# SPDX-License-Identifier: Apache-2.0

[build-system]
requires = ["setuptools>=42", "wheel", "setuptools_scm>=8.0.0"]
build-backend = "setuptools.build_meta"

[project]
name = "reward_hub"
description = "A unified hub for reward models in AI alignment"
readme = "README.md"
authors = [
    {name = "Red Hat AI Innovation", email="gxxu@redhat.com" }
]
license = {text = "Apache-2.0"}
classifiers = [
    "Environment :: Console",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Programming Language :: Python :: 3.11",
    "Programming Language :: Python :: 3.12",
    "License :: OSI Approved :: Apache Software License",
    "License :: OSI Approved :: MIT License",
    "Operating System :: POSIX :: Linux",
    "Topic :: Scientific/Engineering :: Artificial Intelligence",
]
dynamic = ["version"]
requires-python = ">=3.9"
dependencies = [
    "transformers>=4.53.2",
    "numpy",
    "requests",
    "openai>=1.68.2",
    "torch>=2.7.0",
    "litellm>=1.70.0,<1.75.0",
    "python-dotenv>=1.0.0",
    "tenacity>=8.0.0",
]

[tool.setuptools_scm]
version_file = "reward_hub/_version.py"
# do not include +gREV local version, required for Test PyPI upload
local_scheme = "no-local-version"

[project.optional-dependencies]
dev = [
    "pytest>=7.0.0",
    "pytest-mock>=3.10.0",
    "pytest-asyncio>=0.21.0",
    "pre-commit>=3.0.4,<4.0",
    "ruff>=0.10.0"
]
# Optionally install vllm for locally hosted reward models
vllm = [
    "vllm>=0.10.0"
]

# Optionally extend support for Qwen-PRM
prm = [
    "vllm>=0.10.0",
    "transformers==4.53.2"
]

[project.urls]
"Homepage" = "https://github.com/Red-Hat-AI-Innovation-Team/reward_hub/tree/main"
"Bug Tracker" = "https://github.com/Red-Hat-AI-Innovation-Team/reward_hub/issues"

[tool.setuptools.packages.find]
where = ["."]
include = ["reward_hub*"] 

[tool.distutils.bdist_wheel]
universal = true

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = [
    "-v",
    "--strict-markers",
    "--tb=short",
    "-m", "not e2e",  # Skip e2e tests by default
]
markers = [
    "unit: Unit tests with mocks (fast, no model loading)",
    "e2e: End-to-end tests with actual models (requires GPU and --e2e flag)",
    "asyncio: Async test marker",
]
asyncio_mode = "auto"
asyncio_default_fixture_loop_scope = "function"
# Dual-mode operation:
# - Default: pytest              → runs unit tests only (mocked)
# - E2E:     pytest -m e2e       → runs e2e tests only (real models)
# - All:     pytest -m ""        → runs all tests (mocked for unit, real for e2e)
