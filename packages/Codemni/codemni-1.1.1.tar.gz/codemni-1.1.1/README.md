# Codemni

<div align="center">

<img src="https://raw.githubusercontent.com/CodexJitin/Codemni/main/assets/codemni-logo.jpg" alt="Codemni Logo" width="400"/>

**ğŸš€ The Complete AI Agent Framework for Python**

[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: Proprietary](https://img.shields.io/badge/License-Proprietary-red.svg)](https://github.com/CodexJitin/Codemni/blob/main/LICENSE)
[![GitHub](https://img.shields.io/badge/GitHub-CodexJitin%2FCodemni-181717?logo=github)](https://github.com/CodexJitin/Codemni)

*The most powerful framework for building autonomous AI agents - featuring intelligent tool execution, multi-LLM orchestration, and advanced conversational memory*

[Features](#-features) â€¢ [Installation](#-installation) â€¢ [Modules](#-modules) â€¢ [Quick Start](#-quick-start) â€¢ [Documentation](#-documentation)

</div>

---

## ğŸ“– About

**Codemni** is a powerful Python framework for building production-ready AI agents and LLM applications. Unlike simple wrappers, Codemni provides a complete ecosystem with intelligent tool-calling agents, multi-provider LLM integrations, and sophisticated memory systems. Whether you're building chatbots, automation systems, or complex AI workflows, Codemni gives you the foundation to create robust, scalable solutions.

**Why Choose Codemni?**

- ğŸ¤– **Complete Agent Framework**: Not just an LLM wrapper - build agents that can think, decide, and execute tools
- âœ¨ **Production-Ready**: Battle-tested with built-in error handling, retries, and intelligent fallbacks
- ğŸ¯ **Multi-Provider Support**: Seamlessly switch between OpenAI, Google, Anthropic, Groq, and Ollama
- ğŸ§  **Advanced Memory**: 4 memory strategies to maintain context and conversation history
- ğŸ”§ **Developer-Friendly**: Intuitive APIs, comprehensive documentation, and consistent interfaces
- ğŸš€ **Performance-Optimized**: Designed for speed, efficiency, and reliability at scale
- ğŸ›¡ï¸ **Enterprise-Grade**: Robust error handling, logging, and production-ready code

---

## ğŸ§© Modules

### ğŸ¤– [ToolCalling Agent](./TOOL_CALLING_AGENT/) - AI Agent Framework

Powerful and flexible AI agent framework that enables LLMs to intelligently select and execute tools.

**Key Features:**
- ğŸ”§ Dynamic tool execution based on LLM decisions
- ğŸ’¾ Optional conversation memory (4 different strategies)
- ğŸ¨ Custom agent personality/role support
- ğŸ“Š Verbose mode for debugging
- ğŸ”Œ Multi-LLM support (OpenAI, Google Gemini, Anthropic, Groq, Ollama)
- âš ï¸ Designed for standard models (reasoning models like o1, o3 not supported)

**[ğŸ“š Full Agent Documentation â†’](./TOOL_CALLING_AGENT/README.md)**

---

### ğŸ’¾ [Memory Module](./memory/) - Conversation History Management

Flexible conversation memory system for maintaining context in multi-turn interactions.

**Available Memory Types:**
- ğŸ“ **ConversationalBufferMemory** - Store all messages
- ğŸªŸ **ConversationalWindowMemory** - Keep last N exchanges
- ğŸ« **ConversationalTokenBufferMemory** - Limit by token count
- ğŸ“‹ **ConversationalSummaryMemory** - Summarize old conversations

**Key Features:**
- Common API across all memory types
- Easy serialization (save/load)
- Lightweight and efficient
- Integrates seamlessly with ToolCalling Agent

**[ğŸ“š Full Memory Documentation â†’](./memory/README.md)**

---

### ğŸ“¡ [LLM Module](./llm/) - Large Language Model Wrappers

Production-ready wrappers for popular LLM providers with unified interface.

**Supported Providers:**
- ğŸ”· Google Gemini (`gemini-pro`, `gemini-2.0-flash-exp`)
- ğŸŸ¢ OpenAI (`gpt-4`, `gpt-3.5-turbo`, `gpt-4-turbo`)
- ğŸŸ£ Anthropic Claude (`claude-3-opus`, `claude-3-sonnet`, `claude-3-haiku`)
- âš¡ Groq (`llama3-70b`, `mixtral-8x7b`)
- ğŸ¦™ Ollama (Local models: `llama2`, `mistral`, `codellama`)

**Key Features:**
- Automatic retries with exponential backoff
- Configurable timeouts
- Consistent API across all providers
- Both function and class-based interfaces
- Silent operation (no logging)
- Minimal dependencies

**[ğŸ“š Full LLM Documentation â†’](./llm/README.md)**

---

## ğŸ“¦ Installation

### Install from PyPI (Recommended)

```bash
# Install the base package
pip install Codemni

# Install with specific LLM providers
pip install Codemni[openai]        # OpenAI support
pip install Codemni[anthropic]     # Anthropic Claude support
pip install Codemni[groq]          # Groq support
pip install Codemni[google]        # Google Gemini support
pip install Codemni[ollama]        # Ollama (local) support

# Install with all LLM providers
pip install Codemni[all]
```

### Install from Source

```bash
# Clone the repository
git clone https://github.com/CodexJitin/Codemni.git
cd Codemni

# Install in development mode
pip install -e .

# Or install with all dependencies
pip install -e .[all]
```

---

## ğŸš€ Quick Start

### Installation

```bash
pip install Codemni[all]  # Install with all LLM providers
```

### ToolCalling Agent - Basic Usage

```python
from Codemni.TOOL_CALLING_AGENT.agent import Create_ToolCalling_Agent
from Codemni.llm.Google_llm import GoogleLLM

# Initialize LLM
llm = GoogleLLM(
    model="gemini-2.0-flash-exp",
    api_key="YOUR_API_KEY"  # or set GOOGLE_API_KEY env var
)

# Create agent
agent = Create_ToolCalling_Agent(llm=llm, verbose=True)

# Define a tool
def calculator(expression):
    return str(eval(expression))

# Add tool to agent
agent.add_tool("calculator", "Evaluate mathematical expressions", calculator)

# Use the agent
response = agent.invoke("What is 125 * 48?")
print(response)  # Agent will use the calculator tool
```

### ToolCalling Agent with Memory

```python
from Codemni.TOOL_CALLING_AGENT.agent import Create_ToolCalling_Agent
from Codemni.llm.Google_llm import GoogleLLM
from Codemni.memory.conversational_buffer_memory import ConversationalBufferMemory

# Initialize LLM and memory
llm = GoogleLLM(model="gemini-2.0-flash-exp", api_key="YOUR_API_KEY")
memory = ConversationalBufferMemory()

# Create agent with memory
agent = Create_ToolCalling_Agent(llm=llm, memory=memory, verbose=True)
agent.add_tool("calculator", "Evaluate math", calculator)

# Multi-turn conversation with context
response1 = agent.invoke("Calculate 50 + 25")  # Returns: 75
response2 = agent.invoke("Now multiply that by 2")  # Returns: 150 (remembers 75!)
```

### LLM Module - Basic Usage

```python
from Codemni.llm import google_llm, openai_llm, anthropic_llm

# Google Gemini
response = google_llm(
    prompt="Explain quantum computing in simple terms",
    model="gemini-pro",
    api_key="your-api-key"  # or set GOOGLE_API_KEY env var
)
print(response)

# OpenAI GPT
response = openai_llm(
    prompt="Write a Python function to calculate fibonacci",
    model="gpt-4",
    temperature=0.7,
    max_tokens=500
)
print(response)

# Anthropic Claude
response = anthropic_llm(
    prompt="Explain the concept of recursion",
    model="claude-3-sonnet-20240229",
    max_tokens=300
)
print(response)
```

### Error Handling

```python
from Codemni.llm import google_llm, GoogleLLMError, GoogleLLMAPIError

try:
    response = google_llm(
        prompt="Hello, world!",
        model="gemini-pro"
    )
    print(response)
except GoogleLLMAPIError as e:
    print(f"API Error: {e}")
except GoogleLLMError as e:
    print(f"General Error: {e}")
```

---

## ğŸ” Configuration

### Environment Variables

Set these to avoid hardcoding API keys:

```bash
# Linux/Mac
export GOOGLE_API_KEY="your-google-key"
export OPENAI_API_KEY="your-openai-key"
export ANTHROPIC_API_KEY="your-anthropic-key"
export GROQ_API_KEY="your-groq-key"
export OLLAMA_BASE_URL="http://localhost:11434"  # Optional

# Windows PowerShell
$env:GOOGLE_API_KEY="your-google-key"
$env:OPENAI_API_KEY="your-openai-key"
```

### Using .env File

```bash
# Install python-dotenv
pip install python-dotenv
```

```python
from dotenv import load_dotenv
load_dotenv()

# Now your environment variables are loaded
from Codemni.llm import google_llm

response = google_llm(prompt="Hello", model="gemini-pro")
```

---

## ğŸ—ï¸ Project Structure

```
Codemni/
â”œâ”€â”€ ğŸ“„ README.md              # This file - Main documentation
â”œâ”€â”€ ğŸ“„ LICENSE                # License information
â”œâ”€â”€ ğŸ“„ requirements.txt       # Base dependencies
â”œâ”€â”€ ğŸ“„ __init__.py            # Package initialization
â”‚
â”œâ”€â”€ ï¿½ TOOL_CALLING_AGENT/    # AI Agent Module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ README.md             # Agent documentation
â”‚   â”œâ”€â”€ agent.py              # Main agent implementation
â”‚   â””â”€â”€ prompt.py             # Prompt templates
â”‚
â”œâ”€â”€ ï¿½ memory/                # Memory Module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ README.md             # Memory documentation
â”‚   â”œâ”€â”€ conversational_buffer_memory.py
â”‚   â”œâ”€â”€ conversational_window_memory.py
â”‚   â”œâ”€â”€ conversational_token_buffer_memory.py
â”‚   â””â”€â”€ conversational_summary_memory.py
â”‚
â”œâ”€â”€ ğŸ“ llm/                   # LLM Module
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ README.md             # LLM module documentation
â”‚   â”œâ”€â”€ Google_llm.py         # Google Gemini wrapper
â”‚   â”œâ”€â”€ OpenAI_llm.py         # OpenAI wrapper
â”‚   â”œâ”€â”€ Anthropic_llm.py      # Anthropic wrapper
â”‚   â”œâ”€â”€ Groq_llm.py           # Groq wrapper
â”‚   â””â”€â”€ Ollama_llm.py         # Ollama wrapper
â”‚
â”œâ”€â”€ ğŸ“ core/                  # Core utilities
â”‚   â””â”€â”€ adapter.py            # Tool execution adapter
â”‚
â””â”€â”€ ğŸ“ assets/                # Assets and media
    â””â”€â”€ codemni-logo.jpg
```

---

## ğŸ“š Documentation

### Module Documentation

- **[ToolCalling Agent](./TOOL_CALLING_AGENT/README.md)** - AI agent framework guide
  - Complete API reference for all methods
  - Memory integration guide
  - Tool definition best practices
  - Custom prompt guidelines
  - Troubleshooting and examples
  
- **[Memory Module](./memory/README.md)** - Conversation memory guide
  - Memory type comparison
  - Usage examples for each type
  - Serialization and persistence
  - Integration with agents

- **[LLM Module](./llm/README.md)** - Comprehensive guide to LLM wrappers
  - API reference for all providers
  - Advanced usage examples
  - Exception handling guide
  - Provider-specific notes

---

## âœ¨ Features by Module

### ToolCalling Agent

| Feature | Description |
|---------|-------------|
| ğŸ¤– **Multi-LLM Support** | Works with OpenAI, Google Gemini, Anthropic, Groq, Ollama |
| ğŸ”§ **Dynamic Tools** | Automatically selects and executes appropriate tools |
| ğŸ’¾ **Optional Memory** | 4 memory strategies for conversation context |
| ğŸ¨ **Custom Prompts** | Customize agent personality and role |
| ğŸ“Š **Verbose Mode** | Detailed logging for debugging |
| âš ï¸ **Standard Models** | Optimized for instruction-following models (not reasoning models) |

### Memory Module

| Feature | Description |
|---------|-------------|
| ğŸ“ **Buffer Memory** | Store all conversation messages |
| ğŸªŸ **Window Memory** | Keep only recent N exchanges |
| ğŸ« **Token Buffer** | Limit memory by token count |
| ğŸ“‹ **Summary Memory** | Summarize old conversations |
| ğŸ’¾ **Serialization** | Save/load conversation history |
| ğŸ”Œ **Easy Integration** | Works seamlessly with agents |

### LLM Module

| Feature | Description |
|---------|-------------|
| ğŸ”„ **Auto Retry** | Exponential backoff for transient failures |
| â±ï¸ **Timeouts** | Configurable request timeouts (30-60s) |
| ğŸ›¡ï¸ **Error Handling** | Clear exception hierarchy per provider |
| ğŸ”‡ **Silent Mode** | No logging - clean operation |
| ğŸ“¦ **Minimal Deps** | Install only what you need |
| ğŸ¯ **Unified API** | Same interface across all providers |
| âœ… **Validation** | Input validation and defensive coding |
| ğŸ”§ **Dual Interface** | Both function and class-based APIs |

---

## ğŸ¤ Contributing

Contributions are welcome! Here's how you can help:

1. ğŸ´ Fork the repository
2. ğŸŒ¿ Create a feature branch (`git checkout -b feature/AmazingFeature`)
3. ğŸ’¾ Commit your changes (`git commit -m 'Add some AmazingFeature'`)
4. ğŸ“¤ Push to the branch (`git push origin feature/AmazingFeature`)
5. ğŸ”€ Open a Pull Request

### Development Guidelines

- Follow existing code style and patterns
- Add tests for new features
- Update documentation
- Keep dependencies minimal
- Maintain backward compatibility

---

## ğŸ“‹ Requirements

- Python 3.8 or higher
- Optional dependencies (install as needed):
  - `openai>=1.0.0` - For OpenAI support
  - `anthropic>=0.18.0` - For Anthropic support
  - `groq>=0.4.0` - For Groq support
  - `google-generativeai>=0.3.0` - For Google Gemini support
  - `ollama>=0.1.0` - For Ollama support

---

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

---

## ğŸ‘¤ Author

**CodexJitin**

- GitHub: [@CodexJitin](https://github.com/CodexJitin)
- Repository: [Codemni](https://github.com/CodexJitin/Codemni)

---

## ğŸ”– Version

**Current Version: 1.1.0**

### Changelog

#### v1.1.0 (2025-10-25)
- ğŸ‰ Added ToolCalling Agent module
- ğŸ’¾ Added Memory module with 4 memory strategies
- ğŸ”§ LLM module now supports both function and class-based interfaces
- ğŸ“š Comprehensive documentation for all modules
- âš ï¸ Added warnings about reasoning model compatibility

#### v1.0.0 (2025-10-24)
- ğŸ‰ Initial release
- âœ… LLM module with 5 provider support
- âœ… Production-ready error handling
- âœ… Comprehensive documentation

---

## ğŸŒŸ Show Your Support

If you find Codemni useful, please consider:
- â­ Starring the repository
- ğŸ› Reporting bugs
- ğŸ’¡ Suggesting new features
- ğŸ”€ Contributing code

---

## ğŸ“ Support

- ğŸ“§ Issues: [GitHub Issues](https://github.com/CodexJitin/Codemni/issues)
- ğŸ’¬ Discussions: [GitHub Discussions](https://github.com/CodexJitin/Codemni/discussions)

---

<div align="center">

**Made with â¤ï¸ by CodexJitin**

*Empowering developers to build better AI applications*

</div>
