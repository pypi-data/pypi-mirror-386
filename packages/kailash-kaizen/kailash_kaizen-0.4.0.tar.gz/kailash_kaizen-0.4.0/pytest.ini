[pytest]
testpaths = tests
python_files = test_*.py
python_classes = Test*
python_functions = test_*
addopts =
    -v
    --tb=short
    --strict-markers
    --disable-warnings
    --maxfail=10
    --durations=5
    --color=yes
markers =
    unit: Unit tests (fast, isolated, can use mocks) - Tier 1
    integration: Integration tests (real services, no mocking) - Tier 2
    e2e: End-to-end tests (complete workflows, real infrastructure) - Tier 3
    performance: Performance benchmark tests
    slow: Slow running tests (timeout > 5s)
    requires_postgres: Tests requiring PostgreSQL
    requires_redis: Tests requiring Redis
    requires_docker: Tests requiring Docker services
    requires_ollama: Tests requiring Ollama service
    memory_intensive: Tests that use significant memory
    flaky: Tests that may occasionally fail (retry candidates)
    llm_execution: Tests that use real LLM inference (paid API calls)
    mcp: Tests for MCP (Model Context Protocol) integration
    server: Tests for server-related functionality
    real_llm: Tests that use real LLM providers (OpenAI, Anthropic, etc.)
    ollama_validation: Tests validating Ollama integration
    summary: Summary tests that aggregate results
    ollama: Tests using Ollama provider (free, local)
    landing_ai: Tests using Landing AI provider (requires API key)
    cost: Tests that incur API costs (requires API keys)
    batch_processing: Batch document processing tests
    cost_optimization: Cost optimization scenario tests
    rag_workflow: RAG workflow integration tests

# Timeout configurations by tier
timeout = 120
# Individual test timeouts managed by @pytest.mark.timeout decorators:
# - Unit tests: @pytest.mark.timeout(1) - 1 second max
# - Integration tests: @pytest.mark.timeout(5) - 5 seconds max
# - E2E tests: @pytest.mark.timeout(10) - 10 seconds max

# Performance thresholds (validated in conftest.py)
# Unit: <1000ms, Integration: <5000ms, E2E: <10000ms