"""
Tier 1 Unit Tests for MetricsCollector.

Tests the metrics collection system with mocked components.
Validates counter, gauge, histogram functionality and Prometheus export.

Part of Phase 4: Observability & Performance Monitoring (ADR-017)
Target: <2% performance overhead (NFR-1)
"""

import asyncio
import time
from datetime import datetime

import pytest

from kaizen.core.autonomy.observability.metrics import MetricsCollector
from kaizen.core.autonomy.observability.types import Metric, MetricType


class TestMetricsCollectorBasics:
    """Test basic metrics collector initialization and configuration."""

    def test_collector_initialization(self):
        """Test MetricsCollector initializes with default backend."""
        collector = MetricsCollector()

        assert collector.backend == "prometheus"
        assert len(collector._metrics) == 0
        assert len(collector._counters) == 0
        assert len(collector._gauges) == 0
        assert len(collector._histograms) == 0

    def test_collector_custom_backend(self):
        """Test MetricsCollector accepts custom backend."""
        collector = MetricsCollector(backend="custom")

        assert collector.backend == "custom"


class TestCounterMetrics:
    """Test counter metric functionality."""

    def test_counter_increment_default(self):
        """Test counter increments by 1.0 by default."""
        collector = MetricsCollector()

        collector.counter("test_counter")

        assert collector.get_counter_value("test_counter") == 1.0
        assert collector.get_metric_count() == 1

    def test_counter_increment_custom_value(self):
        """Test counter increments by custom value."""
        collector = MetricsCollector()

        collector.counter("test_counter", value=5.0)

        assert collector.get_counter_value("test_counter") == 5.0

    def test_counter_with_labels(self):
        """Test counter with label dimensions."""
        collector = MetricsCollector()

        collector.counter("api_calls", 1.0, labels={"provider": "openai"})
        collector.counter("api_calls", 1.0, labels={"provider": "anthropic"})

        assert collector.get_counter_value("api_calls", {"provider": "openai"}) == 1.0
        assert (
            collector.get_counter_value("api_calls", {"provider": "anthropic"}) == 1.0
        )
        assert collector.get_metric_count() == 2

    def test_counter_multiple_increments(self):
        """Test counter accumulates multiple increments."""
        collector = MetricsCollector()

        collector.counter("test_counter", 1.0)
        collector.counter("test_counter", 2.0)
        collector.counter("test_counter", 3.0)

        assert collector.get_counter_value("test_counter") == 6.0
        assert collector.get_metric_count() == 3

    def test_counter_nonexistent_returns_zero(self):
        """Test getting nonexistent counter returns 0.0."""
        collector = MetricsCollector()

        value = collector.get_counter_value("nonexistent")

        assert value == 0.0


class TestGaugeMetrics:
    """Test gauge metric functionality."""

    def test_gauge_set_value(self):
        """Test gauge sets point-in-time value."""
        collector = MetricsCollector()

        collector.gauge("memory_bytes", 1024000)

        assert collector.get_gauge_value("memory_bytes") == 1024000
        assert collector.get_metric_count() == 1

    def test_gauge_with_labels(self):
        """Test gauge with label dimensions."""
        collector = MetricsCollector()

        collector.gauge("active_agents", 5, labels={"status": "running"})
        collector.gauge("active_agents", 2, labels={"status": "idle"})

        assert collector.get_gauge_value("active_agents", {"status": "running"}) == 5
        assert collector.get_gauge_value("active_agents", {"status": "idle"}) == 2

    def test_gauge_overwrite_value(self):
        """Test gauge overwrites previous value (not accumulate)."""
        collector = MetricsCollector()

        collector.gauge("test_gauge", 100)
        collector.gauge("test_gauge", 200)

        assert collector.get_gauge_value("test_gauge") == 200
        assert collector.get_metric_count() == 2  # Both observations recorded

    def test_gauge_nonexistent_returns_none(self):
        """Test getting nonexistent gauge returns None."""
        collector = MetricsCollector()

        value = collector.get_gauge_value("nonexistent")

        assert value is None


class TestHistogramMetrics:
    """Test histogram metric functionality."""

    def test_histogram_single_observation(self):
        """Test histogram records single observation."""
        collector = MetricsCollector()

        collector.histogram("request_latency_ms", 125.5)

        values = collector.get_histogram_values("request_latency_ms")
        assert len(values) == 1
        assert values[0] == 125.5

    def test_histogram_multiple_observations(self):
        """Test histogram records multiple observations."""
        collector = MetricsCollector()

        collector.histogram("latency", 100)
        collector.histogram("latency", 200)
        collector.histogram("latency", 150)

        values = collector.get_histogram_values("latency")
        assert len(values) == 3
        assert set(values) == {100, 150, 200}

    def test_histogram_with_labels(self):
        """Test histogram with label dimensions."""
        collector = MetricsCollector()

        collector.histogram("tool_latency_ms", 50, labels={"tool": "search"})
        collector.histogram("tool_latency_ms", 100, labels={"tool": "write"})

        search_values = collector.get_histogram_values(
            "tool_latency_ms", {"tool": "search"}
        )
        write_values = collector.get_histogram_values(
            "tool_latency_ms", {"tool": "write"}
        )

        assert len(search_values) == 1
        assert len(write_values) == 1
        assert search_values[0] == 50
        assert write_values[0] == 100

    def test_histogram_nonexistent_returns_empty_list(self):
        """Test getting nonexistent histogram returns empty list."""
        collector = MetricsCollector()

        values = collector.get_histogram_values("nonexistent")

        assert values == []


class TestTimerContextManagers:
    """Test timer context managers for automatic duration recording."""

    @pytest.mark.asyncio
    async def test_async_timer_records_duration(self):
        """Test async timer context manager records duration."""
        collector = MetricsCollector()

        async with collector.timer("operation_duration_ms"):
            await asyncio.sleep(0.01)  # Sleep 10ms

        values = collector.get_histogram_values("operation_duration_ms")
        assert len(values) == 1
        assert values[0] >= 10.0  # At least 10ms

    @pytest.mark.asyncio
    async def test_async_timer_with_labels(self):
        """Test async timer with labels."""
        collector = MetricsCollector()

        async with collector.timer("tool_execution_ms", labels={"tool": "search"}):
            await asyncio.sleep(0.01)

        values = collector.get_histogram_values("tool_execution_ms", {"tool": "search"})
        assert len(values) == 1
        assert values[0] >= 10.0

    def test_sync_timer_records_duration(self):
        """Test sync timer context manager records duration."""
        collector = MetricsCollector()

        with collector.timer_sync("operation_duration_ms"):
            time.sleep(0.01)  # Sleep 10ms

        values = collector.get_histogram_values("operation_duration_ms")
        assert len(values) == 1
        assert values[0] >= 10.0

    def test_sync_timer_with_labels(self):
        """Test sync timer with labels."""
        collector = MetricsCollector()

        with collector.timer_sync("file_read_ms", labels={"operation": "read"}):
            time.sleep(0.01)

        values = collector.get_histogram_values("file_read_ms", {"operation": "read"})
        assert len(values) == 1
        assert values[0] >= 10.0

    @pytest.mark.asyncio
    async def test_async_timer_multiple_calls(self):
        """Test async timer records multiple operations."""
        collector = MetricsCollector()

        for i in range(3):
            async with collector.timer("loop_iteration_ms"):
                await asyncio.sleep(0.01)

        values = collector.get_histogram_values("loop_iteration_ms")
        assert len(values) == 3
        for value in values:
            assert value >= 10.0


class TestPrometheusExport:
    """Test Prometheus text format export."""

    @pytest.mark.asyncio
    async def test_export_empty_metrics(self):
        """Test export with no metrics returns empty string."""
        collector = MetricsCollector()

        export_text = await collector.export()

        assert export_text == ""

    @pytest.mark.asyncio
    async def test_export_counter_format(self):
        """Test counter export format."""
        collector = MetricsCollector()

        collector.counter("test_counter", 5.0)

        export_text = await collector.export()

        assert "test_counter 5.0" in export_text

    @pytest.mark.asyncio
    async def test_export_counter_with_labels(self):
        """Test counter with labels export format."""
        collector = MetricsCollector()

        collector.counter("api_calls", 10.0, labels={"provider": "openai"})

        export_text = await collector.export()

        assert 'api_calls{provider="openai"} 10.0' in export_text

    @pytest.mark.asyncio
    async def test_export_gauge_format(self):
        """Test gauge export format."""
        collector = MetricsCollector()

        collector.gauge("memory_bytes", 1024000)

        export_text = await collector.export()

        assert "memory_bytes 1024000" in export_text

    @pytest.mark.asyncio
    async def test_export_histogram_percentiles(self):
        """Test histogram exports p50, p95, p99 percentiles."""
        collector = MetricsCollector()

        # Record 100 observations from 1 to 100
        for i in range(1, 101):
            collector.histogram("latency", float(i))

        export_text = await collector.export()

        # Verify percentiles present
        assert "latency_p50" in export_text
        assert "latency_p95" in export_text
        assert "latency_p99" in export_text

        # Verify approximate percentile values (with linear interpolation)
        assert "latency_p50 50.5" in export_text  # Median is 50.5 (interpolated)
        assert (
            "latency_p95 95.05" in export_text
        )  # 95th percentile is 95.05 (interpolated)

    @pytest.mark.asyncio
    async def test_export_mixed_metrics(self):
        """Test export with counters, gauges, and histograms."""
        collector = MetricsCollector()

        collector.counter("requests_total", 100)
        collector.gauge("active_connections", 5)
        collector.histogram("response_time_ms", 150)

        export_text = await collector.export()

        assert "requests_total 100" in export_text
        assert "active_connections 5" in export_text
        assert "response_time_ms_p50" in export_text


class TestMetricKeyGeneration:
    """Test metric key generation for label combinations."""

    def test_metric_key_no_labels(self):
        """Test metric key without labels."""
        collector = MetricsCollector()

        key = collector._metric_key("test_metric", {})

        assert key == "test_metric"

    def test_metric_key_single_label(self):
        """Test metric key with single label."""
        collector = MetricsCollector()

        key = collector._metric_key("test_metric", {"env": "prod"})

        assert key == 'test_metric{env="prod"}'

    def test_metric_key_multiple_labels(self):
        """Test metric key with multiple labels (sorted)."""
        collector = MetricsCollector()

        key = collector._metric_key("test_metric", {"env": "prod", "region": "us-east"})

        # Labels should be sorted alphabetically
        assert key == 'test_metric{env="prod",region="us-east"}'

    def test_metric_key_label_sorting(self):
        """Test labels are consistently sorted."""
        collector = MetricsCollector()

        key1 = collector._metric_key("metric", {"b": "2", "a": "1"})
        key2 = collector._metric_key("metric", {"a": "1", "b": "2"})

        assert key1 == key2  # Order shouldn't matter


class TestPercentileCalculation:
    """Test percentile calculation for histograms."""

    def test_percentile_p50_median(self):
        """Test p50 percentile (median)."""
        collector = MetricsCollector()

        values = [1.0, 2.0, 3.0, 4.0, 5.0]
        p50 = collector._calculate_percentile(values, 0.50)

        assert p50 == 3.0  # Middle value

    def test_percentile_p95(self):
        """Test p95 percentile with linear interpolation."""
        collector = MetricsCollector()

        values = list(range(1, 101))  # 1 to 100
        p95 = collector._calculate_percentile(values, 0.95)

        # With linear interpolation: (99 - 1) * 0.95 = 93.05
        # Index 93 = value 94, index 94 = value 95
        # Interpolated: 94 + (95 - 94) * 0.05 = 94 + 0.05 = 94.05
        # But list is [1, 2, ..., 100], so index 93 = 94, index 94 = 95
        # Result: 95.05
        assert p95 == 95.05

    def test_percentile_empty_list(self):
        """Test percentile with empty list returns 0.0."""
        collector = MetricsCollector()

        p50 = collector._calculate_percentile([], 0.50)

        assert p50 == 0.0

    def test_percentile_single_value(self):
        """Test percentile with single value."""
        collector = MetricsCollector()

        p50 = collector._calculate_percentile([42.0], 0.50)

        assert p50 == 42.0


class TestMetricsReset:
    """Test metrics reset functionality."""

    def test_reset_clears_all_metrics(self):
        """Test reset clears all metric types."""
        collector = MetricsCollector()

        # Record metrics of all types
        collector.counter("test_counter", 5.0)
        collector.gauge("test_gauge", 100)
        collector.histogram("test_histogram", 50)

        assert collector.get_metric_count() == 3

        # Reset
        collector.reset()

        # Verify all cleared
        assert collector.get_metric_count() == 0
        assert collector.get_counter_value("test_counter") == 0.0
        assert collector.get_gauge_value("test_gauge") is None
        assert collector.get_histogram_values("test_histogram") == []

    def test_reset_allows_new_metrics(self):
        """Test metrics can be recorded after reset."""
        collector = MetricsCollector()

        collector.counter("test", 1.0)
        collector.reset()

        collector.counter("test", 2.0)

        assert collector.get_counter_value("test") == 2.0
        assert collector.get_metric_count() == 1


class TestMetricObservations:
    """Test metric observation storage."""

    def test_metrics_list_stores_all_observations(self):
        """Test all metric observations are stored in _metrics list."""
        collector = MetricsCollector()

        collector.counter("counter1", 1.0)
        collector.gauge("gauge1", 100)
        collector.histogram("hist1", 50)

        assert len(collector._metrics) == 3
        assert all(isinstance(m, Metric) for m in collector._metrics)

    def test_metric_observations_have_timestamps(self):
        """Test metric observations include timestamps."""
        collector = MetricsCollector()

        before = datetime.utcnow()
        collector.counter("test", 1.0)
        after = datetime.utcnow()

        metric = collector._metrics[0]
        assert before <= metric.timestamp <= after

    def test_metric_observations_have_correct_type(self):
        """Test metric observations have correct type attribute."""
        collector = MetricsCollector()

        collector.counter("c", 1.0)
        collector.gauge("g", 100)
        collector.histogram("h", 50)

        assert collector._metrics[0].type == "counter"
        assert collector._metrics[1].type == "gauge"
        assert collector._metrics[2].type == "histogram"


# Performance validation (ADR-017 NFR-1: <2% overhead)
class TestPerformanceOverhead:
    """Test metrics collection performance overhead."""

    @pytest.mark.asyncio
    async def test_metrics_overhead_is_minimal(self):
        """Test metrics collection adds <5% overhead (ADR-017 NFR-1 relaxed for test env)."""
        collector = MetricsCollector()

        # Baseline: operation without metrics (run 3 times, take median)
        baseline_times = []
        for _ in range(3):
            start = time.perf_counter()
            for i in range(1000):
                await asyncio.sleep(0.0001)  # Simulate work
            baseline_times.append(time.perf_counter() - start)
        baseline_duration = sorted(baseline_times)[1]  # Median

        # With metrics: operation with metrics collection (run 3 times, take median)
        metric_times = []
        for _ in range(3):
            start = time.perf_counter()
            for i in range(1000):
                async with collector.timer("operation_ms"):
                    await asyncio.sleep(0.0001)
            metric_times.append(time.perf_counter() - start)
        with_metrics_duration = sorted(metric_times)[1]  # Median

        # Calculate overhead
        overhead = (with_metrics_duration - baseline_duration) / baseline_duration * 100

        # Should be <5% overhead (relaxed for test environment variability)
        # Production target is <2% but test environment has higher variance
        assert (
            overhead < 5.0
        ), f"Metrics overhead {overhead:.2f}% exceeds 5% test threshold"
