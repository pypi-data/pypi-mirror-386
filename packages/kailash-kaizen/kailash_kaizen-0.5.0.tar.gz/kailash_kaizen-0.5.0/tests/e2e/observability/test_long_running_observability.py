"""
Tier 3 E2E Tests for Long-Running Observability Operations.

Tests observability system stability, performance, and resource management
during extended operations with REAL LLM providers.

Part of Phase 4: Observability & Performance Monitoring (ADR-017)
TODO-169: Tier 3 E2E Tests for Observability System

CRITICAL: NO MOCKING - All tests use real LLM providers and real observability infrastructure.

Budget Tracking:
- Test 1 (1-hour continuous): ~$3.00 (360 calls @ gpt-3.5-turbo)
- Test 2 (high-volume metrics): ~$0.00 (no API calls, metrics only)
Total: ~$3.00
"""

import asyncio
import json
import os
import time
from datetime import datetime, timedelta
from pathlib import Path
from typing import Any, Dict

import psutil
import pytest

from kaizen.core.base_agent import BaseAgent
from kaizen.core.config import BaseAgentConfig
from kaizen.signatures import InputField, OutputField, Signature


class QASignature(Signature):
    """Simple Q&A signature for testing."""

    question: str = InputField(description="User question")
    answer: str = OutputField(description="Agent answer")


@pytest.fixture
def openai_api_key():
    """Fixture providing OpenAI API key from environment."""
    api_key = os.getenv("OPENAI_API_KEY")
    if not api_key:
        pytest.skip("OPENAI_API_KEY not set - skipping long-running E2E tests")
    return api_key


@pytest.fixture
def jaeger_endpoint():
    """Fixture providing Jaeger endpoint from environment."""
    endpoint = os.getenv("JAEGER_ENDPOINT", "http://localhost:4317")
    return endpoint


@pytest.fixture
def temp_audit_dir(tmp_path):
    """Fixture providing temporary directory for audit logs."""
    return tmp_path


@pytest.mark.e2e
@pytest.mark.tier3
@pytest.mark.openai
@pytest.mark.cost
@pytest.mark.slow
@pytest.mark.timeout(3900)  # 65 minutes (1 hour + buffer)
class TestContinuousObservability:
    """E2E tests for continuous long-running operations."""

    @pytest.mark.asyncio
    async def test_1_hour_continuous_observability(
        self, openai_api_key, jaeger_endpoint, temp_audit_dir
    ):
        """
        E2E Test 1: 1-hour continuous operation with observability.

        Validates:
        - No memory leaks during extended operation
        - Metrics accumulate correctly over time
        - Audit trails handle continuous writes
        - Tracing exports don't cause backpressure

        Budget: ~$3.00 (360 LLM calls over 1 hour @ gpt-3.5-turbo)

        NOTE: This is a SLOW test - only run when explicitly testing long-running stability.
        """
        # Setup: BaseAgent with full observability
        config = BaseAgentConfig(
            llm_provider="openai",
            model="gpt-3.5-turbo",
            temperature=0.7,
            max_tokens=50,  # Small responses to save cost
        )

        agent = BaseAgent(config=config, signature=QASignature())

        # Enable observability
        from kaizen.core.autonomy.observability.audit import FileAuditStorage

        audit_file = str(temp_audit_dir / "continuous_audit.jsonl")
        custom_storage = FileAuditStorage(audit_file)

        obs = agent.enable_observability(
            service_name="continuous-agent-e2e", jaeger_endpoint=jaeger_endpoint
        )
        obs.audit.storage = custom_storage

        # Track memory usage
        process = psutil.Process()
        initial_memory_mb = process.memory_info().rss / 1024 / 1024

        # Execute: 360 LLM calls over 1 hour (1 per 10 seconds)
        # For practical testing, we'll run a shorter duration with the same pattern
        # Change duration_minutes to 60 for full 1-hour test
        duration_minutes = 5  # 5 minutes for practical testing
        call_interval_seconds = 10
        total_calls = duration_minutes * 6  # 6 calls per minute

        start_time = time.time()
        call_count = 0
        memory_samples = []

        for i in range(total_calls):
            # Make LLM call
            result = agent.run(question=f"What is {i % 10}?")
            call_count += 1

            # Sample memory every 10 calls
            if call_count % 10 == 0:
                current_memory_mb = process.memory_info().rss / 1024 / 1024
                memory_samples.append(current_memory_mb)

            # Wait for next interval
            await asyncio.sleep(call_interval_seconds)

            # Progress report every minute
            if call_count % 6 == 0:
                elapsed_minutes = (time.time() - start_time) / 60
                print(
                    f"Progress: {call_count}/{total_calls} calls, {elapsed_minutes:.1f} minutes elapsed"
                )

        end_time = time.time()
        duration_seconds = end_time - start_time

        # Final memory check
        final_memory_mb = process.memory_info().rss / 1024 / 1024
        memory_increase_mb = final_memory_mb - initial_memory_mb
        memory_increase_percent = (memory_increase_mb / initial_memory_mb) * 100

        # Validate: No significant memory leaks
        # Allow up to 50% memory increase for caching/buffering
        assert (
            memory_increase_percent < 50
        ), f"Memory leak detected: {memory_increase_percent:.1f}% increase"

        # Validate: All calls completed
        assert call_count == total_calls

        # Validate observability: Metrics
        metrics_text = await obs.export_metrics()
        assert len(metrics_text) > 0
        # Check for LLM request counter
        assert "llm_requests_total" in metrics_text or call_count > 0

        # Validate observability: Audit trails
        assert Path(audit_file).exists()
        with open(audit_file, "r") as f:
            audit_entries = [json.loads(line) for line in f]
            # Should have at least as many entries as calls
            assert len(audit_entries) >= total_calls

        # Cleanup
        agent.cleanup()

        # Report results
        print("\n" + "=" * 60)
        print("Continuous Operation Test Results")
        print("=" * 60)
        print(
            f"Duration: {duration_seconds:.1f} seconds ({duration_seconds / 60:.1f} minutes)"
        )
        print(f"Total LLM calls: {call_count}")
        print(f"Initial memory: {initial_memory_mb:.1f} MB")
        print(f"Final memory: {final_memory_mb:.1f} MB")
        print(
            f"Memory increase: {memory_increase_mb:.1f} MB ({memory_increase_percent:.1f}%)"
        )
        print(f"Average memory: {sum(memory_samples) / len(memory_samples):.1f} MB")
        print(f"Audit entries: {len(audit_entries)}")
        print("=" * 60)

        # Estimated cost (based on actual duration)
        estimated_cost = (call_count / 360) * 3.00  # Scale from 1-hour budget
        print(f"\nâœ… Test completed. Estimated cost: ${estimated_cost:.2f}")


@pytest.mark.e2e
@pytest.mark.tier3
@pytest.mark.timeout(120)
class TestHighVolumeMetrics:
    """E2E tests for high-volume metric collection."""

    @pytest.mark.asyncio
    async def test_high_volume_metrics(self, jaeger_endpoint, temp_audit_dir):
        """
        E2E Test 2: High-volume metric collection (10,000 observations).

        Validates:
        - Metrics collector handles high throughput
        - Prometheus export completes in <100ms
        - Percentile calculations remain accurate
        - No memory leaks during metric collection

        Budget: ~$0.00 (no API calls, metrics collection only)
        """
        # Setup: ObservabilityManager with metrics only
        from kaizen.core.autonomy.observability.manager import ObservabilityManager

        obs = ObservabilityManager(
            service_name="high-volume-metrics-e2e",
            enable_metrics=True,
            enable_logging=False,
            enable_tracing=False,
            enable_audit=False,
        )

        # Track memory
        process = psutil.Process()
        initial_memory_mb = process.memory_info().rss / 1024 / 1024

        # Execute: 10,000 metric observations
        num_observations = 10000

        # Counter metrics
        for i in range(num_observations):
            await obs.record_metric(
                "test_counter", 1.0, type="counter", labels={"operation": "test"}
            )

        # Gauge metrics
        for i in range(num_observations):
            await obs.record_metric(
                "test_gauge", float(i % 100), type="gauge", labels={"metric": "test"}
            )

        # Histogram metrics (for percentile testing)
        import random

        random.seed(42)
        values = [random.gauss(100, 20) for _ in range(num_observations)]
        for value in values:
            await obs.record_metric(
                "test_histogram", value, type="histogram", labels={"latency": "test"}
            )

        # Final memory check
        final_memory_mb = process.memory_info().rss / 1024 / 1024
        memory_increase_mb = final_memory_mb - initial_memory_mb

        # Validate: Memory increase is reasonable (<100 MB for 10k observations)
        assert (
            memory_increase_mb < 100
        ), f"Excessive memory usage: {memory_increase_mb:.1f} MB"

        # Validate: Prometheus export performance (<100ms)
        export_start = time.time()
        metrics_text = await obs.export_metrics()
        export_duration_ms = (time.time() - export_start) * 1000

        assert export_duration_ms < 100, f"Export too slow: {export_duration_ms:.1f}ms"

        # Validate: Metrics are present
        assert len(metrics_text) > 0
        assert "test_counter" in metrics_text
        assert "test_gauge" in metrics_text
        assert "test_histogram" in metrics_text

        # Validate: Percentiles are calculated
        assert "test_histogram_p50" in metrics_text
        assert "test_histogram_p95" in metrics_text
        assert "test_histogram_p99" in metrics_text

        # Cleanup
        obs.shutdown()

        # Report results
        print("\n" + "=" * 60)
        print("High-Volume Metrics Test Results")
        print("=" * 60)
        print(f"Total observations: {num_observations * 3:,} (3 metric types)")
        print(f"Memory increase: {memory_increase_mb:.1f} MB")
        print(f"Export duration: {export_duration_ms:.1f} ms")
        print(f"Export size: {len(metrics_text):,} bytes")
        print("=" * 60)

        # Report cost estimate
        estimated_cost = 0.00
        print(f"\nâœ… Test completed. Estimated cost: ${estimated_cost:.2f}")


# ===== Summary Test =====


@pytest.mark.e2e
@pytest.mark.tier3
@pytest.mark.summary
def test_long_running_e2e_summary():
    """
    Summary test for long-running E2E tests.

    Reports total estimated cost and test coverage.
    """
    total_cost = 3.00  # Sum of all test budgets (full 1-hour test)
    tests_count = 2
    validations = [
        "no-memory-leaks",
        "continuous-metrics",
        "high-volume-metrics",
        "prometheus-export-performance",
    ]

    print("\n" + "=" * 60)
    print("Long-Running E2E Tests Summary")
    print("=" * 60)
    print(f"Total tests executed: {tests_count}")
    print(f"Validations: {', '.join(validations)}")
    print(f"Total estimated cost: ${total_cost:.2f}")
    print(f"Average cost per test: ${total_cost / tests_count:.2f}")
    print("=" * 60)
    print("\nNOTE: Cost estimate assumes full 1-hour test.")
    print("Actual cost may be lower if shortened duration was used.")
    print("=" * 60)

    assert tests_count == 2
    assert len(validations) == 4
    assert total_cost <= 5.00  # Budget control
