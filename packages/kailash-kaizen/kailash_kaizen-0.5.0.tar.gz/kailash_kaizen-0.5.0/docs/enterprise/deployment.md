# Production Deployment Guide

Comprehensive guide for deploying the Kaizen Framework in production environments, covering deployment strategies, scaling patterns, and operational considerations.

## Deployment Overview

**Kaizen supports multiple deployment patterns** optimized for different enterprise requirements:

1. **Single-Instance Deployment**: Simple, self-contained deployment for small teams
2. **Distributed Deployment**: Scalable, multi-service architecture for enterprise use
3. **Cloud-Native Deployment**: Kubernetes-based deployment with auto-scaling
4. **Hybrid Deployment**: On-premises + cloud integration for regulated industries

**Current Status**:
- ✅ **Development Deployment**: Local development and testing
- 🟡 **Production Patterns**: Docker containerization and basic scaling (designed)
- 🟡 **Enterprise Deployment**: Kubernetes, monitoring, and HA (planned)
- 🟡 **Cloud Integration**: Multi-cloud and hybrid deployment (planned)

## Deployment Architectures

### Single-Instance Deployment

**Use Case**: Small teams, development environments, proof of concepts

```yaml
# docker-compose.yml - Single instance deployment
version: '3.8'

services:
  kaizen-app:
    image: kaizen:latest
    ports:
      - "8080:8080"
    environment:
      - KAIZEN_ENV=production
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - DATABASE_URL=postgresql://kaizen:password@db:5432/kaizen
    depends_on:
      - db
      - redis
    volumes:
      - ./config:/app/config
      - ./logs:/app/logs

  db:
    image: postgres:15
    environment:
      POSTGRES_DB: kaizen
      POSTGRES_USER: kaizen
      POSTGRES_PASSWORD: password
    volumes:
      - postgres_data:/var/lib/postgresql/data

  redis:
    image: redis:7
    volumes:
      - redis_data:/data

volumes:
  postgres_data:
  redis_data:
```

**Deployment Steps**:

```bash
# 1. Prepare environment
mkdir kaizen-production
cd kaizen-production

# 2. Create configuration
cat > .env << EOF
KAIZEN_ENV=production
OPENAI_API_KEY=your_openai_key
ANTHROPIC_API_KEY=your_anthropic_key
DATABASE_URL=postgresql://kaizen:secure_password@db:5432/kaizen
REDIS_URL=redis://redis:6379
KAIZEN_SECRET_KEY=your_secret_key
EOF

# 3. Deploy services
docker-compose up -d

# 4. Verify deployment
curl http://localhost:8080/health
```

### Distributed Deployment

**Use Case**: Medium to large teams, high availability requirements

```yaml
# docker-compose.distributed.yml
version: '3.8'

services:
  # Load balancer
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/ssl
    depends_on:
      - kaizen-app-1
      - kaizen-app-2

  # Application instances
  kaizen-app-1:
    image: kaizen:latest
    environment:
      - KAIZEN_ENV=production
      - KAIZEN_INSTANCE_ID=app-1
      - DATABASE_URL=postgresql://kaizen:password@db-primary:5432/kaizen
    depends_on:
      - db-primary
      - redis-cluster

  kaizen-app-2:
    image: kaizen:latest
    environment:
      - KAIZEN_ENV=production
      - KAIZEN_INSTANCE_ID=app-2
      - DATABASE_URL=postgresql://kaizen:password@db-primary:5432/kaizen
    depends_on:
      - db-primary
      - redis-cluster

  # Database cluster
  db-primary:
    image: postgres:15
    environment:
      POSTGRES_DB: kaizen
      POSTGRES_USER: kaizen
      POSTGRES_PASSWORD: secure_password
      POSTGRES_REPLICATION_MODE: master
    volumes:
      - postgres_primary:/var/lib/postgresql/data

  db-replica:
    image: postgres:15
    environment:
      POSTGRES_MASTER_SERVICE: db-primary
      POSTGRES_REPLICATION_MODE: slave
    volumes:
      - postgres_replica:/var/lib/postgresql/data

  # Redis cluster
  redis-cluster:
    image: redis:7-cluster
    environment:
      REDIS_CLUSTER_ENABLED: "yes"
    volumes:
      - redis_cluster:/data

  # Monitoring
  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./prometheus.yml:/etc/prometheus/prometheus.yml

  grafana:
    image: grafana/grafana
    ports:
      - "3000:3000"
    environment:
      GF_SECURITY_ADMIN_PASSWORD: admin_password

volumes:
  postgres_primary:
  postgres_replica:
  redis_cluster:
```

### Cloud-Native Deployment (Kubernetes)

**Use Case**: Enterprise scale, cloud-native infrastructure, auto-scaling

```yaml
# k8s/namespace.yaml
apiVersion: v1
kind: Namespace
metadata:
  name: kaizen-production

---
# k8s/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kaizen-app
  namespace: kaizen-production
  labels:
    app: kaizen
spec:
  replicas: 3
  selector:
    matchLabels:
      app: kaizen
  template:
    metadata:
      labels:
        app: kaizen
    spec:
      containers:
      - name: kaizen
        image: kaizen:v1.0.0
        ports:
        - containerPort: 8080
        env:
        - name: KAIZEN_ENV
          value: "production"
        - name: DATABASE_URL
          valueFrom:
            secretKeyRef:
              name: kaizen-secrets
              key: database-url
        - name: OPENAI_API_KEY
          valueFrom:
            secretKeyRef:
              name: kaizen-secrets
              key: openai-api-key
        resources:
          requests:
            memory: "512Mi"
            cpu: "500m"
          limits:
            memory: "1Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /health
            port: 8080
          initialDelaySeconds: 30
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /ready
            port: 8080
          initialDelaySeconds: 5
          periodSeconds: 5

---
# k8s/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: kaizen-service
  namespace: kaizen-production
spec:
  selector:
    app: kaizen
  ports:
  - port: 80
    targetPort: 8080
  type: ClusterIP

---
# k8s/ingress.yaml
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: kaizen-ingress
  namespace: kaizen-production
  annotations:
    kubernetes.io/ingress.class: nginx
    cert-manager.io/cluster-issuer: letsencrypt-prod
spec:
  tls:
  - hosts:
    - kaizen.company.com
    secretName: kaizen-tls
  rules:
  - host: kaizen.company.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: kaizen-service
            port:
              number: 80

---
# k8s/hpa.yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: kaizen-hpa
  namespace: kaizen-production
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: kaizen-app
  minReplicas: 3
  maxReplicas: 20
  metrics:
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80
```

**Kubernetes Deployment Script**:

```bash
#!/bin/bash
# deploy-k8s.sh

set -e

# Create secrets
kubectl create secret generic kaizen-secrets \
  --from-literal=database-url="postgresql://kaizen:${DB_PASSWORD}@postgres:5432/kaizen" \
  --from-literal=openai-api-key="${OPENAI_API_KEY}" \
  --from-literal=anthropic-api-key="${ANTHROPIC_API_KEY}" \
  --namespace=kaizen-production

# Deploy database
kubectl apply -f k8s/postgres/

# Deploy Redis
kubectl apply -f k8s/redis/

# Deploy application
kubectl apply -f k8s/

# Wait for deployment
kubectl rollout status deployment/kaizen-app -n kaizen-production

# Verify deployment
kubectl get pods -n kaizen-production
kubectl get services -n kaizen-production

echo "✅ Kaizen deployed successfully!"
echo "🌐 Access: https://kaizen.company.com"
```

## Configuration Management

### Environment Configuration

**Production Configuration Pattern**:

```python
# config/production.py
import os
from dataclasses import dataclass
from typing import Dict, List, Optional

@dataclass
class ProductionConfig:
    """Production configuration for Kaizen deployment."""

    # Environment
    environment: str = "production"
    debug: bool = False
    log_level: str = "INFO"

    # Application
    host: str = "0.0.0.0"
    port: int = 8080
    workers: int = 4

    # Database
    database_url: str = os.getenv("DATABASE_URL")
    database_pool_size: int = 20
    database_max_overflow: int = 30

    # Redis
    redis_url: str = os.getenv("REDIS_URL")
    redis_max_connections: int = 50

    # AI Models
    openai_api_key: str = os.getenv("OPENAI_API_KEY")
    anthropic_api_key: str = os.getenv("ANTHROPIC_API_KEY")
    model_timeout: int = 30
    model_retry_attempts: int = 3

    # Security
    secret_key: str = os.getenv("KAIZEN_SECRET_KEY")
    allowed_hosts: List[str] = None
    cors_origins: List[str] = None

    # Monitoring
    metrics_enabled: bool = True
    prometheus_port: int = 9090
    health_check_interval: int = 30

    # Performance
    cache_enabled: bool = True
    cache_ttl: int = 3600
    rate_limiting: bool = True
    max_requests_per_minute: int = 100

    # Logging
    log_format: str = "json"
    log_file: Optional[str] = "/app/logs/kaizen.log"

    def __post_init__(self):
        """Validate production configuration."""
        if not self.database_url:
            raise ValueError("DATABASE_URL is required for production")

        if not self.secret_key:
            raise ValueError("KAIZEN_SECRET_KEY is required for production")

        if not self.openai_api_key:
            raise ValueError("OPENAI_API_KEY is required for production")

        # Set defaults for lists
        if self.allowed_hosts is None:
            self.allowed_hosts = ["kaizen.company.com"]

        if self.cors_origins is None:
            self.cors_origins = ["https://kaizen.company.com"]

# Load configuration
config = ProductionConfig()
```

### Infrastructure as Code

**Terraform Configuration**:

```hcl
# terraform/main.tf
terraform {
  required_providers {
    aws = {
      source  = "hashicorp/aws"
      version = "~> 5.0"
    }
    kubernetes = {
      source  = "hashicorp/kubernetes"
      version = "~> 2.0"
    }
  }
}

# EKS Cluster
module "eks" {
  source  = "terraform-aws-modules/eks/aws"
  version = "~> 19.0"

  cluster_name    = "kaizen-production"
  cluster_version = "1.27"

  vpc_id     = module.vpc.vpc_id
  subnet_ids = module.vpc.private_subnets

  eks_managed_node_groups = {
    kaizen_nodes = {
      min_size     = 3
      max_size     = 20
      desired_size = 5

      instance_types = ["m5.large"]
      capacity_type  = "ON_DEMAND"

      labels = {
        Environment = "production"
        Application = "kaizen"
      }
    }
  }
}

# RDS Database
resource "aws_db_instance" "kaizen_db" {
  identifier = "kaizen-production"

  engine         = "postgres"
  engine_version = "15.3"
  instance_class = "db.r5.large"

  allocated_storage     = 100
  max_allocated_storage = 1000
  storage_encrypted     = true

  db_name  = "kaizen"
  username = "kaizen"
  password = var.db_password

  vpc_security_group_ids = [aws_security_group.rds.id]
  db_subnet_group_name   = aws_db_subnet_group.kaizen.name

  backup_retention_period = 7
  backup_window          = "03:00-04:00"
  maintenance_window     = "sun:04:00-sun:05:00"

  skip_final_snapshot = false
  final_snapshot_identifier = "kaizen-production-final-snapshot"

  tags = {
    Name        = "kaizen-production"
    Environment = "production"
  }
}

# ElastiCache Redis
resource "aws_elasticache_replication_group" "kaizen_redis" {
  replication_group_id         = "kaizen-production"
  description                  = "Redis cluster for Kaizen production"

  node_type            = "cache.r6g.large"
  port                 = 6379
  parameter_group_name = "default.redis7"

  num_cache_clusters = 3
  at_rest_encryption_enabled = true
  transit_encryption_enabled = true

  subnet_group_name = aws_elasticache_subnet_group.kaizen.name
  security_group_ids = [aws_security_group.redis.id]

  tags = {
    Name        = "kaizen-production"
    Environment = "production"
  }
}
```

## Scaling Strategies

### Horizontal Scaling

**Auto-Scaling Configuration**:

```yaml
# Auto-scaling based on multiple metrics
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: kaizen-advanced-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: kaizen-app
  minReplicas: 3
  maxReplicas: 50
  metrics:
  # CPU utilization
  - type: Resource
    resource:
      name: cpu
      target:
        type: Utilization
        averageUtilization: 70

  # Memory utilization
  - type: Resource
    resource:
      name: memory
      target:
        type: Utilization
        averageUtilization: 80

  # Custom metrics
  - type: Pods
    pods:
      metric:
        name: kaizen_requests_per_second
      target:
        type: AverageValue
        averageValue: "100"

  - type: Pods
    pods:
      metric:
        name: kaizen_execution_latency_p95
      target:
        type: AverageValue
        averageValue: "5000m"  # 5 seconds

  behavior:
    scaleUp:
      stabilizationWindowSeconds: 60
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 4
        periodSeconds: 15
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
```

### Vertical Scaling

**Vertical Pod Autoscaler**:

```yaml
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: kaizen-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: kaizen-app
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: kaizen
      minAllowed:
        cpu: 100m
        memory: 128Mi
      maxAllowed:
        cpu: 2
        memory: 4Gi
      controlledResources: ["cpu", "memory"]
```

## High Availability

### Multi-Region Deployment

**Global Load Balancing**:

```yaml
# Global load balancer configuration
apiVersion: v1
kind: ConfigMap
metadata:
  name: global-lb-config
data:
  nginx.conf: |
    upstream kaizen_us_east {
        server kaizen-us-east.company.com;
    }

    upstream kaizen_us_west {
        server kaizen-us-west.company.com;
    }

    upstream kaizen_eu_west {
        server kaizen-eu-west.company.com;
    }

    geo $region {
        default us_east;
        ~^(?:10\.1\.|192\.168\.|172\.(?:1[6-9]|2[0-9]|3[01])\.) us_east;
        ~^(?:50\.1[6-9]\.|70\.(?:1[0-5][0-9]|1[6-9][0-9]|2[0-4][0-9]|25[0-5])\.) us_west;
        ~^(?:80\.(?:2[0-5][0-5]|[0-1][0-9][0-9])\.) eu_west;
    }

    server {
        listen 80;
        server_name kaizen-global.company.com;

        location / {
            proxy_pass http://kaizen_$region;
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        }
    }
```

### Database High Availability

**PostgreSQL High Availability**:

```yaml
# PostgreSQL cluster with replication
apiVersion: postgresql.cnpg.io/v1
kind: Cluster
metadata:
  name: postgres-cluster
spec:
  instances: 3

  postgresql:
    parameters:
      max_connections: "200"
      shared_buffers: "256MB"
      effective_cache_size: "1GB"
      wal_buffers: "16MB"
      checkpoint_completion_target: "0.9"

  bootstrap:
    initdb:
      database: kaizen
      owner: kaizen
      secret:
        name: postgres-credentials

  storage:
    size: 100Gi
    storageClass: fast-ssd

  monitoring:
    enabled: true

  backup:
    retentionPolicy: "30d"
    barmanObjectStore:
      destinationPath: s3://kaizen-backups/postgres
      s3Credentials:
        accessKeyId:
          name: backup-credentials
          key: ACCESS_KEY_ID
        secretAccessKey:
          name: backup-credentials
          key: SECRET_ACCESS_KEY
      wal:
        retention: "5d"
      data:
        retention: "30d"
```

## Monitoring and Observability

### Production Monitoring Stack

**Complete Monitoring Setup**:

```yaml
# monitoring/kustomization.yaml
apiVersion: kustomize.config.k8s.io/v1beta1
kind: Kustomization

resources:
- prometheus/
- grafana/
- alertmanager/
- loki/
- jaeger/

namespace: monitoring

commonLabels:
  app.kubernetes.io/part-of: kaizen-monitoring
```

**Prometheus Configuration**:

```yaml
# monitoring/prometheus/config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
data:
  prometheus.yml: |
    global:
      scrape_interval: 15s
      evaluation_interval: 15s

    rule_files:
    - "/etc/prometheus/rules/*.yml"

    alerting:
      alertmanagers:
      - static_configs:
        - targets:
          - alertmanager:9093

    scrape_configs:
    # Kaizen application metrics
    - job_name: 'kaizen'
      static_configs:
      - targets: ['kaizen-service:9090']
      metrics_path: /metrics
      scrape_interval: 10s

    # Kubernetes metrics
    - job_name: 'kubernetes-pods'
      kubernetes_sd_configs:
      - role: pod
      relabel_configs:
      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]
        action: keep
        regex: true

    # PostgreSQL metrics
    - job_name: 'postgres'
      static_configs:
      - targets: ['postgres-exporter:9187']

    # Redis metrics
    - job_name: 'redis'
      static_configs:
      - targets: ['redis-exporter:9121']
```

### Health Checks and Readiness Probes

**Application Health Endpoints**:

```python
# health_checks.py
from dataclasses import dataclass
from typing import Dict, List
import asyncio
import psutil
import time

@dataclass
class HealthStatus:
    service: str
    status: str
    details: Dict
    timestamp: float

class HealthChecker:
    """Comprehensive health checking for production deployment."""

    def __init__(self, config):
        self.config = config
        self.start_time = time.time()

    async def health_check(self) -> Dict:
        """Complete health check for all components."""
        checks = await asyncio.gather(
            self.check_application_health(),
            self.check_database_health(),
            self.check_redis_health(),
            self.check_ai_model_health(),
            self.check_system_resources(),
            return_exceptions=True
        )

        overall_status = "healthy"
        for check in checks:
            if isinstance(check, HealthStatus) and check.status != "healthy":
                overall_status = "unhealthy"
                break

        return {
            "status": overall_status,
            "timestamp": time.time(),
            "uptime_seconds": time.time() - self.start_time,
            "checks": {
                "application": checks[0],
                "database": checks[1],
                "redis": checks[2],
                "ai_models": checks[3],
                "system": checks[4]
            }
        }

    async def readiness_check(self) -> Dict:
        """Quick readiness check for load balancer."""
        # Fast checks only
        db_ready = await self.quick_db_check()
        redis_ready = await self.quick_redis_check()

        ready = db_ready and redis_ready

        return {
            "ready": ready,
            "timestamp": time.time(),
            "checks": {
                "database": db_ready,
                "redis": redis_ready
            }
        }

    async def check_system_resources(self) -> HealthStatus:
        """Check system resource usage."""
        cpu_percent = psutil.cpu_percent(interval=1)
        memory = psutil.virtual_memory()
        disk = psutil.disk_usage('/')

        status = "healthy"
        if cpu_percent > 90 or memory.percent > 90 or disk.percent > 90:
            status = "degraded"

        return HealthStatus(
            service="system",
            status=status,
            details={
                "cpu_percent": cpu_percent,
                "memory_percent": memory.percent,
                "disk_percent": disk.percent,
                "load_average": psutil.getloadavg()
            },
            timestamp=time.time()
        )
```

## Backup and Disaster Recovery

### Backup Strategy

**Automated Backup Configuration**:

```yaml
# backup/cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: kaizen-backup
spec:
  schedule: "0 2 * * *"  # Daily at 2 AM
  jobTemplate:
    spec:
      template:
        spec:
          containers:
          - name: backup
            image: postgres:15
            env:
            - name: PGPASSWORD
              valueFrom:
                secretKeyRef:
                  name: postgres-credentials
                  key: password
            command:
            - /bin/bash
            - -c
            - |
              # Database backup
              pg_dump -h postgres -U kaizen kaizen > /backup/kaizen-$(date +%Y%m%d-%H%M%S).sql

              # Upload to S3
              aws s3 cp /backup/kaizen-*.sql s3://kaizen-backups/database/

              # Cleanup old local backups
              find /backup -name "kaizen-*.sql" -mtime +7 -delete
            volumeMounts:
            - name: backup-storage
              mountPath: /backup
          volumes:
          - name: backup-storage
            persistentVolumeClaim:
              claimName: backup-pvc
          restartPolicy: OnFailure
```

### Disaster Recovery Plan

**Recovery Procedures**:

```bash
#!/bin/bash
# disaster-recovery.sh

set -e

echo "🚨 Starting Kaizen disaster recovery..."

# 1. Assess damage
echo "📊 Assessing current system state..."
kubectl get pods -n kaizen-production
kubectl get services -n kaizen-production

# 2. Database recovery
echo "🗄️ Recovering database..."
if ! kubectl exec postgres-0 -- pg_isready; then
    echo "Database is down, restoring from backup..."

    # Get latest backup
    LATEST_BACKUP=$(aws s3 ls s3://kaizen-backups/database/ --recursive | sort | tail -n 1 | awk '{print $4}')

    # Restore database
    kubectl exec postgres-0 -- psql -U kaizen -c "DROP DATABASE IF EXISTS kaizen;"
    kubectl exec postgres-0 -- psql -U kaizen -c "CREATE DATABASE kaizen;"
    aws s3 cp "s3://kaizen-backups/database/$LATEST_BACKUP" - | kubectl exec -i postgres-0 -- psql -U kaizen kaizen
fi

# 3. Application recovery
echo "🚀 Recovering application..."
kubectl rollout restart deployment/kaizen-app -n kaizen-production
kubectl rollout status deployment/kaizen-app -n kaizen-production

# 4. Verify recovery
echo "✅ Verifying recovery..."
sleep 30
curl -f http://kaizen.company.com/health || exit 1

echo "🎉 Disaster recovery completed successfully!"
```

## Security Hardening

### Container Security

**Secure Container Configuration**:

```dockerfile
# Dockerfile.production
FROM python:3.11-slim as base

# Security: Create non-root user
RUN groupadd -r kaizen && useradd --no-log-init -r -g kaizen kaizen

# Security: Update packages and remove package manager
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Security: Set up secure directories
WORKDIR /app
RUN chown -R kaizen:kaizen /app

# Install dependencies
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# Copy application
COPY --chown=kaizen:kaizen . .

# Security: Switch to non-root user
USER kaizen

# Security: Run with minimal privileges
EXPOSE 8080
CMD ["gunicorn", "--bind", "0.0.0.0:8080", "--workers", "4", "app:create_app()"]
```

### Network Security

**Network Policies**:

```yaml
# security/network-policy.yaml
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: kaizen-network-policy
  namespace: kaizen-production
spec:
  podSelector:
    matchLabels:
      app: kaizen
  policyTypes:
  - Ingress
  - Egress

  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: ingress-nginx
    ports:
    - protocol: TCP
      port: 8080

  egress:
  # Allow DNS
  - to: []
    ports:
    - protocol: UDP
      port: 53

  # Allow database access
  - to:
    - podSelector:
        matchLabels:
          app: postgres
    ports:
    - protocol: TCP
      port: 5432

  # Allow Redis access
  - to:
    - podSelector:
        matchLabels:
          app: redis
    ports:
    - protocol: TCP
      port: 6379

  # Allow HTTPS outbound (for AI APIs)
  - to: []
    ports:
    - protocol: TCP
      port: 443
```

## Implementation Timeline

### Phase 1: Basic Production Deployment (2-3 weeks)
- ✅ Docker containerization
- ✅ Basic configuration management
- ✅ Health checks and readiness probes
- ✅ Single-instance deployment

### Phase 2: Distributed Deployment (3-4 weeks)
- 🟡 Load balancing and multiple instances
- 🟡 Database clustering and replication
- 🟡 Redis clustering
- 🟡 Basic monitoring and alerting

### Phase 3: Cloud-Native Deployment (4-6 weeks)
- 🟡 Kubernetes deployment
- 🟡 Auto-scaling configuration
- 🟡 Infrastructure as Code
- 🟡 Advanced monitoring

### Phase 4: Enterprise Features (6-8 weeks)
- 🟡 Multi-region deployment
- 🟡 Disaster recovery automation
- 🟡 Advanced security hardening
- 🟡 Compliance and governance

---

**🚀 Production Deployment Ready**: This comprehensive deployment guide provides the foundation for enterprise-scale Kaizen deployments. Start with single-instance deployment and gradually adopt more advanced patterns as your requirements grow.