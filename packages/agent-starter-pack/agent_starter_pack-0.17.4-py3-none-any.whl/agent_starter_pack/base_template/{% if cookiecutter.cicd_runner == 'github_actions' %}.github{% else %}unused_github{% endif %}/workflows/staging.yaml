# Copyright 2025 Google LLC
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

name: Deploy to Staging

on:
  push:
    branches:
      - main
    paths:
      - '{{cookiecutter.agent_directory}}/**'
      - 'data_ingestion/**'
      - 'tests/**'
      - 'deployment/**'
      - 'uv.lock'

jobs:
  deploy_and_test_staging:
    runs-on: ubuntu-latest
    permissions:
      contents: 'read'
      id-token: 'write'

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python 3.12
        uses: actions/setup-python@v4
        with:
          python-version: '3.12'

      - id: 'auth'
        name: 'Authenticate to Google Cloud'
        uses: 'google-github-actions/auth@v2'
        with:
          workload_identity_provider: 'projects/{% raw %}${{ vars.GCP_PROJECT_NUMBER }}{% endraw %}/locations/global/workloadIdentityPools/{% raw %}${{ secrets.WIF_POOL_ID }}{% endraw %}/providers/{% raw %}${{ secrets.WIF_PROVIDER_ID }}{% endraw %}'
          service_account: '{% raw %}${{ secrets.GCP_SERVICE_ACCOUNT }}{% endraw %}'
          create_credentials_file: true
          project_id: {% raw %}${{ vars.CICD_PROJECT_ID }}{% endraw %}
{%- if cookiecutter.deployment_target == 'cloud_run' %}

      - name: Set up Cloud SDK
        uses: 'google-github-actions/setup-gcloud@v2'
{%- endif %}
{%- if cookiecutter.data_ingestion %}

      - name: Deploy data ingestion pipeline (Staging)
        run: |
          cd data_ingestion && pip install uv==0.8.13 && cd data_ingestion_pipeline && \
          uv sync --locked && uv run python submit_pipeline.py
        env:
          PIPELINE_ROOT: {% raw %}${{ vars.PIPELINE_GCS_ROOT_STAGING }}{% endraw %}
          REGION: {% raw %}${{ vars.REGION }}{% endraw %}
{%- if cookiecutter.datastore_type == "vertex_ai_search" %}
          DATA_STORE_REGION: {% raw %}${{ vars.DATA_STORE_REGION }}{% endraw %}
          DATA_STORE_ID: {% raw %}${{ vars.DATA_STORE_ID_STAGING }}{% endraw %}
{%- elif cookiecutter.datastore_type == "vertex_ai_vector_search" %}
          VECTOR_SEARCH_INDEX: {% raw %}${{ vars.VECTOR_SEARCH_INDEX_STAGING }}{% endraw %}
          VECTOR_SEARCH_INDEX_ENDPOINT: {% raw %}${{ vars.VECTOR_SEARCH_INDEX_ENDPOINT_STAGING }}{% endraw %}
          VECTOR_SEARCH_BUCKET: {% raw %}${{ vars.VECTOR_SEARCH_BUCKET_STAGING }}{% endraw %}
{%- endif %}
          PROJECT_ID: {% raw %}${{ vars.STAGING_PROJECT_ID }}{% endraw %}
          SERVICE_ACCOUNT: {% raw %}${{ vars.PIPELINE_SA_EMAIL_STAGING }}{% endraw %}
          PIPELINE_NAME: {% raw %}${{ vars.PIPELINE_NAME }}{% endraw %}
{%- endif %}

{%- if cookiecutter.deployment_target == 'cloud_run' %}

      - name: Configure Docker for Artifact Registry
        run: |
          gcloud auth configure-docker {% raw %}${{ vars.REGION }}{% endraw %}-docker.pkg.dev --quiet
          
      - name: Build and Push Docker Image
        run: |
          docker build -t {% raw %}${{ vars.REGION }}{% endraw %}-docker.pkg.dev/{% raw %}${{ vars.CICD_PROJECT_ID }}{% endraw %}/{% raw %}${{ vars.ARTIFACT_REGISTRY_REPO_NAME }}{% endraw %}/{% raw %}${{ vars.CONTAINER_NAME }}{% endraw %} \
            --build-arg COMMIT_SHA={% raw %}${{ github.sha }}{% endraw %} .
          docker push {% raw %}${{ vars.REGION }}{% endraw %}-docker.pkg.dev/{% raw %}${{ vars.CICD_PROJECT_ID }}{% endraw %}/{% raw %}${{ vars.ARTIFACT_REGISTRY_REPO_NAME }}{% endraw %}/{% raw %}${{ vars.CONTAINER_NAME }}{% endraw %}

      - name: Deploy to Staging (Cloud Run)
        run: |
          gcloud run deploy {{cookiecutter.project_name}} \
            --image {% raw %}${{ vars.REGION }}{% endraw %}-docker.pkg.dev/{% raw %}${{ vars.CICD_PROJECT_ID }}{% endraw %}/{% raw %}${{ vars.ARTIFACT_REGISTRY_REPO_NAME }}{% endraw %}/{% raw %}${{ vars.CONTAINER_NAME }}{% endraw %} \
            --region {% raw %}${{ vars.REGION }}{% endraw %} \
            --project {% raw %}${{ vars.STAGING_PROJECT_ID }}{% endraw %}

      - name: Fetch Staging Service URL
        id: fetch-url
        run: |
          _STAGING_URL=$(gcloud run services describe {{cookiecutter.project_name}} \
            --region {% raw %}${{ vars.REGION }}{% endraw %} --project {% raw %}${{ vars.STAGING_PROJECT_ID }}{% endraw %} --format="value(status.url)")
          echo "_staging_url=${_STAGING_URL}" >> $GITHUB_OUTPUT

      - name: Fetch ID Token
        id: fetch-token
        run: |
          _ID_TOKEN=$(gcloud auth print-identity-token --impersonate-service-account={% raw %}${{ secrets.GCP_SERVICE_ACCOUNT }}{% endraw %} -q)
          echo "::add-mask::${_ID_TOKEN}"
          echo "_id_token=${_ID_TOKEN}" >> $GITHUB_OUTPUT

{%- elif cookiecutter.deployment_target == 'agent_engine' %}

      - name: Install uv and dependencies
        run: |
          pip install uv==0.8.13
          uv sync --locked

      - name: Deploy to Staging (Agent Engine)
        run: |
          uv export --no-hashes --no-sources --no-header --no-dev --no-emit-project --no-annotate --locked > .requirements.txt
          uv run {{cookiecutter.agent_directory}}/agent_engine_app.py \
            --project {% raw %}${{ vars.STAGING_PROJECT_ID }}{% endraw %} \
            --location {% raw %}${{ vars.REGION }}{% endraw %} \
            --artifacts-bucket-name {% raw %}${{ vars.LOGS_BUCKET_NAME_STAGING }}{% endraw %} \
            --set-env-vars="COMMIT_SHA={% raw %}${{ github.sha }}{% endraw %}{%- if cookiecutter.data_ingestion %}{%- if cookiecutter.datastore_type == "vertex_ai_search" %},DATA_STORE_ID={% raw %}${{ vars.DATA_STORE_ID_STAGING }}{% endraw %},DATA_STORE_REGION={% raw %}${{ vars.DATA_STORE_REGION }}{% endraw %}{%- elif cookiecutter.datastore_type == "vertex_ai_vector_search" %},VECTOR_SEARCH_INDEX={% raw %}${{ vars.VECTOR_SEARCH_INDEX_STAGING }}{% endraw %},VECTOR_SEARCH_INDEX_ENDPOINT={% raw %}${{ vars.VECTOR_SEARCH_INDEX_ENDPOINT_STAGING }}{% endraw %},VECTOR_SEARCH_BUCKET={% raw %}${{ vars.VECTOR_SEARCH_BUCKET_STAGING }}{% endraw %}{%- endif %}{%- endif %}"

      - name: Fetch Auth Token
        id: fetch-token
        run: |
          _AUTH_TOKEN=$(gcloud auth print-access-token -q)
          echo "::add-mask::${_AUTH_TOKEN}"
          echo "_auth_token=${_AUTH_TOKEN}" >> $GITHUB_OUTPUT
{%- endif %}

      - name: Run load test
        run: |
{%- if cookiecutter.deployment_target == 'cloud_run' and cookiecutter.agent_name == 'adk_live' %}
          # Install dependencies for load test
          pip install locust==2.31.1 websockets

          # Install Cloud Run proxy component
          gcloud components install cloud-run-proxy --quiet

          # Start Cloud Run proxy in background
          gcloud run services proxy {{cookiecutter.project_name}} \
          --port=8080 \
          --region {% raw %}${{ vars.REGION }}{% endraw %} \
          --project {% raw %}${{ vars.STAGING_PROJECT_ID }}{% endraw %} \
          --quiet &
          PROXY_PID=$!

          # Wait for proxy to be ready
          echo "Waiting for proxy to start..."
          sleep 10

          # Run load test and capture exit code
          locust -f tests/load_test/load_test.py \
          --headless \
          -H http://127.0.0.1:8080 \
          -t 30s -u 2 -r 2 \
          --csv=tests/load_test/.results/results \
          --html=tests/load_test/.results/report.html
          LOCUST_EXIT_CODE=$?

          # Clean up proxy
          kill $PROXY_PID || true

          # Exit with load test result to fail build if tests failed
          if [ $LOCUST_EXIT_CODE -ne 0 ]; then
            echo "Load test failed with exit code $LOCUST_EXIT_CODE"
            exit $LOCUST_EXIT_CODE
          fi
{%- elif cookiecutter.deployment_target == 'cloud_run' %}
          export _ID_TOKEN="{% raw %}${{ steps.fetch-token.outputs._id_token }}{% endraw %}"
          export _STAGING_URL="{% raw %}${{ steps.fetch-url.outputs._staging_url }}{% endraw %}"
          pip install locust==2.31.1
          locust -f tests/load_test/load_test.py \
          --headless \
          -H ${_STAGING_URL} \
          -t 30s -u 10 -r 0.5 \
          --csv=tests/load_test/.results/results \
          --html=tests/load_test/.results/report.html
{%- elif cookiecutter.deployment_target == 'agent_engine' and cookiecutter.is_adk_live %}
          # Start expose app in remote mode (uses deployment_metadata.json by default)
          uv run python -m {{cookiecutter.agent_directory}}.utils.expose_app --mode remote &
          EXPOSE_PID=$!

          # Wait for expose app to be ready
          sleep 10

          # Run load test against local expose app
          uv run --with locust==2.31.1 --with websockets locust -f tests/load_test/load_test.py \
          -H http://127.0.0.1:8000 \
          --headless \
          -t 30s -u 2 -r 1 \
          --csv=tests/load_test/.results/results \
          --html=tests/load_test/.results/report.html
          LOCUST_EXIT_CODE=$?

          # Stop expose app
          kill $EXPOSE_PID

          # Exit with error if locust test failed
          if [ $LOCUST_EXIT_CODE -ne 0 ]; then
            echo "Load test failed with exit code $LOCUST_EXIT_CODE"
            exit $LOCUST_EXIT_CODE
          fi
{%- elif cookiecutter.deployment_target == 'agent_engine' %}
          export _AUTH_TOKEN="{% raw %}${{ steps.fetch-token.outputs._auth_token }}{% endraw %}"
          pip install locust==2.31.1
          locust -f tests/load_test/load_test.py \
          --headless \
          -t 30s -u 2 -r 0.5 \
          --csv=tests/load_test/.results/results \
          --html=tests/load_test/.results/report.html
{%- endif %}

      - name: Export Load Test Results to GCS
        run: |
          TIMESTAMP=$(date +%Y%m%d-%H%M%S)
          gcloud storage cp --recursive tests/load_test/.results gs://{% raw %}${{ vars.BUCKET_NAME_LOAD_TEST_RESULTS }}{% endraw %}/results-${TIMESTAMP} --quiet
          echo "_________________________________________________________________________"
          echo "Load test results copied to gs://{% raw %}${{ vars.BUCKET_NAME_LOAD_TEST_RESULTS }}{% endraw %}/results-${TIMESTAMP}"
          echo "HTTP link: https://console.cloud.google.com/storage/browser/{% raw %}${{ vars.BUCKET_NAME_LOAD_TEST_RESULTS }}{% endraw %}/results-${TIMESTAMP}"
          echo "_________________________________________________________________________"

  call_production_workflow:
    needs: deploy_and_test_staging
    uses: ./.github/workflows/deploy-to-prod.yaml
    permissions:
      contents: 'read'
      id-token: 'write'
    secrets: inherit