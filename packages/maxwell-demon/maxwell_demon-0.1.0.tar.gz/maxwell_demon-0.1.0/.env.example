# Example environment configuration for vibelint
# Copy this file to .env and fill in your API keys

# Hugging Face token (for embedding models)
HF_TOKEN=your_huggingface_token_here

# === LLM Configuration Options (choose one scenario) ===

# Scenario 1: OpenAI (simplest - just add your API key!)
# Automatically configures:
# - Fast LLM: gpt-5-mini ($0.25/$2 per 1M tokens)
# - Orchestrator LLM: gpt-5 ($1.25/$10 per 1M tokens)
# - API URL: https://api.openai.com/v1
OPENAI_API_KEY=your_openai_key_here

# Scenario 2: Custom dual endpoints (advanced)
# FAST_LLM_API_KEY=sk-proxy-your-fast-llm-key-here
# ORCHESTRATOR_LLM_API_KEY=sk-proxy-your-orchestrator-llm-key-here
# Configure models/endpoints in pyproject.toml

# Scenario 3: Single shared endpoint
# LLM_API_KEY=sk-your-shared-llm-key-here
# Configure models/endpoints in pyproject.toml

# Scenario 4: Specialized embeddings (enhanced semantic analysis)
# CODE_EMBEDDING_API_KEY=sk-vanguardone-proxy-your-code-embedding-key-here
# NATURAL_EMBEDDING_API_KEY=sk-vanguardtwo-proxy-your-natural-language-key-here
# Uses specialized models for:
# - Code: Function similarity, architectural patterns, duplicate logic detection
# - Natural: Documentation quality, comment relevance, requirement matching

# Scenario 5: Vector database options (for persistent embeddings)
# QDRANT_API_KEY=your-qdrant-api-key  # For Qdrant cloud
# VECTORIZE_API_TOKEN=your-cloudflare-api-token  # For Cloudflare Vectorize
# VECTORIZE_ACCOUNT_ID=your-cloudflare-account-id  # For Cloudflare Vectorize
# PINECONE_API_KEY=your-pinecone-api-key  # For Pinecone cloud

# Scenario 6: Embeddings-only (no LLM required)
# No API keys needed - uses local embedding models for:
# - Semantic similarity detection
# - Code clustering
# - Duplicate function detection