{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed Test\n",
    "\n",
    "### **Conclusion**\n",
    "- **~1.8X faster on GPU compared to CPU on Apple Silicon. On Intel processors, 2X faster with CPU (but metal is not supported on intel processor)** \n",
    "- **CuDNN** (NVIDIA’s deep learning library) significantly improves performance for LSTM-based models compared to vanilla RNNs. (See LSTM vs RNN lstm in keras documentation)\n",
    "\n",
    "---\n",
    "\n",
    "### **Model Details**\n",
    "- **Model Type:** BiLSTM  \n",
    "- **Total Parameters:** 2,658,945 (~10.14 MB)  \n",
    "- **Training Data:** 25,000 sequences  \n",
    "- **Features:** 1 (number of occurrences in the corpus)\n",
    "\n",
    "---\n",
    "\n",
    "### **Rules of Thumb**\n",
    "1. **Sample Size:**  \n",
    "   `nb_samples > 10 × nb_features × nb_classes`  \n",
    "   (with `nb_classes = 5` for regression).\n",
    "\n",
    "2. **Parameter-Sample Ratio:**  \n",
    "   `nb_parameters < nb_samples / 10`  \n",
    "   (or even `nb_samples / 50` for deep learning).\n",
    "\n",
    "---\n",
    "\n",
    "### **GPU Performance**\n",
    "\n",
    "#### **tf.keras**  \n",
    "- Run 1: **146s** (~179ms/step) — loss: `0.4118`, accuracy: `0.8075`, val_loss: `0.3565`, val_accuracy: `0.8464`  \n",
    "- Run 2 (Apple Silicon): **29s** (~32ms/step) — loss: `0.4167`, accuracy: `0.8018`, val_loss: `0.3612`, val_accuracy: `0.8474`\n",
    "\n",
    "#### **Keras**  \n",
    "- Run 1: **119s** (~146ms/step) — loss: `0.4102`, accuracy: `0.8142`, val_loss: `0.3496`, val_accuracy: `0.8467`  \n",
    "- Run 2 (Apple Silicon): **25s** (~30ms/step) — loss: `0.4167`, accuracy: `0.8018`, val_loss: `0.3611`, val_accuracy: `0.8474`\n",
    "\n",
    "---\n",
    "\n",
    "### **CPU Performance**\n",
    "\n",
    "#### **tf.keras**  \n",
    "- Run 1: **73s** (~89ms/step) — loss: `0.4184`, accuracy: `0.8078`, val_loss: `0.3444`, val_accuracy: `0.8507`  \n",
    "- Run 2 (Apple Silicon): **47s** (~59ms/step) — loss: `0.5113`, accuracy: `0.7238`, val_loss: `0.3405`, val_accuracy: `0.8538`\n",
    "\n",
    "#### **Keras**  \n",
    "- Run 1: **81s** (~97ms/step) — loss: `0.4027`, accuracy: `0.8147`, val_loss: `0.3395`, val_accuracy: `0.8508`  \n",
    "- Run 2 (Apple Silicon): **47s** (~58ms/step) — loss: `0.5113`, accuracy: `0.7238`, val_loss: `0.3405`, val_accuracy: `0.853\n",
    "\n",
    "---\n",
    "\n",
    "### **Key Observations**\n",
    "1. **CPU vs GPU:**  \n",
    "   - GPU training is approximately **2X faster** than CPU for this BiLSTM model.  \n",
    "\n",
    "2. **Accuracy and Loss:**  \n",
    "   - Both CPU and GPU achieve similar final accuracy and loss values, demonstrating functional equivalence.  \n",
    "\n",
    "3. **Recommendation:**  \n",
    "   - Use CPU for smaller datasets or models like BiLSTM.  \n",
    "   - For larger datasets or deeper architectures, GPU with CuDNN can provide significant speedups.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"\\nDevices: \", devices)\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    details = tf.config.experimental.get_device_details(gpus[0])\n",
    "    print(\"GPU details: \", details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)  # for reproducibility\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 100  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(\n",
    "    num_words=max_features\n",
    ")\n",
    "\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print(x_train.shape, x_train[:2], y_train.shape, y_train[:2])\n",
    "print(\n",
    "    f\"{x_train.shape[0]} train samples and {y_test.shape[0]} tests samples, for a total of {x_train.shape[0] + x_test.shape[0]} samples.\"\n",
    ")\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "print(f\"Using tf.keras version {keras.__version__}\")\n",
    "\n",
    "# Define the input layer\n",
    "inputs = keras.Input(shape=(maxlen,))  # maxlen is the input length for each sequence\n",
    "\n",
    "# Embedding layer\n",
    "x = keras.layers.Embedding(max_features, 128)(\n",
    "    inputs\n",
    ")  # max_features is the vocabulary size\n",
    "\n",
    "# Bidirectional LSTM layer\n",
    "x = keras.layers.Bidirectional(keras.layers.LSTM(64))(x)\n",
    "\n",
    "# Dropout layer for regularization\n",
    "x = keras.layers.Dropout(0.5)(x)\n",
    "\n",
    "# Output layer (for binary classification)\n",
    "outputs = keras.layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# Define the model with inputs and outputs\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"BiLSTM\")\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Train the model\n",
    "print(\"Train...\")\n",
    "model.fit(\n",
    "    x_train, y_train, batch_size=batch_size, epochs=1, validation_data=(x_test, y_test)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "print(f\"Using keras version {keras.__version__}\")\n",
    "\n",
    "model = keras.models.Sequential(name=\"BiLSTM\")\n",
    "model.add(keras.layers.Embedding(max_features, 128, input_length=maxlen))\n",
    "model.add(keras.layers.Bidirectional(keras.layers.LSTM(64)))\n",
    "model.add(keras.layers.Dropout(0.5))\n",
    "model.add(keras.layers.Dense(1, activation=\"sigmoid\"))\n",
    "model.summary()\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"Train...\")\n",
    "model.fit(\n",
    "    x_train, y_train, batch_size=batch_size, epochs=1, validation_data=[x_test, y_test]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Comparing GPU performance of LSTM using CuDNN (keras.layers.LSTM) and not using CuDNN (keras.layers.LSTMCell)\n",
    "\n",
    "# from tensorflow import keras\n",
    "# from keras.models import Sequential\n",
    "# from keras.layers import Dense, Dropout, Embedding, LSTM, Input, Bidirectional\n",
    "\n",
    "# keras.utils.set_random_seed(42)\n",
    "\n",
    "\n",
    "# def build_model(allow_cudnn_kernel=True):\n",
    "#     # CuDNN is only available at the layer level, and not at the cell level.\n",
    "#     # This means `LSTM(units)` will use the CuDNN kernel,\n",
    "#     # while RNN(LSTMCell(units)) will run on non-CuDNN kernel.\n",
    "#     if allow_cudnn_kernel:\n",
    "#         # The LSTM layer with default options uses CuDNN.\n",
    "#         model = Sequential(name=\"BiLSTM with CuDNN\")\n",
    "#         lstm_layer = keras.layers.LSTM(64)\n",
    "#     else:\n",
    "#         # Wrapping a LSTMCell in a RNN layer will not use CuDNN.\n",
    "#         model = Sequential(name=\"BiLSTM without CuDNN\")\n",
    "#         lstm_layer = keras.layers.RNN(keras.layers.LSTMCell(64))\n",
    "#     model.add(Embedding(max_features, 128, input_length=maxlen))\n",
    "#     model.add(Bidirectional(lstm_layer))\n",
    "#     model.add(Dropout(0.5))\n",
    "#     model.add(Dense(1, activation=\"sigmoid\"))\n",
    "#     model.summary()\n",
    "#     return model\n",
    "\n",
    "\n",
    "# model = build_model(allow_cudnn_kernel=True)\n",
    "# model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# print(\"Train model using CuDNN kernel...\")\n",
    "# model.fit(\n",
    "#     x_train, y_train, batch_size=batch_size, epochs=1, validation_data=[x_test, y_test]\n",
    "# )\n",
    "\n",
    "\n",
    "# model_noncudnn = build_model(allow_cudnn_kernel=False)\n",
    "# model_noncudnn.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "# print(\"Train model not using CuDNN kernel...\")\n",
    "# model_noncudnn.fit(\n",
    "#     x_train, y_train, batch_size=batch_size, epochs=1, validation_data=[x_test, y_test]\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
