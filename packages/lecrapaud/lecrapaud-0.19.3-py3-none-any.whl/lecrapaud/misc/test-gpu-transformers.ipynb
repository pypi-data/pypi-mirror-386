{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speed Test\n",
    "\n",
    "### **Conclusion**\n",
    "- **~same on CPU compared to GPU** for this model and setup.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Model Details**\n",
    "- **Model Type:** Transformer (encoder only)  \n",
    "- **Total Parameters:** 4,934,529 (18.82 MB)\n",
    "- **Training Data:** 25,000 sequences  \n",
    "- **Features:** 1 (number of occurrences in the corpus)\n",
    "\n",
    "---\n",
    "\n",
    "### **Rules of Thumb**\n",
    "1. **Sample Size:**  \n",
    "   `nb_samples > 10 × nb_features × nb_classes`  \n",
    "   (with `nb_classes = 5` for regression).\n",
    "\n",
    "2. **Parameter-Sample Ratio:**  \n",
    "   `nb_parameters < nb_samples / 10`  \n",
    "   (or even `nb_samples / 50` for deep learning).\n",
    "\n",
    "---\n",
    "\n",
    "### **GPU Performance**\n",
    "\n",
    "#### **tf.keras**  \n",
    "- Run 1 (Apple Silicon) : 412s 525ms/step - loss: 7.6209 - accuracy: 0.4999 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
    "\n",
    "#### **Keras**  \n",
    "- Run 1 (Apple Silicon) : 419s 534ms/step - loss: 7.6191 - accuracy: 0.5002 - val_loss: 7.6246 - val_accuracy: 0.5000\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **CPU Performance**\n",
    "\n",
    "#### **tf.keras**  \n",
    "- Run 1 (Apple Silicon) : 439s 560ms/step - accuracy: 0.4992 - loss: 8.0630 - val_accuracy: 0.5000 - val_loss: 8.0590\n",
    "\n",
    "#### **Keras**  \n",
    "- Run 1 (Apple Silicon) : 417s 531ms/step - accuracy: 0.5006 - loss: 7.9368 - val_accuracy: 0.5000 - val_loss: 7.9712\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "devices = tf.config.list_physical_devices()\n",
    "print(\"\\nDevices: \", devices)\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "if gpus:\n",
    "    details = tf.config.experimental.get_device_details(gpus[0])\n",
    "    print(\"GPU details: \", details)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get data\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)  # for reproducibility\n",
    "\n",
    "max_features = 20000\n",
    "maxlen = 100  # cut texts after this number of words (among top max_features most common words)\n",
    "batch_size = 32\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.imdb.load_data(\n",
    "    num_words=max_features\n",
    ")\n",
    "\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen)\n",
    "print(x_train.shape, x_train[:2], y_train.shape, y_train[:2])\n",
    "print(\n",
    "    f\"{x_train.shape[0]} train samples and {y_test.shape[0]} tests samples, for a total of {x_train.shape[0] + x_test.shape[0]} samples.\"\n",
    ")\n",
    "y_train = np.array(y_train)\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "print(f\"Using tf.keras version {keras.__version__}\")\n",
    "\n",
    "ff_dim = 512\n",
    "head_size = 128\n",
    "num_heads = 16\n",
    "num_layers = 2\n",
    "\n",
    "\n",
    "def transformer_encoder(inputs, num_layers, head_size, num_heads, ff_dim, dropout=0):\n",
    "    for _ in range(num_layers):\n",
    "        # Attention and Normalization\n",
    "        x = keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "        x = keras.layers.MultiHeadAttention(\n",
    "            key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "        )(x, x)\n",
    "        x = keras.layers.Add()([x, inputs])\n",
    "\n",
    "        # Feed Forward Part\n",
    "        y = keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        y = keras.layers.Dense(ff_dim, activation=\"relu\")(y)\n",
    "        y = keras.layers.Dropout(dropout)(y)\n",
    "        y = keras.layers.Dense(inputs.shape[-1])(y)\n",
    "        inputs = keras.layers.Add()([y, x])\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Define the input layer\n",
    "inputs = keras.layers.Input(\n",
    "    shape=(maxlen,)\n",
    ")  # maxlen is the input length for each sequence\n",
    "\n",
    "# Embedding layer\n",
    "x = keras.layers.Embedding(max_features, 128)(\n",
    "    inputs\n",
    ")  # max_features is the vocabulary size\n",
    "x = transformer_encoder(x, num_layers, head_size, num_heads, ff_dim)\n",
    "x = keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "# Define output layer based on target type\n",
    "outputs = keras.layers.Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "# Build the model\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"transformer\")\n",
    "\n",
    "model.summary()\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"Train...\")\n",
    "batch_size = 32\n",
    "epoch = 100\n",
    "model.fit(\n",
    "    x_train, y_train, batch_size=batch_size, epochs=1, validation_data=[x_test, y_test]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "\n",
    "print(f\"Using keras version {keras.__version__}\")\n",
    "\n",
    "ff_dim = 512\n",
    "head_size = 128\n",
    "num_heads = 16\n",
    "num_layers = 2\n",
    "\n",
    "\n",
    "def transformer_encoder(inputs, num_layers, head_size, num_heads, ff_dim, dropout=0):\n",
    "    for _ in range(num_layers):\n",
    "        # Attention and Normalization\n",
    "        x = keras.layers.LayerNormalization(epsilon=1e-6)(inputs)\n",
    "        x = keras.layers.MultiHeadAttention(\n",
    "            key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "        )(x, x)\n",
    "        x = keras.layers.Add()([x, inputs])\n",
    "\n",
    "        # Feed Forward Part\n",
    "        y = keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "        y = keras.layers.Dense(ff_dim, activation=\"relu\")(y)\n",
    "        y = keras.layers.Dropout(dropout)(y)\n",
    "        y = keras.layers.Dense(inputs.shape[-1])(y)\n",
    "        inputs = keras.layers.Add()([y, x])\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "# Define the input layer\n",
    "inputs = keras.layers.Input(\n",
    "    shape=(maxlen,)\n",
    ")  # maxlen is the input length for each sequence\n",
    "\n",
    "# Embedding layer\n",
    "x = keras.layers.Embedding(max_features, 128)(\n",
    "    inputs\n",
    ")  # max_features is the vocabulary size\n",
    "x = transformer_encoder(x, num_layers, head_size, num_heads, ff_dim)\n",
    "x = keras.layers.GlobalAveragePooling1D()(x)\n",
    "x = keras.layers.LayerNormalization(epsilon=1e-6)(x)\n",
    "# Define output layer based on target type\n",
    "outputs = keras.layers.Dense(1, activation=\"linear\")(x)\n",
    "\n",
    "# Build the model\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name=\"transformer\")\n",
    "\n",
    "model.summary()\n",
    "# try using different optimizers and different optimizer configs\n",
    "model.compile(\"adam\", \"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "\n",
    "print(\"Train...\")\n",
    "batch_size = 32\n",
    "epoch = 100\n",
    "model.fit(\n",
    "    x_train, y_train, batch_size=batch_size, epochs=1, validation_data=[x_test, y_test]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
