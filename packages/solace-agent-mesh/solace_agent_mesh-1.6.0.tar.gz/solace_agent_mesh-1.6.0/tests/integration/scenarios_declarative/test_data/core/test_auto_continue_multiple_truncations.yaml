test_case_id: "auto_continue_multiple_truncations_001"
description: "Tests that the agent can handle multiple back-to-back truncations."
tags: ["all","core_a2a"]
skip_intermediate_events: true

gateway_input:
  target_agent_name: "TestAgent"
  user_identity: "auto_continue_tester@example.com"
  parts: [{type: "text", text: "Tell me a very long story about a robot."}]

llm_interactions:
  # 1. First truncation
  - static_response:
      choices:
        - message: {role: "assistant", content: "Part 1 of the story is quite long and it goes on and on until"}
          finish_reason: "length"
      usage: {prompt_tokens: 10, completion_tokens: 50, total_tokens: 60}

  # 2. Second truncation
  - static_response:
      choices:
        - message: {role: "assistant", content: " it reaches the second part which is also very long and continues until"}
          finish_reason: "length"
      usage: {prompt_tokens: 70, completion_tokens: 50, total_tokens: 120}
    expected_request:
      expected_tool_responses_in_llm_messages:
        - tool_name: "_continue_generation"
          response_contains: "You were interrupted"

  # 3. Final part
  - static_response:
      choices:
        - message: {role: "assistant", content: " the final conclusion where the robot saves the day."}
          finish_reason: "stop"
      usage: {prompt_tokens: 130, completion_tokens: 30, total_tokens: 160}
    expected_request:
      expected_tool_responses_in_llm_messages:
        - tool_name: "_continue_generation"
          response_contains: "You were interrupted"

expected_gateway_output:
  - type: "final_response"
    kind: "task"
    id: "*"
    contextId: "*"
    status:
      state: "completed"
      message:
        kind: "message"
        messageId: "*"
        role: "agent"
        parts:
          - kind: "text"
            text_contains:
              - "Part 1 of the story is quite long and it goes on and on until it reaches the second part which is also very long and continues until the final conclusion where the robot saves the day."
