flowcept_version: 0.9.17 # Version of the Flowcept package. This setting file is compatible with this version.

project:
  debug: true # Toggle debug mode. This will add a property `debug: true` to all saved data, making it easier to retrieve/delete them later.
  json_serializer: default # JSON serialization mode: default or complex. If "complex", Flowcept will deal with complex python dicts that may contain JSON unserializable values
  replace_non_json_serializable: true # Replace values that can't be JSON serialized
  performance_logging: false # Enable performance logging if true. Particularly useful for MQ flushes.
  enrich_messages: true # Add extra metadata to task messages, such as IP addresses of the node that executed the task, UTC timestamps, GitHub repo metadata.
  db_flush_mode: online # Mode for flushing DB entries: "online" or "offline". If online, flushes to the DB will happen before the workflow ends.
  dump_buffer: # This is particularly useful if you need to run completely offline. If you omit this, even offline, buffer data will not be persisted.
    enabled: false
    path: flowcept_buffer.jsonl

log:
  log_path: "default" # Path for log file output; "default" will write the log in the directory where the main executable is running from.
  log_file_level: error # Logging level (error, debug, info, critical) for file logs; use "disable" to turn off.
  log_stream_level: error # Logging level (error, debug, info, critical) for console/stream logs; use "disable" to turn off.

telemetry_capture: # This toggles each individual type of telemetry capture. GPU capture is treated different depending on the vendor (AMD or NVIDIA).
  gpu: ~ # ~ means None. This is a list with GPU metrics. AMD=[activity,used,power,temperature,others,id]; NVIDIA=[used,temperature,power,name,id]
  cpu: true
  per_cpu: true
  process_info: true
  mem: true
  disk: true
  network: true
  machine_info: true

instrumentation:
  enabled: true # This toggles data capture for instrumentation.
  torch:
    what: parent_and_children # Scope of instrumentation: "parent_only" -- will capture only at the main model level, "parent_and_children" -- will capture the inner layers, or ~ (disable).
    children_mode: telemetry_and_tensor_inspection   # What to capture if parent_and_children is chosen in the scope. Possible values: "tensor_inspection" (i.e., tensor metadata), "telemetry", "telemetry_and_tensor_inspection"
    epoch_loop: lightweight # lightweight, ~ (disable), or default (default will use the default telemetry capture method)
    batch_loop: lightweight # lightweight, ~ (disable), or default (default will use the default telemetry capture method)
    capture_epochs_at_every: 1 # Will capture data at every N epochs; please use a value that is multiple of the total number of #epochs.
    register_workflow: true # Will store the parent model forward as a workflow itself in the database.

experiment:
  user: root  # Optionally identify the user running the experiment. The logged username will be captured anyways.

mq:
  type: redis  # or kafka or mofka; Please adjust the port (kafka's default is 9092; redis is 6379). If mofka, adjust the group_file.
  host: localhost
  # uri: ?
  # instances: ["localhost:6379"] # We can have multiple MQ instances being accessed by the consumers but each interceptor will currently access one single MQ..
  port: 6379
  # group_file: mofka.json
  channel: interception
  buffer_size: 50
  insertion_buffer_time_secs: 5
  timing: false
  # uri: use Redis connection uri here
  chunk_size: -1  # use 0 or -1 to disable this. Or simply omit this from the config file.
  same_as_kvdb: false # Set this to true if you are using the same Redis instance both as an MQ and as the KV_DB. In that case, no need to repeat connection parameters in MQ. Use only what you define in KV_DB.
#  bin: /usr/local/bin/redis-server # Use this if you want to start redis using the flowcept-cli.
#  conf_file: /etc/redis/redis.conf

kv_db:
  host: localhost
  port: 6379
  enabled: true
  # uri: use Redis connection uri here

web_server:
  host: 0.0.0.0
  port: 5000

sys_metadata:
  environment_id: "laptop"   # We use this to keep track of the environment used to run an experiment. Typical values include the cluster name, but it can be anything that you think will help identify your experimentation environment.

extra_metadata: # We use this to store any extra metadata you want to keep track of during an experiment.
  place_holder: ""

analytics:
  sort_orders:
    generated.loss: minimum_first
    generated.accuracy: maximum_first

db_buffer:
  insertion_buffer_time_secs: 5   # Time interval (in seconds) to buffer incoming records before flushing to the database
  buffer_size: 50    # Maximum number of records to hold in the buffer before forcing a flush
  remove_empty_fields: false    # If true, fields with null/empty values will be removed before insertion
  stop_max_trials: 300    # Maximum number of trials before giving up when waiting for a fully safe stop (i.e., all records have been inserted as expected).
  stop_trials_sleep: 0.1  # Sleep duration (in seconds) between trials when waiting for a fully safe stop.

agent:
  enabled: false
  mcp_host: localhost
  mcp_port: 8000
  llm_server_url: '?'
  api_key: '?'
  model: '?'
  service_provider: '?'
  model_kwargs: {}
  audio_enabled: false

databases:

  lmdb:
    enabled: true
    path: flowcept_lmdb

  mongodb:
    enabled: true
    host: localhost
    port: 27017
    db: flowcept
    create_collection_index: true  # Whether flowcept should create collection indices if they haven't been created yet. This is done only at the Flowcept start up.
#    bin: /usr/bin/mongod
#    db_path:
#    log_path: /var/log/mongodb/mongod.log
#    lock_file_path: /var/run/mongod.pid


adapters:
  # For each key below, you can have multiple instances. Like mlflow1, mlflow2; zambeze1, zambeze2. Use an empty dict, {}, if you won't use any adapter.

  broker_mqtt:
    kind: broker
    host: localhost
    port: 30011
    protocol: mqtt3.1.1
    queues: ["#"]
    username: postman
    password: p
    qos: 2
    task_subtype: intersect_msg
    tracked_keys:
      used: payload
      generated: ~
      custom_metadata: [headers, msgId]
      activity_id: operationId
      submitted_at: ~
      started_at: ~
      ended_at: ~
      registered_at: ~

  mlflow:
    kind: mlflow
    file_path: mlflow.db
    log_params: ['*']
    log_metrics: ['*']
    watch_interval_sec: 2

  tensorboard:
    kind: tensorboard
    file_path: tensorboard_events
    log_tags: ['scalars', 'hparams', 'tensors']
    log_metrics: ['accuracy']
    watch_interval_sec: 5

  dask:
    kind: dask
    worker_should_get_input: true
    scheduler_should_get_input: true
    worker_should_get_output: true
    scheduler_create_timestamps: true
    worker_create_timestamps: false

