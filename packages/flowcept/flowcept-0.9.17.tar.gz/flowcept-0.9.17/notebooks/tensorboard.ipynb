{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf1fc999",
   "metadata": {},
   "source": [
    "# Demo for Tensorboard FlowCept Adapter\n",
    "\n",
    "Please note that this Notebook demonstrates the linkage between a previous workflow run with Dask and this Tensorflow training script. \n",
    "Thus, this demo expects that you have exected the `dask.ipynb` Notebook previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd68f0d-c6f3-40cb-9674-2b82f32758b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip install flowcept[tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b29990bd-0526-4186-9b40-6399e2b28c78",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sleeps are used because these notebooks are being tested automatically as part of the CI/CD. \n",
    "# In a normal user interaction, these sleeps would not be necessary.\n",
    "from time import sleep\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from uuid import uuid4\n",
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e128d2b0-c23a-4428-b142-0cd13b6eaced",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def run_tensorboard_hparam_tuning(tensorboard_events_dir, epochs=2):\n",
    "    # Code based on: https://www.tensorflow.org/tensorboard/hyperparameter_tuning_with_hparams\n",
    "    import tensorflow as tf\n",
    "    from tensorboard.plugins.hparams import api as hp\n",
    "\n",
    "    fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "\n",
    "    (x_train, y_train), (x_test, y_test) = fashion_mnist.load_data()\n",
    "    x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "    # Reduce the dataset size for faster debugging\n",
    "    DEBUG_SAMPLES_TRAIN = 100  # Number of training samples to keep\n",
    "    DEBUG_SAMPLES_TEST = 20  # Number of test samples to keep\n",
    "    \n",
    "    x_train, y_train = x_train[:DEBUG_SAMPLES_TRAIN], y_train[:DEBUG_SAMPLES_TRAIN]\n",
    "    x_test, y_test = x_test[:DEBUG_SAMPLES_TEST], y_test[:DEBUG_SAMPLES_TEST]\n",
    "\n",
    "    HP_NUM_UNITS = hp.HParam(\"num_units\", hp.Discrete([16, 32]))\n",
    "    HP_DROPOUT = hp.HParam(\"dropout\", hp.RealInterval(0.1, 0.2))\n",
    "    HP_OPTIMIZER = hp.HParam(\"optimizer\", hp.Discrete([\"adam\", \"sgd\"]))\n",
    "    HP_BATCHSIZES = hp.HParam(\"batch_size\", hp.Discrete([32, 64]))\n",
    "\n",
    "    HP_MODEL_CONFIG = hp.HParam(\"model_config\")\n",
    "    HP_OPTIMIZER_CONFIG = hp.HParam(\"optimizer_config\")\n",
    "\n",
    "    METRIC_ACCURACY = \"accuracy\"\n",
    "\n",
    "    with tf.summary.create_file_writer(tensorboard_events_dir).as_default():\n",
    "        hp.hparams_config(\n",
    "            hparams=[\n",
    "                HP_NUM_UNITS,\n",
    "                HP_DROPOUT,\n",
    "                HP_OPTIMIZER,\n",
    "                HP_BATCHSIZES,\n",
    "                HP_MODEL_CONFIG,\n",
    "                HP_OPTIMIZER_CONFIG,\n",
    "            ],\n",
    "            metrics=[hp.Metric(METRIC_ACCURACY, display_name=\"Accuracy\")],\n",
    "        )\n",
    "\n",
    "    def train_test_model(hparams, tensorboard_events_dir):\n",
    "        model = tf.keras.models.Sequential(\n",
    "            [\n",
    "                tf.keras.layers.Flatten(),\n",
    "                tf.keras.layers.Dense(\n",
    "                    hparams[HP_NUM_UNITS], activation=tf.nn.relu\n",
    "                ),\n",
    "                tf.keras.layers.Dropout(hparams[HP_DROPOUT]),\n",
    "                tf.keras.layers.Dense(10, activation=tf.nn.softmax),\n",
    "            ]\n",
    "        )\n",
    "        model.compile(\n",
    "            optimizer=hparams[HP_OPTIMIZER],\n",
    "            loss=\"sparse_categorical_crossentropy\",\n",
    "            metrics=[\"accuracy\"],\n",
    "        )\n",
    "\n",
    "        model.fit(\n",
    "            x_train,\n",
    "            y_train,\n",
    "            epochs=epochs,\n",
    "            callbacks=[\n",
    "                tf.keras.callbacks.TensorBoard(tensorboard_events_dir),\n",
    "                # log metrics\n",
    "                hp.KerasCallback(tensorboard_events_dir, hparams),  # log hparams\n",
    "            ],\n",
    "            batch_size=hparams[HP_BATCHSIZES],\n",
    "        )  # Run with 1 epoch to speed things up for tests\n",
    "        _, accuracy = model.evaluate(x_test, y_test)\n",
    "        return accuracy\n",
    "\n",
    "    def run(run_dir, hparams):\n",
    "        with tf.summary.create_file_writer(run_dir).as_default():\n",
    "            hp.hparams(hparams)  # record the values used in this trial\n",
    "            accuracy = train_test_model(hparams, tensorboard_events_dir)\n",
    "            tf.summary.scalar(METRIC_ACCURACY, accuracy, step=1)\n",
    "\n",
    "    session_num = 0\n",
    "\n",
    "    for num_units in HP_NUM_UNITS.domain.values:\n",
    "        for dropout_rate in (\n",
    "            HP_DROPOUT.domain.min_value,\n",
    "            HP_DROPOUT.domain.max_value,\n",
    "        ):\n",
    "            for optimizer in HP_OPTIMIZER.domain.values:\n",
    "                for batch_size in HP_BATCHSIZES.domain.values:\n",
    "                    # These two added ids below are optional and useful\n",
    "                    # just to contextualize this run.\n",
    "                    hparams = {\n",
    "                        \"activity_id\": \"hyperparam_evaluation\",\n",
    "                        \"epochs\": epochs,\n",
    "                        HP_NUM_UNITS: num_units,\n",
    "                        HP_DROPOUT: dropout_rate,\n",
    "                        HP_OPTIMIZER: optimizer,\n",
    "                        HP_BATCHSIZES: batch_size,\n",
    "                    }\n",
    "                    run_name = f\"wf_id_{wf_id}_{session_num}\"\n",
    "                    print(\"--- Starting trial: %s\" % run_name)\n",
    "                    print(f\"{hparams}\")\n",
    "                    run(f\"{tensorboard_events_dir}/\" + run_name, hparams)\n",
    "                    session_num += 1\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9516c23a-66a5-4f0d-8a22-bad62b64793f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optionally set up env vars to control Flowcept's log level\n",
    "%env LOG_STREAM_LEVEL=\"error\"\n",
    "%env LOG_FILE_LEVEL=\"debug\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ea0e28-78ca-4846-9a91-ddc4481b57e9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Set the env var pointing to the conf file where the ports, hostnames, and other conf variables are read from.\n",
    "\n",
    "There is an exemplary conf file available in the `resources` directory in FlowCept repository. You can use it as is if running this Notebook on your local laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ff0914b-1e50-46f9-9e07-ab7a1d98ee1e",
   "metadata": {},
   "source": [
    "## Set up tensorboard events directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b3923b-4cf8-4ffb-a7c8-44583e9d8316",
   "metadata": {},
   "outputs": [],
   "source": [
    "from flowcept.configs import settings\n",
    "#tensorboard_events_dir = \"my_tb_dir\"\n",
    "tensorboard_events_dir = settings[\"adapters\"][\"tensorboard\"][\"file_path\"] # For convenience for these tests, we're getting the file path from the yaml settings file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfa081e-8a69-4f5e-bb4d-773bcf0d7793",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Optional: Delete old tensorboard directories\n",
    "if os.path.exists(tensorboard_events_dir):\n",
    "    shutil.rmtree(tensorboard_events_dir)\n",
    "    sleep(0.5)\n",
    "    os.mkdir(tensorboard_events_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "649bc15f-21fa-41f7-b265-8613a48fbe50",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get training parameters from previous Dask workflow run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7823ecc6-5ab9-46a0-8850-6d1ce6161b7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from flowcept import TaskQueryAPI\n",
    "from flowcept.commons.utils import get_utc_minutes_ago\n",
    "query_api = TaskQueryAPI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010599b2-e040-4d2d-ac07-88e5b9aa9133",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_filter = {\n",
    "    \"utc_timestamp\": { \"$gte\" : get_utc_minutes_ago(60) },\n",
    "    \"generated.epochs\": { \"$gte\" : 0 }\n",
    "}\n",
    "docs = query_api.query(filter=_filter)\n",
    "epochs_params = set()\n",
    "for doc in docs:\n",
    "    print(f\"task={doc['task_id']}, generated epochs={doc['generated']['epochs']}\")\n",
    "    epochs_params.add(doc['generated']['epochs'])\n",
    "epochs_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48eb62a2-f9af-45a1-8291-e3c25b6a01a0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Initialize Tensorboard's interceptor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732755c0-8b6f-4788-be96-d26fecc3c811",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from flowcept import Flowcept\n",
    "flowcept = Flowcept(\"tensorboard\")\n",
    "flowcept.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "433fff30-7fd6-4d72-823a-239e2dd8deaa",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Now start a train using the `epochs_params` generated by the Dask workflow.\n",
    "\n",
    "This example assumes that you have run the Dask notebook example before. If you haven't run it, `epochs_params` will be empty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a4adc3-55c0-4d1e-bb83-a8b48351481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_params = epochs_params if len(epochs_params) else {1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7c6108-2d07-49a9-a8dd-c7061a4d70ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for epochs in epochs_params:\n",
    "    workflow_id = run_tensorboard_hparam_tuning(tensorboard_events_dir)\n",
    "    print(f\"{epochs}, {workflow_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300474cd-77e9-4728-8b81-77cae4c3db7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sleep(10)\n",
    "flowcept.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41546ea7-d7ac-40e1-ad74-ea637ad686b5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Get the training metadata stored from this workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68d24f1-febc-4417-85d4-c35ee52709f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "_filter = {\n",
    "    \"workflow_id\": workflow_id\n",
    "}\n",
    "docs = query_api.query(filter=_filter)\n",
    "docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
