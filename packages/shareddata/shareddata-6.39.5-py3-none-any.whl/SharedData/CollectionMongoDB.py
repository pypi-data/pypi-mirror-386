import pandas as pd
import numpy as np
import datetime
import math
import bson
from bson import ObjectId
import hashlib
import json
import asyncio

from SharedData.Database import *
from SharedData.Utils import datetype
from pymongo import ASCENDING, DESCENDING,UpdateOne
from SharedData.Logger import Logger
from SharedData.IO.MongoDBClient import MongoDBClient

class CollectionMongoDB:

    # TODO: create partitioning option yearly, monthly, daily
    """
    '''
    A class to manage MongoDB collections with support for upsert and append operations,
    automatic index handling, and data serialization/deserialization between MongoDB documents
    and pandas DataFrames.
    
    Attributes:
        type (int): Storage type of the collection (1: DISK, 2: MEMORY).
        shareddata: Shared data object containing MongoDB client.
        user (str): User namespace for MongoDB client.
        database (str): Database name.
        period (str): Period identifier (e.g., 'D1', 'M15', 'M1') used for date normalization.
        source (str): Source identifier.
        tablename (str): Collection name.
        subscription_thread: Placeholder for subscription thread (not implemented).
        publish_thread: Placeholder for publish thread (not implemented).
        names (list): Optional list of field names.
        formats (list): Optional list of field formats.
        size (int): Optional size parameter.
        hasindex (bool): Whether the collection has an index on primary keys.
        overwrite (bool): Whether to overwrite existing data.
        partitioning: Partitioning option (currently not implemented).
        _collection: The underlying pymongo collection object.
        mongodb: MongoDB shared data interface.
        mongodb_client: MongoDB
    """
    def __init__(self, shareddata, database, period, source, tablename,
                 hasindex=True,create_if_not_exists = True,overwrite=False, 
                 user='master', partitioning=None):
        # tabletype 1: DISK, 2: MEMORY
        """
        Initialize a MongoDB collection handler with specified configuration.
        
        Parameters:
            shareddata (object): Shared resources container, including MongoDB client.
            database (str): Name of the database.
            period (str): Time period identifier for the data.
            source (str): Source identifier for the data.
            tablename (str): Name of the collection.
            hasindex (bool, optional): Whether to create and maintain indexes on the collection. Defaults to True.
            create_if_not_exists (bool, optional): Whether to create the collection if it does not exist. Defaults to True.
            overwrite (bool, optional): Whether to overwrite existing collection data. Defaults to False.
            user (str, optional): MongoDB user or namespace. Defaults to 'master'.
            partitioning (optional): Partitioning scheme or parameter. Defaults to None.
        
        Raises:
            Exception: If the collection does not exist and create_if_not_exists is False.
        
        This constructor sets up the MongoDB collection, creates it if necessary, and manages indexes based on primary key columns and modification time.
        """

        self.shareddata = shareddata
        self.user = user
        self.database = database
        self.period = period
        self.source = source
        self.tablename = tablename
        self.subscription_thread = None
        self.publish_thread = None
        
        self.hasindex = hasindex
        self.overwrite = overwrite
        self.partitioning = partitioning
                
        self._collection = None

        self.mongodb = MongoDBClient(self.user)
        
        self.path = f'{user}/{database}/{period}/{source}/collection/{tablename}'
        self.relpath = f'{database}/{period}/{source}/collection/{tablename}'
        self.pkey_columns = DATABASE_PKEYS[self.database]
        self.exists = self.relpath in self.mongodb.client[self.user].list_collection_names()
        if (not self.exists) and (not create_if_not_exists):
            raise Exception(f'Collection {self.relpath} does not exist')
        
        if not self.exists:
            if self.overwrite:
                self.mongodb.client[self.user].drop_collection(self.relpath)
            # Create collection
            self.mongodb.client[self.user].create_collection(self.relpath)
            self._collection = self.mongodb.client[self.user][self.relpath]
            if self.hasindex:
                pkey_fields = [(f"{field}", ASCENDING) for field in self.pkey_columns]
                pkey_name = '_'.join(f"{field}_1" for field in self.pkey_columns)
                self.mongodb.client[self.user][self.relpath].create_index(pkey_fields, unique=True, name=pkey_name)
                # Create index on mtime for timestamp queries
                self.mongodb.client[self.user][self.relpath].create_index([("mtime", DESCENDING)])
        else:
            self._collection = self.mongodb.client[self.user][self.relpath]
            pkey_fields = [(f"{field}", ASCENDING) for field in self.pkey_columns]
            pkey_name = '_'.join(f"{field}_1" for field in self.pkey_columns)            
            # Get collection indexes information
            index_info = self._collection.index_information()
            # Check for indexes other than the default _id index
            self.hasindex = any(index_name == pkey_name for index_name in index_info)
        
    @property
    def collection(self):
        """
        Property that returns the value of the private attribute `_collection`.
        """
        return self._collection    

    def upsert(self, data):
        """
        Perform upsert operations on the collection, handling single or multiple documents.
        
        This method inserts new documents or updates existing ones based on the primary key columns.
        It supports input as a dictionary, a list of dictionaries, or a pandas DataFrame. Documents missing
        any primary key fields are skipped and logged as errors.
        
        Key features:
        - Validates presence of primary key fields in each document.
        - Removes MongoDB restricted fields like '_id'.
        - Adds or updates the modification time ('mtime') to the current UTC timestamp if not present.
        - Normalizes or floors 'date' fields according to the collection's period setting ('D1', 'M15', 'M1').
        - Cleans field names by removing dots and replacing empty strings with spaces.
        - Performs bulk upsert operations using MongoDB's UpdateOne with upsert=True.
        
        Parameters:
            data (dict, list of dict, or pandas.DataFrame): Document(s) to upsert into the collection.
        
        Returns:
            pymongo.results.BulkWriteResult or list: Result of the bulk write operation if performed,
            otherwise an empty list.
        
        Raises:
            ValueError: If the collection does not have an index, upsert is not supported.
        """
        if not self.hasindex:
            raise ValueError("Upsert operation is not supported for collections without index.")
        
        # If data is a DataFrame, serialize it into a list of dictionaries
        if isinstance(data, pd.DataFrame):
            data = self.df2documents(data)
        # If data is a dictionary, convert it into a list so both cases are handled uniformly
        if isinstance(data, dict):
            data = [data]

        operations = []
        missing_pkey_items = []
        for item in data:
            # Check if the item contains all primary key columns
            if not all(field in item for field in self.pkey_columns):
                missing_pkey_items.append(item)
                continue  # Skip this item if it doesn't contain all primary key columns
            
            # Remove '_id' field if present
            if '_id' in item:
                del item['_id']

            # Add modification time if not present
            if 'mtime' not in item:
                item['mtime'] = pd.Timestamp.utcnow()

            # Check if date needs to be floored to specific intervals
            if 'date' in item:
                if self.period == 'D1':
                    item = item.copy()
                    item['date'] = pd.Timestamp(item['date']).normalize()
                elif self.period == 'M15':
                    item = item.copy()
                    item['date'] = pd.Timestamp(item['date']).floor('15T')                
                elif self.period == 'M1':
                    item = item.copy()
                    item['date'] = pd.Timestamp(item['date']).floor('T')

            # MongoDB does not allow '.' in field names, so replace them with spaces
            item = {k.replace('.', ''): v for k, v in item.items()}
                
            # MongoDB does not allow '' field names, change to ' ' if name == ''
            item = {k if k != '' else ' ': v for k, v in item.items()}

            # Construct the filter condition using the primary key columns
            filter_condition = {field: item[field] for field in self.pkey_columns if field in item}
            
            # Prepare the update operation
            update_data = {'$set': item}

            # Add the upsert operation to the operations list
            operations.append(UpdateOne(filter_condition, update_data, upsert=True))
        
        # Execute all operations in bulk if more than one, otherwise perform single update
        result = []
        if len(operations) > 0:
            # Use ordered=False for parallel execution and better performance
            # PROS: 10-100x faster, parallel execution, better resource utilization
            # CONS: Operations may complete out of order, partial failures more complex
            # Safe for upserts where operations are independent (unique date+symbol pairs)
            try:
                result = self._collection.bulk_write(operations, ordered=False)
                
                # Check for write errors (even with ordered=False, some ops may succeed)
                if hasattr(result, 'bulk_api_result') and result.bulk_api_result.get('writeErrors'):
                    error_count = len(result.bulk_api_result['writeErrors'])
                    Logger.log.warning(f"upsert:{self.relpath} {error_count}/{len(operations)} operations had errors (check for duplicates or constraint violations)")
                    
            except Exception as e:
                Logger.log.error(f"upsert:{self.relpath} bulk_write failed: {e}")
                raise

        if len(missing_pkey_items) > 0:
            Logger.log.error(f"upsert:{self.relpath} {len(missing_pkey_items)}/{len(data)} missing pkey!")
        
        return result
    
    def extend(self, data):
        """
        Appends one or more documents to the collection after preprocessing.
        
        Parameters:
            data (dict, list of dict, or pd.DataFrame): A single document as a dictionary,
                multiple documents as a list of dictionaries, or a DataFrame to be serialized
                into documents.
        
        Raises:
            ValueError: If the collection has an index, as extend operation is not supported.
        
        Behavior:
            - Converts a DataFrame input into a list of dictionaries.
            - Ensures input is a list of documents.
            - Removes the '_id' field from each document if present.
            - Adds a modification timestamp ('mtime') if missing.
            - Normalizes or floors the 'date' field based on the collection's period attribute:
                - 'D1': normalize to midnight.
                - 'M15': floor to nearest 15 minutes.
                - 'M1': floor to nearest minute.
            - Inserts all processed documents into the underlying collection.
        
        Returns:
            The result of the insert_many operation, or an empty list if no documents were inserted.
        """
        if self.hasindex:
            raise ValueError("Extend operation is not supported for collections with index.")
        
        # Check if data is a DataFrame and serialize it into a list of dictionaries
        if isinstance(data, pd.DataFrame):
            data = self.serialize(data)
        # Convert a single dictionary into a list for uniform handling
        if isinstance(data, dict):
            data = [data]

        documents_to_insert = []
        for item in data:
            # Remove '_id' field if present
            if '_id' in item:
                del item['_id']
                
            # Add modification time if not present
            if 'mtime' not in item:
                item['mtime'] = pd.Timestamp.utcnow()
            
            # Check and adjust date if necessary
            if 'date' in item:
                if self.period == 'D1':
                    item = item.copy()
                    item['date'] = pd.Timestamp(item['date']).normalize()
                elif self.period == 'M15':
                    item = item.copy()
                    item['date'] = pd.Timestamp(item['date']).floor('15T')
                elif self.period == 'M1':
                    item = item.copy()
                    item['date'] = pd.Timestamp(item['date']).floor('T')

            documents_to_insert.append(item)

        # Insert all prepared documents into the collection        
        result = []
        if documents_to_insert:
            result = self._collection.insert_many(documents_to_insert)
        
        return result        
    
    def find(self, query, projection=None, sort=None, limit=None, skip=None):
        """
        Retrieve documents from the collection that match the given query criteria.
        
        Args:
            query (dict): A dictionary specifying the filter conditions for the documents.
            projection (dict, optional): A dictionary specifying the fields to include or exclude in the returned documents.
            sort (list, optional): A list of tuples specifying the fields and directions to sort the results by.
            limit (int, optional): The maximum number of documents to return.
            skip (int, optional): The number of documents to skip before returning results.
        
        Returns:
            list: A list of documents matching the query, optionally filtered, sorted, skipped, and limited.
        """
        if projection:
            cursor = self._collection.find(query, projection)
        else:
            cursor = self._collection.find(query)
        
        if sort:
            cursor = cursor.sort(sort)
        elif self.hasindex:
            # If no sort is specified, sort by the primary key(s) to ensure consistent order.
            sort = [(pkey, 1) for pkey in DATABASE_PKEYS[self.database]]
            cursor = cursor.sort(sort)
        elif not self.hasindex:
            # If no sort is specified, sort by the primary key(s) to ensure consistent order.
            sort = {'_id': 1}
            cursor = cursor.sort(sort)

        if skip:
            cursor = cursor.skip(skip)
        
        if limit:
            cursor = cursor.limit(limit)
        return list(cursor)
    
    def delete(self, query):
        """
        Delete documents from the collection that match the specified query.
        
        Args:
            query (dict): A dictionary representing the filter criteria for documents to delete.
        
        Returns:
            int: The count of documents that were deleted.
        """
        result = self._collection.delete_many(query)
        return result.deleted_count
    
    @staticmethod
    def serialize(obj, iso_dates=False):
        """
        Recursively serialize a Python object into a nested dictionary or list structure,
        removing any "empty" values as defined by the is_empty() method.
        
        Supports special handling for pandas Timestamps, datetime objects, dictionaries,
        pandas DataFrames, lists, tuples, sets, objects with __dict__, and ObjectId instances.
        
        Parameters:
            obj (any): The Python object to serialize.
            iso_dates (bool): If True, datetime and Timestamp objects are converted to ISO 8601 strings.
        
        Returns:
            dict, list, str, or None: A serialized representation of the input object with empty values removed,
            or None if the entire structure is empty.
        """

        # 1) Special-case Timestamps so they don't get recursed:
        if isinstance(obj, pd.Timestamp):
            # Return None if it's considered 'empty' (e.g. NaT),
            # otherwise treat it as a scalar (string, raw Timestamps, etc.)
            if iso_dates:
                return None if CollectionMongoDB.is_empty(obj) else obj.isoformat()
            else:
                return None if CollectionMongoDB.is_empty(obj) else obj

        # # Handle Python datetime.datetime objects
        if isinstance(obj, datetime.datetime):
            if iso_dates:        
                return None if CollectionMongoDB.is_empty(obj) else obj.isoformat()
            else:
                return None if CollectionMongoDB.is_empty(obj) else obj
        
        # 2) Dict
        if isinstance(obj, dict):
            new_dict = {}
            for k, v in obj.items():
                # Recurse
                serialized_v = CollectionMongoDB.serialize(v, iso_dates)
                # Only keep non-empty values
                if serialized_v is not None and not CollectionMongoDB.is_empty(serialized_v):
                    new_dict[k] = serialized_v

            # If the resulting dict is empty, return None instead of {}
            return new_dict if new_dict else None

        # 3) DataFrame
        if isinstance(obj, pd.DataFrame):
            records = obj.to_dict(orient='records')
            # Each record is a dict, so we re-serialize it
            return [
                r for r in (CollectionMongoDB.serialize(rec, iso_dates) for rec in records)
                if r is not None and not CollectionMongoDB.is_empty(r)
            ]

        # 4) List/tuple/set
        if isinstance(obj, (list, tuple, set)):
            new_list = [
                CollectionMongoDB.serialize(item, iso_dates)
                for item in obj
                if not CollectionMongoDB.is_empty(item)
            ]
            # If the list ends up empty, return None
            return new_list if new_list else None

        # 5) For other objects with __dict__, treat them like a dict
        if hasattr(obj, "__dict__"):
            return CollectionMongoDB.serialize(vars(obj), iso_dates)
        
        # 6) Convert ObjectId to string for JSON serialization
        if isinstance(obj, ObjectId):
            return str(obj)

        # 7) Otherwise, just return the raw value if it's not "empty"
        return obj if not CollectionMongoDB.is_empty(obj) else None

    EMPTY_VALUES = {
        str: ["", "1.7976931348623157E308", "nan", "NaN",],
        int: [2147483647],
        float: [1.7976931348623157e+308, np.nan, np.inf, -np.inf],
        datetime.datetime: [datetime.datetime(1, 1, 1, 0, 0)],
        # pd.Timestamp: [pd.Timestamp("1970-01-01 00:00:00")],
        pd.NaT: [pd.NaT],
        pd.Timedelta: [pd.Timedelta(0)],
        pd.Interval: [pd.Interval(0, 0)],
        type(None): [None],
        bool: [False],
    }
    
    @staticmethod
    def is_empty(value):
        """
        Determine if the given value should be considered empty or a sentinel value.
        
        This method checks for various conditions that signify emptiness, including:
        - Floating point NaN, infinity, zero, and maximum float values.
        - Pandas Timestamp objects that are NaT (Not a Time).
        - Values contained in predefined empty sets specific to their type.
        - Empty containers such as lists, tuples, sets, and dictionaries.
        - Pandas NaTType instances.
        
        Parameters:
            value (any): The value to check for emptiness.
        
        Returns:
            bool: True if the value is considered empty or a sentinel, False otherwise.
        """
        # Special handling for floats
        if isinstance(value, float):
            if math.isnan(value) or math.isinf(value):
                return True
            if value == 1.7976931348623157e+308:
                return True

        # If it's a Timestamp and is NaT, treat as empty
        if isinstance(value, pd.Timestamp):
            if pd.isna(value):  # True for pd.NaT
                return True

        # Check if value is in our known empty sets
        value_type = type(value)
        if value_type in CollectionMongoDB.EMPTY_VALUES:
            if value in CollectionMongoDB.EMPTY_VALUES[value_type]:
                return True

        # Empty containers
        if isinstance(value, (list, tuple, set)) and len(value) == 0:
            return True
        if isinstance(value, dict) and len(value) == 0:
            return True
        if isinstance(value, pd._libs.tslibs.nattype.NaTType):
            return True

        return False

    def flatten_dict(self, d, parent_key='', sep='->'):
        """
        Recursively flattens a nested dictionary by concatenating keys with a specified separator.
        
        Parameters:
            d (dict): The dictionary to flatten.
            parent_key (str, optional): The base key string to prepend to keys during recursion. Defaults to ''.
            sep (str, optional): The separator string used between concatenated keys. Defaults to '->'.
        
        Returns:
            dict: A new dictionary with nested keys flattened into single-level keys joined by the separator.
        """
        items = []
        for k, v in d.items():
            new_key = f"{parent_key}{sep}{k}" if parent_key else k
            if isinstance(v, dict):
                items.extend(self.flatten_dict(v, new_key, sep=sep).items())
            else:
                items.append((new_key, v))
        return dict(items)

    def documents2df(self, documents, flatten=True, drop_empty=True, serialize=True):
        """
        Convert a list of document dictionaries into a pandas DataFrame.
        
        Parameters:
            documents (list): A list of dictionaries representing documents.
            flatten (bool): If True, flatten nested dictionaries within each document. Default is True.
            drop_empty (bool): If True, drop columns that contain only None values. Default is True.
            serialize (bool): If True, serialize ObjectId and datetime objects before processing. Default is True.
        
        Returns:
            pandas.DataFrame: A DataFrame representation of the documents, with optional flattening,
            serialization, and empty column removal. If primary key columns are present, they are set as the DataFrame index.
        """
        if len(documents) == 0:
            return pd.DataFrame()
        # Serialize ObjectId and datetime objects
        if serialize:
            documents = self.serialize(documents)
        # Flatten each document
        if flatten:
            documents = [self.flatten_dict(doc) for doc in documents]
        
        # Convert the list of dictionaries into a DataFrame 
        df = pd.DataFrame(documents)
        
        if drop_empty:
            # Remove columns with all None values
            df = df.dropna(axis=1, how='all')
        
        # Set primary key as index        
        pkey_columns = DATABASE_PKEYS[self.database]
        if all(col in df.columns for col in pkey_columns):
            df.set_index(pkey_columns, inplace=True)

        return df
    
    def df2documents(self, df, unflatten=True, drop_empty=True):
        """
        Convert a pandas DataFrame into a list of dictionary documents suitable for MongoDB insertion.
        
        Parameters:
            df (pandas.DataFrame): The DataFrame to convert.
            unflatten (bool, optional): If True, unflatten nested keys in the documents. Defaults to True.
            drop_empty (bool, optional): If True, drop rows and columns that are completely empty and remove empty fields from documents. Defaults to True.
        
        Returns:
            list of dict: A list of documents representing the DataFrame rows, with primary key validation, cleaned column names, and optional unflattening.
        
        Raises:
            ValueError: If the DataFrame's primary key columns do not match the expected primary key columns for the database.
        """
        if df.empty:
            return []
        # Retrieve the expected primary key columns for this database
        pkey_columns = DATABASE_PKEYS[self.database]
        # Convert index to columns
        df = df.reset_index()
        if len(df.columns) >= len(pkey_columns):
            for icol, col in enumerate(pkey_columns):
                if df.columns[icol]!=pkey_columns[icol]:
                    raise ValueError(f"df2documents:Expected primary key column {pkey_columns}!")


        # MongoDB does not allow '.' in field names, so replace them with spaces
        df.columns = [str(s).replace('.','') for s in df.columns]

        # Drop rows and columns with all None/NaN values
        if drop_empty:
            df = df.dropna(axis=0, how='all').dropna(axis=1, how='all')

        # Convert DataFrame to list of dictionaries
        documents = df.to_dict(orient='records')

        if drop_empty:
            # Remove empty fields in the documents
            for doc in documents:
                keys_to_remove = [key for key, value in doc.items() if CollectionMongoDB.is_empty(value)]
                for key in keys_to_remove:
                    del doc[key]

        # Unflatten the documents if needed
        if unflatten:
            documents = [self.unflatten_dict(doc) for doc in documents]

        return documents
    
    def unflatten_dict(self, d, sep='->'):
        """
        Convert a flat dictionary with compound keys into a nested dictionary.
        
        Each key in the input dictionary `d` is split by the specified separator `sep` to create a hierarchy of nested dictionaries. The values are assigned to the innermost keys.
        
        Parameters:
            d (dict): The flat dictionary with compound keys.
            sep (str): The separator string used to split keys into nested levels. Default is '->'.
        
        Returns:
            dict: A nested dictionary constructed from the flat dictionary.
        """
        result = {}
        for key, value in d.items():
            parts = key.split(sep)
            current = result
            for part in parts[:-1]:
                if part not in current:
                    current[part] = {}
                    current = current[part]

                else:
                    if not isinstance(current[part], dict):
                        current[part] = {}
                    current = current[part]
            current[parts[-1]] = value
        return result
            
    def documents2json(self, documents, serialize=True, drop_empty=True):
        """
        Convert a list of document dictionaries to a JSON string.
        
        This method optionally serializes special data types such as ObjectId and datetime
        to JSON-compatible formats and can remove keys with empty values from each document.
        
        Parameters:
            documents (list): A list of document dictionaries to be converted.
            serialize (bool): If True, serialize special types like ObjectId and datetime to JSON-compatible formats. Default is True.
            drop_empty (bool): If True, remove keys with empty values from each document before conversion. Default is True.
        
        Returns:
            str: A JSON-formatted string representing the list of documents.
        """
        if len(documents) == 0:
            return json.dumps([])
        # Serialize ObjectId and datetime objects
        if serialize:
            documents = self.serialize(documents, iso_dates=True)

        if drop_empty:
            # Remove empty fields in the documents
            for doc in documents:
                keys_to_remove = [key for key, value in doc.items() if CollectionMongoDB.is_empty(value)]
                for key in keys_to_remove:
                    del doc[key]

        return json.dumps(documents)

    def recursive_update(self, original, updates):
        """
        Recursively updates the original dictionary with values from the updates dictionary.
        
        For each key in updates:
        - If the corresponding value is a dictionary and the original dictionary has a dictionary at that key,
          the update is applied recursively.
        - Otherwise, the value from updates overwrites the value in the original dictionary.
        
        This method preserves keys in the original dictionary that are not present in updates.
        
        Args:
            original (dict): The dictionary to be updated.
            updates (dict): The dictionary containing updates.
        
        Returns:
            dict: The updated original dictionary.
        """
        for key, value in updates.items():
            if isinstance(value, dict):
                # Get existing nested dictionary or use an empty dict if not present
                original_value = original.get(key, {})
                if isinstance(original_value, dict):
                    # Merge recursively
                    original[key] = self.recursive_update(original_value, value)
                else:
                    # Directly assign if original is not a dict
                    original[key] = value
            else:
                # Non-dict values are directly overwritten
                original[key] = value
        return original
        
    @staticmethod
    def sort_dict(obj: object) -> object:
        """
        Recursively sort dictionaries by their keys and apply the sorting to nested dictionaries and lists.
        
        For a dictionary input, returns a new dictionary with keys sorted in ascending order, recursively sorting all nested dictionaries and lists.
        For a list input, recursively sorts each element.
        For other data types, returns the object unchanged.
        
        This function is useful for ensuring consistent ordering of dictionary keys, which can aid in hashing or comparing nested data structures.
        
        Args:
            obj (object): The input object to sort, which may be a dictionary, list, or any other data type.
        
        Returns:
            object: A new object with dictionaries sorted by keys recursively, or the original object if it is not a dictionary or list.
        """
        if isinstance(obj, dict):
            return {k: CollectionMongoDB.sort_dict(obj[k]) for k in sorted(obj)}
        elif isinstance(obj, list):
            return [CollectionMongoDB.sort_dict(item) for item in obj]
        else:
            return obj
            
    @staticmethod
    def get_hash(obj: dict) -> str:
        """
        Generate a SHA-256 hash for a given dictionary.
        
        This method sorts the dictionary to ensure consistent key order, encodes it into BSON format,
        and computes the SHA-256 hash of the encoded bytes.
        
        Args:
            obj (dict): The dictionary to hash.
        
        Returns:
            str: The hexadecimal SHA-256 hash string of the BSON-encoded sorted dictionary.
        """
        obj = CollectionMongoDB.sort_dict(obj)
        return hashlib.sha256(bson.BSON.encode(obj)).hexdigest()
    
    