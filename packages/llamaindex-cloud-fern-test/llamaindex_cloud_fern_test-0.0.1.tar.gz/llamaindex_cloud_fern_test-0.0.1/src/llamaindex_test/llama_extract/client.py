# This file was auto-generated by Fern from our API Definition.

import typing

from .. import core
from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.request_options import RequestOptions
from ..types.extract_agent import ExtractAgent
from ..types.extract_config import ExtractConfig
from ..types.extract_job import ExtractJob
from ..types.extract_resultset import ExtractResultset
from ..types.extract_run import ExtractRun
from ..types.extract_schema_generate_response import ExtractSchemaGenerateResponse
from ..types.extract_schema_validate_response import ExtractSchemaValidateResponse
from ..types.file_data import FileData
from ..types.paginated_extract_runs_response import PaginatedExtractRunsResponse
from ..types.webhook_configuration import WebhookConfiguration
from .raw_client import AsyncRawLlamaExtractClient, RawLlamaExtractClient
from .types.extract_agent_create_data_schema import ExtractAgentCreateDataSchema
from .types.extract_agent_update_data_schema import ExtractAgentUpdateDataSchema
from .types.extract_job_create_batch_data_schema_override import ExtractJobCreateBatchDataSchemaOverride
from .types.extract_job_create_data_schema_override import ExtractJobCreateDataSchemaOverride
from .types.extract_job_create_priority import ExtractJobCreatePriority
from .types.extract_schema_validate_request_data_schema import ExtractSchemaValidateRequestDataSchema
from .types.extract_stateless_request_data_schema import ExtractStatelessRequestDataSchema

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class LlamaExtractClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._raw_client = RawLlamaExtractClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> RawLlamaExtractClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        RawLlamaExtractClient
        """
        return self._raw_client

    def list_jobs(
        self, *, extraction_agent_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[ExtractJob]:
        """
        Parameters
        ----------
        extraction_agent_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[ExtractJob]
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.llama_extract.list_jobs(
            extraction_agent_id="extraction_agent_id",
        )
        """
        _response = self._raw_client.list_jobs(extraction_agent_id=extraction_agent_id, request_options=request_options)
        return _response.data

    def run_job(
        self,
        *,
        extraction_agent_id: str,
        file_id: str,
        from_ui: typing.Optional[bool] = None,
        priority: typing.Optional[ExtractJobCreatePriority] = OMIT,
        webhook_configurations: typing.Optional[typing.Sequence[WebhookConfiguration]] = OMIT,
        data_schema_override: typing.Optional[ExtractJobCreateDataSchemaOverride] = OMIT,
        config_override: typing.Optional[ExtractConfig] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractJob:
        """
        Parameters
        ----------
        extraction_agent_id : str
            The id of the extraction agent

        file_id : str
            The id of the file

        from_ui : typing.Optional[bool]

        priority : typing.Optional[ExtractJobCreatePriority]
            The priority for the request. This field may be ignored or overwritten depending on the organization tier.

        webhook_configurations : typing.Optional[typing.Sequence[WebhookConfiguration]]
            The outbound webhook configurations

        data_schema_override : typing.Optional[ExtractJobCreateDataSchemaOverride]
            The data schema to override the extraction agent's data schema with

        config_override : typing.Optional[ExtractConfig]
            The config to override the extraction agent's config with

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractJob
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.llama_extract.run_job(
            from_ui=True,
            extraction_agent_id="extraction_agent_id",
            file_id="file_id",
        )
        """
        _response = self._raw_client.run_job(
            extraction_agent_id=extraction_agent_id,
            file_id=file_id,
            from_ui=from_ui,
            priority=priority,
            webhook_configurations=webhook_configurations,
            data_schema_override=data_schema_override,
            config_override=config_override,
            request_options=request_options,
        )
        return _response.data

    def get_job(self, job_id: str, *, request_options: typing.Optional[RequestOptions] = None) -> ExtractJob:
        """
        Parameters
        ----------
        job_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractJob
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.llama_extract.get_job(
            job_id="job_id",
        )
        """
        _response = self._raw_client.get_job(job_id, request_options=request_options)
        return _response.data

    def run_job_on_file(
        self,
        *,
        extraction_agent_id: str,
        file: core.File,
        from_ui: typing.Optional[bool] = None,
        data_schema_override: typing.Optional[str] = OMIT,
        config_override: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractJob:
        """
        Parameters
        ----------
        extraction_agent_id : str
            The id of the extraction agent

        file : core.File
            See core.File for more documentation

        from_ui : typing.Optional[bool]

        data_schema_override : typing.Optional[str]
            The data schema to override the extraction agent's data schema with as a JSON string

        config_override : typing.Optional[str]
            The config to override the extraction agent's config with as a JSON string

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractJob
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.llama_extract.run_job_on_file(
            from_ui=True,
            extraction_agent_id="extraction_agent_id",
        )
        """
        _response = self._raw_client.run_job_on_file(
            extraction_agent_id=extraction_agent_id,
            file=file,
            from_ui=from_ui,
            data_schema_override=data_schema_override,
            config_override=config_override,
            request_options=request_options,
        )
        return _response.data

    def run_batch_jobs(
        self,
        *,
        extraction_agent_id: str,
        file_ids: typing.Sequence[str],
        from_ui: typing.Optional[bool] = None,
        webhook_configurations: typing.Optional[typing.Sequence[WebhookConfiguration]] = OMIT,
        data_schema_override: typing.Optional[ExtractJobCreateBatchDataSchemaOverride] = OMIT,
        config_override: typing.Optional[ExtractConfig] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[ExtractJob]:
        """
        Parameters
        ----------
        extraction_agent_id : str
            The id of the extraction agent

        file_ids : typing.Sequence[str]
            The ids of the files

        from_ui : typing.Optional[bool]

        webhook_configurations : typing.Optional[typing.Sequence[WebhookConfiguration]]
            The outbound webhook configurations

        data_schema_override : typing.Optional[ExtractJobCreateBatchDataSchemaOverride]
            The data schema to override the extraction agent's data schema with

        config_override : typing.Optional[ExtractConfig]
            The config to override the extraction agent's config with

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[ExtractJob]
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.llama_extract.run_batch_jobs(
            from_ui=True,
            extraction_agent_id="extraction_agent_id",
            file_ids=["file_ids"],
        )
        """
        _response = self._raw_client.run_batch_jobs(
            extraction_agent_id=extraction_agent_id,
            file_ids=file_ids,
            from_ui=from_ui,
            webhook_configurations=webhook_configurations,
            data_schema_override=data_schema_override,
            config_override=config_override,
            request_options=request_options,
        )
        return _response.data

    def get_job_result(
        self,
        job_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractResultset:
        """
        Parameters
        ----------
        job_id : str

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractResultset
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.llama_extract.get_job_result(
            job_id="job_id",
            project_id="project_id",
            organization_id="organization_id",
        )
        """
        _response = self._raw_client.get_job_result(
            job_id, project_id=project_id, organization_id=organization_id, request_options=request_options
        )
        return _response.data

    def list_runs(
        self,
        *,
        extraction_agent_id: str,
        skip: typing.Optional[int] = None,
        limit: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PaginatedExtractRunsResponse:
        """
        Parameters
        ----------
        extraction_agent_id : str

        skip : typing.Optional[int]

        limit : typing.Optional[int]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PaginatedExtractRunsResponse
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.llama_extract.list_runs(
            extraction_agent_id="extraction_agent_id",
            skip=1,
            limit=1,
        )
        """
        _response = self._raw_client.list_runs(
            extraction_agent_id=extraction_agent_id, skip=skip, limit=limit, request_options=request_options
        )
        return _response.data

    def get_latest_ui_run(
        self, *, extraction_agent_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.Optional[ExtractRun]:
        """
        Parameters
        ----------
        extraction_agent_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Optional[ExtractRun]
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.llama_extract.get_latest_ui_run(
            extraction_agent_id="extraction_agent_id",
        )
        """
        _response = self._raw_client.get_latest_ui_run(
            extraction_agent_id=extraction_agent_id, request_options=request_options
        )
        return _response.data

    def get_run_by_job(
        self,
        job_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractRun:
        """
        Parameters
        ----------
        job_id : str

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractRun
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.llama_extract.get_run_by_job(
            job_id="job_id",
            project_id="project_id",
            organization_id="organization_id",
        )
        """
        _response = self._raw_client.get_run_by_job(
            job_id, project_id=project_id, organization_id=organization_id, request_options=request_options
        )
        return _response.data

    def get_run(
        self,
        run_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractRun:
        """
        Parameters
        ----------
        run_id : str

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractRun
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.llama_extract.get_run(
            run_id="run_id",
            project_id="project_id",
            organization_id="organization_id",
        )
        """
        _response = self._raw_client.get_run(
            run_id, project_id=project_id, organization_id=organization_id, request_options=request_options
        )
        return _response.data

    def delete_run(
        self,
        run_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Optional[typing.Any]:
        """
        Parameters
        ----------
        run_id : str

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Optional[typing.Any]
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.llama_extract.delete_run(
            run_id="run_id",
            project_id="project_id",
            organization_id="organization_id",
        )
        """
        _response = self._raw_client.delete_run(
            run_id, project_id=project_id, organization_id=organization_id, request_options=request_options
        )
        return _response.data

    def extract_stateless(
        self,
        *,
        data_schema: ExtractStatelessRequestDataSchema,
        config: ExtractConfig,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        webhook_configurations: typing.Optional[typing.Sequence[WebhookConfiguration]] = OMIT,
        file_id: typing.Optional[str] = OMIT,
        text: typing.Optional[str] = OMIT,
        file: typing.Optional[FileData] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractJob:
        """
        Stateless extraction endpoint that uses a default extraction agent in the user's default project.
        Requires data_schema, config, and either file_id, text, or base64 encoded file data.

        Parameters
        ----------
        data_schema : ExtractStatelessRequestDataSchema
            The schema of the data to extract

        config : ExtractConfig
            The configuration parameters for the extraction

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        webhook_configurations : typing.Optional[typing.Sequence[WebhookConfiguration]]
            The outbound webhook configurations

        file_id : typing.Optional[str]
            The ID of the file to extract from

        text : typing.Optional[str]
            The text content to extract from

        file : typing.Optional[FileData]
            The file data with base64 content and MIME type

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractJob
            Successful Response

        Examples
        --------
        from llamaindex_test import ExtractConfig, LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.llama_extract.extract_stateless(
            project_id="project_id",
            organization_id="organization_id",
            data_schema={},
            config=ExtractConfig(),
        )
        """
        _response = self._raw_client.extract_stateless(
            data_schema=data_schema,
            config=config,
            project_id=project_id,
            organization_id=organization_id,
            webhook_configurations=webhook_configurations,
            file_id=file_id,
            text=text,
            file=file,
            request_options=request_options,
        )
        return _response.data

    def list_agents(
        self,
        *,
        include_default: typing.Optional[bool] = None,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[ExtractAgent]:
        """
        Parameters
        ----------
        include_default : typing.Optional[bool]
            Whether to include default agents in the results

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[ExtractAgent]
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.llama_extract.list_agents(
            include_default=True,
            project_id="project_id",
            organization_id="organization_id",
        )
        """
        _response = self._raw_client.list_agents(
            include_default=include_default,
            project_id=project_id,
            organization_id=organization_id,
            request_options=request_options,
        )
        return _response.data

    def create_agent(
        self,
        *,
        name: str,
        data_schema: ExtractAgentCreateDataSchema,
        config: ExtractConfig,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractAgent:
        """
        Parameters
        ----------
        name : str
            The name of the extraction schema

        data_schema : ExtractAgentCreateDataSchema
            The schema of the data.

        config : ExtractConfig
            The configuration parameters for the extraction agent.

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractAgent
            Successful Response

        Examples
        --------
        from llamaindex_test import ExtractConfig, LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.llama_extract.create_agent(
            project_id="project_id",
            organization_id="organization_id",
            name="name",
            data_schema={},
            config=ExtractConfig(),
        )
        """
        _response = self._raw_client.create_agent(
            name=name,
            data_schema=data_schema,
            config=config,
            project_id=project_id,
            organization_id=organization_id,
            request_options=request_options,
        )
        return _response.data

    def validate_schema(
        self,
        *,
        data_schema: ExtractSchemaValidateRequestDataSchema,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractSchemaValidateResponse:
        """
        Validates an extraction agent's schema definition.
        Returns the normalized and validated schema if valid, otherwise raises an HTTP 400.

        Parameters
        ----------
        data_schema : ExtractSchemaValidateRequestDataSchema

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractSchemaValidateResponse
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.llama_extract.validate_schema(
            data_schema={},
        )
        """
        _response = self._raw_client.validate_schema(data_schema=data_schema, request_options=request_options)
        return _response.data

    def generate_schema(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        prompt: typing.Optional[str] = OMIT,
        file_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractSchemaGenerateResponse:
        """
        Generates an extraction agent's schema definition from a file and/or natural language prompt.

        Parameters
        ----------
        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        prompt : typing.Optional[str]
            Natural language description of the data structure to extract

        file_id : typing.Optional[str]
            Optional file ID to analyze for schema generation

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractSchemaGenerateResponse
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.llama_extract.generate_schema(
            project_id="project_id",
            organization_id="organization_id",
        )
        """
        _response = self._raw_client.generate_schema(
            project_id=project_id,
            organization_id=organization_id,
            prompt=prompt,
            file_id=file_id,
            request_options=request_options,
        )
        return _response.data

    def get_agent_by_name(
        self,
        name: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractAgent:
        """
        Parameters
        ----------
        name : str

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractAgent
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.llama_extract.get_agent_by_name(
            name="name",
            project_id="project_id",
            organization_id="organization_id",
        )
        """
        _response = self._raw_client.get_agent_by_name(
            name, project_id=project_id, organization_id=organization_id, request_options=request_options
        )
        return _response.data

    def get_or_create_default_agent(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractAgent:
        """
        Get or create a default extraction agent for the current project.
        The default agent has an empty schema and default configuration.

        Parameters
        ----------
        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractAgent
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.llama_extract.get_or_create_default_agent(
            project_id="project_id",
            organization_id="organization_id",
        )
        """
        _response = self._raw_client.get_or_create_default_agent(
            project_id=project_id, organization_id=organization_id, request_options=request_options
        )
        return _response.data

    def get_agent(
        self, extraction_agent_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> ExtractAgent:
        """
        Parameters
        ----------
        extraction_agent_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractAgent
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.llama_extract.get_agent(
            extraction_agent_id="extraction_agent_id",
        )
        """
        _response = self._raw_client.get_agent(extraction_agent_id, request_options=request_options)
        return _response.data

    def update_agent(
        self,
        extraction_agent_id: str,
        *,
        data_schema: ExtractAgentUpdateDataSchema,
        config: ExtractConfig,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractAgent:
        """
        Parameters
        ----------
        extraction_agent_id : str

        data_schema : ExtractAgentUpdateDataSchema
            The schema of the data

        config : ExtractConfig
            The configuration parameters for the extraction agent.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractAgent
            Successful Response

        Examples
        --------
        from llamaindex_test import ExtractConfig, LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.llama_extract.update_agent(
            extraction_agent_id="extraction_agent_id",
            data_schema={},
            config=ExtractConfig(),
        )
        """
        _response = self._raw_client.update_agent(
            extraction_agent_id, data_schema=data_schema, config=config, request_options=request_options
        )
        return _response.data

    def delete_agent(
        self, extraction_agent_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.Optional[typing.Any]:
        """
        Parameters
        ----------
        extraction_agent_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Optional[typing.Any]
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.llama_extract.delete_agent(
            extraction_agent_id="extraction_agent_id",
        )
        """
        _response = self._raw_client.delete_agent(extraction_agent_id, request_options=request_options)
        return _response.data


class AsyncLlamaExtractClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._raw_client = AsyncRawLlamaExtractClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> AsyncRawLlamaExtractClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        AsyncRawLlamaExtractClient
        """
        return self._raw_client

    async def list_jobs(
        self, *, extraction_agent_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[ExtractJob]:
        """
        Parameters
        ----------
        extraction_agent_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[ExtractJob]
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.llama_extract.list_jobs(
                extraction_agent_id="extraction_agent_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.list_jobs(
            extraction_agent_id=extraction_agent_id, request_options=request_options
        )
        return _response.data

    async def run_job(
        self,
        *,
        extraction_agent_id: str,
        file_id: str,
        from_ui: typing.Optional[bool] = None,
        priority: typing.Optional[ExtractJobCreatePriority] = OMIT,
        webhook_configurations: typing.Optional[typing.Sequence[WebhookConfiguration]] = OMIT,
        data_schema_override: typing.Optional[ExtractJobCreateDataSchemaOverride] = OMIT,
        config_override: typing.Optional[ExtractConfig] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractJob:
        """
        Parameters
        ----------
        extraction_agent_id : str
            The id of the extraction agent

        file_id : str
            The id of the file

        from_ui : typing.Optional[bool]

        priority : typing.Optional[ExtractJobCreatePriority]
            The priority for the request. This field may be ignored or overwritten depending on the organization tier.

        webhook_configurations : typing.Optional[typing.Sequence[WebhookConfiguration]]
            The outbound webhook configurations

        data_schema_override : typing.Optional[ExtractJobCreateDataSchemaOverride]
            The data schema to override the extraction agent's data schema with

        config_override : typing.Optional[ExtractConfig]
            The config to override the extraction agent's config with

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractJob
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.llama_extract.run_job(
                from_ui=True,
                extraction_agent_id="extraction_agent_id",
                file_id="file_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.run_job(
            extraction_agent_id=extraction_agent_id,
            file_id=file_id,
            from_ui=from_ui,
            priority=priority,
            webhook_configurations=webhook_configurations,
            data_schema_override=data_schema_override,
            config_override=config_override,
            request_options=request_options,
        )
        return _response.data

    async def get_job(self, job_id: str, *, request_options: typing.Optional[RequestOptions] = None) -> ExtractJob:
        """
        Parameters
        ----------
        job_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractJob
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.llama_extract.get_job(
                job_id="job_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.get_job(job_id, request_options=request_options)
        return _response.data

    async def run_job_on_file(
        self,
        *,
        extraction_agent_id: str,
        file: core.File,
        from_ui: typing.Optional[bool] = None,
        data_schema_override: typing.Optional[str] = OMIT,
        config_override: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractJob:
        """
        Parameters
        ----------
        extraction_agent_id : str
            The id of the extraction agent

        file : core.File
            See core.File for more documentation

        from_ui : typing.Optional[bool]

        data_schema_override : typing.Optional[str]
            The data schema to override the extraction agent's data schema with as a JSON string

        config_override : typing.Optional[str]
            The config to override the extraction agent's config with as a JSON string

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractJob
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.llama_extract.run_job_on_file(
                from_ui=True,
                extraction_agent_id="extraction_agent_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.run_job_on_file(
            extraction_agent_id=extraction_agent_id,
            file=file,
            from_ui=from_ui,
            data_schema_override=data_schema_override,
            config_override=config_override,
            request_options=request_options,
        )
        return _response.data

    async def run_batch_jobs(
        self,
        *,
        extraction_agent_id: str,
        file_ids: typing.Sequence[str],
        from_ui: typing.Optional[bool] = None,
        webhook_configurations: typing.Optional[typing.Sequence[WebhookConfiguration]] = OMIT,
        data_schema_override: typing.Optional[ExtractJobCreateBatchDataSchemaOverride] = OMIT,
        config_override: typing.Optional[ExtractConfig] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[ExtractJob]:
        """
        Parameters
        ----------
        extraction_agent_id : str
            The id of the extraction agent

        file_ids : typing.Sequence[str]
            The ids of the files

        from_ui : typing.Optional[bool]

        webhook_configurations : typing.Optional[typing.Sequence[WebhookConfiguration]]
            The outbound webhook configurations

        data_schema_override : typing.Optional[ExtractJobCreateBatchDataSchemaOverride]
            The data schema to override the extraction agent's data schema with

        config_override : typing.Optional[ExtractConfig]
            The config to override the extraction agent's config with

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[ExtractJob]
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.llama_extract.run_batch_jobs(
                from_ui=True,
                extraction_agent_id="extraction_agent_id",
                file_ids=["file_ids"],
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.run_batch_jobs(
            extraction_agent_id=extraction_agent_id,
            file_ids=file_ids,
            from_ui=from_ui,
            webhook_configurations=webhook_configurations,
            data_schema_override=data_schema_override,
            config_override=config_override,
            request_options=request_options,
        )
        return _response.data

    async def get_job_result(
        self,
        job_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractResultset:
        """
        Parameters
        ----------
        job_id : str

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractResultset
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.llama_extract.get_job_result(
                job_id="job_id",
                project_id="project_id",
                organization_id="organization_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.get_job_result(
            job_id, project_id=project_id, organization_id=organization_id, request_options=request_options
        )
        return _response.data

    async def list_runs(
        self,
        *,
        extraction_agent_id: str,
        skip: typing.Optional[int] = None,
        limit: typing.Optional[int] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PaginatedExtractRunsResponse:
        """
        Parameters
        ----------
        extraction_agent_id : str

        skip : typing.Optional[int]

        limit : typing.Optional[int]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PaginatedExtractRunsResponse
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.llama_extract.list_runs(
                extraction_agent_id="extraction_agent_id",
                skip=1,
                limit=1,
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.list_runs(
            extraction_agent_id=extraction_agent_id, skip=skip, limit=limit, request_options=request_options
        )
        return _response.data

    async def get_latest_ui_run(
        self, *, extraction_agent_id: str, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.Optional[ExtractRun]:
        """
        Parameters
        ----------
        extraction_agent_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Optional[ExtractRun]
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.llama_extract.get_latest_ui_run(
                extraction_agent_id="extraction_agent_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.get_latest_ui_run(
            extraction_agent_id=extraction_agent_id, request_options=request_options
        )
        return _response.data

    async def get_run_by_job(
        self,
        job_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractRun:
        """
        Parameters
        ----------
        job_id : str

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractRun
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.llama_extract.get_run_by_job(
                job_id="job_id",
                project_id="project_id",
                organization_id="organization_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.get_run_by_job(
            job_id, project_id=project_id, organization_id=organization_id, request_options=request_options
        )
        return _response.data

    async def get_run(
        self,
        run_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractRun:
        """
        Parameters
        ----------
        run_id : str

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractRun
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.llama_extract.get_run(
                run_id="run_id",
                project_id="project_id",
                organization_id="organization_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.get_run(
            run_id, project_id=project_id, organization_id=organization_id, request_options=request_options
        )
        return _response.data

    async def delete_run(
        self,
        run_id: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Optional[typing.Any]:
        """
        Parameters
        ----------
        run_id : str

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Optional[typing.Any]
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.llama_extract.delete_run(
                run_id="run_id",
                project_id="project_id",
                organization_id="organization_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.delete_run(
            run_id, project_id=project_id, organization_id=organization_id, request_options=request_options
        )
        return _response.data

    async def extract_stateless(
        self,
        *,
        data_schema: ExtractStatelessRequestDataSchema,
        config: ExtractConfig,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        webhook_configurations: typing.Optional[typing.Sequence[WebhookConfiguration]] = OMIT,
        file_id: typing.Optional[str] = OMIT,
        text: typing.Optional[str] = OMIT,
        file: typing.Optional[FileData] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractJob:
        """
        Stateless extraction endpoint that uses a default extraction agent in the user's default project.
        Requires data_schema, config, and either file_id, text, or base64 encoded file data.

        Parameters
        ----------
        data_schema : ExtractStatelessRequestDataSchema
            The schema of the data to extract

        config : ExtractConfig
            The configuration parameters for the extraction

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        webhook_configurations : typing.Optional[typing.Sequence[WebhookConfiguration]]
            The outbound webhook configurations

        file_id : typing.Optional[str]
            The ID of the file to extract from

        text : typing.Optional[str]
            The text content to extract from

        file : typing.Optional[FileData]
            The file data with base64 content and MIME type

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractJob
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud, ExtractConfig

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.llama_extract.extract_stateless(
                project_id="project_id",
                organization_id="organization_id",
                data_schema={},
                config=ExtractConfig(),
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.extract_stateless(
            data_schema=data_schema,
            config=config,
            project_id=project_id,
            organization_id=organization_id,
            webhook_configurations=webhook_configurations,
            file_id=file_id,
            text=text,
            file=file,
            request_options=request_options,
        )
        return _response.data

    async def list_agents(
        self,
        *,
        include_default: typing.Optional[bool] = None,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[ExtractAgent]:
        """
        Parameters
        ----------
        include_default : typing.Optional[bool]
            Whether to include default agents in the results

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[ExtractAgent]
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.llama_extract.list_agents(
                include_default=True,
                project_id="project_id",
                organization_id="organization_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.list_agents(
            include_default=include_default,
            project_id=project_id,
            organization_id=organization_id,
            request_options=request_options,
        )
        return _response.data

    async def create_agent(
        self,
        *,
        name: str,
        data_schema: ExtractAgentCreateDataSchema,
        config: ExtractConfig,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractAgent:
        """
        Parameters
        ----------
        name : str
            The name of the extraction schema

        data_schema : ExtractAgentCreateDataSchema
            The schema of the data.

        config : ExtractConfig
            The configuration parameters for the extraction agent.

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractAgent
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud, ExtractConfig

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.llama_extract.create_agent(
                project_id="project_id",
                organization_id="organization_id",
                name="name",
                data_schema={},
                config=ExtractConfig(),
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.create_agent(
            name=name,
            data_schema=data_schema,
            config=config,
            project_id=project_id,
            organization_id=organization_id,
            request_options=request_options,
        )
        return _response.data

    async def validate_schema(
        self,
        *,
        data_schema: ExtractSchemaValidateRequestDataSchema,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractSchemaValidateResponse:
        """
        Validates an extraction agent's schema definition.
        Returns the normalized and validated schema if valid, otherwise raises an HTTP 400.

        Parameters
        ----------
        data_schema : ExtractSchemaValidateRequestDataSchema

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractSchemaValidateResponse
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.llama_extract.validate_schema(
                data_schema={},
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.validate_schema(data_schema=data_schema, request_options=request_options)
        return _response.data

    async def generate_schema(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        prompt: typing.Optional[str] = OMIT,
        file_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractSchemaGenerateResponse:
        """
        Generates an extraction agent's schema definition from a file and/or natural language prompt.

        Parameters
        ----------
        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        prompt : typing.Optional[str]
            Natural language description of the data structure to extract

        file_id : typing.Optional[str]
            Optional file ID to analyze for schema generation

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractSchemaGenerateResponse
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.llama_extract.generate_schema(
                project_id="project_id",
                organization_id="organization_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.generate_schema(
            project_id=project_id,
            organization_id=organization_id,
            prompt=prompt,
            file_id=file_id,
            request_options=request_options,
        )
        return _response.data

    async def get_agent_by_name(
        self,
        name: str,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractAgent:
        """
        Parameters
        ----------
        name : str

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractAgent
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.llama_extract.get_agent_by_name(
                name="name",
                project_id="project_id",
                organization_id="organization_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.get_agent_by_name(
            name, project_id=project_id, organization_id=organization_id, request_options=request_options
        )
        return _response.data

    async def get_or_create_default_agent(
        self,
        *,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractAgent:
        """
        Get or create a default extraction agent for the current project.
        The default agent has an empty schema and default configuration.

        Parameters
        ----------
        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractAgent
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.llama_extract.get_or_create_default_agent(
                project_id="project_id",
                organization_id="organization_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.get_or_create_default_agent(
            project_id=project_id, organization_id=organization_id, request_options=request_options
        )
        return _response.data

    async def get_agent(
        self, extraction_agent_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> ExtractAgent:
        """
        Parameters
        ----------
        extraction_agent_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractAgent
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.llama_extract.get_agent(
                extraction_agent_id="extraction_agent_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.get_agent(extraction_agent_id, request_options=request_options)
        return _response.data

    async def update_agent(
        self,
        extraction_agent_id: str,
        *,
        data_schema: ExtractAgentUpdateDataSchema,
        config: ExtractConfig,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ExtractAgent:
        """
        Parameters
        ----------
        extraction_agent_id : str

        data_schema : ExtractAgentUpdateDataSchema
            The schema of the data

        config : ExtractConfig
            The configuration parameters for the extraction agent.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ExtractAgent
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud, ExtractConfig

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.llama_extract.update_agent(
                extraction_agent_id="extraction_agent_id",
                data_schema={},
                config=ExtractConfig(),
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.update_agent(
            extraction_agent_id, data_schema=data_schema, config=config, request_options=request_options
        )
        return _response.data

    async def delete_agent(
        self, extraction_agent_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.Optional[typing.Any]:
        """
        Parameters
        ----------
        extraction_agent_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Optional[typing.Any]
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.llama_extract.delete_agent(
                extraction_agent_id="extraction_agent_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.delete_agent(extraction_agent_id, request_options=request_options)
        return _response.data
