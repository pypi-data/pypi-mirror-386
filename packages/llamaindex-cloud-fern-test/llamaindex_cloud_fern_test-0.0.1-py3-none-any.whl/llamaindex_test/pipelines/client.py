# This file was auto-generated by Fern from our API Definition.

import typing

from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.request_options import RequestOptions
from ..types.chat_data import ChatData
from ..types.chat_message import ChatMessage
from ..types.chat_message_chunk import ChatMessageChunk
from ..types.cloud_document import CloudDocument
from ..types.cloud_document_create import CloudDocumentCreate
from ..types.data_sink_create import DataSinkCreate
from ..types.input_message import InputMessage
from ..types.llama_parse_parameters import LlamaParseParameters
from ..types.managed_ingestion_status_response import ManagedIngestionStatusResponse
from ..types.metadata_filters import MetadataFilters
from ..types.paginated_list_cloud_documents_response import PaginatedListCloudDocumentsResponse
from ..types.pipeline import Pipeline
from ..types.pipeline_create_embedding_config import PipelineCreateEmbeddingConfig
from ..types.pipeline_create_transform_config import PipelineCreateTransformConfig
from ..types.pipeline_metadata_config import PipelineMetadataConfig
from ..types.pipeline_type import PipelineType
from ..types.playground_session import PlaygroundSession
from ..types.preset_retrieval_params import PresetRetrievalParams
from ..types.retrieval_mode import RetrievalMode
from ..types.retrieve_results import RetrieveResults
from ..types.sparse_model_config import SparseModelConfig
from ..types.text_node import TextNode
from .raw_client import AsyncRawPipelinesClient, RawPipelinesClient
from .types.pipeline_update_embedding_config import PipelineUpdateEmbeddingConfig
from .types.pipeline_update_transform_config import PipelineUpdateTransformConfig
from .types.pipelines_list_documents_paginated_request_status_refresh_policy import (
    PipelinesListDocumentsPaginatedRequestStatusRefreshPolicy,
)
from .types.pipelines_list_documents_request_status_refresh_policy import (
    PipelinesListDocumentsRequestStatusRefreshPolicy,
)
from .types.retrieval_params_search_filters_inference_schema_value import (
    RetrievalParamsSearchFiltersInferenceSchemaValue,
)

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class PipelinesClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._raw_client = RawPipelinesClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> RawPipelinesClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        RawPipelinesClient
        """
        return self._raw_client

    def search(
        self,
        *,
        project_id: typing.Optional[str] = None,
        project_name: typing.Optional[str] = None,
        pipeline_name: typing.Optional[str] = None,
        pipeline_type: typing.Optional[PipelineType] = None,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[Pipeline]:
        """
        Search for pipelines by various parameters.

        Parameters
        ----------
        project_id : typing.Optional[str]

        project_name : typing.Optional[str]

        pipeline_name : typing.Optional[str]

        pipeline_type : typing.Optional[PipelineType]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[Pipeline]
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipelines.search(
            project_id="project_id",
            project_name="project_name",
            pipeline_name="pipeline_name",
            pipeline_type="PLAYGROUND",
            organization_id="organization_id",
        )
        """
        _response = self._raw_client.search(
            project_id=project_id,
            project_name=project_name,
            pipeline_name=pipeline_name,
            pipeline_type=pipeline_type,
            organization_id=organization_id,
            request_options=request_options,
        )
        return _response.data

    def create(
        self,
        *,
        name: str,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        embedding_config: typing.Optional[PipelineCreateEmbeddingConfig] = OMIT,
        transform_config: typing.Optional[PipelineCreateTransformConfig] = OMIT,
        sparse_model_config: typing.Optional[SparseModelConfig] = OMIT,
        data_sink_id: typing.Optional[str] = OMIT,
        embedding_model_config_id: typing.Optional[str] = OMIT,
        data_sink: typing.Optional[DataSinkCreate] = OMIT,
        preset_retrieval_parameters: typing.Optional[PresetRetrievalParams] = OMIT,
        llama_parse_parameters: typing.Optional[LlamaParseParameters] = OMIT,
        status: typing.Optional[str] = OMIT,
        metadata_config: typing.Optional[PipelineMetadataConfig] = OMIT,
        pipeline_type: typing.Optional[PipelineType] = OMIT,
        managed_pipeline_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Pipeline:
        """
        Create a new pipeline for a project.

        Parameters
        ----------
        name : str

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        embedding_config : typing.Optional[PipelineCreateEmbeddingConfig]

        transform_config : typing.Optional[PipelineCreateTransformConfig]
            Configuration for the transformation.

        sparse_model_config : typing.Optional[SparseModelConfig]
            Configuration for the sparse model used in hybrid search.

        data_sink_id : typing.Optional[str]
            Data sink ID. When provided instead of data_sink, the data sink will be looked up by ID.

        embedding_model_config_id : typing.Optional[str]
            Embedding model config ID. When provided instead of embedding_config, the embedding model config will be looked up by ID.

        data_sink : typing.Optional[DataSinkCreate]
            Data sink. When provided instead of data_sink_id, the data sink will be created.

        preset_retrieval_parameters : typing.Optional[PresetRetrievalParams]
            Preset retrieval parameters for the pipeline.

        llama_parse_parameters : typing.Optional[LlamaParseParameters]

        status : typing.Optional[str]
            Status of the pipeline deployment.

        metadata_config : typing.Optional[PipelineMetadataConfig]
            Metadata configuration for the pipeline.

        pipeline_type : typing.Optional[PipelineType]
            Type of pipeline. Either PLAYGROUND or MANAGED.

        managed_pipeline_id : typing.Optional[str]
            The ID of the ManagedPipeline this playground pipeline is linked to.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipelines.create(
            project_id="project_id",
            organization_id="organization_id",
            name="name",
        )
        """
        _response = self._raw_client.create(
            name=name,
            project_id=project_id,
            organization_id=organization_id,
            embedding_config=embedding_config,
            transform_config=transform_config,
            sparse_model_config=sparse_model_config,
            data_sink_id=data_sink_id,
            embedding_model_config_id=embedding_model_config_id,
            data_sink=data_sink,
            preset_retrieval_parameters=preset_retrieval_parameters,
            llama_parse_parameters=llama_parse_parameters,
            status=status,
            metadata_config=metadata_config,
            pipeline_type=pipeline_type,
            managed_pipeline_id=managed_pipeline_id,
            request_options=request_options,
        )
        return _response.data

    def upsert(
        self,
        *,
        name: str,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        embedding_config: typing.Optional[PipelineCreateEmbeddingConfig] = OMIT,
        transform_config: typing.Optional[PipelineCreateTransformConfig] = OMIT,
        sparse_model_config: typing.Optional[SparseModelConfig] = OMIT,
        data_sink_id: typing.Optional[str] = OMIT,
        embedding_model_config_id: typing.Optional[str] = OMIT,
        data_sink: typing.Optional[DataSinkCreate] = OMIT,
        preset_retrieval_parameters: typing.Optional[PresetRetrievalParams] = OMIT,
        llama_parse_parameters: typing.Optional[LlamaParseParameters] = OMIT,
        status: typing.Optional[str] = OMIT,
        metadata_config: typing.Optional[PipelineMetadataConfig] = OMIT,
        pipeline_type: typing.Optional[PipelineType] = OMIT,
        managed_pipeline_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Pipeline:
        """
        Upsert a pipeline for a project.
        Updates if a pipeline with the same name and project_id already exists. Otherwise, creates a new pipeline.

        Parameters
        ----------
        name : str

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        embedding_config : typing.Optional[PipelineCreateEmbeddingConfig]

        transform_config : typing.Optional[PipelineCreateTransformConfig]
            Configuration for the transformation.

        sparse_model_config : typing.Optional[SparseModelConfig]
            Configuration for the sparse model used in hybrid search.

        data_sink_id : typing.Optional[str]
            Data sink ID. When provided instead of data_sink, the data sink will be looked up by ID.

        embedding_model_config_id : typing.Optional[str]
            Embedding model config ID. When provided instead of embedding_config, the embedding model config will be looked up by ID.

        data_sink : typing.Optional[DataSinkCreate]
            Data sink. When provided instead of data_sink_id, the data sink will be created.

        preset_retrieval_parameters : typing.Optional[PresetRetrievalParams]
            Preset retrieval parameters for the pipeline.

        llama_parse_parameters : typing.Optional[LlamaParseParameters]

        status : typing.Optional[str]
            Status of the pipeline deployment.

        metadata_config : typing.Optional[PipelineMetadataConfig]
            Metadata configuration for the pipeline.

        pipeline_type : typing.Optional[PipelineType]
            Type of pipeline. Either PLAYGROUND or MANAGED.

        managed_pipeline_id : typing.Optional[str]
            The ID of the ManagedPipeline this playground pipeline is linked to.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipelines.upsert(
            project_id="project_id",
            organization_id="organization_id",
            name="name",
        )
        """
        _response = self._raw_client.upsert(
            name=name,
            project_id=project_id,
            organization_id=organization_id,
            embedding_config=embedding_config,
            transform_config=transform_config,
            sparse_model_config=sparse_model_config,
            data_sink_id=data_sink_id,
            embedding_model_config_id=embedding_model_config_id,
            data_sink=data_sink,
            preset_retrieval_parameters=preset_retrieval_parameters,
            llama_parse_parameters=llama_parse_parameters,
            status=status,
            metadata_config=metadata_config,
            pipeline_type=pipeline_type,
            managed_pipeline_id=managed_pipeline_id,
            request_options=request_options,
        )
        return _response.data

    def get(self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None) -> Pipeline:
        """
        Get a pipeline by ID for a given project.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipelines.get(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._raw_client.get(pipeline_id, request_options=request_options)
        return _response.data

    def update(
        self,
        pipeline_id: str,
        *,
        embedding_config: typing.Optional[PipelineUpdateEmbeddingConfig] = OMIT,
        transform_config: typing.Optional[PipelineUpdateTransformConfig] = OMIT,
        sparse_model_config: typing.Optional[SparseModelConfig] = OMIT,
        data_sink_id: typing.Optional[str] = OMIT,
        embedding_model_config_id: typing.Optional[str] = OMIT,
        data_sink: typing.Optional[DataSinkCreate] = OMIT,
        preset_retrieval_parameters: typing.Optional[PresetRetrievalParams] = OMIT,
        llama_parse_parameters: typing.Optional[LlamaParseParameters] = OMIT,
        status: typing.Optional[str] = OMIT,
        metadata_config: typing.Optional[PipelineMetadataConfig] = OMIT,
        name: typing.Optional[str] = OMIT,
        managed_pipeline_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Pipeline:
        """
        Update an existing pipeline for a project.

        Parameters
        ----------
        pipeline_id : str

        embedding_config : typing.Optional[PipelineUpdateEmbeddingConfig]

        transform_config : typing.Optional[PipelineUpdateTransformConfig]
            Configuration for the transformation.

        sparse_model_config : typing.Optional[SparseModelConfig]
            Configuration for the sparse model used in hybrid search.

        data_sink_id : typing.Optional[str]
            Data sink ID. When provided instead of data_sink, the data sink will be looked up by ID.

        embedding_model_config_id : typing.Optional[str]
            Embedding model config ID. When provided instead of embedding_config, the embedding model config will be looked up by ID.

        data_sink : typing.Optional[DataSinkCreate]
            Data sink. When provided instead of data_sink_id, the data sink will be created.

        preset_retrieval_parameters : typing.Optional[PresetRetrievalParams]
            Preset retrieval parameters for the pipeline.

        llama_parse_parameters : typing.Optional[LlamaParseParameters]
            Settings that can be configured for how to use LlamaParse to parse files within a LlamaCloud pipeline.

        status : typing.Optional[str]
            Status of the pipeline deployment.

        metadata_config : typing.Optional[PipelineMetadataConfig]
            Metadata configuration for the pipeline.

        name : typing.Optional[str]

        managed_pipeline_id : typing.Optional[str]
            The ID of the ManagedPipeline this playground pipeline is linked to.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipelines.update(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._raw_client.update(
            pipeline_id,
            embedding_config=embedding_config,
            transform_config=transform_config,
            sparse_model_config=sparse_model_config,
            data_sink_id=data_sink_id,
            embedding_model_config_id=embedding_model_config_id,
            data_sink=data_sink,
            preset_retrieval_parameters=preset_retrieval_parameters,
            llama_parse_parameters=llama_parse_parameters,
            status=status,
            metadata_config=metadata_config,
            name=name,
            managed_pipeline_id=managed_pipeline_id,
            request_options=request_options,
        )
        return _response.data

    def delete(self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Delete a pipeline by ID.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipelines.delete(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._raw_client.delete(pipeline_id, request_options=request_options)
        return _response.data

    def get_status(
        self,
        pipeline_id: str,
        *,
        full_details: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ManagedIngestionStatusResponse:
        """
        Get the status of a pipeline by ID.

        Parameters
        ----------
        pipeline_id : str

        full_details : typing.Optional[bool]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ManagedIngestionStatusResponse
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipelines.get_status(
            pipeline_id="pipeline_id",
            full_details=True,
        )
        """
        _response = self._raw_client.get_status(pipeline_id, full_details=full_details, request_options=request_options)
        return _response.data

    def sync(self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None) -> Pipeline:
        """
        Run ingestion for the pipeline by incrementally updating the data-sink with upstream changes from data-sources & files.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipelines.sync(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._raw_client.sync(pipeline_id, request_options=request_options)
        return _response.data

    def cancel_sync(self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None) -> Pipeline:
        """
        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipelines.cancel_sync(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._raw_client.cancel_sync(pipeline_id, request_options=request_options)
        return _response.data

    def force_delete(self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipelines.force_delete(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._raw_client.force_delete(pipeline_id, request_options=request_options)
        return _response.data

    def copy(self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None) -> Pipeline:
        """
        Copy a pipeline by ID.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipelines.copy(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._raw_client.copy(pipeline_id, request_options=request_options)
        return _response.data

    def retrieve(
        self,
        pipeline_id: str,
        *,
        query: str,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        dense_similarity_top_k: typing.Optional[int] = OMIT,
        dense_similarity_cutoff: typing.Optional[float] = OMIT,
        sparse_similarity_top_k: typing.Optional[int] = OMIT,
        enable_reranking: typing.Optional[bool] = OMIT,
        rerank_top_n: typing.Optional[int] = OMIT,
        alpha: typing.Optional[float] = OMIT,
        search_filters: typing.Optional[MetadataFilters] = OMIT,
        search_filters_inference_schema: typing.Optional[
            typing.Dict[str, typing.Optional[RetrievalParamsSearchFiltersInferenceSchemaValue]]
        ] = OMIT,
        files_top_k: typing.Optional[int] = OMIT,
        retrieval_mode: typing.Optional[RetrievalMode] = OMIT,
        retrieve_image_nodes: typing.Optional[bool] = OMIT,
        retrieve_page_screenshot_nodes: typing.Optional[bool] = OMIT,
        retrieve_page_figure_nodes: typing.Optional[bool] = OMIT,
        class_name: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> RetrieveResults:
        """
        Get retrieval results for a managed pipeline and a query

        Parameters
        ----------
        pipeline_id : str

        query : str
            The query to retrieve against.

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        dense_similarity_top_k : typing.Optional[int]
            Number of nodes for dense retrieval.

        dense_similarity_cutoff : typing.Optional[float]
            Minimum similarity score wrt query for retrieval

        sparse_similarity_top_k : typing.Optional[int]
            Number of nodes for sparse retrieval.

        enable_reranking : typing.Optional[bool]
            Enable reranking for retrieval

        rerank_top_n : typing.Optional[int]
            Number of reranked nodes for returning.

        alpha : typing.Optional[float]
            Alpha value for hybrid retrieval to determine the weights between dense and sparse retrieval. 0 is sparse retrieval and 1 is dense retrieval.

        search_filters : typing.Optional[MetadataFilters]
            Search filters for retrieval.

        search_filters_inference_schema : typing.Optional[typing.Dict[str, typing.Optional[RetrievalParamsSearchFiltersInferenceSchemaValue]]]
            JSON Schema that will be used to infer search_filters. Omit or leave as null to skip inference.

        files_top_k : typing.Optional[int]
            Number of files to retrieve (only for retrieval mode files_via_metadata and files_via_content).

        retrieval_mode : typing.Optional[RetrievalMode]
            The retrieval mode for the query.

        retrieve_image_nodes : typing.Optional[bool]
            Whether to retrieve image nodes.

        retrieve_page_screenshot_nodes : typing.Optional[bool]
            Whether to retrieve page screenshot nodes.

        retrieve_page_figure_nodes : typing.Optional[bool]
            Whether to retrieve page figure nodes.

        class_name : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        RetrieveResults
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipelines.retrieve(
            pipeline_id="pipeline_id",
            project_id="project_id",
            organization_id="organization_id",
            query="query",
        )
        """
        _response = self._raw_client.retrieve(
            pipeline_id,
            query=query,
            project_id=project_id,
            organization_id=organization_id,
            dense_similarity_top_k=dense_similarity_top_k,
            dense_similarity_cutoff=dense_similarity_cutoff,
            sparse_similarity_top_k=sparse_similarity_top_k,
            enable_reranking=enable_reranking,
            rerank_top_n=rerank_top_n,
            alpha=alpha,
            search_filters=search_filters,
            search_filters_inference_schema=search_filters_inference_schema,
            files_top_k=files_top_k,
            retrieval_mode=retrieval_mode,
            retrieve_image_nodes=retrieve_image_nodes,
            retrieve_page_screenshot_nodes=retrieve_page_screenshot_nodes,
            retrieve_page_figure_nodes=retrieve_page_figure_nodes,
            class_name=class_name,
            request_options=request_options,
        )
        return _response.data

    def get_playground_session(
        self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> PlaygroundSession:
        """
        Get a playground session for a user and pipeline.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PlaygroundSession
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipelines.get_playground_session(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._raw_client.get_playground_session(pipeline_id, request_options=request_options)
        return _response.data

    def chat_stream(
        self,
        pipeline_id: str,
        *,
        messages: typing.Optional[typing.Sequence[InputMessage]] = OMIT,
        data: typing.Optional[ChatData] = OMIT,
        class_name: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Iterator[ChatMessageChunk]:
        """
        Make a retrieval query + chat completion for a managed pipeline.

        Parameters
        ----------
        pipeline_id : str

        messages : typing.Optional[typing.Sequence[InputMessage]]

        data : typing.Optional[ChatData]

        class_name : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.Iterator[ChatMessageChunk]


        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        response = client.pipelines.chat_stream(
            pipeline_id="pipeline_id",
        )
        for chunk in response:
            yield chunk
        """
        with self._raw_client.chat_stream(
            pipeline_id, messages=messages, data=data, class_name=class_name, request_options=request_options
        ) as r:
            yield from r.data

    def chat(
        self,
        pipeline_id: str,
        *,
        messages: typing.Optional[typing.Sequence[InputMessage]] = OMIT,
        data: typing.Optional[ChatData] = OMIT,
        class_name: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ChatMessage:
        """
        Make a retrieval query + chat completion for a managed pipeline.

        Parameters
        ----------
        pipeline_id : str

        messages : typing.Optional[typing.Sequence[InputMessage]]

        data : typing.Optional[ChatData]

        class_name : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ChatMessage


        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipelines.chat(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._raw_client.chat(
            pipeline_id, messages=messages, data=data, class_name=class_name, request_options=request_options
        )
        return _response.data

    def list_documents(
        self,
        pipeline_id: str,
        *,
        skip: typing.Optional[int] = None,
        limit: typing.Optional[int] = None,
        file_id: typing.Optional[str] = None,
        only_direct_upload: typing.Optional[bool] = None,
        only_api_data_source_documents: typing.Optional[bool] = None,
        status_refresh_policy: typing.Optional[PipelinesListDocumentsRequestStatusRefreshPolicy] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[CloudDocument]:
        """
        Return a list of documents for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        skip : typing.Optional[int]

        limit : typing.Optional[int]

        file_id : typing.Optional[str]

        only_direct_upload : typing.Optional[bool]

        only_api_data_source_documents : typing.Optional[bool]

        status_refresh_policy : typing.Optional[PipelinesListDocumentsRequestStatusRefreshPolicy]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[CloudDocument]
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipelines.list_documents(
            pipeline_id="pipeline_id",
            skip=1,
            limit=1,
            file_id="file_id",
            only_direct_upload=True,
            only_api_data_source_documents=True,
            status_refresh_policy="cached",
        )
        """
        _response = self._raw_client.list_documents(
            pipeline_id,
            skip=skip,
            limit=limit,
            file_id=file_id,
            only_direct_upload=only_direct_upload,
            only_api_data_source_documents=only_api_data_source_documents,
            status_refresh_policy=status_refresh_policy,
            request_options=request_options,
        )
        return _response.data

    def create_batch_documents(
        self,
        pipeline_id: str,
        *,
        request: typing.Sequence[CloudDocumentCreate],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[CloudDocument]:
        """
        Batch create documents for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        request : typing.Sequence[CloudDocumentCreate]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[CloudDocument]
            Successful Response

        Examples
        --------
        from llamaindex_test import CloudDocumentCreate, LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipelines.create_batch_documents(
            pipeline_id="pipeline_id",
            request=[
                CloudDocumentCreate(
                    text="text",
                    metadata={"key": "value"},
                )
            ],
        )
        """
        _response = self._raw_client.create_batch_documents(
            pipeline_id, request=request, request_options=request_options
        )
        return _response.data

    def upsert_batch_documents(
        self,
        pipeline_id: str,
        *,
        request: typing.Sequence[CloudDocumentCreate],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[CloudDocument]:
        """
        Batch create or update a document for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        request : typing.Sequence[CloudDocumentCreate]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[CloudDocument]
            Successful Response

        Examples
        --------
        from llamaindex_test import CloudDocumentCreate, LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipelines.upsert_batch_documents(
            pipeline_id="pipeline_id",
            request=[
                CloudDocumentCreate(
                    text="text",
                    metadata={"key": "value"},
                )
            ],
        )
        """
        _response = self._raw_client.upsert_batch_documents(
            pipeline_id, request=request, request_options=request_options
        )
        return _response.data

    def list_documents_paginated(
        self,
        pipeline_id: str,
        *,
        skip: typing.Optional[int] = None,
        limit: typing.Optional[int] = None,
        file_id: typing.Optional[str] = None,
        only_direct_upload: typing.Optional[bool] = None,
        only_api_data_source_documents: typing.Optional[bool] = None,
        status_refresh_policy: typing.Optional[PipelinesListDocumentsPaginatedRequestStatusRefreshPolicy] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PaginatedListCloudDocumentsResponse:
        """
        Return a list of documents for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        skip : typing.Optional[int]

        limit : typing.Optional[int]

        file_id : typing.Optional[str]

        only_direct_upload : typing.Optional[bool]

        only_api_data_source_documents : typing.Optional[bool]

        status_refresh_policy : typing.Optional[PipelinesListDocumentsPaginatedRequestStatusRefreshPolicy]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PaginatedListCloudDocumentsResponse
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipelines.list_documents_paginated(
            pipeline_id="pipeline_id",
            skip=1,
            limit=1,
            file_id="file_id",
            only_direct_upload=True,
            only_api_data_source_documents=True,
            status_refresh_policy="cached",
        )
        """
        _response = self._raw_client.list_documents_paginated(
            pipeline_id,
            skip=skip,
            limit=limit,
            file_id=file_id,
            only_direct_upload=only_direct_upload,
            only_api_data_source_documents=only_api_data_source_documents,
            status_refresh_policy=status_refresh_policy,
            request_options=request_options,
        )
        return _response.data

    def get_document(
        self, pipeline_id: str, document_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> CloudDocument:
        """
        Return a single document for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        document_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        CloudDocument
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipelines.get_document(
            pipeline_id="pipeline_id",
            document_id="document_id",
        )
        """
        _response = self._raw_client.get_document(pipeline_id, document_id, request_options=request_options)
        return _response.data

    def delete_document(
        self, pipeline_id: str, document_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Delete a document from a pipeline.
        Initiates an async job that will:
        1. Delete vectors from the vector store
        2. Delete the document from MongoDB after vectors are successfully deleted

        Parameters
        ----------
        pipeline_id : str

        document_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipelines.delete_document(
            pipeline_id="pipeline_id",
            document_id="document_id",
        )
        """
        _response = self._raw_client.delete_document(pipeline_id, document_id, request_options=request_options)
        return _response.data

    def get_document_status(
        self, pipeline_id: str, document_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> ManagedIngestionStatusResponse:
        """
        Return a single document for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        document_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ManagedIngestionStatusResponse
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipelines.get_document_status(
            pipeline_id="pipeline_id",
            document_id="document_id",
        )
        """
        _response = self._raw_client.get_document_status(pipeline_id, document_id, request_options=request_options)
        return _response.data

    def sync_document(
        self, pipeline_id: str, document_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.Optional[typing.Any]:
        """
        Sync a specific document for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        document_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Optional[typing.Any]
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipelines.sync_document(
            pipeline_id="pipeline_id",
            document_id="document_id",
        )
        """
        _response = self._raw_client.sync_document(pipeline_id, document_id, request_options=request_options)
        return _response.data

    def list_document_chunks(
        self, pipeline_id: str, document_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[TextNode]:
        """
        Return a list of chunks for a pipeline document.

        Parameters
        ----------
        pipeline_id : str

        document_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[TextNode]
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipelines.list_document_chunks(
            pipeline_id="pipeline_id",
            document_id="document_id",
        )
        """
        _response = self._raw_client.list_document_chunks(pipeline_id, document_id, request_options=request_options)
        return _response.data

    def force_sync_all_documents(
        self,
        pipeline_id: str,
        *,
        batch_size: typing.Optional[int] = None,
        only_failed: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Optional[typing.Any]:
        """
        Force sync all documents in a pipeline by batching document ingestion jobs.

        - Iterates all document refs for the pipeline
        - Enqueues document ingestion jobs in batches of `batch_size`

        Parameters
        ----------
        pipeline_id : str

        batch_size : typing.Optional[int]

        only_failed : typing.Optional[bool]
            Only sync retriable documents (failed/cancelled/not-started/stalled-in-progress)

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Optional[typing.Any]
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipelines.force_sync_all_documents(
            pipeline_id="pipeline_id",
            batch_size=1,
            only_failed=True,
        )
        """
        _response = self._raw_client.force_sync_all_documents(
            pipeline_id, batch_size=batch_size, only_failed=only_failed, request_options=request_options
        )
        return _response.data


class AsyncPipelinesClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._raw_client = AsyncRawPipelinesClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> AsyncRawPipelinesClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        AsyncRawPipelinesClient
        """
        return self._raw_client

    async def search(
        self,
        *,
        project_id: typing.Optional[str] = None,
        project_name: typing.Optional[str] = None,
        pipeline_name: typing.Optional[str] = None,
        pipeline_type: typing.Optional[PipelineType] = None,
        organization_id: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[Pipeline]:
        """
        Search for pipelines by various parameters.

        Parameters
        ----------
        project_id : typing.Optional[str]

        project_name : typing.Optional[str]

        pipeline_name : typing.Optional[str]

        pipeline_type : typing.Optional[PipelineType]

        organization_id : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[Pipeline]
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipelines.search(
                project_id="project_id",
                project_name="project_name",
                pipeline_name="pipeline_name",
                pipeline_type="PLAYGROUND",
                organization_id="organization_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.search(
            project_id=project_id,
            project_name=project_name,
            pipeline_name=pipeline_name,
            pipeline_type=pipeline_type,
            organization_id=organization_id,
            request_options=request_options,
        )
        return _response.data

    async def create(
        self,
        *,
        name: str,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        embedding_config: typing.Optional[PipelineCreateEmbeddingConfig] = OMIT,
        transform_config: typing.Optional[PipelineCreateTransformConfig] = OMIT,
        sparse_model_config: typing.Optional[SparseModelConfig] = OMIT,
        data_sink_id: typing.Optional[str] = OMIT,
        embedding_model_config_id: typing.Optional[str] = OMIT,
        data_sink: typing.Optional[DataSinkCreate] = OMIT,
        preset_retrieval_parameters: typing.Optional[PresetRetrievalParams] = OMIT,
        llama_parse_parameters: typing.Optional[LlamaParseParameters] = OMIT,
        status: typing.Optional[str] = OMIT,
        metadata_config: typing.Optional[PipelineMetadataConfig] = OMIT,
        pipeline_type: typing.Optional[PipelineType] = OMIT,
        managed_pipeline_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Pipeline:
        """
        Create a new pipeline for a project.

        Parameters
        ----------
        name : str

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        embedding_config : typing.Optional[PipelineCreateEmbeddingConfig]

        transform_config : typing.Optional[PipelineCreateTransformConfig]
            Configuration for the transformation.

        sparse_model_config : typing.Optional[SparseModelConfig]
            Configuration for the sparse model used in hybrid search.

        data_sink_id : typing.Optional[str]
            Data sink ID. When provided instead of data_sink, the data sink will be looked up by ID.

        embedding_model_config_id : typing.Optional[str]
            Embedding model config ID. When provided instead of embedding_config, the embedding model config will be looked up by ID.

        data_sink : typing.Optional[DataSinkCreate]
            Data sink. When provided instead of data_sink_id, the data sink will be created.

        preset_retrieval_parameters : typing.Optional[PresetRetrievalParams]
            Preset retrieval parameters for the pipeline.

        llama_parse_parameters : typing.Optional[LlamaParseParameters]

        status : typing.Optional[str]
            Status of the pipeline deployment.

        metadata_config : typing.Optional[PipelineMetadataConfig]
            Metadata configuration for the pipeline.

        pipeline_type : typing.Optional[PipelineType]
            Type of pipeline. Either PLAYGROUND or MANAGED.

        managed_pipeline_id : typing.Optional[str]
            The ID of the ManagedPipeline this playground pipeline is linked to.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipelines.create(
                project_id="project_id",
                organization_id="organization_id",
                name="name",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.create(
            name=name,
            project_id=project_id,
            organization_id=organization_id,
            embedding_config=embedding_config,
            transform_config=transform_config,
            sparse_model_config=sparse_model_config,
            data_sink_id=data_sink_id,
            embedding_model_config_id=embedding_model_config_id,
            data_sink=data_sink,
            preset_retrieval_parameters=preset_retrieval_parameters,
            llama_parse_parameters=llama_parse_parameters,
            status=status,
            metadata_config=metadata_config,
            pipeline_type=pipeline_type,
            managed_pipeline_id=managed_pipeline_id,
            request_options=request_options,
        )
        return _response.data

    async def upsert(
        self,
        *,
        name: str,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        embedding_config: typing.Optional[PipelineCreateEmbeddingConfig] = OMIT,
        transform_config: typing.Optional[PipelineCreateTransformConfig] = OMIT,
        sparse_model_config: typing.Optional[SparseModelConfig] = OMIT,
        data_sink_id: typing.Optional[str] = OMIT,
        embedding_model_config_id: typing.Optional[str] = OMIT,
        data_sink: typing.Optional[DataSinkCreate] = OMIT,
        preset_retrieval_parameters: typing.Optional[PresetRetrievalParams] = OMIT,
        llama_parse_parameters: typing.Optional[LlamaParseParameters] = OMIT,
        status: typing.Optional[str] = OMIT,
        metadata_config: typing.Optional[PipelineMetadataConfig] = OMIT,
        pipeline_type: typing.Optional[PipelineType] = OMIT,
        managed_pipeline_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Pipeline:
        """
        Upsert a pipeline for a project.
        Updates if a pipeline with the same name and project_id already exists. Otherwise, creates a new pipeline.

        Parameters
        ----------
        name : str

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        embedding_config : typing.Optional[PipelineCreateEmbeddingConfig]

        transform_config : typing.Optional[PipelineCreateTransformConfig]
            Configuration for the transformation.

        sparse_model_config : typing.Optional[SparseModelConfig]
            Configuration for the sparse model used in hybrid search.

        data_sink_id : typing.Optional[str]
            Data sink ID. When provided instead of data_sink, the data sink will be looked up by ID.

        embedding_model_config_id : typing.Optional[str]
            Embedding model config ID. When provided instead of embedding_config, the embedding model config will be looked up by ID.

        data_sink : typing.Optional[DataSinkCreate]
            Data sink. When provided instead of data_sink_id, the data sink will be created.

        preset_retrieval_parameters : typing.Optional[PresetRetrievalParams]
            Preset retrieval parameters for the pipeline.

        llama_parse_parameters : typing.Optional[LlamaParseParameters]

        status : typing.Optional[str]
            Status of the pipeline deployment.

        metadata_config : typing.Optional[PipelineMetadataConfig]
            Metadata configuration for the pipeline.

        pipeline_type : typing.Optional[PipelineType]
            Type of pipeline. Either PLAYGROUND or MANAGED.

        managed_pipeline_id : typing.Optional[str]
            The ID of the ManagedPipeline this playground pipeline is linked to.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipelines.upsert(
                project_id="project_id",
                organization_id="organization_id",
                name="name",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.upsert(
            name=name,
            project_id=project_id,
            organization_id=organization_id,
            embedding_config=embedding_config,
            transform_config=transform_config,
            sparse_model_config=sparse_model_config,
            data_sink_id=data_sink_id,
            embedding_model_config_id=embedding_model_config_id,
            data_sink=data_sink,
            preset_retrieval_parameters=preset_retrieval_parameters,
            llama_parse_parameters=llama_parse_parameters,
            status=status,
            metadata_config=metadata_config,
            pipeline_type=pipeline_type,
            managed_pipeline_id=managed_pipeline_id,
            request_options=request_options,
        )
        return _response.data

    async def get(self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None) -> Pipeline:
        """
        Get a pipeline by ID for a given project.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipelines.get(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.get(pipeline_id, request_options=request_options)
        return _response.data

    async def update(
        self,
        pipeline_id: str,
        *,
        embedding_config: typing.Optional[PipelineUpdateEmbeddingConfig] = OMIT,
        transform_config: typing.Optional[PipelineUpdateTransformConfig] = OMIT,
        sparse_model_config: typing.Optional[SparseModelConfig] = OMIT,
        data_sink_id: typing.Optional[str] = OMIT,
        embedding_model_config_id: typing.Optional[str] = OMIT,
        data_sink: typing.Optional[DataSinkCreate] = OMIT,
        preset_retrieval_parameters: typing.Optional[PresetRetrievalParams] = OMIT,
        llama_parse_parameters: typing.Optional[LlamaParseParameters] = OMIT,
        status: typing.Optional[str] = OMIT,
        metadata_config: typing.Optional[PipelineMetadataConfig] = OMIT,
        name: typing.Optional[str] = OMIT,
        managed_pipeline_id: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> Pipeline:
        """
        Update an existing pipeline for a project.

        Parameters
        ----------
        pipeline_id : str

        embedding_config : typing.Optional[PipelineUpdateEmbeddingConfig]

        transform_config : typing.Optional[PipelineUpdateTransformConfig]
            Configuration for the transformation.

        sparse_model_config : typing.Optional[SparseModelConfig]
            Configuration for the sparse model used in hybrid search.

        data_sink_id : typing.Optional[str]
            Data sink ID. When provided instead of data_sink, the data sink will be looked up by ID.

        embedding_model_config_id : typing.Optional[str]
            Embedding model config ID. When provided instead of embedding_config, the embedding model config will be looked up by ID.

        data_sink : typing.Optional[DataSinkCreate]
            Data sink. When provided instead of data_sink_id, the data sink will be created.

        preset_retrieval_parameters : typing.Optional[PresetRetrievalParams]
            Preset retrieval parameters for the pipeline.

        llama_parse_parameters : typing.Optional[LlamaParseParameters]
            Settings that can be configured for how to use LlamaParse to parse files within a LlamaCloud pipeline.

        status : typing.Optional[str]
            Status of the pipeline deployment.

        metadata_config : typing.Optional[PipelineMetadataConfig]
            Metadata configuration for the pipeline.

        name : typing.Optional[str]

        managed_pipeline_id : typing.Optional[str]
            The ID of the ManagedPipeline this playground pipeline is linked to.

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipelines.update(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.update(
            pipeline_id,
            embedding_config=embedding_config,
            transform_config=transform_config,
            sparse_model_config=sparse_model_config,
            data_sink_id=data_sink_id,
            embedding_model_config_id=embedding_model_config_id,
            data_sink=data_sink,
            preset_retrieval_parameters=preset_retrieval_parameters,
            llama_parse_parameters=llama_parse_parameters,
            status=status,
            metadata_config=metadata_config,
            name=name,
            managed_pipeline_id=managed_pipeline_id,
            request_options=request_options,
        )
        return _response.data

    async def delete(self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Delete a pipeline by ID.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipelines.delete(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.delete(pipeline_id, request_options=request_options)
        return _response.data

    async def get_status(
        self,
        pipeline_id: str,
        *,
        full_details: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ManagedIngestionStatusResponse:
        """
        Get the status of a pipeline by ID.

        Parameters
        ----------
        pipeline_id : str

        full_details : typing.Optional[bool]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ManagedIngestionStatusResponse
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipelines.get_status(
                pipeline_id="pipeline_id",
                full_details=True,
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.get_status(
            pipeline_id, full_details=full_details, request_options=request_options
        )
        return _response.data

    async def sync(self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None) -> Pipeline:
        """
        Run ingestion for the pipeline by incrementally updating the data-sink with upstream changes from data-sources & files.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipelines.sync(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.sync(pipeline_id, request_options=request_options)
        return _response.data

    async def cancel_sync(
        self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> Pipeline:
        """
        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipelines.cancel_sync(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.cancel_sync(pipeline_id, request_options=request_options)
        return _response.data

    async def force_delete(self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipelines.force_delete(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.force_delete(pipeline_id, request_options=request_options)
        return _response.data

    async def copy(self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None) -> Pipeline:
        """
        Copy a pipeline by ID.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        Pipeline
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipelines.copy(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.copy(pipeline_id, request_options=request_options)
        return _response.data

    async def retrieve(
        self,
        pipeline_id: str,
        *,
        query: str,
        project_id: typing.Optional[str] = None,
        organization_id: typing.Optional[str] = None,
        dense_similarity_top_k: typing.Optional[int] = OMIT,
        dense_similarity_cutoff: typing.Optional[float] = OMIT,
        sparse_similarity_top_k: typing.Optional[int] = OMIT,
        enable_reranking: typing.Optional[bool] = OMIT,
        rerank_top_n: typing.Optional[int] = OMIT,
        alpha: typing.Optional[float] = OMIT,
        search_filters: typing.Optional[MetadataFilters] = OMIT,
        search_filters_inference_schema: typing.Optional[
            typing.Dict[str, typing.Optional[RetrievalParamsSearchFiltersInferenceSchemaValue]]
        ] = OMIT,
        files_top_k: typing.Optional[int] = OMIT,
        retrieval_mode: typing.Optional[RetrievalMode] = OMIT,
        retrieve_image_nodes: typing.Optional[bool] = OMIT,
        retrieve_page_screenshot_nodes: typing.Optional[bool] = OMIT,
        retrieve_page_figure_nodes: typing.Optional[bool] = OMIT,
        class_name: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> RetrieveResults:
        """
        Get retrieval results for a managed pipeline and a query

        Parameters
        ----------
        pipeline_id : str

        query : str
            The query to retrieve against.

        project_id : typing.Optional[str]

        organization_id : typing.Optional[str]

        dense_similarity_top_k : typing.Optional[int]
            Number of nodes for dense retrieval.

        dense_similarity_cutoff : typing.Optional[float]
            Minimum similarity score wrt query for retrieval

        sparse_similarity_top_k : typing.Optional[int]
            Number of nodes for sparse retrieval.

        enable_reranking : typing.Optional[bool]
            Enable reranking for retrieval

        rerank_top_n : typing.Optional[int]
            Number of reranked nodes for returning.

        alpha : typing.Optional[float]
            Alpha value for hybrid retrieval to determine the weights between dense and sparse retrieval. 0 is sparse retrieval and 1 is dense retrieval.

        search_filters : typing.Optional[MetadataFilters]
            Search filters for retrieval.

        search_filters_inference_schema : typing.Optional[typing.Dict[str, typing.Optional[RetrievalParamsSearchFiltersInferenceSchemaValue]]]
            JSON Schema that will be used to infer search_filters. Omit or leave as null to skip inference.

        files_top_k : typing.Optional[int]
            Number of files to retrieve (only for retrieval mode files_via_metadata and files_via_content).

        retrieval_mode : typing.Optional[RetrievalMode]
            The retrieval mode for the query.

        retrieve_image_nodes : typing.Optional[bool]
            Whether to retrieve image nodes.

        retrieve_page_screenshot_nodes : typing.Optional[bool]
            Whether to retrieve page screenshot nodes.

        retrieve_page_figure_nodes : typing.Optional[bool]
            Whether to retrieve page figure nodes.

        class_name : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        RetrieveResults
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipelines.retrieve(
                pipeline_id="pipeline_id",
                project_id="project_id",
                organization_id="organization_id",
                query="query",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.retrieve(
            pipeline_id,
            query=query,
            project_id=project_id,
            organization_id=organization_id,
            dense_similarity_top_k=dense_similarity_top_k,
            dense_similarity_cutoff=dense_similarity_cutoff,
            sparse_similarity_top_k=sparse_similarity_top_k,
            enable_reranking=enable_reranking,
            rerank_top_n=rerank_top_n,
            alpha=alpha,
            search_filters=search_filters,
            search_filters_inference_schema=search_filters_inference_schema,
            files_top_k=files_top_k,
            retrieval_mode=retrieval_mode,
            retrieve_image_nodes=retrieve_image_nodes,
            retrieve_page_screenshot_nodes=retrieve_page_screenshot_nodes,
            retrieve_page_figure_nodes=retrieve_page_figure_nodes,
            class_name=class_name,
            request_options=request_options,
        )
        return _response.data

    async def get_playground_session(
        self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> PlaygroundSession:
        """
        Get a playground session for a user and pipeline.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PlaygroundSession
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipelines.get_playground_session(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.get_playground_session(pipeline_id, request_options=request_options)
        return _response.data

    async def chat_stream(
        self,
        pipeline_id: str,
        *,
        messages: typing.Optional[typing.Sequence[InputMessage]] = OMIT,
        data: typing.Optional[ChatData] = OMIT,
        class_name: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.AsyncIterator[ChatMessageChunk]:
        """
        Make a retrieval query + chat completion for a managed pipeline.

        Parameters
        ----------
        pipeline_id : str

        messages : typing.Optional[typing.Sequence[InputMessage]]

        data : typing.Optional[ChatData]

        class_name : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Yields
        ------
        typing.AsyncIterator[ChatMessageChunk]


        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            response = await client.pipelines.chat_stream(
                pipeline_id="pipeline_id",
            )
            async for chunk in response:
                yield chunk


        asyncio.run(main())
        """
        async with self._raw_client.chat_stream(
            pipeline_id, messages=messages, data=data, class_name=class_name, request_options=request_options
        ) as r:
            async for _chunk in r.data:
                yield _chunk

    async def chat(
        self,
        pipeline_id: str,
        *,
        messages: typing.Optional[typing.Sequence[InputMessage]] = OMIT,
        data: typing.Optional[ChatData] = OMIT,
        class_name: typing.Optional[str] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> ChatMessage:
        """
        Make a retrieval query + chat completion for a managed pipeline.

        Parameters
        ----------
        pipeline_id : str

        messages : typing.Optional[typing.Sequence[InputMessage]]

        data : typing.Optional[ChatData]

        class_name : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ChatMessage


        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipelines.chat(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.chat(
            pipeline_id, messages=messages, data=data, class_name=class_name, request_options=request_options
        )
        return _response.data

    async def list_documents(
        self,
        pipeline_id: str,
        *,
        skip: typing.Optional[int] = None,
        limit: typing.Optional[int] = None,
        file_id: typing.Optional[str] = None,
        only_direct_upload: typing.Optional[bool] = None,
        only_api_data_source_documents: typing.Optional[bool] = None,
        status_refresh_policy: typing.Optional[PipelinesListDocumentsRequestStatusRefreshPolicy] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[CloudDocument]:
        """
        Return a list of documents for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        skip : typing.Optional[int]

        limit : typing.Optional[int]

        file_id : typing.Optional[str]

        only_direct_upload : typing.Optional[bool]

        only_api_data_source_documents : typing.Optional[bool]

        status_refresh_policy : typing.Optional[PipelinesListDocumentsRequestStatusRefreshPolicy]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[CloudDocument]
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipelines.list_documents(
                pipeline_id="pipeline_id",
                skip=1,
                limit=1,
                file_id="file_id",
                only_direct_upload=True,
                only_api_data_source_documents=True,
                status_refresh_policy="cached",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.list_documents(
            pipeline_id,
            skip=skip,
            limit=limit,
            file_id=file_id,
            only_direct_upload=only_direct_upload,
            only_api_data_source_documents=only_api_data_source_documents,
            status_refresh_policy=status_refresh_policy,
            request_options=request_options,
        )
        return _response.data

    async def create_batch_documents(
        self,
        pipeline_id: str,
        *,
        request: typing.Sequence[CloudDocumentCreate],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[CloudDocument]:
        """
        Batch create documents for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        request : typing.Sequence[CloudDocumentCreate]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[CloudDocument]
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud, CloudDocumentCreate

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipelines.create_batch_documents(
                pipeline_id="pipeline_id",
                request=[
                    CloudDocumentCreate(
                        text="text",
                        metadata={"key": "value"},
                    )
                ],
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.create_batch_documents(
            pipeline_id, request=request, request_options=request_options
        )
        return _response.data

    async def upsert_batch_documents(
        self,
        pipeline_id: str,
        *,
        request: typing.Sequence[CloudDocumentCreate],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[CloudDocument]:
        """
        Batch create or update a document for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        request : typing.Sequence[CloudDocumentCreate]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[CloudDocument]
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud, CloudDocumentCreate

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipelines.upsert_batch_documents(
                pipeline_id="pipeline_id",
                request=[
                    CloudDocumentCreate(
                        text="text",
                        metadata={"key": "value"},
                    )
                ],
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.upsert_batch_documents(
            pipeline_id, request=request, request_options=request_options
        )
        return _response.data

    async def list_documents_paginated(
        self,
        pipeline_id: str,
        *,
        skip: typing.Optional[int] = None,
        limit: typing.Optional[int] = None,
        file_id: typing.Optional[str] = None,
        only_direct_upload: typing.Optional[bool] = None,
        only_api_data_source_documents: typing.Optional[bool] = None,
        status_refresh_policy: typing.Optional[PipelinesListDocumentsPaginatedRequestStatusRefreshPolicy] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PaginatedListCloudDocumentsResponse:
        """
        Return a list of documents for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        skip : typing.Optional[int]

        limit : typing.Optional[int]

        file_id : typing.Optional[str]

        only_direct_upload : typing.Optional[bool]

        only_api_data_source_documents : typing.Optional[bool]

        status_refresh_policy : typing.Optional[PipelinesListDocumentsPaginatedRequestStatusRefreshPolicy]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PaginatedListCloudDocumentsResponse
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipelines.list_documents_paginated(
                pipeline_id="pipeline_id",
                skip=1,
                limit=1,
                file_id="file_id",
                only_direct_upload=True,
                only_api_data_source_documents=True,
                status_refresh_policy="cached",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.list_documents_paginated(
            pipeline_id,
            skip=skip,
            limit=limit,
            file_id=file_id,
            only_direct_upload=only_direct_upload,
            only_api_data_source_documents=only_api_data_source_documents,
            status_refresh_policy=status_refresh_policy,
            request_options=request_options,
        )
        return _response.data

    async def get_document(
        self, pipeline_id: str, document_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> CloudDocument:
        """
        Return a single document for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        document_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        CloudDocument
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipelines.get_document(
                pipeline_id="pipeline_id",
                document_id="document_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.get_document(pipeline_id, document_id, request_options=request_options)
        return _response.data

    async def delete_document(
        self, pipeline_id: str, document_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Delete a document from a pipeline.
        Initiates an async job that will:
        1. Delete vectors from the vector store
        2. Delete the document from MongoDB after vectors are successfully deleted

        Parameters
        ----------
        pipeline_id : str

        document_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipelines.delete_document(
                pipeline_id="pipeline_id",
                document_id="document_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.delete_document(pipeline_id, document_id, request_options=request_options)
        return _response.data

    async def get_document_status(
        self, pipeline_id: str, document_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> ManagedIngestionStatusResponse:
        """
        Return a single document for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        document_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ManagedIngestionStatusResponse
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipelines.get_document_status(
                pipeline_id="pipeline_id",
                document_id="document_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.get_document_status(
            pipeline_id, document_id, request_options=request_options
        )
        return _response.data

    async def sync_document(
        self, pipeline_id: str, document_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.Optional[typing.Any]:
        """
        Sync a specific document for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        document_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Optional[typing.Any]
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipelines.sync_document(
                pipeline_id="pipeline_id",
                document_id="document_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.sync_document(pipeline_id, document_id, request_options=request_options)
        return _response.data

    async def list_document_chunks(
        self, pipeline_id: str, document_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.List[TextNode]:
        """
        Return a list of chunks for a pipeline document.

        Parameters
        ----------
        pipeline_id : str

        document_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[TextNode]
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipelines.list_document_chunks(
                pipeline_id="pipeline_id",
                document_id="document_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.list_document_chunks(
            pipeline_id, document_id, request_options=request_options
        )
        return _response.data

    async def force_sync_all_documents(
        self,
        pipeline_id: str,
        *,
        batch_size: typing.Optional[int] = None,
        only_failed: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.Optional[typing.Any]:
        """
        Force sync all documents in a pipeline by batching document ingestion jobs.

        - Iterates all document refs for the pipeline
        - Enqueues document ingestion jobs in batches of `batch_size`

        Parameters
        ----------
        pipeline_id : str

        batch_size : typing.Optional[int]

        only_failed : typing.Optional[bool]
            Only sync retriable documents (failed/cancelled/not-started/stalled-in-progress)

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Optional[typing.Any]
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipelines.force_sync_all_documents(
                pipeline_id="pipeline_id",
                batch_size=1,
                only_failed=True,
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.force_sync_all_documents(
            pipeline_id, batch_size=batch_size, only_failed=only_failed, request_options=request_options
        )
        return _response.data
