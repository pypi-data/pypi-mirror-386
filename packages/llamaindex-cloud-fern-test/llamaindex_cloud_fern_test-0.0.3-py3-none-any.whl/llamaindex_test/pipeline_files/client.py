# This file was auto-generated by Fern from our API Definition.

import typing

from .. import core
from ..core.client_wrapper import AsyncClientWrapper, SyncClientWrapper
from ..core.request_options import RequestOptions
from ..types.file_count_by_status_response import FileCountByStatusResponse
from ..types.managed_ingestion_status_response import ManagedIngestionStatusResponse
from ..types.paginated_list_pipeline_files_response import PaginatedListPipelineFilesResponse
from ..types.pipeline_file import PipelineFile
from ..types.pipeline_file_create import PipelineFileCreate
from .raw_client import AsyncRawPipelineFilesClient, RawPipelineFilesClient
from .types.pipeline_file_update_custom_metadata_value import PipelineFileUpdateCustomMetadataValue

# this is used as the default value for optional parameters
OMIT = typing.cast(typing.Any, ...)


class PipelineFilesClient:
    def __init__(self, *, client_wrapper: SyncClientWrapper):
        self._raw_client = RawPipelineFilesClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> RawPipelineFilesClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        RawPipelineFilesClient
        """
        return self._raw_client

    def list(
        self,
        pipeline_id: str,
        *,
        data_source_id: typing.Optional[str] = None,
        only_manually_uploaded: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[PipelineFile]:
        """
        Get files for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        data_source_id : typing.Optional[str]

        only_manually_uploaded : typing.Optional[bool]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[PipelineFile]
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipeline_files.list(
            pipeline_id="pipeline_id",
            data_source_id="data_source_id",
            only_manually_uploaded=True,
        )
        """
        _response = self._raw_client.list(
            pipeline_id,
            data_source_id=data_source_id,
            only_manually_uploaded=only_manually_uploaded,
            request_options=request_options,
        )
        return _response.data

    def add(
        self,
        pipeline_id: str,
        *,
        request: typing.Sequence[PipelineFileCreate],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[PipelineFile]:
        """
        Add files to a pipeline.

        Parameters
        ----------
        pipeline_id : str

        request : typing.Sequence[PipelineFileCreate]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[PipelineFile]
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud, PipelineFileCreate

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipeline_files.add(
            pipeline_id="pipeline_id",
            request=[
                PipelineFileCreate(
                    file_id="file_id",
                )
            ],
        )
        """
        _response = self._raw_client.add(pipeline_id, request=request, request_options=request_options)
        return _response.data

    def list_2(
        self,
        pipeline_id: str,
        *,
        data_source_id: typing.Optional[str] = None,
        only_manually_uploaded: typing.Optional[bool] = None,
        file_name_contains: typing.Optional[str] = None,
        limit: typing.Optional[int] = None,
        offset: typing.Optional[int] = None,
        order_by: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PaginatedListPipelineFilesResponse:
        """
        Get files for a pipeline.

        Args:
            pipeline_id: ID of the pipeline
            data_source_id: Optional filter by data source ID
            only_manually_uploaded: Filter for only manually uploaded files
            file_name_contains: Optional filter by file name (substring match)
            limit: Limit number of results
            offset: Offset for pagination
            order_by: Field to order by

        Parameters
        ----------
        pipeline_id : str

        data_source_id : typing.Optional[str]

        only_manually_uploaded : typing.Optional[bool]

        file_name_contains : typing.Optional[str]

        limit : typing.Optional[int]

        offset : typing.Optional[int]

        order_by : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PaginatedListPipelineFilesResponse
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipeline_files.list_2(
            pipeline_id="pipeline_id",
            data_source_id="data_source_id",
            only_manually_uploaded=True,
            file_name_contains="file_name_contains",
            limit=1,
            offset=1,
            order_by="order_by",
        )
        """
        _response = self._raw_client.list_2(
            pipeline_id,
            data_source_id=data_source_id,
            only_manually_uploaded=only_manually_uploaded,
            file_name_contains=file_name_contains,
            limit=limit,
            offset=offset,
            order_by=order_by,
            request_options=request_options,
        )
        return _response.data

    def get_status_counts(
        self,
        pipeline_id: str,
        *,
        data_source_id: typing.Optional[str] = None,
        only_manually_uploaded: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> FileCountByStatusResponse:
        """
        Get files for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        data_source_id : typing.Optional[str]

        only_manually_uploaded : typing.Optional[bool]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        FileCountByStatusResponse
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipeline_files.get_status_counts(
            pipeline_id="pipeline_id",
            data_source_id="data_source_id",
            only_manually_uploaded=True,
        )
        """
        _response = self._raw_client.get_status_counts(
            pipeline_id,
            data_source_id=data_source_id,
            only_manually_uploaded=only_manually_uploaded,
            request_options=request_options,
        )
        return _response.data

    def get_status(
        self, pipeline_id: str, file_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> ManagedIngestionStatusResponse:
        """
        Get status of a file for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        file_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ManagedIngestionStatusResponse
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipeline_files.get_status(
            pipeline_id="pipeline_id",
            file_id="file_id",
        )
        """
        _response = self._raw_client.get_status(pipeline_id, file_id, request_options=request_options)
        return _response.data

    def update(
        self,
        pipeline_id: str,
        file_id: str,
        *,
        custom_metadata: typing.Optional[
            typing.Dict[str, typing.Optional[PipelineFileUpdateCustomMetadataValue]]
        ] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PipelineFile:
        """
        Update a file for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        file_id : str

        custom_metadata : typing.Optional[typing.Dict[str, typing.Optional[PipelineFileUpdateCustomMetadataValue]]]
            Custom metadata for the file

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PipelineFile
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipeline_files.update(
            pipeline_id="pipeline_id",
            file_id="file_id",
        )
        """
        _response = self._raw_client.update(
            pipeline_id, file_id, custom_metadata=custom_metadata, request_options=request_options
        )
        return _response.data

    def delete(
        self, pipeline_id: str, file_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Delete a file from a pipeline.

        Parameters
        ----------
        pipeline_id : str

        file_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipeline_files.delete(
            pipeline_id="pipeline_id",
            file_id="file_id",
        )
        """
        _response = self._raw_client.delete(pipeline_id, file_id, request_options=request_options)
        return _response.data

    def import_metadata(
        self, pipeline_id: str, *, upload_file: core.File, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.Dict[str, str]:
        """
        Import metadata for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        upload_file : core.File
            See core.File for more documentation

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Dict[str, str]
            Successful Response

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipeline_files.import_metadata(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._raw_client.import_metadata(
            pipeline_id, upload_file=upload_file, request_options=request_options
        )
        return _response.data

    def delete_metadata(self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None) -> None:
        """
        Delete metadata for all files in a pipeline.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        from llamaindex_test import LlamaCloud

        client = LlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )
        client.pipeline_files.delete_metadata(
            pipeline_id="pipeline_id",
        )
        """
        _response = self._raw_client.delete_metadata(pipeline_id, request_options=request_options)
        return _response.data


class AsyncPipelineFilesClient:
    def __init__(self, *, client_wrapper: AsyncClientWrapper):
        self._raw_client = AsyncRawPipelineFilesClient(client_wrapper=client_wrapper)

    @property
    def with_raw_response(self) -> AsyncRawPipelineFilesClient:
        """
        Retrieves a raw implementation of this client that returns raw responses.

        Returns
        -------
        AsyncRawPipelineFilesClient
        """
        return self._raw_client

    async def list(
        self,
        pipeline_id: str,
        *,
        data_source_id: typing.Optional[str] = None,
        only_manually_uploaded: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[PipelineFile]:
        """
        Get files for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        data_source_id : typing.Optional[str]

        only_manually_uploaded : typing.Optional[bool]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[PipelineFile]
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipeline_files.list(
                pipeline_id="pipeline_id",
                data_source_id="data_source_id",
                only_manually_uploaded=True,
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.list(
            pipeline_id,
            data_source_id=data_source_id,
            only_manually_uploaded=only_manually_uploaded,
            request_options=request_options,
        )
        return _response.data

    async def add(
        self,
        pipeline_id: str,
        *,
        request: typing.Sequence[PipelineFileCreate],
        request_options: typing.Optional[RequestOptions] = None,
    ) -> typing.List[PipelineFile]:
        """
        Add files to a pipeline.

        Parameters
        ----------
        pipeline_id : str

        request : typing.Sequence[PipelineFileCreate]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.List[PipelineFile]
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud, PipelineFileCreate

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipeline_files.add(
                pipeline_id="pipeline_id",
                request=[
                    PipelineFileCreate(
                        file_id="file_id",
                    )
                ],
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.add(pipeline_id, request=request, request_options=request_options)
        return _response.data

    async def list_2(
        self,
        pipeline_id: str,
        *,
        data_source_id: typing.Optional[str] = None,
        only_manually_uploaded: typing.Optional[bool] = None,
        file_name_contains: typing.Optional[str] = None,
        limit: typing.Optional[int] = None,
        offset: typing.Optional[int] = None,
        order_by: typing.Optional[str] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PaginatedListPipelineFilesResponse:
        """
        Get files for a pipeline.

        Args:
            pipeline_id: ID of the pipeline
            data_source_id: Optional filter by data source ID
            only_manually_uploaded: Filter for only manually uploaded files
            file_name_contains: Optional filter by file name (substring match)
            limit: Limit number of results
            offset: Offset for pagination
            order_by: Field to order by

        Parameters
        ----------
        pipeline_id : str

        data_source_id : typing.Optional[str]

        only_manually_uploaded : typing.Optional[bool]

        file_name_contains : typing.Optional[str]

        limit : typing.Optional[int]

        offset : typing.Optional[int]

        order_by : typing.Optional[str]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PaginatedListPipelineFilesResponse
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipeline_files.list_2(
                pipeline_id="pipeline_id",
                data_source_id="data_source_id",
                only_manually_uploaded=True,
                file_name_contains="file_name_contains",
                limit=1,
                offset=1,
                order_by="order_by",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.list_2(
            pipeline_id,
            data_source_id=data_source_id,
            only_manually_uploaded=only_manually_uploaded,
            file_name_contains=file_name_contains,
            limit=limit,
            offset=offset,
            order_by=order_by,
            request_options=request_options,
        )
        return _response.data

    async def get_status_counts(
        self,
        pipeline_id: str,
        *,
        data_source_id: typing.Optional[str] = None,
        only_manually_uploaded: typing.Optional[bool] = None,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> FileCountByStatusResponse:
        """
        Get files for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        data_source_id : typing.Optional[str]

        only_manually_uploaded : typing.Optional[bool]

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        FileCountByStatusResponse
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipeline_files.get_status_counts(
                pipeline_id="pipeline_id",
                data_source_id="data_source_id",
                only_manually_uploaded=True,
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.get_status_counts(
            pipeline_id,
            data_source_id=data_source_id,
            only_manually_uploaded=only_manually_uploaded,
            request_options=request_options,
        )
        return _response.data

    async def get_status(
        self, pipeline_id: str, file_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> ManagedIngestionStatusResponse:
        """
        Get status of a file for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        file_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        ManagedIngestionStatusResponse
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipeline_files.get_status(
                pipeline_id="pipeline_id",
                file_id="file_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.get_status(pipeline_id, file_id, request_options=request_options)
        return _response.data

    async def update(
        self,
        pipeline_id: str,
        file_id: str,
        *,
        custom_metadata: typing.Optional[
            typing.Dict[str, typing.Optional[PipelineFileUpdateCustomMetadataValue]]
        ] = OMIT,
        request_options: typing.Optional[RequestOptions] = None,
    ) -> PipelineFile:
        """
        Update a file for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        file_id : str

        custom_metadata : typing.Optional[typing.Dict[str, typing.Optional[PipelineFileUpdateCustomMetadataValue]]]
            Custom metadata for the file

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        PipelineFile
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipeline_files.update(
                pipeline_id="pipeline_id",
                file_id="file_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.update(
            pipeline_id, file_id, custom_metadata=custom_metadata, request_options=request_options
        )
        return _response.data

    async def delete(
        self, pipeline_id: str, file_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Delete a file from a pipeline.

        Parameters
        ----------
        pipeline_id : str

        file_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipeline_files.delete(
                pipeline_id="pipeline_id",
                file_id="file_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.delete(pipeline_id, file_id, request_options=request_options)
        return _response.data

    async def import_metadata(
        self, pipeline_id: str, *, upload_file: core.File, request_options: typing.Optional[RequestOptions] = None
    ) -> typing.Dict[str, str]:
        """
        Import metadata for a pipeline.

        Parameters
        ----------
        pipeline_id : str

        upload_file : core.File
            See core.File for more documentation

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        typing.Dict[str, str]
            Successful Response

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipeline_files.import_metadata(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.import_metadata(
            pipeline_id, upload_file=upload_file, request_options=request_options
        )
        return _response.data

    async def delete_metadata(
        self, pipeline_id: str, *, request_options: typing.Optional[RequestOptions] = None
    ) -> None:
        """
        Delete metadata for all files in a pipeline.

        Parameters
        ----------
        pipeline_id : str

        request_options : typing.Optional[RequestOptions]
            Request-specific configuration.

        Returns
        -------
        None

        Examples
        --------
        import asyncio

        from llamaindex_test import AsyncLlamaCloud

        client = AsyncLlamaCloud(
            token="YOUR_TOKEN",
            base_url="https://yourhost.com/path/to/api",
        )


        async def main() -> None:
            await client.pipeline_files.delete_metadata(
                pipeline_id="pipeline_id",
            )


        asyncio.run(main())
        """
        _response = await self._raw_client.delete_metadata(pipeline_id, request_options=request_options)
        return _response.data
