#pylint: disable=too-many-lines
"""
This module contains the SAP HANA APL Time Series algorithm.

The following class is available:

    * :class:`AutoTimeSeries`
"""
import logging
import warnings
from enum import Enum, unique, auto
import numpy as np
import pandas as pd
from hdbcli import dbapi
from hana_ml.dataframe import (
    DataFrame,
    quotename)
from hana_ml.ml_exceptions import Error, FitIncompleteError
from hana_ml.algorithms.apl.apl_base import (
    APLBase,
    APLArtifactTable,
    APLArtifactApplyOutTable)
from hana_ml.ml_base import (
    parse_one_dtype,
    ListOfStrings,
)
from hana_ml.visualizers.report_builder import (
    ReportBuilder,
    Page,
    DescriptionItem,
    TableItem,
    ChartItem
)
from hana_ml.algorithms.apl.apl_base import SqlGenerationMode
from hana_ml.algorithms.apl.apl_artifact import APLArtifactView

logger = logging.getLogger(__name__) #pylint: disable=invalid-name

@unique
class _ModelReportDetailLevel(Enum):
    NO_FORECAST = auto()
    FULL = auto()

class AutoTimeSeries(APLBase): #pylint: disable=too-many-instance-attributes
    """
    SAP HANA APL Time Series algorithm.

    Parameters
    ----------
    target: str
        The name of the column containing the time series data points.
    time_column_name: str
        The name of the column containing the time series time points.
        The time column is used as table key. It can be overridden by setting the 'key' parameter
        through the fit() method.
    last_training_time_point: str, optional
        The last time point used for model training.
        The training dataset will contain all data points up to this date.
        By default, this parameter will be set as the last time point until which the target is
        not null.
    horizon: int, optional
        The number of forecasts to be generated by the model upon apply.
        The time series model will be trained to optimize accuracy on the requested horizon only.
        The default value is 1.
    with_extra_predictable: bool, optional
        If set to true, all input variables will be used by the model to generate forecasts.
        If set to false, only the time and target columns will be used. All other variables
        will be ignored.
        This parameter is set to true by default.
    variable_storages: dict, optional
        Specifies the variable data types (string, integer, number).
        For example, {'VAR1': 'string', 'VAR2': 'number'}.
        See notes below for more details.
    variable_value_types: dict, optional
        Specifies the variable value types (continuous, nominal, ordinal).
        For example, {'VAR1': 'continuous', 'VAR2': 'nominal'}.
        See notes below for more details.
    variable_missing_strings: dict, optional
        Specifies the variable values that will be taken as missing.
        For example, {'VAR1': '???'} means anytime the variable value equals '???',
        it will be taken as missing.
    extra_applyout_settings: dict, optional
        Specifies the prediction outputs.
        See documentation on predict() method for more details.
    sort_data: bool
        If True, a temporary view is created on the dataset to sort data by time.
        However, users can provide directly a view with sorted dates.
        In this case, they must set *sort_data* to *False* to avoid creating a new view.
        The default value is True.
        WARNING: it is recommended to leave this parameter by default so the data is guaranteed
        to be read in sorted order. If the data is not sorted, the model will fail.
    other_params: dict, optional
        Corresponds to the advanced settings.
        The dictionary contains {<parameter_name>: <parameter_value>}.
        The possible parameters are:

        - 'max_tasks'
        - 'segment_column_name'
        - 'force_negative_forecast'
        - 'force_positive_forecast'
        - 'forecast_fallback_method'
        - 'forecast_max_cyclics'
        - 'forecast_max_lags'
        - 'forecast_method'
        - 'smoothing_cycle_length'

        See `Common APL Aliases for Model Training
        <https://help.sap.com/viewer/7223667230cb471ea916200712a9c682/latest/en-US/de2e28eaef79418799b9f4e588b04b53.html>`_
        in the SAP HANA APL Developer Guide.

        For 'max_tasks', see `FUNC_HEADER
        <https://help.sap.com/viewer/7223667230cb471ea916200712a9c682/latest/en-US/d8faaa27841341cbac41353d862484af.html>`_.
    other_train_apl_aliases: dict, optional
        Users can provide APL aliases as advanced settings to the model.
        Unlike 'other_params' described above, users are free to input any possible value.
        There is no control in python.

    Attributes
    ----------
    model_: hana_ml DataFrame
        The trained model content
    summary_: APLArtifactTable
        The reference to the "SUMMARY" table generated by the model training.
        This table contains the summary about the model training.
    indicators_: APLArtifactTable
        The reference to the "INDICATORS" table generated by the model training.
        This table contains the various metrics related to the model and its variables.
    fit_operation_logs_: APLArtifactTable
        The reference to the "OPERATION_LOG" table generated by the model training
    var_desc_: APLArtifactTable
        The reference to the "VARIABLE_DESCRIPTION" table that was built during the model training
    applyout_: hana_ml DataFrame
        The predictions generated the last time the model was applied
    predict_operation_logs_: APLArtifactTable
        The reference to the "OPERATION_LOG" table that is produced when making predictions.
    train_data_: hana_ml DataFrame
        The train dataset

    Examples
    --------
    >>> from hana_ml.algorithms.apl.time_series import AutoTimeSeries
    >>> from hana_ml.dataframe import ConnectionContext, DataFrame

    Connecting to SAP HANA

    >>> CONN = ConnectionContext(HDB_HOST, HDB_PORT, HDB_USER, HDB_PASS)
    >>> # -- Creates Hana DataFrame
    >>> hana_df = DataFrame(CONN, 'select * from APL_SAMPLES.CASHFLOWS_FULL')

    Creating and fitting the model

    >>> model = AutoTimeSeries(time_column_name='Date', target='Cash', horizon=3)
    >>> model.fit(data=hana_df)

    Debriefing

    >>> model.get_model_components()
    {'Trend': 'Polynom( Date)',
     'Cycles': 'PeriodicExtrasPred_MondayMonthInd',
     'Fluctuations': 'AR(46)'}

    >>> model.get_performance_metrics()
    {'MAPE': [0.12853715702893018, 0.12789963348617622, 0.12969031859857874], ...}


    Generating forecasts using the **forecast()** method

    This method is used to generate forecasts using a signature similar to the one used in PAL.
    There are two variants of usage as described below:

    1) If the model does not use **extra-predictable** variables (no exogenous variable), users
    must simply specify the number of forecasts.

    >>> train_df = DataFrame(CONN,
                            'SELECT "Date" , "Cash" '
                            'from APL_SAMPLES.CASHFLOWS_FULL ORDER BY 1 LIMIT 100')
    >>> model = AutoTimeSeries(time_column_name='Date', target='Cash', horizon=3)
    >>> model.fit(train_df)
    >>> out = model.forecast(forecast_length=3)
    >>> out.collect().tail(5)
               Date                            ACTUAL    PREDICTED  LOWER_INT_95PCT  UPPER_INT_95PCT
    98   2001-05-23  3057.812544999999772699132909775  4593.966530              NaN              NaN
    99   2001-05-25  3037.539714999999887176132440567  4307.893346              NaN              NaN
    100  2001-05-26                              None  4206.023158     -3609.599872     12021.646187
    101  2001-05-27                              None  4575.162651     -3392.283802     12542.609104
    102  2001-05-28                              None  4830.352462     -3239.507360     12900.212284


    2) If the model uses **extra-predictable** variables, users must provide the values of all
    extra-predictable variables for each time point of the forecast period.
    These values must be provided as a hana_ml dataframe with the same structure as the
    training dataset.

    >>> # Trains the dataset with extra-predictable variables
    >>> train_df = DataFrame(CONN,
    ...                     'SELECT * '
    ...                     'from APL_SAMPLES.CASHFLOWS_FULL '
    ...                     'WHERE "Cash" is not null')
    >>> # Extra-predictable variables' values on the forecast period
    >>> forecast_df = DataFrame(CONN,
    ...                        'SELECT * '
    ...                        'from APL_SAMPLES.CASHFLOWS_FULL '
    ...                        'WHERE "Cash" is null LIMIT 5')
    >>> model = AutoTimeSeries(time_column_name='Date', target='Cash', horizon=3)
    >>> model.fit(train_df)
    >>> out = model.forecast(data=forecast_df)
    >>> out.collect().tail(5)
              Date ACTUAL    PREDICTED  LOWER_INT_95PCT  UPPER_INT_95PCT
    251  2001-12-29   None  6864.371407      -224.079492     13952.822306
    252  2001-12-30   None  6889.515324      -211.264912     13990.295559
    253  2001-12-31   None  6914.766513      -187.180923     14016.713949
    254  2002-01-01   None  6940.124974              NaN              NaN
    255  2002-01-02   None  6965.590706              NaN              NaN


    Generating forecasts with the **predict()** method.

    The predict() method allows users to apply a fitted model on a dataset different from the
    training dataset.
    For example, users can train a dataset on the first quarter (January to March) and apply
    the model on a dataset of different period (March to May).

    >>> # Trains the model on the first quarter, from January to March
    >>> train_df = DataFrame(CONN,
    ...                     'SELECT "Date" , "Cash" '
    ...                     'from APL_SAMPLES.CASHFLOWS_FULL '
    ...                     "where \"Date\" between '2001-01-01' and '2001-03-31'"
    ...                     " ORDER BY 1")
    >>> model.fit(train_df)
    >>> # Forecasts on a shifted period, from March to May
    >>> test_df = DataFrame(CONN,
    ...                    'SELECT "Date", "Cash" '
    ...                    'from APL_SAMPLES.CASHFLOWS_FULL '
    ...                    "where \"Date\" between '2001-03-01' and '2001-05-31'"
    ...                    " ORDER BY 1")
    >>> out = model.predict(test_df)
    >>> out.collect().tail(5)
              Date                            ACTUAL     PREDICTED  LOWER_INT_95PCT  UPPER_INT_95PCT
    60  2001-05-30  3837.196734000000105879735597214   4630.223083              NaN              NaN
    61  2001-05-31  2911.884261000000151398126928726   4635.265982              NaN              NaN
    62  2001-06-01                              None   4538.516542     -1087.461104     10164.494188
    63  2001-06-02                              None   4848.815364     -5090.167255     14787.797983
    64  2001-06-03                              None   4853.858263     -5138.553275     14846.269801

    Using the **fit_predict()** method

    This method enables the user to fit a model and generate forecasts on a single call, and thus
    get results faster. However, the model is created on the fly and deleted after use, so the user
    will not be able to save the resulting model.

    >>> model.fit_predict(hana_df)
    >>> out.collect().tail(5)
               Date            ACTUAL    PREDICTED  LOWER_INT_95PCT  UPPER_INT_95PCT
    249  2001-12-27  5995.42329499999  6055.761105              NaN              NaN
    250  2001-12-28  7111.41669699999  6314.336098              NaN              NaN
    251  2002-01-03                           None  7033.880804      4529.462710      9538.298899
    252  2002-01-04                           None  6464.557223      3965.343397      8963.771049
    253  2002-01-07                           None  6469.141663      3961.414900      8976.868427

    Breaking down the time series into trend, cycles, fluctuations and residuals components.

    If the parameter *extra_applyout_settings* is set to **{'ExtraMode': True}**, anytime a forecast
    method is called, predict(), forecast() or fit_predict(), the output will contain time series
    components and their corresponding residuals. The prediction columns are suffixed by the
    horizon number. For instance, 'Cycles_RESIDUALS_3' means the residual of the cycle component in
    the third horizon.

    >>> model.fit(train_df)
    >>> model.set_params(extra_applyout_settings={'ExtraMode': True})
    >>> out = model.predict(hana_df)
    >>> out.collect().tail(5)
                   Date              ACTUAL        ...  Cycles_RESIDUALS_3  Fluctuations_RESIDUALS_3
    249  2001-12-27  5995.42329499392507553        ...               32.51                  4.48e-13
    250  2001-12-28  7111.41669699455205917        ...             -644.77                  1.14e-13
    251  2002-01-03                    None        ...                 NaN                       NaN
    252  2002-01-04                    None        ...                 NaN                       NaN
    253  2002-01-07                    None        ...                 NaN                       NaN

    Users can change the fields that are included in the output by using the APL/ApplyExtraMode alias
    in *extra_applyout_settings*, for instance:
    **{'APL/ApplyExtraMode': 'First Forecast with Stable Components and Residues and Error Bars'}**.
    Please check the SAP HANA APL documentation to know which values are available for
    APL/ApplyExtraMode.
    See Function Reference > Predictive Model Services > *APPLY_MODEL* > Advanced Apply Settings
    in the `SAP HANA APL Developer Guide <https://help.sap.com/viewer/p/apl>`_.

    Notes
    -----
    The input dataset, given as an hana_ml dataframe, must not be a temporary table because the
    API tries to create a view sorted by the time column.
    SAP HANA does not allow user to create a view on temporary table.
    However, even though it is **not recommended**, to avoid creating the view, user can force
    the parameter sort_data to False.

    When calling the fit_predict() method, the time series model is generated on the fly and
    not returned. If a model must be saved, please consider using the fit() method instead.

    When extra-predictable variables are involved, it is usual to have a single dataset used
    both for the model training and the forecasting. In this case, the dataset should contain two
    successive periods:

    - The first one is used for the model training, ranging from the beginning to the last
      date where the target value **is not null**.
    - The second one is used for the model training, ranging from the the first date where
      the target value **is null**.

    The content of the output of the get_performance_metrics() method may change depending of
    the version of SAP HANA APL used with this API. Please refer to the SAP HANA APL documentation
    to know which metrics will be provided.

    """
    # Define APL aliases for TimeSeries
    # See APL Help
    #     "Function Reference" > "Common APL Aliases for Model Training" > Time-Series Models
    # Override upper class static variable
    APL_ALIAS_KEYS = {
        'cutting_strategy': 'APL/CuttingStrategy',
        'force_negative_forecast': 'APL/ForceNegativeForecast',
        'force_positive_forecast': 'APL/ForcePositiveForecast',
        'forecast_fallback_method': 'APL/ForecastFallbackMethod',
        'forecast_max_cyclics': 'APL/ForecastMaxCyclics',
        'forecast_max_lags': 'APL/ForecastMaxLags',
        'forecast_method': 'APL/ForecastMethod',
        'smoothing_cycle_length': 'APL/SmoothingCycleLength',
        'segment_column_name': 'APL/SegmentColumnName'
    }

    EXTRA_MODES = (
        'No Extra',
        'Forecasts and Error Bars',
        'Stable Components and Error Bars',
        'First Forecast with Stable Components and Residues and Error Bars'
    )

    def __init__(self,
                 conn_context=None,
                 time_column_name=None,
                 target=None,
                 horizon=1,
                 with_extra_predictable=True,
                 last_training_time_point=None,
                 variable_storages=None,
                 variable_value_types=None,
                 variable_missing_strings=None,
                 extra_applyout_settings=None,
                 train_data_=None,
                 sort_data=True,
                 ** other_params): #pylint: disable=too-many-arguments
        self.target = None
        self.with_extra_predictable = None
        self.set_params(time_column_name=time_column_name)
        self.set_params(target=target)
        self.set_params(horizon=horizon)
        self.set_params(with_extra_predictable=with_extra_predictable)
        self.set_params(last_training_time_point=last_training_time_point)
        self.set_params(extra_applyout_settings=extra_applyout_settings)
        self.set_params(sort_data=sort_data)

        super(AutoTimeSeries, self).__init__(
            conn_context,
            variable_storages,
            variable_value_types,
            variable_missing_strings,
            extra_applyout_settings,
            ** other_params)

        self._model_type = 'timeseries'
        self.model_ = None
        # dataframe used in the last fit, required for forecast() method
        self.train_data_ = train_data_
        self._report_builder = None

    def set_params(self, **parameters):
        """
        Sets attributes of the current model.

        Parameters
        ----------
        parameters: dict
            Contains attribute names and values in the form of keyword arguments
        """
        if 'target' in parameters:
            self.target = self._arg('target', parameters.pop('target'), str)
        if 'time_column_name' in parameters:
            self.time_column_name = self._arg('time_column_name',
                                              parameters.pop('time_column_name'),
                                              str)
        if 'horizon' in parameters:
            self.horizon = self._arg('horizon', parameters.pop('horizon'), int)
        if 'with_extra_predictable' in parameters:
            self.with_extra_predictable = self._arg('with_extra_predictable',
                                                    parameters.pop('with_extra_predictable'),
                                                    bool)
        if 'last_training_time_point' in parameters:
            self.last_training_time_point = self._arg(
                'last_training_time_point',
                parameters.pop('last_training_time_point'),
                str)
        if 'extra_applyout_settings' in parameters:
            param = self._arg(
                'extra_applyout_settings',
                parameters.pop('extra_applyout_settings'),
                dict)
            if param:
                # It is possible to reset the param to None
                if not set(param.keys()).issubset(['ExtraMode', 'APL/ApplyExtraMode']):
                    msg = ("The extra_applyout_settings parameter must only contain "
                           "'ExtraMode' or 'APL/ApplyExtraMode' as key")
                    raise ValueError(msg)
                # 'APL/ApplyExtraMode' overrides 'ExtraMode'
                extra_mode = param.get('APL/ApplyExtraMode', None)
                if extra_mode is not None:
                    if extra_mode not in self.EXTRA_MODES:
                        msg = "Invalid 'APL/ApplyExtraMode' value"
                        raise ValueError(msg)
                    param = {'APL/ApplyExtraMode': extra_mode}
                else:
                    extra_mode = param.get('ExtraMode', None)
                    if extra_mode is not None:
                        if not isinstance(extra_mode, bool):
                            msg = "'ExtraMode' must be a boolean"
                            raise TypeError(msg)
                        if extra_mode:
                            param = {'APL/ApplyExtraMode': 'Stable Components and Error Bars'}
                        else:
                            param = {'APL/ApplyExtraMode': 'Forecasts and Error Bars'}
            self.extra_applyout_settings = param
        if 'train_data_' in parameters:
            self.train_data_ = self._arg(
                'train_data_',
                parameters.pop('train_data_'),
                DataFrame)
        if 'sort_data' in parameters:
            self.sort_data = self._arg('sort_data', parameters.pop('sort_data'), bool)

        if parameters:
            super(AutoTimeSeries, self).set_params(**parameters)
        return self

    def _check_params_before_fit(self, data, key, features): #pylint: disable=too-many-branches
        """
        Checks the validity of the parameters before fit().
        Infers certain parameters if not set (features, self.target and self.with_extra_predictable)

        Returns
        -------
        RuntimeError raised if something is wrong.
        Otherwise, features
        """
        if key:
            self.set_params(time_column_name=key)
        if not self.time_column_name:
            raise ValueError('The time column name is unknown. '
                             'Please set the time_column_name parameter.')
        if self.time_column_name not in data.columns:
            raise ValueError('The time column {} is missing in the dataset.'.format(
                self.time_column_name))

        if not self.target:
            cols = data.columns
            if self.time_column_name and (len(cols) == 2):
                # There are only 2 columns, we can deduce the target
                self.target = [col for col in cols if col != self.time_column_name][0]
        if not self.target:
            raise ValueError('The target column name is required.'
                             ' Please set the target parameter.')
        if self.target not in data.columns:
            raise ValueError('The target column {} is missing in the dataset.'.format(self.target))
        self._check_valid(data, features=features)
        if not features:
            segment_column_name = getattr(self, 'segment_column_name', None)
            features = []
            if self.with_extra_predictable:
                # No features specified, take all as features except the target
                for col_name in data.columns:
                    if col_name not in (self.target, segment_column_name):
                        features.append(col_name)
        if self.time_column_name not in features:
            features.append(self.time_column_name)
        if features == [self.time_column_name]:
            # If the features only contains DATE,
            # then reset 'with_extra_predictable' to false
            # (avoid APL from claiming about last_traning_time_point)
            self.with_extra_predictable = False
        return features

    def fit(self, data, key=None, features=None, build_report=False): #pylint: disable=too-many-branches
        """
        Fits the model.

        Parameters
        ----------
        data: hana_ml DataFrame
            The training dataset
        key: str, optional
            The column used as row identifier of the dataset.
            This column corresponds to the time column name.
            As a result, setting this parameter will overwrite the time_column_name model setting.
        features: list of str, optional
            The names of the feature columns, meaning the date column and the extra-predictive
            variables.
            If `features` is not provided, it defaults to all columns except the target column.
        build_report: bool, optional
            Whether to build report or not.
            Defaults to False.

        Returns
        -------
        self: object
        """
        features = self._check_params_before_fit(data, key, features)
        self._set_conn_context(data.connection_context)
        new_view_name = None
        try:
            # Creates a view on the dataset with ascending sort on the time column.
            new_view_name, data_sort = self._create_sorted_view(data)
            self.train_data_ = data
            self._fit(
                data=data_sort,
                key=self.time_column_name,
                features=features,
                label=self.target)
        except dbapi.Error as db_er:
            logger.error("An issue occurred during model fitting: %s",
                         db_er, exc_info=True)
            self._drop_artifact_tables()
            raise
        finally:
            if new_view_name:
                self._try_drop_view([new_view_name])
        if build_report:
            self.build_report()
        return self

    def predict(self, data, apply_horizon=None, apply_last_time_point=None, build_report=False):
        """
        Uses the fitted model to generate forecasts.

        Parameters
        ----------
        data: hana_ml DataFrame
            The input dataset used for predictions
        apply_horizon: int, optional
            The number of forecasts to generate.
            By default, the number of forecasts is the horizon on which the model was trained.
        apply_last_time_point: str, optional
            The time point corresponding to the start of the forecast period. Forecasts will be
            generated starting from the next time point after the 'apply_last_time_point'.
            By default, this parameter is set to the value of 'last_training_time_point' known from
            the model training.
        build_report: bool, optional
            Whether to build report or not.
            Defaults to False.

        Returns
        -------
        hana_ml DataFrame
            By default the output contains the following columns:

            - <the name of the time column>
            - ACTUAL: the actual value of time series
            - PREDICTED: the forecast value
            - LOWER_INT_95PCT: the lower limit of 95% confidence interval
            - UPPER_INT_95PCT: the upper limit of 95% confidence interval

            If ExtraMode is set to true, the output dataframe will also contain the breaking down
            of the time series into a trend, cycles, fluctuations and residuals components.

        Examples
        --------

        Default output

        >>> out = model.predict(hana_df)
        >>> out.collect().tail(5)
               Date            ACTUAL    PREDICTED  LOWER_INT_95PCT  UPPER_INT_95PCT
        249  2001-12-27  5995.42329499999  6055.761105              NaN              NaN
        250  2001-12-28  7111.41669699999  6314.336098              NaN              NaN
        251  2002-01-03                           None  7033.88080      4529.46271      9538.29889
        252  2002-01-04                           None  6464.55722      3965.34339      8963.77104
        253  2002-01-07                           None  6469.14166      3961.41490      8976.86842


        Retrieving forecasts and components (predicted, trend, cycles and fluctuations).

        The output columns are suffixed with the horizon index. For example, Trend_1 means the
        trend component of the first horizon.

        >>> model.set_params(extra_applyout_settings={'ExtraMode': True})
        >>> out = model.predict(hana_df)
        >>> out.collect().tail(5)
                Date                               ACTUAL  PREDICTED_1      Trend_1  \
        249  2001-12-27  5995.423294999999598076101392507553  6055.761105  6814.405390   ...
        250  2001-12-28  7111.416696999999658146407455205917  6314.336098  6839.334762   ...
        251  2002-01-03                                 None  7033.880804  6991.163710   ...
        252  2002-01-04                                 None  6464.557223  7016.843985   ...
        253  2002-01-07                                 None  6469.141663  7094.528433   ...

        Users can change the fields that are included in the output by using the APL/ApplyExtraMode alias
        in *extra_applyout_settings*, for instance:
        **{'APL/ApplyExtraMode': 'First Forecast with Stable Components and Residues and Error Bars'}**.
        Please check the SAP HANA APL documentation to know which values are available for
        APL/ApplyExtraMode.
        See Function Reference > Predictive Model Services > *APPLY_MODEL* > Advanced Apply Settings
        in the `SAP HANA APL Developer Guide <https://help.sap.com/viewer/p/apl>`_.
        """
        if apply_horizon is None:
            apply_horizon = self.horizon
        if apply_last_time_point is None:
            apply_last_time_point = self.last_training_time_point
        previous_applyout_settings = self.extra_applyout_settings
        if build_report:
            extra_mode = 'First Forecast with Stable Components and Residues and Error Bars'
            self.set_params(extra_applyout_settings={'APL/ApplyExtraMode': extra_mode})
        apply_config_data_df = self._build_apply_config_dataframe(
            apply_horizon=apply_horizon,
            apply_last_time_point=apply_last_time_point,
            last_row_with_forecasting_information=data.count())
        new_view_name = None
        try:
            # Creates a view on the dataset with ascending sort on the time column.
            new_view_name, data_sort = self._create_sorted_view(data)
            applyout_df = super(AutoTimeSeries, self)._predict(
                data=data_sort,
                apply_config_data_df=apply_config_data_df,
                generate_debrief=True)
            self.applyout_ = self._rewrite_applyout_df(applyout_df=applyout_df)
        except dbapi.Error as db_er:
            logger.error("The model failed to generate forecasts: %s", db_er, exc_info=True)
            raise
        finally:
            self.set_params(extra_applyout_settings=previous_applyout_settings)
            if new_view_name:
                self._try_drop_view([new_view_name])
        if build_report:
            self.build_report()
        return self.applyout_

    def fit_predict(self, data, key=None, features=None, horizon=None, build_report=False):
        #pylint: disable=too-many-branches
        """
        Fits a model and generate forecasts in a single call to the FORECAST APL function.
        This method offers a faster way to perform the model training and forecasting.

        However, the user will not have access to the model used internally since it is deleted
        after the computation of the forecasts.

        Parameters
        ----------
        data: hana_ml DataFrame
            The input time series dataset
        key: str, optional
            The date column name.
            By default, it is equal to the model parameter **time_column_name**.
            If it is given, the model parameter **time_column_name** will be overwritten.
        features: list of str, optional
            The column names corresponding to the extra-predictable variables (exogenous variables).
            If `features` is not provided, it is equal to all columns except the target column.
        horizon: int, optional
            The number of forecasts to generate.
            The default value equals to the horizon parameter of the model.
        build_report : bool, optional
            Whether to build report or not.
            Defaults to False.

        Returns
        -------
        hana_ml DataFrame
            The output is the same as the predict() method.
        """
        if horizon:
            self.set_params(horizon=horizon)
        features = self._check_params_before_fit(data, key, features)
        self._set_conn_context(data.connection_context)
        new_view_name = None
        previous_applyout_settings = self.extra_applyout_settings
        if build_report:
            extra_mode = 'First Forecast with Stable Components and Residues and Error Bars'
            self.set_params(extra_applyout_settings={'APL/ApplyExtraMode': extra_mode})
        try:
            # Creates a view on the dataset with ascending sort on the time column.
            new_view_name, data_sort = self._create_sorted_view(data)
            self.train_data_ = data
            applyout_df = self._call_apl_forecast(
                data=data_sort,
                key=self.time_column_name,
                features=features,
                target=self.target,
                horizon=self.horizon)
            self.applyout_ = self._rewrite_applyout_df(applyout_df=applyout_df)
        except dbapi.Error as db_er:
            logger.error("An issue was encounted during the model fitting: %s",
                         db_er, exc_info=True)
            raise
        finally:
            self.set_params(extra_applyout_settings=previous_applyout_settings)
            if new_view_name:
                self._try_drop_view([new_view_name])
        if build_report:
            self.build_report()
        return self.applyout_

    def forecast(self, forecast_length=None, data=None, build_report=False): #pylint: disable=too-many-branches
        """
        Uses the fitted model to generate out-of-sample forecasts.
        The model is supposed to be already fitted with a given dataset (training dataset).
        This method forecasts over a number of steps after the end of the training dataset.
        When there are extra-predictive variable (exogenous variables), the input parameter
        **data** is required. It must contain the values of the extra-predictable variables
        for the forecast period.
        If there is no extra-predictive variable, only the **forecast_length** parameter is needed.

        Parameters
        ----------
        forecast_length: int, optional
            The number of forecasts to generate from the end of the train dataset.
            This parameter is by default the horizon specified in the model parameter.
        data: hana_ml DataFrame, optional
            The time series with extra-predictable variables used for forecasting.
            This parameter is required if extra-predictive variables are used in the model.
            When this parameter is given, the parameter 'forecast_length' is ignored.
        build_report : bool, optional
            Whether to build report or not.
            Defaults to False.

        Returns
        -------
        hana_ml DataFrame
            The output is the same as the predict() method.

        Examples
        --------

        Case where there is no extra-predictable variable:

        >>> train_df = DataFrame(CONN,
                                 'SELECT "Date" , "Cash" '
                                 'from APL_SAMPLES.CASHFLOWS_FULL '
                                 'where "Cash" is not null '
                                 'ORDER BY 1')
        >>> print(train_df.collect().tail(5))
                    Date         Cash
        246  2001-12-20  6382.441052
        247  2001-12-21  5652.882539
        248  2001-12-26  5081.372996
        249  2001-12-27  5995.423295
        250  2001-12-28  7111.416697

        >>> model = AutoTimeSeries(CONN, time_column_name='Date',
                                   target='Cash',
                                   horizon=3)
        >>> model.fit(train_df)
        >>> out = model.forecast(forecast_length=3)
        >>> out.collect().tail(5)
                   Date                        ACTUAL    PREDICTED  LOWER_INT_95PCT  UPPER_INT_95PCT
        249  2001-12-27  5995.42329499999901392507553  6814.405390              NaN              NaN
        250  2001-12-28  7111.41669699999907455205917  6839.334762              NaN              NaN
        251  2001-12-29                          None  6864.371407      -224.079492     13952.822306
        252  2001-12-30                          None  6889.515324      -211.264912     13990.295559
        253  2001-12-31                          None  6914.766513      -187.180923     14016.713949

        Case where there are extra-predictable variables:

        >>> train_df = DataFrame(CONN,
                                'SELECT * '
                                'from APL_SAMPLES.CASHFLOWS_FULL '
                                'WHERE "Cash" is not null '
                                'ORDER BY 1')
        >>> print(train_df.collect().tail(5))
                   Date  WorkingDaysIndices     ...       BeforeLastWMonth         Cash
        246  2001-12-20                  13     ...                      1  6382.441052
        247  2001-12-21                  14     ...                      1  5652.882539
        248  2001-12-26                  15     ...                      0  5081.372996
        249  2001-12-27                  16     ...                      0  5995.423295
        250  2001-12-28                  17     ...                      0  7111.416697

        >>> # Extra-predictable variables to be provided as the forecast period
        >>> forecast_df = DataFrame(CONN,
                                   'SELECT * '
                                   'from APL_SAMPLES.CASHFLOWS_FULL '
                                   'WHERE "Cash" is null '
                                   'ORDER BY 1 '
                                   'LIMIT 3')
        >>> print(forecast_df.collect())
                 Date  WorkingDaysIndices  ...   BeforeLastWMonth  Cash
        0  2002-01-03                   0  ...                  0  None
        1  2002-01-04                   1  ...                  0  None
        2  2002-01-07                   2  ...                  0  None

        >>> model = AutoTimeSeries(CONN,
                                   time_column_name='Date',
                                   target='Cash',
                                   horizon=3)
        >>> model.fit(train_df)
        >>> out = model.forecast(data=forecast_df)
        >>> out.collect().tail(5)
                   Date                          ACTUAL  PREDICTED  LOWER_INT_95PCT  UPPER_INT_95PCT
        249  2001-12-27  5995.4232949999996101392507553    6814.41              NaN              NaN
        250  2001-12-28  7111.4166969999996407455205917    6839.33              NaN              NaN
        251  2001-12-29                            None    6864.37          -224.08         13952.82
        252  2001-12-30                            None    6889.52          -211.26         13990.30
        253  2001-12-31                            None    6914.77          -187.18         14016.71
        """

        if not self.model_ or not self.model_table_:
            raise FitIncompleteError('Please fit the model first.')
        if not self.train_data_:
            raise FitIncompleteError('The train dataset is unknown.'
                                     ' Please set the train_data_ parameter'
                                     ' or fit the model again.')
        cols = self.train_data_.columns
        if self.time_column_name and (len(cols) == 2):
            self.with_extra_predictable = False
        if not self.target:
            if not self.with_extra_predictable:
                # There are only 2 columns, we can deduce the target
                self.target = [col for col in cols if col != self.time_column_name][0]
            else:
                raise RuntimeError('Cannot create view. The target column is not defined.')
        if self.with_extra_predictable:
            if not data:
                raise RuntimeError("Please provide the extra-predictive values in the 'data'"
                                   " parameter")
        if data:
            forecast_length = data.count()
            # Checks if data has all required extra-predictable columns
            extra_prd_cols = data.columns
            for col in self.train_data_.columns:
                if col == self.target:
                    continue
                if col not in extra_prd_cols:
                    raise RuntimeError('Column %s is missing in the input dataset' % col)
        else:
            if not forecast_length:
                forecast_length = self.horizon
        if not forecast_length and not data:
            raise RuntimeError('Please set either forecast_length or data parameter')
        previous_applyout_settings = self.extra_applyout_settings
        if build_report:
            extra_mode = 'First Forecast with Stable Components and Residues and Error Bars'
            self.set_params(extra_applyout_settings={'APL/ApplyExtraMode': extra_mode})
        new_view_name = None
        try:
            # Creates a view on the dataset extended with data
            # with ascending sort on the time column.
            new_view_name, data_sort = self._create_forecast_input_view(data)
            apply_config_data_df = self._build_apply_config_dataframe(
                apply_horizon=forecast_length,
                apply_last_time_point=self.last_training_time_point,
                last_row_with_forecasting_information=data_sort.count())
            applyout_df = super(AutoTimeSeries, self)._predict(
                data=data_sort,
                apply_config_data_df=apply_config_data_df,
                generate_debrief=True)
            self.applyout_ = self._rewrite_applyout_df(applyout_df=applyout_df)
        except dbapi.Error as db_er:
            logger.error("Predict failed with error message: %s", db_er, exc_info=True)
            raise
        finally:
            self.set_params(extra_applyout_settings=previous_applyout_settings)
            if new_view_name:
                self._try_drop_view([new_view_name])
        if build_report:
            self.build_report()
        return self.applyout_

    def get_model_components(self):
        """
        Returns the description of the model components, that is trend, cycles and fluctuations,
        used by the model to generate forecasts.

        Returns
        -------
        Dictionary or pandas DataFrame
            If no segment column is given, a dictionary with 3 possible keys:
            'Trend', 'Cycles', 'Fluctuations'.

            If a segment column is given, a pandas DataFrame which contains the model components
            for each segment.

        Examples
        --------
        >>> model.get_model_components()
        {
            "Trend": "Linear(TIME)",
            "Cycles": None,
            "Fluctuations": "AR(36)"
        }
        """
        if not hasattr(self, 'indicators_'):
            raise FitIncompleteError(
                "The indicators table was not found. Please fit the model.")
        df_ind = self.indicators_
        cond = "KEY in ('Trend', 'Cycles', 'Fluctuations')"
        df_ind = df_ind.filter(cond).collect()
        segment_column = getattr(self, 'segment_column_name', None)
        if segment_column is None:
            return {row[1].KEY: row[1].VALUE for row in df_ind.iterrows()}
        else:
            df_ind = df_ind[['OID', 'KEY', 'VALUE']]
            df_ind.columns = [segment_column, 'Component', 'Value']
            return df_ind

    def get_performance_metrics(self):
        """
        Returns the performance metrics of the model.
        The metrics are provided for each forecast horizon.

        Returns
        -------
        Dictionary or pandas DataFrame
            If no segment column is given, a dictionary in which each metric is associated with a
            list containing <horizon> elements.

            If a segment column is given, a pandas DataFrame which contains the metric values for
            each segment.

        Examples
        --------

        A model is trained with 4 horizons. The returned value will be:

        >>> model.get_performance_metrics()
        {'MAPE': [
              0.1529961017445385,
              0.1538823292343699,
              0.1564376267423695,
              0.15170398377407046}
        """
        if not hasattr(self, 'indicators_'):
            raise FitIncompleteError(
                "The indicators table was not found. Please fit the model.")
        df_ind = self.indicators_
        # Get metrics per horizon (MAPE)
        cond = "VARIABLE is null and TARGET is null and DETAIL is not null"
        df_ind = df_ind.filter(cond)   # hana_ml DataFrame
        df_ind = df_ind.collect()  # pd.DataFrame
        # Extracts the horizon number from the text 'Forecast n'
        df_ind['HORIZON_NO'] = df_ind['DETAIL'].str.extract(r'Forecast (\d+)')
        df_ind['HORIZON_NO'] = df_ind['HORIZON_NO'].astype(int)
        df_ind.sort_values(by=['OID', 'KEY', 'HORIZON_NO'], inplace=True, ignore_index=True)
        df_ind['VALUE'] = df_ind['VALUE'].astype(float)

        segment_column = getattr(self, 'segment_column_name', None)
        if segment_column is None:
            df_metrics = df_ind[['KEY', 'HORIZON_NO', 'VALUE']]

            # Pivots table so we will have:
            # HORIZON_NO   1    2    3    4    5    6    7    8    9     10
            # KEY
            # MAPE        1.0  2.0  3.0  4.0  5.0  6.0  7.0  8.0  9.0  10.0
            df_metrics = df_metrics.pivot(index='KEY', columns='HORIZON_NO', values='VALUE')

            # Converts to dictionary for return
            ret = {metric_name: [val for _, val in vals.items()]
                   for metric_name, vals in df_metrics.iterrows()}
            return ret
        else:
            df_metrics = df_ind[['OID', 'KEY', 'HORIZON_NO', 'VALUE']]
            df_metrics.columns = [segment_column, 'Metric', 'Horizon', 'Value']
            return df_metrics

    def get_horizon_wide_metric(self, metric_name='MAPE'):
        """
        Returns value of performance metric (MAPE, sMAPE, ...) averaged on the forecast horizon.

        Parameters
        ----------
        metric_name: str
            Default value equals 'MAPE'.
            Possible values: 'MAPE', 'MPE', 'MeanAbsoluteError', 'RootMeanSquareError', 'SMAPE',
            'L1', 'L2', 'P2', 'R2', 'U2'

        Returns
        -------
        Float or pandas DataFrame
            If no segment column is given, the average metric value on the forecast horizon.
            It is based on validation partition.

            If a segment column is given, a pandas DataFrame which contains the average metric
            value on the forecast horizon for each segment.
        """
        metrics = self.get_performance_metrics()
        segment_column = getattr(self, 'segment_column_name', None)
        if segment_column is None:
            if metrics:
                return np.mean(metrics[metric_name])
        else:
            if not metrics.empty:
                metrics = metrics[metrics['Metric'] == metric_name]
                metrics.drop(columns=['Horizon', 'Metric'], inplace=True)
                return metrics.groupby([segment_column], as_index=False).mean()
        raise FitIncompleteError("No metrics available. Please fit the model first.")

    def load_model(self, schema_name, table_name, oid=None):
        """
        Loads the model from a table.

        Parameters
        ----------
        schema_name: str
            The schema name
        table_name: str
            The table name
        oid : str, optional
            If the table contains several models,
            the OID must be given as an identifier.
            If it is not provided, the whole table is read.

        Notes
        -----

        Before using a reloaded model for a new prediction, set the following parameters again:
        'time_column_name', 'target'.
        The SAP HANA ML library needs these parameters to prepare the dataset view.
        Otherwise, methods such as forecast() and predict() will fail.

        Examples
        --------

        >>> # Sets time_column_name and target again
        >>> model = AutoTimeSeries(conn_context=CONN, time_column_name='Date', target='Cash')
        >>> model.load_model(schema_name='MY_SCHEMA', table_name='MY_MODEL_TABLE')
        >>> model.predict(hana_df,
        ...               apply_horizon=(NB_HORIZON_TRAIN + 5),
        ...               apply_last_time_point=LAST_TRAIN_DATE)

        """
        super(AutoTimeSeries, self).load_model(schema_name=schema_name,
                                               table_name=table_name,
                                               oid=oid)
        if not self.time_column_name:
            logger.warning("The time_column_name parameter is empty."
                           "Please set it to a correct value before making predictions.")
        if not self.target:
            logger.warning("The target parameter is empty."
                           "Please set it to a correct value before making predictions.")
        if not self.train_data_:
            logger.warning("The train_data_ parameter is empty."
                           "Please set it to a correct value before making predictions.")

    def _create_train_config_table(self):
        if self.train_data_ is not None:
            last_row_with_forecasting_information = self.train_data_.count()
            segment_column_name = getattr(self, 'segment_column_name', None)
            if segment_column_name is None and self.last_training_time_point is None:
                #pylint:disable=attribute-defined-outside-init
                self.last_training_time_point = self.train_data_.dropna(subset=[self.target]) \
                    .tail(ref_col=self.time_column_name).collect()[self.time_column_name].iloc[0] \
                    .strftime("%Y-%m-%d %H:%M:%S")
        else:
            last_row_with_forecasting_information = None

        config_df = self._build_train_config_dataframe(
            last_row_with_forecasting_information=last_row_with_forecasting_information)
        config_table = self._create_aplartifact_table_with_data_frame(
            name='#OPERATION_CONFIG_{}'.format(self.id),
            type_name=APLArtifactTable.OPERATION_CONFIG_EXTENDED,
            data_df=config_df)

        return config_table

    def _create_forecast_config_table(self, horizon=None):
        segment_column_name = getattr(self, 'segment_column_name', None)
        if segment_column_name is None and self.last_training_time_point is None:
            #pylint:disable=attribute-defined-outside-init
            self.last_training_time_point = self.train_data_.dropna(subset=[self.target]) \
                .tail(ref_col=self.time_column_name).collect()[self.time_column_name].iloc[0] \
                .strftime("%Y-%m-%d %H:%M:%S")

        train_config_df = self._build_train_config_dataframe(
            last_row_with_forecasting_information=self.train_data_.count())
        apply_config_df = self._build_apply_config_dataframe(
            apply_horizon=horizon,
            apply_last_time_point=None,
            last_row_with_forecasting_information=None)

        config_df = pd.concat([train_config_df, apply_config_df], ignore_index=True)
        config_table = self._create_aplartifact_table_with_data_frame(
            name='#OPERATION_CONFIG_{}'.format(self.id),
            type_name=APLArtifactTable.OPERATION_CONFIG_EXTENDED,
            data_df=config_df)

        return config_table

    def _build_train_config_dataframe(self, last_row_with_forecasting_information=None):
        config_ar = []

        config_ar.append(('APL/ModelType', self._model_type, None))

        if self.time_column_name:
            config_ar.append(('APL/TimePointColumnName', self.time_column_name, None))

        if self.horizon:
            config_ar.append(('APL/Horizon', str(self.horizon), None))

        if self.with_extra_predictable:
            config_ar.append(('APL/WithExtraPredictable', 'true', None))
        else:
            config_ar.append(('APL/WithExtraPredictable', 'false', None))

        segment_column_name = getattr(self, 'segment_column_name', None)
        if segment_column_name is None and last_row_with_forecasting_information is not None:
            config_ar.append(('APL/LastRowWithExtraPredictable',
                              str(last_row_with_forecasting_information), None))
            # 'APL/LastRowWithExtraPredictable' was ignored by APPLY_MODEL until APL 2225
            config_ar.append(('Protocols/Default/Transforms/Kxen.TimeSeries/Parameters/' \
                            + 'LastRowWithForecastingInformation',
                              str(last_row_with_forecasting_information), None))

        if self.last_training_time_point:
            config_ar.append(('APL/LastTrainingTimePoint',
                              self.last_training_time_point, None))
        else:
            param = 'Protocols/Default/Transforms/Kxen.TimeSeries/Parameters/AutoDetectLastRow'
            config_ar.append((param, 'true', None))

        config_ar = config_ar + self._get_train_config_data()

        approx_seasonal_path = 'Protocols/Default/Transforms/Kxen.TimeSeries/Parameters/' \
                             + 'Smoothing/ActivateApproximativeSeasonalCycleForTES'
        config_ar.append((approx_seasonal_path, 'true', None))
        # ignored if a different value (false) is provided by user
        config_ar.append(('APL/ActivateExplanations', 'true', None))
        config_ar.append(('APL/LocalExplanations/Activate', 'true', None))

        return pd.DataFrame(config_ar).drop_duplicates(subset=0, keep='first')

    def _build_apply_config_dataframe(self, apply_horizon=None, apply_last_time_point=None, last_row_with_forecasting_information=None):
        config_ar = []

        segment_column_name = getattr(self, 'segment_column_name', None)
        if segment_column_name is None and last_row_with_forecasting_information is not None:
            config_ar.append(('APL/LastRowWithExtraPredictable',
                              str(last_row_with_forecasting_information), None))
            # 'APL/LastRowWithExtraPredictable' was ignored by APPLY_MODEL until APL 2225
            config_ar.append(('Protocols/Default/Transforms/Kxen.TimeSeries/Parameters/' \
                            + 'LastRowWithForecastingInformation',
                              str(last_row_with_forecasting_information), None))

        if apply_horizon:
            config_ar.append(('APL/AppliedHorizon', str(apply_horizon), None))

        if hasattr(self, 'extra_applyout_settings') and self.extra_applyout_settings:
            # if extra_applyout_settings is given
            extra_mode = self.extra_applyout_settings['APL/ApplyExtraMode']
            if extra_mode == 'First Forecast with Stable Components and Residues and Error Bars':
                if self._get_apl_version() < 2121:
                    msg = "The provided 'ApplyExtraMode' value requires APL 2121+"
                    raise ValueError(msg)
            config_ar.append(('APL/ApplyExtraMode', extra_mode, None))
        else:
            # default output
            config_ar.append(('APL/ApplyExtraMode', 'Forecasts and Error Bars', None))

        if apply_last_time_point:
            config_ar.append(('APL/ApplyLastTimePoint', apply_last_time_point, None))
        else:
            param = 'Protocols/Default/Transforms/Kxen.TimeSeries/Parameters/AutoDetectLastRow'
            config_ar.append((param, 'true', None))

        return pd.DataFrame(config_ar)

    def _rewrite_applyout_df(self, applyout_df):
        #pylint: disable=too-many-branches
        #pylint: disable=too-many-statements
        #pylint: disable=too-many-locals
        """
        Rewrites the applyout dataframe so it outputs standardized column names.
        Parameters:
        ---------
        applyout_df : hana_ml DataFrame
            The initial output of predict

        Returns
        ------
        A new hana_ml DataFrame with renamed columns
        """

        # Determines the mapping of old columns to new columns
        # Stores the mapping into different list of tuples [(old_column, new_columns)]
        start_cols = []   # starting columns: ID, ACTUAL
        predicted_cols = []  # [('kts_1', 'PREDICTED_1), ('kts_2', 'PREDICTED_2), ...]
        components_cols = []  # Same thing for components [('kts_1AR', 'AR_1), ...]
        residues_cols = []  # for residues

        segment_column_name = getattr(self, 'segment_column_name', None)

        for old_col in applyout_df.columns:
            # if old_col in [self.time_column_name, self.target]:
            if old_col == segment_column_name:
                new_col = old_col
                start_cols.insert(0, (old_col, new_col))
            elif old_col == self.time_column_name:
                new_col = old_col
                start_cols.append((old_col, new_col))
            elif old_col == self.target:
                new_col = 'ACTUAL'
                start_cols.append((old_col, new_col))
            elif (old_col == 'kts_1') and (self.extra_applyout_settings is None):
                new_col = 'PREDICTED'
                start_cols.append((old_col, new_col))
            elif old_col == 'kts_1_lowerlimit_95%':
                new_col = 'LOWER_INT_95PCT'
                start_cols.append((old_col, new_col))
            elif old_col == 'kts_1_upperlimit_95%':
                new_col = 'UPPER_INT_95PCT'
                start_cols.append((old_col, new_col))
            elif self.extra_applyout_settings:
                # 'ExtraMode' or 'APL/ApplyExtraMode' is set.
                # many columns are now available: kts_n, kts_n<component>, kts_nResidues<component>
                # kts_[n}Residues{Component} -> {Component}_RESIDUALS_{n}
                found = self._get_new_column_name(
                    old_col_re=r'kts_([0-9]+)Residues([a-zA-Z_]+)',
                    old_col=old_col,
                    new_col_re=r'\2_RESIDUALS_\1')
                if found:
                    new_col = found
                    residues_cols.append((old_col, new_col))
                else:
                    # kts_[n}{Component} -> {Component}_{n}
                    found = self._get_new_column_name(
                        old_col_re=r'kts_([0-9]+)([a-zA-Z_]+)',
                        old_col=old_col,
                        new_col_re=r'\2_\1')
                    if found:
                        new_col = found
                        components_cols.append((old_col, new_col))
                    else:
                        found = self._get_new_column_name(
                            old_col_re=r'kts_([0-9]+)',
                            old_col=old_col,
                            new_col_re=r'PREDICTED_\1')
                        if found:
                            new_col = found
                            predicted_cols.append((old_col, new_col))
        # Writes the select SQL by renaming the columns
        sql = 'SELECT '
        # Starting columns
        for i, (old_col, new_col) in enumerate(start_cols):
            if i > 0:
                sql = sql + ', '
            sql = (sql + '{old_col} {new_col}'.format(
                old_col=quotename(old_col),
                new_col=quotename(new_col)))

        # kts_*, kts_*Components, kts_*Residues columns
        # Ordered by horizon, predicted, components, residues
        nb_horizons = len(predicted_cols)
        nb_components = 0
        if nb_horizons > 0:
            nb_components = int(len(components_cols)/nb_horizons)
        for horizon in range(nb_horizons):
            old_col, new_col = predicted_cols[horizon]
            sql = sql + ', '
            sql = (sql + '{old_col} {new_col}'.format(
                old_col=quotename(old_col),
                new_col=quotename(new_col)))
            for l_maps in [components_cols, residues_cols]:
                if l_maps:  # if the mapping is not empty
                    for i_component in range(nb_components):
                        old_col, new_col = l_maps[(horizon * nb_components) + i_component]
                        sql = sql + ', '
                        sql = (sql + '{old_col} {new_col}'.format(
                            old_col=quotename(old_col),
                            new_col=quotename(new_col)))
        if segment_column_name is None:
            order_by = quotename(self.time_column_name)
        else:
            order_by = quotename(segment_column_name) + ',' + quotename(self.time_column_name)
        sql = (sql
               + ' FROM ' + self.applyout_table_.name
               + ' ORDER BY ' + order_by)
        applyout_df_new = DataFrame(connection_context=self.conn_context,
                                    select_statement=sql)
        logger.info('DataFrame for predict ouput: %s', sql)
        return applyout_df_new

    def _create_sorted_view(self, data):
        """
        Creates a view on the dataset with ascending sort on the time column.
        It is required for APL TimeSeries.
        However, if the parameter 'sort_data' is False, the view is not created.
        Parameters
        ----------
        data : hana_ml DataFrame
            Input dataset

        Returns
        -------
        (new_view_name, data_sort)
            new_view_name : the newly created view name or None if no view is created
            data_sort : the new hana_ml DataFrame mapped to this new view or the original dataframe

        Note
        ----
        The dataset must not be a temporary table, otherwise the creation will fail in SAP HANA.
        """
        if not self.sort_data:
            # User can provide directly a (tmp) sorted view. View must not be created.
            return None, data
        if not self.time_column_name:
            raise Error('Cannot create view. time_column_name parameter is not defined.')
        new_view_name = 'TS_DATA_SORT_VIEW_{}'.format(self.id)
        segment_column_name = getattr(self, 'segment_column_name', None)
        if segment_column_name is None:
            order_by = quotename(self.time_column_name)
        else:
            order_by = quotename(segment_column_name) + ',' + quotename(self.time_column_name)
        # Create a hana view and return a APLArtifactView instance
        artifact_view = self._create_view(view_name=new_view_name,
                          data=data,
                          order_by=order_by)
        data_sort = DataFrame(self.conn_context, artifact_view.select_clause)
        return new_view_name, data_sort

    def _create_forecast_input_view(self, extra_pred_data):
        """
        Creates a view on the dataset extended with the out-of-sample extra-predictable variables.
        The resulted view will be used by the forecast() method.

        Returns
        ------
        (new_view_name, data_sort)
            new_view_name : the newly created view
            data_sort : the new hana_ml DataFrame mapped to this new view
        """
        if extra_pred_data:
            # Union train_dataset + extra_pred_data
            sql = 'select * from ({TRAIN}) union '
            if self.target in extra_pred_data.columns:
                sql = (sql +
                       'select * from ({EXTRA_PRED})')
            else:
                sql = (sql +
                       'select *, null {TARGET_VAR} from ({EXTRA_PRED})')
            sql = sql.format(TRAIN=(self.train_data_.select_statement),
                             TARGET_VAR=quotename(self.target),
                             EXTRA_PRED=extra_pred_data.select_statement,
                            )
            new_df = DataFrame(connection_context=self.conn_context, select_statement=sql)
        else:
            # When there is no extra-predictable variable, forecast is only based on train dataset
            # + forecast_length
            new_df = self.train_data_
        return self._create_sorted_view(new_df)

    def _create_forecast_out(self, data, horizon):
        #pylint: disable=too-many-locals
        """
        Creates the Table object for the FORECAST APL function output (fit_predict() method).
        Because we are limited to a single call to FORECAST function, the table definition cannot
        be determined via intermediary calls to APL.
        Hence, we have to hard-code the table definition based on the extra_applyout_settings.

        Parameters
        ----------
        data: hana_ml DataFrame
            The input dataset
        horizon: int
            The horizon
        Returns
        -------
        A APLArtifactApplyOutTable object containing the table definition
        """

        if not horizon:
            horizon = self.horizon

        data_types_dict = {}  # {'ColumnName' : 'HanaType'}
        for dtype in data.dtypes():
            name, sqltype = dtype[0], dtype[1]
            if sqltype == 'DECIMAL':
                # parse_one_dtype raise TypeError if DECIMAL
                colname, sqltype = (name, 'DOUBLE')
            else:
                colname, sqltype = parse_one_dtype(dtype)
            data_types_dict[colname] = sqltype

        if hasattr(self, 'extra_applyout_settings') and self.extra_applyout_settings:
            extra_mode = self.extra_applyout_settings['APL/ApplyExtraMode']
        else:
            extra_mode = 'Forecasts and Error Bars'

        decompose_influencers = False
        if extra_mode == 'First Forecast with Stable Components and Residues and Error Bars':
            decompose_influencers = True
            if hasattr(self, 'other_train_apl_aliases') and self.other_train_apl_aliases and \
               'APL/ActivateExplanations' in self.other_train_apl_aliases and \
               self.other_train_apl_aliases['APL/ActivateExplanations'] == 'false':
                decompose_influencers = False

        if extra_mode in ['First Forecast with Stable Components and Residues and Error Bars',
                          'Forecasts and Error Bars']:
            horizon = 1

        applyout_tbl_specs = [
            (self.time_column_name, data_types_dict[self.time_column_name]),
            (self.target, data_types_dict[self.target]),
        ]

        segment_column_name = getattr(self, 'segment_column_name', None)
        if segment_column_name is not None:
            segment_column_type = data.get_table_structure()[segment_column_name]
            applyout_tbl_specs.insert(0, (segment_column_name, segment_column_type))

        for i in range(1, horizon + 1):
            applyout_tbl_specs.append(('kts_%d' % i, 'DOUBLE'))

        if extra_mode != 'No Extra':
            applyout_tbl_specs.extend([('kts_1_lowerlimit_95%', 'DOUBLE'),
                                       ('kts_1_upperlimit_95%', 'DOUBLE')])

        if extra_mode not in ['No Extra', 'Forecasts and Error Bars']:
            for i in range(1, horizon + 1):
                if decompose_influencers:
                    components = ['Trend', 'Cycles', 'ExtraPreds', 'Fluctuations', 'Residues']
                else:
                    components = ['Trend', 'Cycles', 'Fluctuations']
                for comp in components:
                    applyout_tbl_specs.extend([('kts_{}{}'.format(i, comp), 'DOUBLE')])
                    if not decompose_influencers:
                        applyout_tbl_specs.extend([('kts_{}Residues{}'.format(i, comp), 'DOUBLE')])

        # Create Apply-out table
        applyout_tbl_name = '#APPLYOUT_%s' % (self.id)  # name of table
        # applyout_table = Table(applyout_tbl_name, applyout_tbl_specs)
        applyout_table_def = ', '.join(['{col_name} {col_type}'.format(col_name=quotename(col_name),
                                                                       col_type=col_type)
                                        for col_name, col_type in applyout_tbl_specs
                                       ]
                                      )
        applyout_table_def = '({})'.format(applyout_table_def)
        applyout_table = APLArtifactApplyOutTable(
            name=applyout_tbl_name,
            table_definition=applyout_table_def,
            apl_version=self._apl_version,
            data=[]
        )
        return applyout_table

    def _call_apl_forecast(self, data, key, features, target, horizon):
        #pylint: disable=too-many-arguments
        #pylint: disable=too-many-branches
        #pylint:disable=too-many-locals
        #pylint: disable=too-many-statements
        """
        Calls the FORECAST APL function.

        Parameters
        ----------
        data : DataFrame
            The training dataset
        key : str, optional
            The name of the ID column
        features : list of str, optional
            The names of the feature columns
        target : str, optional
            The name of the target variable
        horizon : int, optional
            The number of forecasts

        """
        # -- get and check the validity of input params key, features, target
        key = self._arg('key', key, str)
        features = self._arg('features', features, ListOfStrings)
        target = self._arg('target', target, str)
        self._check_valid(data, key=key, features=features, label=target)

        # -- prepare TRAIN_CONFIG artifact
        forecast_config_table = self._create_forecast_config_table(horizon)

        try:
            # ---- prepare train dataset
            # Before doing a guess variable, need to materialize the dataset.
            # We can either make a view or create a new table
            # View offers more performance
            # But view is impossible when the dataset is temporary table
            try:
                data_view_name = "TRAIN_DATA_VIEW_{}".format(self.id)
                # Create a hana view and return a APLArtifactView instance
                artifact_view = self._create_view(view_name=data_view_name, data=data)
            except dbapi.Error:
                # fall back if view can't be created
                # (because the original dataset is a temp table)
                data_view_name = "#TRAIN_DATA_VIEW_{}".format(self.id)
                artifact_view = self._materialize_w_type_conv(name=data_view_name, data=data)

            # applyout table
            applyout_table = self._create_forecast_out(data, horizon)

            # ---- VARIABLE_DESCRIPTION
            var_desc_table = self._create_var_desc_table(key, target, data_view_name)

            # --- HEADER_FUNCTION
            func_header_table = self._create_func_header_table()

            # --- VARIABLE_ROLES
            var_roles_table = self._create_var_roles_table(
                data=data,
                key=key,
                label=target,
                features=features,
                weight=None,
            )

            # --- Prepare Output tables
            # OPERATION_LOG,
            operation_log_table = self._create_operation_log_table(
                '#FIT_LOG_{}'.format(self.id))

            # SUMMARY,
            summary_table = self._create_summary_table()

            # INDICATORS
            indicators_table = APLArtifactTable(
                name='#INDICATORS_{}'.format(self.id),
                type_name=APLArtifactTable.INDICATORS,
                apl_version=self._apl_version)

            # DEBRIEF_METRIC
            debrief_metric_table = self._create_debrief_metric_table()

            # DEBRIEF_PROPERTY
            debrief_property_table = self._create_debrief_property_table()

            # Materialize artifacts prior to calling APL
            self._create_artifact_table(func_header_table)
            self._create_artifact_table(forecast_config_table)
            self._create_artifact_table(var_desc_table)
            self._create_artifact_table(var_roles_table)
            self._create_artifact_table(applyout_table)
            self._create_artifact_table(operation_log_table)
            self._create_artifact_table(summary_table)
            self._create_artifact_table(indicators_table)
            self._create_artifact_table(debrief_metric_table)
            self._create_artifact_table(debrief_property_table)

            # --- Call procedure
            # If DU is used, applyout_table must be passed as string
            applyout_table_arg = applyout_table
            if self._sql_generation_mode == SqlGenerationMode.SQLBLOCK_DU:
                procedure_name = "APL_FORECAST_AND_DEBRIEF"
                self._call_apl(
                    procedure_name,
                    input_tables=[
                        func_header_table,
                        forecast_config_table,
                        var_desc_table,
                        var_roles_table,
                        artifact_view,
                        # For DU, the applyout table must be given as string (it suppose it was created beforehand)
                        APLArtifactView(name=applyout_table.name,
                                        select_clause=f"SELECT * FROM {applyout_table_arg.name}"),
                    ],
                    output_tables=[
                        operation_log_table,
                        summary_table,
                        indicators_table,
                        debrief_metric_table,
                        debrief_property_table
                    ])
            else:
                procedure_name = "APL_FORECAST__OVERLOAD_5_6"
                self._call_apl(
                    procedure_name,
                    input_tables=[
                        func_header_table,
                        forecast_config_table,
                        var_desc_table,
                        var_roles_table,
                        artifact_view,
                    ],
                    output_tables=[
                        applyout_table_arg,
                        operation_log_table,
                        summary_table,
                        indicators_table,
                        debrief_metric_table,
                        debrief_property_table
                    ])
            # Records the SQL artifacts that can be restituted later for HDI
            self._record_sql_artifact(
                model_method='fit_predict',
                apl_function="APL_FORECAST__OVERLOAD_5_6",
                input_tables=[
                    func_header_table,
                    forecast_config_table,
                    var_desc_table,
                    var_roles_table,
                    artifact_view,
                ],
                output_tables=[
                    applyout_table_arg,
                    operation_log_table,
                    summary_table,
                    indicators_table,
                    debrief_metric_table,
                    debrief_property_table
                ])
        except dbapi.Error as db_er:
            # do stuff with the error, and also raise to the caller,
            # clean up the table used in fit function
            logger.error("An issue was encounted during the model fitting: %s",
                         db_er, exc_info=True)
            self._drop_artifact_tables()
            raise
        finally:
            # clean created view
            self._try_drop_view(data_view_name)

        # --- Save returned artifacts
        # No model returned by APL_FORECAST
        self.model_table_ = None
        self.model_ = None
        # It is useless to keep the model as Dataframe, just for compatibility
        # --- capture the other output artifacts as hana Dataframes
        self.summary_ = self.conn_context.table(summary_table.name) #pylint:disable=attribute-defined-outside-init
        self.indicators_ = self.conn_context.table(indicators_table.name) #pylint:disable=attribute-defined-outside-init
        self.debrief_metric_table_ = debrief_metric_table #pylint:disable=attribute-defined-outside-init
        self.debrief_property_table_ = debrief_property_table #pylint:disable=attribute-defined-outside-init
        self.fit_operation_log_ = self.conn_context.table( #pylint:disable=attribute-defined-outside-init
            operation_log_table.name)
        self.predict_operation_log_ = self.fit_operation_log_  #pylint:disable=attribute-defined-outside-init
        self.var_desc_ = self.conn_context.table(var_desc_table.name) #pylint:disable=attribute-defined-outside-init

        self.applyout_ = self.conn_context.table(applyout_table.name) #pylint:disable=attribute-defined-outside-init
        self.applyout_table_ = applyout_table #pylint:disable=attribute-defined-outside-init
        return self.applyout_

    def export_apply_code(self, code_type, key=None, label=None, schema_name=None, table_name=None,
                          other_params=None):
        raise NotImplementedError

    def build_report(self, segment_name=None, max_local_explanations=100):
        """
        Build model report.

        Parameters
        ----------
        segment_name: str, optional
            If the model is segmented, the segment name for which the report will be built.
        max_local_explanations: int, optional
            The maximum number of local explanations displayed in the report.
        """
        overview_df = self.get_debrief_report('TimeSeries_ModelOverview').collect()
        perf_df = self.get_debrief_report('TimeSeries_Performance').collect()
        outliers_df = self.get_debrief_report('TimeSeries_Outliers').collect()
        components_df = self.get_debrief_report('TimeSeries_Decomposition').collect() \
            .sort_values(by=['Row'], ascending=False).reset_index()
        change_points_df = self.get_debrief_report('TimeSeries_ChangePoints').collect()
        num_influencers_df = self.get_debrief_report('TimeSeries_ImpactInfluencersNumerical') \
            .collect()
        cat_influencers_df = self.get_debrief_report('TimeSeries_ImpactInfluencersCategorical') \
            .collect()
        cycles_df = self.get_debrief_report('TimeSeries_ImpactCycles').collect() \
            .sort_values(by=['Cycle Rank', 'Row']).reset_index()
        local_explanations_df = pd.DataFrame(columns=['Oid'])
        apl_version = self._get_apl_version()
        if apl_version >= 2325:
            local_explanations_df = self.get_debrief_report('TimeSeries_ForecastBreakdown').collect()

        segment_column = getattr(self, 'segment_column_name', None)
        segments = overview_df['Oid'].unique().tolist()

        if len(segments) > 1 and segment_name is None:
            raise ValueError(
                'The model is segmented. Please provide a value for the parameter "segment_name".')
        if segment_name is not None and segment_name not in segments:
            raise ValueError('Unknown segment name.')

        offset = 1 if segment_column is None else 2
        expected_columns = ['ACTUAL', 'LOWER_INT_95PCT', 'UPPER_INT_95PCT',
                            'PREDICTED_1', 'Trend_1', 'Cycles_1',
                            'ExtraPreds_1', 'Fluctuations_1', 'Residues_1']
        if self.applyout_ is not None and \
           self.applyout_.columns[offset:] == expected_columns:
            detail_level = _ModelReportDetailLevel.FULL
            forecast_df = self.applyout_.collect()
        else:
            if self.applyout_ is not None:
                warnings.warn('Please set the "extra_applyout_settings" parameter to '
                              '"{\'APL/ApplyExtraMode\': \'First Forecast with Stable Components '
                              'and Residues and Error Bars\'}" before calling "fit_predict", '
                              '"predict" or "forecast" to get the expected forecast chart in the '
                              'model report.')
            detail_level = _ModelReportDetailLevel.NO_FORECAST

        if segment_name is not None:
            overview_df = overview_df[overview_df['Oid'] == segment_name]
            perf_df = perf_df[perf_df['Oid'] == segment_name]
            outliers_df = outliers_df[outliers_df['Oid'] == segment_name]
            components_df = components_df[components_df['Oid'] == segment_name]
            change_points_df = change_points_df[change_points_df['Oid'] == segment_name]
            num_influencers_df = num_influencers_df[num_influencers_df['Oid'] == segment_name]
            cat_influencers_df = cat_influencers_df[cat_influencers_df['Oid'] == segment_name]
            cycles_df = cycles_df[cycles_df['Oid'] == segment_name]
            local_explanations_df = local_explanations_df[local_explanations_df['Oid'] == segment_name]

            if detail_level != _ModelReportDetailLevel.NO_FORECAST:
                forecast_df = forecast_df[forecast_df[segment_column].astype(str) == segment_name]

        date = overview_df['Date Variable'].iloc[0]
        target = overview_df['Target Variable'].iloc[0]
        last_date = overview_df['Last Date'].iloc[0]

        self._report_builder = ReportBuilder(title='Time Series Model Report (APL)')

        overview_page = Page('Overview')
        model_summary = DescriptionItem('Model Summary')
        model_summary.add('Building Date', overview_df['Build Date'].astype(str).iloc[0])
        model_summary.add('Learning Time (sec)', overview_df['Learn Time (sec)'].iloc[0])
        model_summary.add('Date Variable', date)
        model_summary.add('Target Variable', target)
        model_summary.add('First Date', overview_df['First Date'].iloc[0])
        model_summary.add('Last Date', overview_df['Last Date'].iloc[0])
        if segment_name is not None:
            model_summary.add('Segment Variable', segment_column)
            model_summary.add('Segment', segment_name)
        model_summary.add('Horizon', overview_df['Horizon'].iloc[0])
        overview_page.addItem(model_summary)
        perf_summary = DescriptionItem('Performance Summary')
        perf_summary.add('Horizon-Wide MAPE',
                         str(perf_df[perf_df['Partition'] == 'Validation']['MAPE'].iloc[0]) + '%')
        overview_page.addItem(perf_summary)
        self._report_builder.addPage(overview_page)

        if detail_level != _ModelReportDetailLevel.NO_FORECAST:
            forecast_page = Page('Forecast')
            forecast_df['ACTUAL'] = forecast_df['ACTUAL'].fillna(value=np.nan).astype(float)
            forecast_df = forecast_df.round(decimals=3)
            forecast_df[date] = forecast_df[date].astype(str)
            forecast_chart_data = []
            actual_data = forecast_df[[date, 'ACTUAL']].dropna()
            forecast_chart_data.append({'source': [actual_data[date].tolist(),
                                                   actual_data['ACTUAL'].tolist()]})
            forecast_data = forecast_df[[date, 'PREDICTED_1']].dropna()
            forecast_chart_data.append({'source': [forecast_data[date].tolist(),
                                                   forecast_data['PREDICTED_1'].tolist()]})
            forecast_chart_data.append({'source': [outliers_df['Date'].astype(str).tolist(),
                                                   outliers_df['Signal'].tolist()]})
            error_max_data = forecast_df[[date, 'UPPER_INT_95PCT']].dropna()
            forecast_chart_data.append({'source': [error_max_data[date].tolist(),
                                                   error_max_data['UPPER_INT_95PCT'].tolist()]})
            error_min_data = forecast_df[[date, 'LOWER_INT_95PCT']].dropna()
            forecast_chart_data.append({'source': [error_min_data[date].tolist(),
                                                   error_min_data['LOWER_INT_95PCT'].tolist()]})
            forecast_chart_series = []
            forecast_chart_series_infos = [('Actual', '#93bfeb', 'line', 'solid'),
                                           ('Forecast', '#2f6497', 'line', 'solid'),
                                           ('Outliers', '#b90c0d', 'scatter', 'solid'),
                                           ('Error Max', '#2f6497', 'line', 'dashed'),
                                           ('Error Min', '#2f6497', 'line', 'dashed')]
            for i, (name, color, serie_type, line_type) in enumerate(forecast_chart_series_infos):
                serie = {
                    'datasetIndex': i,
                    'type': serie_type,
                    'seriesLayoutBy': 'row',
                    'name': name,
                    'emphasis': {'focus': 'self'},
                    'lineStyle': {'color': color, 'type': line_type},
                    'itemStyle': {'color': color},
                    'showSymbol': 'false'
                }
                forecast_chart_series.append(serie)
            forecast_chart_config = {
                'tips': [
                    'This graph shows curves for the predicted values (forecast) and actual values (target) over time.'
                ],
                'dataset': forecast_chart_data,
                'grid': {'show': 'true', 'containLabel': 'true'},
                'legend': {},
                'xAxis': {
                    'name': date,
                    'type': 'time',
                },
                'yAxis': {
                    'name': target,
                    'type': 'value',
                    'axisLine': {'show': 'true'},
                    'axisTick': {'show': 'true'},
                },
                'tooltip': {'trigger': 'axis'},
                'series': forecast_chart_series,
                'lazyLoad': 'false',
                'toolbox': {
                    'feature': {
                        'dataZoom': {
                            'yAxisIndex': 'none'
                        },
                        'restore': {},
                        'saveAsImage': {
                            'name': 'forecast'
                        }
                    }
                },
            }
            forecast_chart = ChartItem('Forecast vs. Actual', forecast_chart_config)
            forecast_page.addItem(forecast_chart)
            forecast_table = TableItem('Forecasts')
            forecast_table_df = forecast_df[[date, 'PREDICTED_1',
                                             'UPPER_INT_95PCT', 'LOWER_INT_95PCT']].dropna()
            forecast_table.addColumn(date, forecast_table_df[date].tolist())
            forecast_table.addColumn('Forecast', forecast_table_df['PREDICTED_1'].tolist())
            forecast_table.addColumn('Error Max', forecast_table_df['UPPER_INT_95PCT'].tolist())
            forecast_table.addColumn('Error Min', forecast_table_df['LOWER_INT_95PCT'].tolist())
            forecast_page.addItem(forecast_table)
            if not outliers_df.empty:
                outliers_table = TableItem('Outliers')
                outliers_table.addColumn(date, outliers_df['Date'].astype(str).tolist())
                outliers_table.addColumn('Actual', outliers_df['Signal'].tolist())
                outliers_table.addColumn('Forecast', outliers_df['Forecast'].tolist())
                forecast_page.addItem(outliers_table)
            self._report_builder.addPage(forecast_page)

        perf_page = Page('Performance')
        for partition_name in ['Test', 'Validation', 'Estimation']:
            partition_perf = TableItem(partition_name)
            partition_perf_df = perf_df[perf_df['Partition'] == partition_name].copy()
            if not partition_perf_df.empty:
                partition_perf_df.drop(['Oid', 'Partition'], axis=1, inplace=True)
                metric_values = []
                for metric_name in partition_perf_df.columns:
                    metric_values.append(partition_perf_df[metric_name].iloc[0])
                partition_perf.addColumn('Metric Name', partition_perf_df.columns.tolist())
                partition_perf.addColumn('Metric Value', metric_values)
                perf_page.addItem(partition_perf)
        self._report_builder.addPage(perf_page)

        explanation_page = Page('Components')
        components_df['Relative Impact'] = 100 * components_df['Relative Impact']
        components_df = components_df.round(decimals=3)
        unused_components = components_df[components_df['Relative Impact'] <= 0]['Type'].tolist()
        components_df = components_df[components_df['Relative Impact'] > 0]
        components_df['Type'] = components_df['Type'].replace({'Cycles': 'Cycle', 'Influencers': 'Influencer'})
        components_impacts = components_df['Relative Impact'].tolist()
        components_types = components_df['Type'].tolist()
        components_labels = components_df[['Type', 'Item']].stack().groupby(level=0) \
            .agg(': '.join).tolist()
        components_data = []
        color_palette = {'Trend': '#ed4a7b', 'Cycle': '#e8743b', 'Residuals': '#6c8893',
                         'Influencer': '#19a979', 'Fluctuations': '#945ecf'}
        for i, component_type in enumerate(components_types):
            components_data.append({'value': components_impacts[i],
                                    'itemStyle': {'color': color_palette[component_type]}})
        components_config = {
            'tips': [
                'This graph represents the relative impact of each component found by the predictive model.',
                'See <a href="https://help.sap.com/docs/apl/7223667230cb471ea916200712a9c682/774b073b60734d448dd010a8953beb20.html" target="_blank">Time Series Modeling</a> in the SAP HANA APL Developer Guide.'
            ],
            'tooltip': {
                'trigger': 'axis',
                'axisPointer': {'type': 'shadow'}
            },
            'legend': {},
            'grid': {'show': 'true', 'containLabel': 'true'},
            'xAxis': {
                'name': 'Impact (%)',
                'type': 'value',
                'axisLine': {'show': 'true'},
                'axisTick': {'show': 'true'},
            },
            'yAxis': {
                'name': 'Component',
                'type': 'category',
                'data': components_labels,
                'axisLabel': {
                    'interval': 0
                }
            },
            'series': [
                {
                    'type': 'bar',
                    'data': components_data
                }
            ],
            'toolbox': {
                'feature': {
                    'saveAsImage': {
                        'name': 'components'
                    }
                }
            },
        }
        components_chart = ChartItem('Component Impact', components_config)
        explanation_page.addItem(components_chart)
        if detail_level != _ModelReportDetailLevel.NO_FORECAST:
            breakdown_chart_datasets = []
            breakdown_chart_datasets.append({'source': [actual_data[date].tolist(),
                                                        actual_data['ACTUAL'].tolist()]})
            forecast_data_future = forecast_data[forecast_data[date] > last_date]
            breakdown_chart_datasets.append(
                {'source': [forecast_data_future[date].tolist(),
                            forecast_data_future['PREDICTED_1'].tolist()]})
            component_cols = ['Trend_1', 'Cycles_1', 'ExtraPreds_1',
                              'Fluctuations_1', 'Residues_1']
            for component_col in component_cols:
                component_data = forecast_df[[date, component_col]].dropna()
                breakdown_chart_datasets.append(
                    {'source': [component_data[date].tolist(),
                                component_data[component_col].tolist()]})
            breakdown_chart_series = []
            for i, (name, color) in enumerate([('Actual', '#93bfeb'),
                                               ('Forecast', '#2f6497'),
                                               ('Trend', '#ed4a7b'),
                                               ('Cycles', '#e8743b'),
                                               ('Influencers', '#19a979'),
                                               ('Fluctuations', '#945ecf'),
                                               ('Residuals', '#6c8893')]):
                if name in unused_components:
                    continue
                serie = {
                    'datasetIndex': i,
                    'type': 'line',
                    'seriesLayoutBy': 'row',
                    'name': name,
                    'emphasis': {'focus': 'self'},
                    'lineStyle': {'color': color, 'type': 'solid'},
                    'itemStyle': {'color': color},
                    'showSymbol': 'false',
                }
                if name == 'Trend':
                    first_trend_point = breakdown_chart_datasets[i]['source'][0][0]

                    change_points_data = []
                    for change_point in change_points_df['Date'].tolist():
                        if change_point >= first_trend_point:
                            change_points_data.append({'xAxis': change_point})
                    serie['markLine'] = {
                        'symbol': 'none',
                        'lineStyle': {'color': '#ed4a7b'},
                        'data': change_points_data
                    }
                breakdown_chart_series.append(serie)
            breakdown_chart_config = {
                'tips': [
                    'This graph shows how the predictive model decomposed the series (horizontal lines); a vertical dotted line appears if there is a change in the orientation of the trend.',
                    'See <a href="https://help.sap.com/docs/apl/7223667230cb471ea916200712a9c682/774b073b60734d448dd010a8953beb20.html" target="_blank">Time Series Modeling</a> in the SAP HANA APL Developer Guide.'
                ],
                'dataset': breakdown_chart_datasets,
                'grid': {'show': 'true', 'containLabel': 'true'},
                'legend': {},
                'xAxis': {
                    'name': date,
                    'type': 'time',
                    'axisLine': {'onZero': 'false', 'show': 'true'},
                    'axisTick': {'show': 'true'},
                },
                'yAxis': {
                    'name': target,
                    'type': 'value',
                    'axisLine': {'onZero': 'false', 'show': 'true'},
                    'axisTick': {'show': 'true'},
                },
                'tooltip': {'trigger': 'axis'},
                'series': breakdown_chart_series,
                'lazyLoad': 'false',
                'toolbox': {
                    'feature': {
                        'dataZoom': {
                            'yAxisIndex': 'none'
                        },
                        'restore': {},
                        'saveAsImage': {
                            'name': 'breakdown'
                        }
                    }
                },
            }
            breakdown_chart = ChartItem('Breakdown', breakdown_chart_config)
            explanation_page.addItem(breakdown_chart)
        self._report_builder.addPage(explanation_page)

        influencers_page = Page('Influencers')
        influencers_df = pd.concat([num_influencers_df, cat_influencers_df],
                                   axis=0, ignore_index=True) \
            .sort_values(by=['Influencer Rank', 'Category Rank'],
                         ascending=[True, False]).reset_index()
        if not influencers_df.empty:
            influencers = influencers_df['Influencer Name'].unique().tolist()
            for influencer in influencers:
                influencer_df = influencers_df[influencers_df['Influencer Name'] == influencer]
                if pd.isna(influencer_df['Influencer Value'].iloc[0]):
                    # categorical influencer
                    influencer_values = influencer_df['Category Impact'].tolist()
                    influencer_data = []
                    for influencer_value in influencer_values:
                        influencer_data.append({'value': influencer_value,
                                                'itemStyle': {'color': '#19a979'}})
                    influencer_config = {
                        'tips': [
                            'This graph represents the impact of the influencer, that is how much the influencer increases or decreases the value predicted for each influencer value.',
                            'See <a href="https://help.sap.com/docs/apl/7223667230cb471ea916200712a9c682/774b073b60734d448dd010a8953beb20.html" target="_blank">Time Series Modeling</a> in the SAP HANA APL Developer Guide.'
                        ],
                        'tooltip': {
                            'trigger': 'axis',
                            'axisPointer': {'type': 'shadow'}
                        },
                        'legend': {},
                        'grid': {'show': 'true', 'containLabel': 'true'},
                        'xAxis': {
                            'name': 'Impact',
                            'type': 'value',
                            'axisLine': {'show': 'true'},
                            'axisTick': {'show': 'true'},
                        },
                        'yAxis': {
                            'name': 'Influencer Value',
                            'type': 'category',
                            'data': influencer_df['Category Value'].tolist(),
                        },
                        'series': [
                            {
                                'type': 'bar',
                                'data': influencer_data
                            }
                        ],
                        'toolbox': {
                            'feature': {
                                'saveAsImage': {
                                    'name': 'influencer_' + influencer
                                }
                            }
                        },
                    }
                    influencer_chart = ChartItem(influencer, influencer_config)
                    influencers_page.addItem(influencer_chart)
                else:
                    # numerical influencer
                    influencer_config = {
                        'tips': [
                            'This graph represents the impact of the influencer, that is how much the influencer increases or decreases the value predicted for each influencer value.',
                            'See <a href="https://help.sap.com/docs/apl/7223667230cb471ea916200712a9c682/774b073b60734d448dd010a8953beb20.html" target="_blank">Time Series Modeling</a> in the SAP HANA APL Developer Guide.'
                        ],
                        'dataset': [{'source': [influencer_df['Influencer Value'].tolist(),
                                                influencer_df['Impact'].tolist()]}],
                        'grid': {'show': 'true', 'containLabel': 'true'},
                        'xAxis': {
                            'name': 'Influencer Value',
                            'type': 'value',
                        },
                        'yAxis': {
                            'name': 'Impact',
                            'type': 'value',
                            'axisLine': {'show': 'true'},
                            'axisTick': {'show': 'true'},
                        },
                        'tooltip': {'trigger': 'axis'},
                        'series': [
                            {
                                'datasetIndex': 0,
                                'type': 'line',
                                'seriesLayoutBy': 'row',
                                'name': 'Impact',
                                'emphasis': {'focus': 'self'},
                                'lineStyle': {'color': '#19a979', 'type': 'solid'},
                                'itemStyle': {'color': '#19a979'},
                                'showSymbol': 'true'
                            }
                        ],
                        'lazyLoad': 'false',
                        'toolbox': {
                            'feature': {
                                'dataZoom': {
                                    'yAxisIndex': 'none'
                                },
                                'saveAsImage': {
                                    'name': 'influencer_' + influencer
                                }
                            }
                        },
                    }
                    influencer_chart = ChartItem(influencer, influencer_config)
                    influencers_page.addItem(influencer_chart)
            self._report_builder.addPage(influencers_page)

        if not cycles_df.empty:
            cycles_page = Page('Cycles')
            cycles = cycles_df['Cycle name'].unique().tolist()
            for cycle in cycles:
                cycle_df = cycles_df[cycles_df['Cycle name'] == cycle]
                cycle_periods = cycle_df['Period'].unique().tolist()
                seasons = cycle_df['Season'].dropna().unique().tolist()
                seasons = [''] if not seasons else seasons
                cycle_series = []
                season_colors = ['#e8743b', '#f08956', '#ffb18a']
                for i in range(len(seasons)):
                    cycle_series.insert(0, {'type': 'bar',
                                            'itemStyle': {'color': season_colors[i]}})
                cycle_data = [['Period', *seasons]]
                for period in cycle_periods:
                    period_impacts = cycle_df[cycle_df['Period'] == period]['Impact'].tolist()
                    cycle_data.append([period, *period_impacts])
                cycle_config = {
                    'tips': [
                        'This graph represents the impact of the cycle, that is how much the cycle increases or decreases the value predicted for each period.',
                        'See <a href="https://help.sap.com/docs/apl/7223667230cb471ea916200712a9c682/774b073b60734d448dd010a8953beb20.html" target="_blank">Time Series Modeling</a> in the SAP HANA APL Developer Guide.'
                    ],
                    'tooltip': {
                        'trigger': 'axis',
                        'axisPointer': {'type': 'shadow'}
                    },
                    'legend': {},
                    'dataset': {
                        'source': cycle_data
                    },
                    'grid': {'show': 'true', 'containLabel': 'true'},
                    'xAxis': {
                        'name': 'Period',
                        'type': 'category',
                        'axisLine': {'show': 'true'},
                        'axisTick': {'show': 'true'},
                        'axisLabel': {
                            'rotate': 30
                        }
                    },
                    'yAxis': {
                        'name': 'Impact',
                        'type': 'value',
                        'axisLine': {'show': 'true'},
                        'axisTick': {'show': 'true'},
                    },
                    'series': cycle_series,
                    'toolbox': {
                        'feature': {
                            'saveAsImage': {
                                'name': 'cycle_' + cycle
                            }
                        }
                    },
                }
                cycle_chart = ChartItem(cycle, cycle_config)
                cycles_page.addItem(cycle_chart)
            self._report_builder.addPage(cycles_page)

        local_explanations_df.dropna(how='all', inplace=True)
        if (apl_version >= 2325) and (not local_explanations_df.empty):
            explanations_page = Page('Local Explanations')
            local_explanations_df['Type'] = local_explanations_df['Type'].replace({'Cycles': 'Cycle', 'Influencers': 'Influencer'})
            dates = local_explanations_df['Date'].unique()
            variable_stats = self.get_debrief_report('Statistics_Variables').collect()

            def get_component_label(row):
                if row['Value'] is None:
                    return row['Type'] + ': ' + row['Item']
                else:
                    value = row['Value']
                    if row['Type'] == 'Influencer':
                        storage = variable_stats[variable_stats['Variable'] == row['Item']]['Storage'].iloc[0]
                        if storage == 'number':
                            value = '{:.3f}'.format(float(row['Value']))
                    return row['Type'] + ': ' + row['Item'] + ' [' + value + ']'

            for i, date in enumerate(dates):
                if i == max_local_explanations:
                    logger.warning("The number of local explanations displayed in the report is "
                                   "limited to %s. Please use the 'max_local_explanations' "
                                   "parameter of the 'build_report' method to modify this value.",
                                   str(max_local_explanations))
                    break

                explanation_df = local_explanations_df[local_explanations_df['Date'] == date].sort_values(by='Row', ascending=True, ignore_index=True)
                explanation_df['Component Labels'] = explanation_df.apply(get_component_label, axis=1) \
                    .replace({'Other: Others-': 'Negative Others',
                              'Other: Others+': 'Positive Others'})

                start_values = []
                end_values = []

                sum_impacts = 0

                for contribution in explanation_df['Impact'].tolist():
                    start_values.append(sum_impacts)
                    sum_impacts = sum_impacts + contribution
                    end_values.append(sum_impacts)

                explanation_df['Start Values'] = start_values
                explanation_df['End Values'] = end_values

                waterfall_data_df = explanation_df.sort_values(by='Row', ascending=False, ignore_index=True)[['Component Labels', 'Start Values', 'End Values']]
                waterfall_data = [waterfall_data_df.columns.values.tolist()] + [['Total', 0, sum_impacts]] + waterfall_data_df.values.tolist()

                explanations_config = {
                    'customFn': ['series[0].label.formatter', 'series[0].renderItem', 'tooltip.formatter'],
                    'tooltip': {
                        'trigger': 'axis',
                        'axisPointer': {'type': 'none'},
                        'formatter': {
                            'params': ['params'],
                            'body': 'return params[0].axisValueLabel;'
                        }
                    },
                    'grid': {'show': 'true', 'containLabel': 'true'},
                    'dataset': {
                        'source': waterfall_data,
                    },
                    'xAxis': {
                        'name': 'Impact',
                        'type': 'value',
                        'axisLine': {'show': 'true'},
                        'axisTick': {'show': 'true'},
                        'min': min(0, min(end_values)),
                        'max': max(0, max(end_values))
                    },
                    'yAxis': {
                        'name': 'Component',
                        'type': 'category',
                        'axisLabel': {
                            'interval': 0
                        }
                    },
                    'series': [
                        {
                            'type': 'custom',
                            'label': {
                                'show': 'true',
                                'position': 'right',
                                'color': '#60594F',
                                'fontWeight': 'bold',
                                'formatter': {
                                    'params': ['params'],
                                    'body': ''.join([
                                        "const labelData = params.data[2] - params.data[1];",
                                        "return labelData.toFixed(3);"
                                    ])
                                }
                            },
                            'datasetIndex': 0,
                            'encode': {
                                'y': 0,
                            },
                            'renderItem': {
                                'params': ['params', 'api'],
                                'body': ''.join([
                                    "const dataIndex = api.value(0);",
                                    "const barStartValue = api.value(1);",
                                    "const barEndValue = api.value(2);",
                                    "const startCoord = api.coord([barStartValue, dataIndex]);",
                                    "const endCoord = api.coord([barEndValue, dataIndex]);",
                                    "const rectHeight = 10;",
                                    "const rectMinWidth = 1;",
                                    "let rectWidth = startCoord[0] - endCoord[0];",
                                    "const style = api.style();",
                                    "if (dataIndex === 0) {",
                                    "style.fill = '#6C8893';",
                                    "} else if (rectWidth > 0) {",
                                    "style.fill = '#ED5F5F';"
                                    "} else if (rectWidth === 0) {",
                                    "style.fill = '#ededed';",
                                    "} else {",
                                    "style.fill = '#2F6497';"
                                    "}",
                                    "rectWidth = rectWidth === 0 ? rectMinWidth : rectWidth;",
                                    "const rectItem = {",
                                    "type: 'rect',",
                                    "shape: {",
                                    "x: endCoord[0],",
                                    "y: endCoord[1] - rectHeight / 2,",
                                    "width: rectWidth,",
                                    "height: rectHeight,",
                                    "},",
                                    "style: style,",
                                    "};",
                                    "return rectItem;"
                                ])
                            }
                        },
                    ],
                    'toolbox': {
                        'feature': {
                            'saveAsImage': {
                                'name': 'local_explanations_' + str(date)
                            }
                        }
                    },
                }

                chart = ChartItem(str(date), explanations_config)
                explanations_page.addItem(chart)

            self._report_builder.addPage(explanations_page)

        self._report_builder.build()

    def generate_html_report(self, filename):
        """
        Save model report as a html file.

        Parameters
        ----------
        filename : str
            Html file name.
        """
        if self._report_builder is None:
            raise Exception('To generate a report, you must call the build_report method firstly.')

        self._report_builder.generate_html(filename)

    def generate_notebook_iframe_report(self):
        """
        Render model report as a notebook iframe.
        """
        if self._report_builder is None:
            raise Exception('To generate a report, you must call the build_report method firstly.')

        self._report_builder.generate_notebook_iframe()
