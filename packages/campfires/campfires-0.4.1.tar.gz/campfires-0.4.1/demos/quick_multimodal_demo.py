"""
Quick Multimodal Demo - Focused demonstration of key features.

This script provides a streamlined demonstration of:
1. Creating multimodal content
2. Using multimodal torches
3. Basic prompt engineering
4. Asset management
"""

import base64
from pathlib import Path

# Import key multimodal components
from campfires import (
    ContentType,
    MultimodalContent,
    MultimodalTorch,
    MultimodalPromptBuilder,
    get_prompt_for_content_types,
    generate_torch_id
)


def demo_multimodal_basics():
    """Demonstrate basic multimodal functionality."""
    print("ğŸ­ Campfires Multimodal Quick Demo")
    print("=" * 40)
    
    # 1. Create different types of content
    print("\nğŸ“ Creating multimodal content...")
    
    # Text content
    text_content = MultimodalContent(
        content_type=ContentType.TEXT,
        data="Welcome to Campfires multimodal capabilities!",
        metadata={"source": "demo", "language": "en"}
    )
    
    # Image content (simple SVG)
    svg_image = '''<svg width="100" height="100" xmlns="http://www.w3.org/2000/svg">
        <rect width="100" height="100" fill="#FF6B6B"/>
        <text x="50" y="55" text-anchor="middle" fill="white" font-size="14">ğŸ”¥</text>
    </svg>'''
    
    image_content = MultimodalContent(
        content_type=ContentType.IMAGE,
        data=base64.b64encode(svg_image.encode()).decode(),
        metadata={"format": "svg", "description": "Campfire logo"}
    )
    
    # Audio content (placeholder)
    audio_content = MultimodalContent(
        content_type=ContentType.AUDIO,
        data=base64.b64encode(b"fake_audio_data").decode(),
        metadata={"format": "wav", "duration": 3.0}
    )
    
    print(f"âœ… Created {len([text_content, image_content, audio_content])} content items")
    
    # 2. Create a multimodal torch
    print("\nğŸ”¥ Creating multimodal torch...")
    
    import time
    
    torch = MultimodalTorch(
        torch_id=generate_torch_id(
            claim="Quick multimodal demo torch",
            source="demo_campfire",
            timestamp=time.time()
        ),
        contents=[text_content, image_content, audio_content],
        primary_claim="Quick multimodal demo torch",
        source_campfire="demo_campfire",
        channel="demo_channel",
        metadata={"demo": "quick_demo", "version": "1.0"}
    )
    
    print(f"ğŸ†” Torch ID: {torch.torch_id}")
    print(f"ğŸ“Š Total size: {torch.get_total_size()} bytes")
    print(f"ğŸ­ Content types: {torch.get_content_types()}")
    
    # 3. Demonstrate content filtering
    print("\nğŸ” Filtering content by type...")
    
    text_items = torch.get_text_contents()
    image_items = torch.get_image_contents()
    audio_items = torch.get_audio_contents()
    
    print(f"ğŸ“ Text items: {len(text_items)}")
    print(f"ğŸ–¼ï¸  Image items: {len(image_items)}")
    print(f"ğŸµ Audio items: {len(audio_items)}")
    
    # 4. Convert to different formats
    print("\nğŸ”„ Converting to different formats...")
    
    # Convert to MCP message
    mcp_message = torch.to_mcp_message()
    print(f"ğŸ“¨ MCP message parts: {len(mcp_message.get('content', []))}")
    
    # Convert to legacy torch (if needed)
    legacy_torch = torch.to_legacy_torch()
    print(f"ğŸ”™ Legacy torch ID: {legacy_torch.torch_id}")
    
    return torch


def demo_prompt_engineering():
    """Demonstrate multimodal prompt engineering."""
    print("\nğŸ’­ Demonstrating Prompt Engineering...")
    
    # Initialize prompt builder
    builder = MultimodalPromptBuilder()
    
    # 1. Get prompts for different content types
    print("\nğŸ¯ Getting specialized prompts...")
    
    # Vision analysis prompt
    vision_prompt = get_prompt_for_content_types([ContentType.IMAGE])
    print(f"ğŸ‘ï¸  Vision prompt (first 80 chars): {vision_prompt[:80]}...")
    
    # Audio analysis prompt
    audio_prompt = get_prompt_for_content_types([ContentType.AUDIO])
    print(f"ğŸµ Audio prompt (first 80 chars): {audio_prompt[:80]}...")
    
    # Multimodal prompt
    multimodal_prompt = get_prompt_for_content_types([
        ContentType.TEXT, 
        ContentType.IMAGE, 
        ContentType.AUDIO
    ])
    print(f"ğŸ­ Multimodal prompt (first 80 chars): {multimodal_prompt[:80]}...")
    
    # 2. Build custom prompts
    print("\nğŸ› ï¸  Building custom prompts...")
    
    try:
        # Simple prompt
        simple_prompt = (builder
                        .add_instruction("Analyze the provided multimodal content")
                        .add_content_analysis("multimodal", "comprehensive understanding")
                        .build())
        print(f"ğŸ“ Simple prompt created: {len(simple_prompt)} characters")
        
        # Reset builder for next prompt
        builder = MultimodalPromptBuilder()
        
        # Prompt with variables
        custom_prompt = (builder
                        .add_instruction("Analyze the image content")
                        .add_content_analysis("image", "objects and colors")
                        .add_output_format("structured JSON")
                        .build())
        print(f"ğŸ¨ Custom prompt created: {len(custom_prompt)} characters")
        
    except Exception as e:
        print(f"ğŸ“ Note: Custom prompts require template library setup")
        print(f"   Error: {str(e)[:50]}...")
    
    return {
        "vision": vision_prompt,
        "audio": audio_prompt,
        "multimodal": multimodal_prompt
    }


def demo_content_operations(torch):
    """Demonstrate content operations and analysis."""
    print("\nâš™ï¸  Demonstrating Content Operations...")
    
    # 1. Content analysis
    print("\nğŸ” Analyzing content...")
    
    for i, content in enumerate(torch.contents):
        print(f"\nContent {i+1}:")
        print(f"  Type: {content.content_type.value}")
        print(f"  Size: {content.get_size()} bytes")
        print(f"  Binary: {content.is_binary()}")
        print(f"  Metadata keys: {list(content.metadata.keys())}")
    
    # 2. Content statistics
    print(f"\nğŸ“Š Torch Statistics:")
    print(f"  Total contents: {len(torch.contents)}")
    print(f"  Total size: {torch.get_total_size()} bytes")
    print(f"  Unique types: {len(torch.get_content_types())}")
    
    # 3. Content validation
    print(f"\nâœ… Content Validation:")
    for content in torch.contents:
        is_valid = len(content.data) > 0 and content.content_type is not None
        print(f"  {content.content_type.value}: {'âœ… Valid' if is_valid else 'âŒ Invalid'}")


def demo_integration_examples():
    """Show examples of how to integrate with other systems."""
    print("\nğŸ”— Integration Examples...")
    
    print("\nğŸ“¡ OpenRouter Integration:")
    print("  # Create multimodal message for OpenRouter")
    print("  client = MultimodalOpenRouterClient(config)")
    print("  message = client.create_multimodal_message(")
    print("      text='Analyze this content',")
    print("      torch=multimodal_torch")
    print("  )")
    
    print("\nğŸ•ï¸  Campfire Integration:")
    print("  # Use in Campfire orchestration")
    print("  class MultimodalCamper(Camper, MultimodalLLMCamperMixin):")
    print("      async def process_torch(self, torch):")
    print("          return await self.analyze_multimodal_content(torch)")
    
    print("\nğŸ“¦ Party Box Integration:")
    print("  # Store and manage multimodal assets")
    print("  asset_manager = MultimodalAssetManager(storage_path)")
    print("  asset_hash = asset_manager.add_asset_from_torch(torch)")
    print("  assets = asset_manager.search_by_content_type('image')")


def main():
    """Run the quick multimodal demo."""
    try:
        # Run demonstrations
        torch = demo_multimodal_basics()
        prompts = demo_prompt_engineering()
        demo_content_operations(torch)
        demo_integration_examples()
        
        print("\n" + "=" * 40)
        print("ğŸ‰ Quick Demo Completed!")
        print("âœ¨ Campfires multimodal capabilities demonstrated")
        print("ğŸ“š Check the full demo for comprehensive examples")
        print("=" * 40)
        
    except Exception as e:
        print(f"\nâŒ Demo error: {str(e)}")
        print("ğŸ”§ This may be due to missing dependencies in demo environment")
        print("âœ… The multimodal structure has been demonstrated")


if __name__ == "__main__":
    main()