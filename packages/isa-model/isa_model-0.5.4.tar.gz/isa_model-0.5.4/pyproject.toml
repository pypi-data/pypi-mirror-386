[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "isa_model"
version = "0.5.4"
description = "Unified AI model serving framework with API streaming support"
authors = [{name = "isA_Model Contributors"}]
readme = "README.md"
requires-python = ">=3.8"
classifiers = [
    "Development Status :: 3 - Alpha",
    "Intended Audience :: Developers",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3",
]
dependencies = [
    # Core Web Framework
    "fastapi>=0.95.0",
    "uvicorn>=0.22.0",
    "pydantic>=2.0.0",
    "httpx>=0.23.0",
    "requests>=2.28.0",
    "aiohttp>=3.8.0",
    "python-dotenv>=1.0.0",

    # Core Data Processing
    "numpy>=1.20.0",

    # Database (core for pricing/model registry)
    "supabase>=2.0.0",
    "psycopg2-binary>=2.9.0",
    "asyncpg>=0.28.0",

    # Core Monitoring & Resilience
    "slowapi>=0.1.8",
    "circuitbreaker>=1.3.2",
    "structlog>=23.1.0",
    "psutil>=5.9.0",
    "redis>=4.5.0",
    "tenacity>=8.2.0",
]

[tool.setuptools.packages.find]
include = ["isa_model*"]

[tool.setuptools.package-data]
isa_model = [
    "core/config/providers/*.yaml",
    "core/config/environments/*.yaml",
    "core/config/*.yaml"
]

[project.optional-dependencies]
# Cloud API providers (lightweight - no local models)
cloud = [
    "openai>=1.10.0",
    "replicate>=0.23.0",
    "cerebras-cloud-sdk>=1.0.0",  # Cerebras ultra-fast inference
    "modal>=0.63.0",  # Modal for serverless GPU deployment
    "grpclib>=0.4.7",  # Required by Modal SDK
    "python-logging-loki>=0.3.1",  # Centralized logging
    "huggingface-hub>=0.16.0",  # For model metadata and deployment
    "docker>=6.0.0",  # TODO: Should not be needed for cloud mode, fix import structure (see issue3.md)
    "influxdb-client>=1.36.0",  # For logging/monitoring
    "tiktoken>=0.5.0",  # For token counting in LLM services
]

# Local ML inference (heavyweight - PyTorch + transformers)
local = [
    "torch>=2.0.0",
    "transformers>=4.30.0",
    "accelerate>=0.20.0",
    "huggingface-hub>=0.16.0",
    "safetensors>=0.4.1",
    "sentencepiece>=0.1.99",
]

# Training capabilities
training = [
    "datasets>=2.10.0",
    "peft>=0.4.0",
    "trl>=0.4.0",
    "bitsandbytes>=0.39.0",
]

# Audio processing
audio = [
    "librosa>=0.10.1",
    "soundfile>=0.12.1",
    "numba>=0.57.0",
]

# Image processing
vision = [
    "Pillow>=10.0.1",
    "torchvision>=0.15.2",
]

# LangChain integration
langchain = [
    "langchain-core>=0.1.0",
    "langchain-openai>=0.0.2",
]

# Cloud storage
storage = [
    "boto3>=1.26.0",
    "google-cloud-storage>=2.7.0",
]

# Advanced monitoring
monitoring = [
    "mlflow>=2.4.0",
    "redis>=4.5.0",
    "prometheus-fastapi-instrumentator>=6.1.0",
    "influxdb-client>=1.36.0",
    "pgvector>=0.2.0",
    "python-logging-loki>=0.3.1",  # Centralized logging with Loki
]

# Kubernetes orchestration
k8s = [
    "kubernetes>=25.3.0",
]

# GPU cloud providers
gpu-cloud = [
    "runpod>=1.0.0",
    "ollama>=0.3.0",
]

# Development tools
dev = [
    "pytest>=7.0.0",
    "black>=22.0.0",
    "flake8>=4.0.0",
    "mypy>=0.991",
    "twine>=4.0.0",
]

# Common presets
api-only = [
    "isa-model[cloud,langchain]",
]

full-local = [
    "isa-model[local,training,audio,vision,langchain]",
]

production = [
    "isa-model[cloud,monitoring,storage,k8s]",
]

# Optimized for staging (inference only, no training/eval)
staging = [
    "isa-model[cloud,monitoring,storage,langchain]",
    "python-consul>=1.1.0",  # Service discovery
]

# Ultra-lightweight staging (no MLflow monitoring)
staging-minimal = [
    "isa-model[cloud,storage,langchain]",
    "influxdb-client>=1.36.0",  # Basic logging only
    "python-logging-loki>=0.3.1",  # Centralized logging
    "python-consul>=1.1.0",  # Service discovery
]

all = [
    "isa-model[cloud,local,training,audio,vision,langchain,storage,monitoring,k8s,gpu-cloud]",
]