{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ğŸ§  Dasein Examples: Universal Memory for Agentic AI\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/nickswami/dasein-core/blob/main/examples/dasein_examples.ipynb)\n",
        "\n",
        "This notebook demonstrates **Dasein** - a universal memory system that learns from your agent's execution history and automatically improves performance, reduces costs, and increases reliability.\n",
        "\n",
        "## What You'll Learn\n",
        "\n",
        "1. **SQL Agent** - Learn query patterns for database interactions\n",
        "2. **Browser Agent** - Learn web scraping and navigation strategies\n",
        "3. **Deep Research Agent** - Optimize multi-agent research workflows\n",
        "\n",
        "## Key Features\n",
        "\n",
        "- âœ¨ **Zero-friction integration** - Wrap any agent in one line\n",
        "- ğŸ§  **Automatic learning** - Agents learn from successes and failures\n",
        "- ğŸ“Š **Performance tracking** - See token usage, timing, and improvement metrics\n",
        "- ğŸ”„ **Retry logic** - Intelligent retry with learned optimizations\n",
        "- â˜ï¸ **Cloud-powered** - Distributed rule synthesis and storage\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Dasein and dependencies\n",
        "!pip install -q dasein-core\n",
        "\n",
        "# For browser agent example (Use Case 2)\n",
        "!pip install -q playwright\n",
        "!playwright install chromium\n",
        "\n",
        "# For deep research agent (Use Case 3)\n",
        "!pip install -q git+https://github.com/langchain-ai/open_deep_research.git\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup API Keys\n",
        "\n",
        "Replace the placeholders below with your actual API keys:\n",
        "\n",
        "- **GOOGLE_API_KEY**: Get from [Google AI Studio](https://makersuite.google.com/app/apikey)\n",
        "- **TAVILY_API_KEY**: Get from [Tavily](https://tavily.com) (free tier available, 1000 searches/month)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# âš ï¸ REPLACE THESE WITH YOUR ACTUAL API KEYS\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"YOUR_GOOGLE_API_KEY_HERE\"\n",
        "os.environ[\"TAVILY_API_KEY\"] = \"YOUR_TAVILY_API_KEY_HERE\"  # Only needed for Use Case 3\n",
        "\n",
        "# Suppress warnings\n",
        "os.environ[\"GOOGLE_CLOUD_DISABLE_DIRECT_PATH\"] = \"true\"\n",
        "\n",
        "print(\"âœ… API keys configured\")\n",
        "print(\"\\nâš ï¸ Note: You may see dependency warnings in Colab - these are safe to ignore.\")\n",
        "print(\"âš ï¸ Note: Brief timeout warnings on first run are normal (cold start) - services will activate automatically.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Use Case 1: SQL Agent with Learning\n",
        "\n",
        "This example demonstrates how Dasein learns SQL query patterns and improves database agent performance.\n",
        "\n",
        "**What Happens:**\n",
        "1. **Baseline Run**: Agent queries the Chinook database without optimization\n",
        "2. **Learning**: Dasein captures the execution trace and sends it to cloud services for rule synthesis\n",
        "3. **Enhanced Run**: Agent runs again with learned rules injected, showing improved performance\n",
        "\n",
        "**Expected Improvements:**\n",
        "- 30-50% reduction in tokens used\n",
        "- Fewer SQL query iterations\n",
        "- Better query formulation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "from pathlib import Path\n",
        "\n",
        "# Download Chinook sample database\n",
        "db_path = Path(\"chinook.db\")\n",
        "if not db_path.exists():\n",
        "    print(\"Downloading Chinook database...\")\n",
        "    urllib.request.urlretrieve(\n",
        "        \"https://raw.githubusercontent.com/lerocha/chinook-database/master/ChinookDatabase/DataSources/Chinook_Sqlite.sqlite\",\n",
        "        str(db_path)\n",
        "    )\n",
        "    print(\"âœ… Database downloaded\")\n",
        "else:\n",
        "    print(\"âœ… Database already exists\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.utilities import SQLDatabase\n",
        "from langchain_community.agent_toolkits import SQLDatabaseToolkit\n",
        "from langchain_community.agent_toolkits.sql.base import create_sql_agent\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from dasein import cognate\n",
        "\n",
        "# Set up SQL agent\n",
        "print(\"Setting up SQL agent...\")\n",
        "db = SQLDatabase.from_uri(f\"sqlite:///{db_path}\")\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
        "toolkit = SQLDatabaseToolkit(db=db, llm=llm)\n",
        "agent = create_sql_agent(llm=llm, toolkit=toolkit, verbose=True, agent_executor_kwargs={'handle_parsing_errors': True})\n",
        "\n",
        "print(\"âœ… SQL agent created\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wrap with Dasein for automatic learning and improvement\n",
        "agent = cognate(\n",
        "    agent,\n",
        "    retry=2,  # Run twice: baseline + enhanced\n",
        "    performance_tracking=True,  # Show improvement metrics\n",
        "    verbose=False  # Set to True to see rule injection details\n",
        ")\n",
        "\n",
        "# Run the query - Dasein handles everything automatically!\n",
        "query = \"Which media format accounts for the highest number of individual line-item sales?\"\n",
        "\n",
        "print(f\"\\nQuery: {query}\\n\")\n",
        "print(\"=\" * 70)\n",
        "print(\"PHASE 1: Baseline run...\")\n",
        "print(\"PHASE 2: Enhanced run with learned rules...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "result = agent.invoke({\"input\": query})\n",
        "\n",
        "print(f\"\\nâœ… Final Answer: {result.get('output', '')}\")\n",
        "print(\"\\nğŸ“Š Check the performance metrics above to see improvement!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Use Case 2: Browser Agent with Learning\n",
        "\n",
        "This example demonstrates how Dasein learns web scraping and navigation patterns.\n",
        "\n",
        "**What Happens:**\n",
        "1. **Baseline Run**: Browser agent navigates to a website without optimization\n",
        "2. **Learning**: Dasein learns which tools to use, navigation patterns, and element selection strategies\n",
        "3. **Enhanced Run**: Agent runs again with learned rules, showing faster and more efficient browsing\n",
        "\n",
        "**Expected Improvements:**\n",
        "- Fewer navigation steps\n",
        "- Better tool selection\n",
        "- More efficient element targeting\n",
        "- Reduced token usage\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from langchain_community.agent_toolkits import PlayWrightBrowserToolkit\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langgraph.prebuilt import create_react_agent\n",
        "from dasein import cognate\n",
        "import asyncio\n",
        "from playwright.async_api import async_playwright\n",
        "\n",
        "async def create_browser_agent():\n",
        "    \"\"\"Create a Playwright browser agent.\"\"\"\n",
        "    print(\"Setting up Playwright browser agent...\")\n",
        "    \n",
        "    # Create browser\n",
        "    playwright = await async_playwright().start()\n",
        "    async_browser = await playwright.chromium.launch(headless=True)\n",
        "    \n",
        "    # Create toolkit and tools\n",
        "    toolkit = PlayWrightBrowserToolkit.from_browser(async_browser=async_browser)\n",
        "    tools = toolkit.get_tools()\n",
        "    \n",
        "    print(f\"Available browser tools: {[tool.name for tool in tools]}\")\n",
        "    \n",
        "    # Create LLM\n",
        "    llm = ChatGoogleGenerativeAI(\n",
        "        model=\"gemini-2.5-flash\",\n",
        "        temperature=0,\n",
        "        request_timeout=180,\n",
        "        max_retries=2\n",
        "    )\n",
        "    \n",
        "    # Create agent\n",
        "    agent_chain = create_react_agent(model=llm, tools=tools)\n",
        "    agent_chain = agent_chain.with_config({\"recursion_limit\": 75})\n",
        "    agent_chain.llm = llm  # Add LLM reference for Dasein\n",
        "    \n",
        "    print(\"âœ… Browser agent created\")\n",
        "    return agent_chain\n",
        "\n",
        "# Create the agent\n",
        "browser_agent = await create_browser_agent()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wrap with Dasein\n",
        "browser_agent = cognate(\n",
        "    browser_agent,\n",
        "    retry=2,\n",
        "    performance_tracking=True,\n",
        "    verbose=False  # Set to True to see rule injection details\n",
        ")\n",
        "\n",
        "# Test query\n",
        "query = \"Go to the LangChain site (https://python.langchain.com). Is there a tutorial on how to create a browsing agent? If so where is it?\"\n",
        "\n",
        "print(f\"\\nQuery: {query}\\n\")\n",
        "print(\"=\" * 70)\n",
        "print(\"PHASE 1: Baseline run...\")\n",
        "print(\"PHASE 2: Enhanced run with learned rules...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "result = await browser_agent.ainvoke({\"messages\": [(\"user\", query)]})\n",
        "\n",
        "print(f\"\\nâœ… Final Answer: {result}\")\n",
        "print(\"\\nğŸ“Š Check the performance metrics above to see improvement!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "# Use Case 3: Deep Research Agent with Learning\n",
        "\n",
        "This example demonstrates Dasein with LangChain's **Open Deep Research Agent** - a complex multi-agent system that orchestrates research tasks.\n",
        "\n",
        "**What Happens:**\n",
        "1. **Baseline Run**: Multi-agent research system runs without optimization\n",
        "   - Supervisor delegates to sub-agents\n",
        "   - Parallel research with Tavily search\n",
        "   - Context accumulation and synthesis\n",
        "   - Typically uses 15x more tokens than normal chat!\n",
        "\n",
        "2. **Learning**: Dasein learns:\n",
        "   - Better search query formulation\n",
        "   - Context filtering to reduce token bloat\n",
        "   - Improved delegation strategies\n",
        "   - Efficient tool usage patterns\n",
        "\n",
        "3. **Enhanced Run**: Agent runs with learned rules\n",
        "\n",
        "**Expected Improvements:**\n",
        "- 20-40% reduction in tokens used\n",
        "- Fewer redundant searches\n",
        "- Better query formulation\n",
        "- Faster convergence to quality results\n",
        "\n",
        "**Requirements:**\n",
        "- TAVILY_API_KEY (free tier: 1000 searches/month)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Critical fix: Force LangChain to use Google GenAI instead of Vertex AI\n",
        "from langchain.chat_models import base as chat_models_base\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "original_init_helper = chat_models_base._init_chat_model_helper\n",
        "_model_cache = {}\n",
        "_patch_logged = set()\n",
        "\n",
        "def patched_init_helper(*args, **kwargs):\n",
        "    \"\"\"Force gemini models to use ChatGoogleGenerativeAI.\"\"\"\n",
        "    model = kwargs.get('model', '')\n",
        "    if not model and args:\n",
        "        model = args[0] if len(args) > 0 and isinstance(args[0], str) else ''\n",
        "    \n",
        "    if model and 'gemini' in model.lower():\n",
        "        if model not in _patch_logged:\n",
        "            print(f\"ğŸ”§ Using ChatGoogleGenerativeAI for {model}\")\n",
        "            _patch_logged.add(model)\n",
        "        \n",
        "        if model not in _model_cache:\n",
        "            _model_cache[model] = ChatGoogleGenerativeAI(model=model)\n",
        "        return _model_cache[model]\n",
        "    return original_init_helper(*args, **kwargs)\n",
        "\n",
        "chat_models_base._init_chat_model_helper = patched_init_helper\n",
        "print(\"âœ… Patched LangChain to use Google GenAI\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from open_deep_research.deep_researcher import deep_researcher_builder\n",
        "from open_deep_research.configuration import Configuration\n",
        "from dasein import cognate\n",
        "import asyncio\n",
        "\n",
        "async def create_research_agent():\n",
        "    \"\"\"Create Open Deep Research agent.\"\"\"\n",
        "    print(\"Setting up Open Deep Research Agent...\")\n",
        "    print(\"Source: https://github.com/langchain-ai/open_deep_research\\n\")\n",
        "    \n",
        "    # Configure with Gemini models and Tavily search\n",
        "    config = Configuration(\n",
        "        summarization_model=\"gemini-2.5-flash\",\n",
        "        research_model=\"gemini-2.5-flash\",\n",
        "        compression_model=\"gemini-2.5-flash\",\n",
        "        final_report_model=\"gemini-2.5-flash\",\n",
        "        allow_clarification=False,\n",
        "        max_concurrent_research_units=2,\n",
        "        max_researcher_iterations=4,\n",
        "        max_react_tool_calls=8,\n",
        "    )\n",
        "    \n",
        "    print(\"Configuration:\")\n",
        "    print(f\"  - Models: {config.research_model}\")\n",
        "    print(f\"  - Search API: Tavily\")\n",
        "    print(f\"  - Max concurrent researchers: {config.max_concurrent_research_units}\\n\")\n",
        "    \n",
        "    # Create the deep researcher graph\n",
        "    graph = deep_researcher_builder.compile()\n",
        "    \n",
        "    print(\"âœ… Research agent created\\n\")\n",
        "    print(\"Graph structure:\")\n",
        "    print(\"  1. Brief Generation\")\n",
        "    print(\"  2. Research Supervisor\")\n",
        "    print(\"  3. Research Sub-Agents (parallel)\")\n",
        "    print(\"  4. Report Writing\\n\")\n",
        "    \n",
        "    # Add LLM reference for Dasein\n",
        "    graph.llm = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\", temperature=0)\n",
        "    \n",
        "    return graph\n",
        "\n",
        "# Create the research agent\n",
        "research_agent = await create_research_agent()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Wrap with Dasein\n",
        "research_agent = cognate(\n",
        "    research_agent,\n",
        "    retry=2,\n",
        "    performance_tracking=True,\n",
        "    verbose=False  # Set to True to see rule injection details\n",
        ")\n",
        "\n",
        "# Research query\n",
        "query = \"\"\"Research and compare LangChain vs LlamaIndex for building LLM applications.\n",
        "\n",
        "I want to understand:\n",
        "1. Their core philosophies and approaches\n",
        "2. Key features and strengths\n",
        "3. Which use cases each is best suited for\n",
        "\n",
        "Provide a concise comparison with sources.\"\"\"\n",
        "\n",
        "print(f\"Research Query: {query}\\n\")\n",
        "print(\"=\" * 70)\n",
        "print(\"This will trigger:\")\n",
        "print(\"  - Supervisor creating 2 sub-agents (one per framework)\")\n",
        "print(\"  - Parallel research with Tavily search\")\n",
        "print(\"  - Multiple search rounds per sub-agent\")\n",
        "print(\"  - Synthesis and report writing\\n\")\n",
        "print(\"PHASE 1: Baseline run...\")\n",
        "print(\"PHASE 2: Enhanced run with learned rules...\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "result = await research_agent.ainvoke(\n",
        "    {\"messages\": [(\"user\", query)]},\n",
        "    config={\n",
        "        \"configurable\": {\n",
        "            \"allow_clarification\": False,\n",
        "            \"summarization_model\": \"gemini-2.5-flash\",\n",
        "            \"research_model\": \"gemini-2.5-flash\",\n",
        "        }\n",
        "    }\n",
        ")\n",
        "\n",
        "# Display report (truncated for readability)\n",
        "report = str(result.get('report', result))\n",
        "if len(report) > 500:\n",
        "    print(f\"\\nâœ… Research Report (truncated):\\n{report[:500]}...\\n[{len(report)} total chars]\")\n",
        "else:\n",
        "    print(f\"\\nâœ… Research Report:\\n{report}\")\n",
        "\n",
        "print(\"\\nğŸ“Š Check the performance metrics above to see massive token savings!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Summary\n",
        "\n",
        "You've just seen **Dasein** in action across three different agent types:\n",
        "\n",
        "1. **SQL Agent** - Learned query optimization patterns\n",
        "2. **Browser Agent** - Learned navigation and tool usage strategies\n",
        "3. **Deep Research Agent** - Learned to optimize complex multi-agent workflows\n",
        "\n",
        "### Key Takeaways\n",
        "\n",
        "- âœ¨ **One-line integration**: `cognate(agent)` is all you need\n",
        "- ğŸ§  **Automatic learning**: Agents improve themselves without manual rule engineering\n",
        "- ğŸ“Š **Transparent metrics**: See exactly how much you're improving\n",
        "- â˜ï¸ **Cloud-powered**: Rules are stored and synthesized using distributed services\n",
        "- ğŸ”„ **Intelligent retry**: `retry=2` automatically runs baseline + enhanced\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "1. **Try your own agents** - Dasein works with any LangChain/LangGraph agent\n",
        "2. **Customize optimization** - Use the `weights` parameter to optimize for what matters to you\n",
        "3. **Build rule libraries** - Run multiple times to accumulate learned rules\n",
        "4. **Scale up** - The more you use Dasein, the smarter your agents become\n",
        "\n",
        "### Resources\n",
        "\n",
        "- ğŸ“– [Documentation](https://github.com/nickswami/dasein-core#readme)\n",
        "- ğŸ’» [GitHub](https://github.com/nickswami/dasein-core)\n",
        "- ğŸ’¬ [Discord Community](https://discord.gg/dasein)\n",
        "- ğŸ› [Report Issues](https://github.com/nickswami/dasein-core/issues)\n",
        "\n",
        "---\n",
        "\n",
        "**Built with â¤ï¸ for the agentic AI community**\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
