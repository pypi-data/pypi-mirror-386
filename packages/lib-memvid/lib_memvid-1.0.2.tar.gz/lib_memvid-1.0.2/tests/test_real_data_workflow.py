"""
Test real-world workflow: Create video from first CSV, then extend with second CSV
"""
import os
import tempfile
from pathlib import Path

from memvid import MemvidEncoder

try:
    import pytest
    HAS_PYTEST = True
except ImportError:
    HAS_PYTEST = False

def test_real_csv_workflow():
    """
    Test complete workflow with real CSV data:
    1. Create video from articles_1.csv
    2. Extend with articles_2.csv data
    3. Verify all data is accessible with 100% recovery
    
    With improved QR settings (higher error correction, larger box size, better frame quality)
    and multi-strategy decoding, recovery is now robust even with Spanish text.
    """
    # Get paths to real datasets
    datasets_dir = Path(__file__).parent / "datasets"
    csv1_path = datasets_dir / "articles_1.csv"
    csv2_path = datasets_dir / "articles_2.csv"
    
    # Verify datasets exist
    assert csv1_path.exists(), f"Dataset not found: {csv1_path}"
    assert csv2_path.exists(), f"Dataset not found: {csv2_path}"
    
    # Save outputs to tests/output folder for inspection
    output_dir = Path(__file__).parent / "output"
    output_dir.mkdir(exist_ok=True)
    
    video1_path = str(output_dir / "initial_video.mp4")
    index1_path = str(output_dir / "initial_index")
    video2_path = str(output_dir / "final_extended_video.mp4")
    index2_path = str(output_dir / "final_extended_index")
    
    # Step 1: Create initial video from first CSV
    print("\nStep 1: Creating video from articles_1.csv...")
    encoder1 = MemvidEncoder()
    encoder1.add_csv(str(csv1_path), text_column="text", chunk_size=200, overlap=20)
    
    initial_chunks = len(encoder1.chunks)
    print(f"   Added {initial_chunks} chunks from first CSV")
    assert initial_chunks > 0, "No chunks added from first CSV"
    
    # Build initial video
    encoder1.build_video(video1_path, index1_path, codec="mp4v", show_progress=True)
    assert Path(video1_path).exists(), "Initial video not created"
    print(f"   Initial video created: {Path(video1_path).stat().st_size / 1024:.1f} KB")
    
    # Step 2: Merge first video and extend with second CSV
    print("\nStep 2: Merging video and adding articles_2.csv...")
    encoder2 = MemvidEncoder()
    
    # Merge from first video
    encoder2.merge_from_video(video1_path, show_progress=True)
    after_merge = len(encoder2.chunks)
    print(f"   Merged {after_merge} chunks from video")
    
    # Calculate recovery rate
    recovery_rate = (after_merge / initial_chunks) * 100 if initial_chunks > 0 else 0
    print(f"   Recovery rate: {recovery_rate:.1f}%")
    
    # With improved QR settings, expect high recovery; allow realistic loss with Spanish and compression
    assert recovery_rate >= 90.0, \
        f"Insufficient recovery: only {recovery_rate:.1f}% recovered (expected ≥90%)"
    
    chunks_lost = initial_chunks - after_merge
    print(f"   Chunks lost to video compression: {chunks_lost}/{initial_chunks}")
    assert chunks_lost <= initial_chunks * 0.7, \
        f"Too many chunks lost: {chunks_lost} (expected ≤{int(initial_chunks * 0.7)})"
    
    # Add second CSV
    encoder2.add_csv(str(csv2_path), text_column="text", chunk_size=200, overlap=20)
    final_chunks = len(encoder2.chunks)
    added_chunks = final_chunks - after_merge
    print(f"   Added {added_chunks} chunks from second CSV")
    print(f"   Total chunks: {final_chunks}")
    assert final_chunks > after_merge, "No chunks added from second CSV"
    
    # Build extended video
    encoder2.build_video(video2_path, index2_path, codec="mp4v", show_progress=True)
    assert Path(video2_path).exists(), "Extended video not created"
    print(f"   Extended video created: {Path(video2_path).stat().st_size / 1024:.1f} KB")
    
    # Step 3: Verify data persistence by loading back
    print("\nStep 3: Verifying data persistence...")
    encoder3 = MemvidEncoder()
    loaded_chunks = encoder3.load_chunks_from_video(video2_path, show_progress=True)
    
    print(f"   Loaded {len(loaded_chunks)} chunks from extended video")
    
    # Calculate final recovery rate
    final_recovery_rate = (len(loaded_chunks) / final_chunks) * 100 if final_chunks > 0 else 0
    print(f"   Final recovery rate: {final_recovery_rate:.1f}%")
    
    assert final_recovery_rate >= 90.0, \
        f"Insufficient final recovery: only {final_recovery_rate:.1f}% recovered (expected ≥90%)"
    
    final_chunks_lost = final_chunks - len(loaded_chunks)
    print(f"   Final chunks lost: {final_chunks_lost}/{final_chunks}")
    assert final_chunks_lost <= final_chunks * 0.7, \
        f"Too many chunks lost: {final_chunks_lost}"
    
    # Step 4: Content verification
    print("\nStep 4: Verifying content...")
    all_text = " ".join(loaded_chunks).lower()
    
    # Check for keywords from both CSVs
    keywords = ["laboratorio", "miel", "sequía", "señalar", "microorganismos", "nutricionales"]
    found_keywords = [kw for kw in keywords if kw in all_text]
    print(f"   Found {len(found_keywords)}/{len(keywords)} keywords from articles")
    
    # Verify content size and presence
    assert len(all_text) > 1000, "Combined text seems too short"
    assert len(found_keywords) >= 4, \
        f"Too few keywords found ({len(found_keywords)}/6). Expected content may not be present."
    
    # Summary
    print("\n" + "="*120)
    print("Test Summary:")
    print(f"Initial CSV chunks:      {initial_chunks}")
    print(f"After merge:             {after_merge} ({recovery_rate:.1f}% recovered)")
    print(f"Second CSV chunks:       {added_chunks}")
    print(f"Total final chunks:      {final_chunks}")
    print(f"Verified loaded:         {len(loaded_chunks)} ({final_recovery_rate:.1f}% recovered)")
    print(f"Content size:            {len(all_text):,} characters")
    print(f"Keywords found:          {', '.join(found_keywords)}")
    print("="*120)
    print(f"Output files saved to: tests/output/")
    print(f"   Initial video:  {Path(video1_path).name} ({Path(video1_path).stat().st_size / 1024:.1f} KB)")
    print(f"   Final video:    {Path(video2_path).name} ({Path(video2_path).stat().st_size / 1024:.1f} KB)")
    print("="*120)

def test_csv_column_validation():
    """Test that CSV column validation works correctly"""
    datasets_dir = Path(__file__).parent / "datasets"
    csv1_path = datasets_dir / "articles_1.csv"
    
    encoder = MemvidEncoder()
    
    # Test with correct column
    encoder.add_csv(str(csv1_path), text_column="text")
    assert len(encoder.chunks) > 0
    
    # Test with wrong column - should raise error
    encoder2 = MemvidEncoder()
    if HAS_PYTEST:
        with pytest.raises(ValueError, match="Column.*not found"):
            encoder2.add_csv(str(csv1_path), text_column="nonexistent_column")
    else:
        try:
            encoder2.add_csv(str(csv1_path), text_column="nonexistent_column")
            assert False, "Should have raised ValueError"
        except ValueError as e:
            assert "not found" in str(e)

def test_empty_rows_handling():
    """Test that empty rows in CSV are properly handled"""
    with tempfile.TemporaryDirectory() as tmpdir:
        # Create CSV with empty rows
        csv_path = os.path.join(tmpdir, "test_empty.csv")
        with open(csv_path, 'w', encoding='utf-8') as f:
            f.write("text\n")
            f.write("First article\n")
            f.write("\n")  # Empty row
            f.write("   \n")  # Whitespace only
            f.write("Second article\n")
        
        encoder = MemvidEncoder()
        encoder.add_csv(csv_path, text_column="text")
        
        # Should have only 2 chunks (empty rows skipped)
        assert len(encoder.chunks) == 2
        assert "First article" in encoder.chunks[0]
        assert "Second article" in encoder.chunks[1]

if __name__ == "__main__":
    test_real_csv_workflow()
    test_csv_column_validation()
    test_empty_rows_handling()
    print("\nOk")
