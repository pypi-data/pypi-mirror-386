# Testing Guide

## 🎯 Testing Philosophy

MCP Vector Search follows a comprehensive testing strategy to ensure reliability, performance, and maintainability. Our testing approach includes unit tests, integration tests, and end-to-end testing.

---

## 🏗️ Test Structure

### Directory Organization
```
tests/
├── __init__.py
├── conftest.py                     # Pytest configuration & fixtures
├── test_simple.py                  # Basic smoke tests
├── unit/                           # Unit tests (fast, isolated)
│   ├── __init__.py
│   ├── core/                       # Core module tests
│   │   ├── test_search.py         # Search engine tests (✅ IMPLEMENTED)
│   │   ├── test_indexer.py        # Indexer tests (✅ IMPLEMENTED)
│   │   ├── test_database.py       # Database tests (✅ IMPLEMENTED)
│   │   ├── test_connection_pool.py # Connection pool tests (✅ IMPLEMENTED)
│   │   ├── test_auto_indexer.py   # Auto-indexer tests
│   │   └── test_factory.py        # Component factory tests
│   ├── parsers/                    # Parser tests
│   │   ├── test_python_parser.py
│   │   ├── test_javascript_parser.py
│   │   └── test_registry.py
│   └── cli/                        # CLI tests
│       ├── test_search_command.py
│       ├── test_index_command.py
│       └── test_auto_index_command.py
├── integration/                    # Integration tests (slower)
│   ├── __init__.py
│   ├── test_indexing_workflow.py  # Full indexing workflow (✅ IMPLEMENTED)
│   ├── test_search_workflow.py    # Search integration
│   ├── test_auto_indexing.py      # Auto-indexing features
│   └── test_project_management.py # Project management
├── e2e/                           # End-to-end tests
│   ├── test_cli_commands.py       # CLI command testing (✅ IMPLEMENTED)
│   └── test_full_workflow.py      # Complete user workflows
├── fixtures/                       # Test data and fixtures
│   ├── sample_projects/           # Sample project structures
│   └── test_files/                # Individual test files
└── performance/                    # Performance tests
    ├── test_indexing_speed.py
    └── test_search_latency.py
```

---

## 🧪 Test Categories

### Unit Tests
**Purpose**: Test individual components in isolation
**Speed**: Fast (< 1 second each)
**Scope**: Single function/class

```python
# tests/unit/test_parsers.py
import pytest
from mcp_vector_search.parsers.python import PythonParser
from mcp_vector_search.core.models import ChunkType

class TestPythonParser:
    """Unit tests for Python parser."""
    
    def test_parse_simple_function(self):
        """Test parsing a simple function."""
        parser = PythonParser()
        code = '''
def hello_world():
    """Say hello to the world."""
    print("Hello, world!")
'''
        
        chunks = parser.parse(code)
        
        assert len(chunks) == 1
        assert chunks[0].chunk_type == ChunkType.FUNCTION
        assert "hello_world" in chunks[0].content
        assert chunks[0].start_line == 2
        assert chunks[0].end_line == 4
    
    def test_parse_class_with_methods(self):
        """Test parsing a class with methods."""
        parser = PythonParser()
        code = '''
class Calculator:
    """A simple calculator."""
    
    def add(self, a, b):
        return a + b
    
    def subtract(self, a, b):
        return a - b
'''
        
        chunks = parser.parse(code)
        
        # Should have class chunk and method chunks
        assert len(chunks) >= 3
        class_chunks = [c for c in chunks if c.chunk_type == ChunkType.CLASS]
        method_chunks = [c for c in chunks if c.chunk_type == ChunkType.FUNCTION]
        
        assert len(class_chunks) == 1
        assert len(method_chunks) == 2
    
    def test_parse_invalid_syntax(self):
        """Test handling of invalid Python syntax."""
        parser = PythonParser()
        code = "def invalid_function(\n    # Missing closing parenthesis"
        
        # Should fall back to regex parsing
        chunks = parser.parse(code)
        assert len(chunks) >= 0  # Should not crash
```

### Integration Tests
**Purpose**: Test component interactions
**Speed**: Medium (1-10 seconds each)
**Scope**: Multiple components working together

```python
# tests/integration/test_workflow.py
import pytest
import tempfile
from pathlib import Path
from mcp_vector_search.core.indexer import SemanticIndexer
from mcp_vector_search.core.search import SemanticSearch

@pytest.mark.asyncio
class TestIndexingWorkflow:
    """Integration tests for indexing workflow."""
    
    async def test_full_indexing_and_search(self, tmp_path, test_database, test_embeddings):
        """Test complete indexing and search workflow."""
        # Create test files
        python_file = tmp_path / "test.py"
        python_file.write_text('''
def authenticate_user(username, password):
    """Authenticate user with username and password."""
    if not username or not password:
        return False
    return check_credentials(username, password)

def check_credentials(username, password):
    """Check user credentials against database."""
    # Implementation here
    return True
''')
        
        js_file = tmp_path / "auth.js"
        js_file.write_text('''
function loginUser(email, password) {
    /**
     * Log in user with email and password
     */
    if (!email || !password) {
        return false;
    }
    return validateCredentials(email, password);
}
''')
        
        # Index files
        indexer = SemanticIndexer(test_database, test_embeddings, parser_registry)
        result = await indexer.index_files([python_file, js_file])
        
        assert result.chunks_created > 0
        assert result.files_processed == 2
        assert len(result.errors) == 0
        
        # Search for authentication-related code
        search = SemanticSearch(test_database, test_embeddings)
        results = await search.search("user authentication", limit=5)
        
        assert len(results) > 0
        assert any("authenticate" in r.chunk.content.lower() for r in results)
        assert any("login" in r.chunk.content.lower() for r in results)
```

### End-to-End Tests
**Purpose**: Test complete user workflows
**Speed**: Slow (10+ seconds each)
**Scope**: Full application behavior

```python
# tests/integration/test_cli.py
import pytest
import subprocess
from pathlib import Path

class TestCLIWorkflow:
    """End-to-end CLI tests."""
    
    def test_init_index_search_workflow(self, tmp_path):
        """Test complete CLI workflow: init -> index -> search."""
        # Change to test directory
        original_cwd = Path.cwd()
        try:
            os.chdir(tmp_path)
            
            # Create sample code
            (tmp_path / "main.py").write_text('''
def main():
    """Main application entry point."""
    print("Hello, world!")

if __name__ == "__main__":
    main()
''')
            
            # Initialize project
            result = subprocess.run(
                ["mcp-vector-search", "init"],
                capture_output=True,
                text=True
            )
            assert result.returncode == 0
            assert Path(".mcp-vector-search").exists()
            
            # Index codebase
            result = subprocess.run(
                ["mcp-vector-search", "index"],
                capture_output=True,
                text=True
            )
            assert result.returncode == 0
            assert "indexed" in result.stdout.lower()
            
            # Search code
            result = subprocess.run(
                ["mcp-vector-search", "search", "main function"],
                capture_output=True,
                text=True
            )
            assert result.returncode == 0
            assert "main.py" in result.stdout
            
        finally:
            os.chdir(original_cwd)
```

---

## 🔧 Test Fixtures

### Pytest Configuration
```python
# tests/conftest.py
import pytest
import tempfile
from pathlib import Path
from unittest.mock import Mock, AsyncMock

from mcp_vector_search.core.database import VectorDatabase
from mcp_vector_search.core.embeddings import EmbeddingGenerator
from mcp_vector_search.parsers.registry import ParserRegistry

@pytest.fixture
def tmp_path():
    """Temporary directory for test files."""
    with tempfile.TemporaryDirectory() as tmp_dir:
        yield Path(tmp_dir)

@pytest.fixture
def test_database():
    """Mock vector database for testing."""
    database = Mock(spec=VectorDatabase)
    database.store_chunks = AsyncMock()
    database.search_similar = AsyncMock(return_value=[])
    database.get_stats = AsyncMock()
    return database

@pytest.fixture
def test_embeddings():
    """Mock embedding generator for testing."""
    embeddings = Mock(spec=EmbeddingGenerator)
    embeddings.generate_embedding = AsyncMock(return_value=[0.1] * 384)
    embeddings.generate_embeddings = AsyncMock(return_value=[[0.1] * 384])
    embeddings.embedding_dimension = 384
    return embeddings

@pytest.fixture
def sample_python_code():
    """Sample Python code for testing."""
    return '''
def fibonacci(n):
    """Calculate the nth Fibonacci number."""
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

class MathUtils:
    """Utility class for mathematical operations."""
    
    @staticmethod
    def factorial(n):
        """Calculate factorial of n."""
        if n <= 1:
            return 1
        return n * MathUtils.factorial(n-1)
'''

@pytest.fixture
def sample_javascript_code():
    """Sample JavaScript code for testing."""
    return '''
function calculateSum(numbers) {
    /**
     * Calculate sum of array of numbers
     * @param {number[]} numbers - Array of numbers
     * @returns {number} Sum of all numbers
     */
    return numbers.reduce((sum, num) => sum + num, 0);
}

class DataProcessor {
    constructor(data) {
        this.data = data;
    }
    
    process() {
        return this.data.map(item => item * 2);
    }
}
'''
```

---

## 🚀 Running Tests

### Basic Test Execution
```bash
# Run all tests
uv run pytest

# Run with verbose output
uv run pytest -v

# Run specific test file
uv run pytest tests/unit/test_parsers.py

# Run specific test method
uv run pytest tests/unit/test_parsers.py::TestPythonParser::test_parse_simple_function

# Run tests matching pattern
uv run pytest -k "test_python"
```

### Coverage Testing
```bash
# Run tests with coverage
uv run pytest --cov=src/mcp_vector_search

# Generate HTML coverage report
uv run pytest --cov=src/mcp_vector_search --cov-report=html

# View coverage report
open htmlcov/index.html

# Fail if coverage below threshold
uv run pytest --cov=src/mcp_vector_search --cov-fail-under=80
```

### Parallel Testing
```bash
# Install pytest-xdist for parallel execution
uv add --dev pytest-xdist

# Run tests in parallel
uv run pytest -n auto

# Run with specific number of workers
uv run pytest -n 4
```

### **Comprehensive Test Runner** (Recommended)
We provide a comprehensive test runner script that handles all test types:

```bash
# Run all tests (unit, integration, e2e, performance, linting)
python scripts/run_tests.py --all

# Run fast tests only (unit + smoke tests) - Great for development
python scripts/run_tests.py --fast

# Run specific test types
python scripts/run_tests.py --unit          # Unit tests only
python scripts/run_tests.py --integration   # Integration tests only
python scripts/run_tests.py --e2e          # End-to-end tests only
python scripts/run_tests.py --performance  # Performance tests only
python scripts/run_tests.py --lint         # Linting checks only
python scripts/run_tests.py --smoke        # Smoke tests only

# Run tests matching a pattern
python scripts/run_tests.py --pattern "search"
```

**Features:**
- ✅ Comprehensive coverage of all test types
- ✅ Performance timing and detailed reporting
- ✅ Graceful handling of missing dependencies
- ✅ CI/CD ready with proper exit codes

### Test Categories
```bash
# Run only unit tests
uv run pytest tests/unit/

# Run only integration tests
uv run pytest tests/integration/

# Run only fast tests
uv run pytest -m "not slow"

# Run performance tests
uv run pytest tests/performance/
```

---

## 📊 Test Markers

### Defining Markers
```python
# pytest.ini or pyproject.toml
[tool.pytest.ini_options]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
    "unit: marks tests as unit tests",
    "performance: marks tests as performance tests",
    "requires_network: marks tests that require network access",
]
```

### Using Markers
```python
import pytest

@pytest.mark.slow
@pytest.mark.integration
async def test_large_codebase_indexing():
    """Test indexing a large codebase (slow test)."""
    # Test implementation

@pytest.mark.unit
def test_chunk_creation():
    """Test code chunk creation (fast unit test)."""
    # Test implementation

@pytest.mark.performance
def test_search_latency():
    """Test search response time."""
    # Performance test implementation
```

---

## 🎭 Mocking Strategies

### Database Mocking
```python
@pytest.fixture
def mock_database():
    """Mock database with realistic behavior."""
    database = Mock(spec=VectorDatabase)
    
    # Mock store_chunks to track stored data
    stored_chunks = []
    async def mock_store_chunks(chunks):
        stored_chunks.extend(chunks)
    database.store_chunks.side_effect = mock_store_chunks
    
    # Mock search to return relevant results
    async def mock_search(query_embedding, limit=10, **kwargs):
        # Return mock results based on stored chunks
        return [
            SearchResult(chunk=chunk, similarity_score=0.8, rank=i)
            for i, chunk in enumerate(stored_chunks[:limit])
        ]
    database.search_similar.side_effect = mock_search
    
    return database
```

### File System Mocking
```python
from unittest.mock import patch, mock_open

def test_file_reading():
    """Test file reading with mocked file system."""
    mock_content = "def test(): pass"
    
    with patch("builtins.open", mock_open(read_data=mock_content)):
        with patch("pathlib.Path.exists", return_value=True):
            # Test code that reads files
            result = read_and_parse_file(Path("test.py"))
            assert "test" in result
```

---

## 📈 Performance Testing

### Benchmarking
```python
# tests/performance/test_indexing_speed.py
import time
import pytest
from mcp_vector_search.core.indexer import SemanticIndexer

@pytest.mark.performance
class TestIndexingPerformance:
    """Performance tests for indexing operations."""
    
    def test_indexing_speed(self, large_codebase_fixture):
        """Test indexing speed on large codebase."""
        indexer = SemanticIndexer(test_database, test_embeddings, parser_registry)
        
        start_time = time.time()
        result = await indexer.index_files(large_codebase_fixture)
        end_time = time.time()
        
        indexing_time = end_time - start_time
        files_per_second = len(large_codebase_fixture) / indexing_time
        
        # Assert performance requirements
        assert files_per_second > 10  # At least 10 files per second
        assert result.chunks_created > 0
        
        print(f"Indexed {len(large_codebase_fixture)} files in {indexing_time:.2f}s")
        print(f"Speed: {files_per_second:.1f} files/second")
```

### Memory Testing
```python
import psutil
import os

def test_memory_usage():
    """Test memory usage during indexing."""
    process = psutil.Process(os.getpid())
    initial_memory = process.memory_info().rss
    
    # Perform memory-intensive operation
    indexer = SemanticIndexer(test_database, test_embeddings, parser_registry)
    await indexer.index_files(large_file_list)
    
    final_memory = process.memory_info().rss
    memory_increase = final_memory - initial_memory
    
    # Assert memory usage is reasonable
    assert memory_increase < 500 * 1024 * 1024  # Less than 500MB increase
```

---

## 🐛 Debugging Tests

### Debugging Strategies
```bash
# Run tests with debugging
uv run pytest --pdb

# Drop into debugger on first failure
uv run pytest --pdb -x

# Show local variables in tracebacks
uv run pytest --tb=long

# Capture and show print statements
uv run pytest -s

# Show warnings
uv run pytest --disable-warnings
```

### Logging in Tests
```python
import logging

def test_with_logging(caplog):
    """Test with log capture."""
    with caplog.at_level(logging.INFO):
        # Code that logs
        logger.info("Test message")
    
    assert "Test message" in caplog.text
```

---

## 📋 Test Checklist

### Before Committing
- [ ] All tests pass locally
- [ ] New features have tests
- [ ] Test coverage meets requirements (80%+)
- [ ] No test warnings or errors
- [ ] Performance tests pass (if applicable)

### Test Quality
- [ ] Tests are independent (can run in any order)
- [ ] Tests are deterministic (same result every time)
- [ ] Tests have clear, descriptive names
- [ ] Tests follow AAA pattern (Arrange, Act, Assert)
- [ ] Edge cases are covered
- [ ] Error conditions are tested

---

## 🎯 Testing Best Practices

### 1. Test Naming
```python
# Good: Descriptive test names
def test_python_parser_extracts_function_with_docstring():
def test_search_returns_empty_list_for_no_matches():
def test_indexer_handles_file_not_found_error():

# Bad: Unclear test names
def test_parser():
def test_search():
def test_error():
```

### 2. Test Structure (AAA Pattern)
```python
def test_search_filters_by_similarity_threshold():
    # Arrange
    search = SemanticSearch(mock_database, mock_embeddings)
    mock_database.search_similar.return_value = [
        SearchResult(chunk=mock_chunk, similarity_score=0.9, rank=0),
        SearchResult(chunk=mock_chunk, similarity_score=0.5, rank=1),
    ]
    
    # Act
    results = await search.search("test query", threshold=0.7)
    
    # Assert
    assert len(results) == 1
    assert results[0].similarity_score >= 0.7
```

### 3. Test Independence
```python
# Good: Each test is independent
def test_parser_handles_empty_string():
    parser = PythonParser()  # Fresh instance
    result = parser.parse("")
    assert result == []

def test_parser_handles_single_function():
    parser = PythonParser()  # Fresh instance
    result = parser.parse("def test(): pass")
    assert len(result) == 1
```

---

## 📚 Resources

### Testing Tools
- **[Pytest](https://docs.pytest.org/)** - Testing framework
- **[pytest-asyncio](https://pytest-asyncio.readthedocs.io/)** - Async test support
- **[pytest-cov](https://pytest-cov.readthedocs.io/)** - Coverage reporting
- **[pytest-xdist](https://pytest-xdist.readthedocs.io/)** - Parallel testing

### Best Practices
- **[Testing Best Practices](https://docs.python-guide.org/writing/tests/)**
- **[Effective Python Testing](https://realpython.com/python-testing/)**
- **[Test-Driven Development](https://testdriven.io/)**
