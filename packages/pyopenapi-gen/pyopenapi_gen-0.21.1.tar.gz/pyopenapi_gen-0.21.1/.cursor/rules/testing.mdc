---
description: 
globs: tests/**/*.py
alwaysApply: false
---
1. üéØ Purpose

To ensure all tests ‚Äî both unit and integration ‚Äî are:

- Precise in intent
- Fully verifying the contract of the component under test
- Isolated from unrelated dependencies
- Described clearly for human readability and maintainability

This cursor rule must be followed both by human developers and any AI writing tests.

2. ‚úÖ Scope of Tests

2.1 Unit Tests

Must test the smallest testable part of the application in isolation.

- Only one unit (e.g., function, class method) should be tested per test case.
- External dependencies (DBs, APIs, services, time, randomness, etc.) must be mocked or stubbed.

2.2 Integration Tests

Should test cooperation between units (e.g., service layer + repository).

- Can include real implementations of multiple components, but external system boundaries must still be mocked unless explicitly testing integrations.
- Only use real I/O for designated "integration" or "e2e" test environments, never in unit tests.

3. üìã Test Case Structure

All tests must follow this standard format:

```python
def test_<unit_of_work>__<condition>__<expected_outcome>():
    """
    Scenario:
        - Description of the test scenario in 2‚Äì4 lines of natural language.
        - Describe what is being tested and why it's important.

    Expected Outcome:
        - Explicitly state what the correct result should be (including side effects).
    """
    # Arrange
    # Setup necessary inputs and mocks

    # Act
    # Run the function or method being tested

    # Assert
    # Check for correct result and side effects
```  

4. üß† AI Agent Testing Rules

When AI is tasked to write tests, it must:

- Use the above structure.
- Generate a clear, natural-language docstring summarizing the scenario and expected result.
- Mock all dependencies not directly under test.
- Avoid redundant assertions or low-value "happy path only" testing ‚Äî focus on variation in input and edge cases.
- Ensure high coverage of:
  - Control flow (if/else, error branches)
  - Contract fidelity (input/output invariants, exceptions)
  - State changes (data updates, method calls)
- Do not test third-party libraries (assume their correctness).

5. üî¨ Coverage Requirements

- All public methods and functions must be tested with 100% logical branch coverage.
- Edge cases must be included (e.g. empty list, None, 0, large inputs).
- For integration tests, ensure interface expectations between units are fully verified.
- Use pytest-cov or equivalent to enforce and review coverage.

6. üîÑ Isolation Rule

- Only the unit or flow under test may contain logic.
- Everything else (e.g., repositories, downstream APIs, services, clocks, logging, randomness) must be mocked using unittest.mock, pytest-mock, or equivalent.
- Use fixtures where applicable, but avoid overly abstract or reused test setups that reduce test clarity.

7. üè∑Ô∏è Naming & File Structure

Test files mirror the code structure: foo.py ‚Üí test_foo.py

Use descriptive function names:

- ‚úÖ test_user_creation__invalid_email__raises_error
- ‚ùå test_invalid_email

8. üìö Example

```python
def test_calculate_discount__vip_customer__applies_20_percent():
    """
    Scenario:
        A VIP customer is eligible for a 20% discount. We want to verify that
        the discount logic applies the correct rate based on the customer tier.

    Expected Outcome:
        The function should return the original price multiplied by 0.8.
    """
    # Arrange
    price = 100.0
    customer = Customer(tier="VIP")

    # Act
    result = calculate_discount(price, customer)

    # Assert
    assert result == 80.0
```  

9. üßµ Integration Test Example

```python
def test_user_registration_flow__valid_data__creates_user_and_sends_email():
    """
    Scenario:
        A new user registers with valid data. The service should create the user,
        send a welcome email, and return a success status.

    Expected Outcome:
        - UserRepository.save is called with correct user
        - EmailService.send_welcome_email is called
        - Function returns success result
    """
    # Arrange
    user_data = {"email": "test@example.com", "name": "Alice"}
    repo = Mock()
    email_service = Mock()
    service = UserService(repo, email_service)

    # Act
    result = service.register_user(user_data)

    # Assert
    repo.save.assert_called_once()
    email_service.send_welcome_email.assert_called_once()
    assert result == {"status": "ok"}
```
## 10. Preferred Testing Framework & Style

**10.1. Framework Choice:**
    - All new Python unit tests and integration tests written for this project **MUST** use the `pytest` framework.
    - Existing tests written using `unittest.TestCase` **SHOULD** be refactored to `pytest` style when those files are being significantly modified or as a dedicated refactoring effort to improve test suite consistency and leverage `pytest` features.

**10.2. `pytest` Idiomatic Style:**
    - **Assertions:** Tests **MUST** use plain `assert` statements for assertions (e.g., `assert result == expected`).
    - **Fixtures:** Test setup, teardown, and dependency injection **MUST** be managed using `pytest` fixtures. Fixtures should be defined in the test file itself or in a relevant `conftest.py`.
    - **Parameterization:** For testing multiple variations of inputs or conditions against the same test logic, `pytest.mark.parametrize` **MUST** be used to keep tests DRY (Don't Repeat Yourself) and data-driven.
    - **Test Structure:** Test classes (if used) **SHOULD NOT** inherit from `unittest.TestCase`. They should be plain Python classes. Test functions can also be defined at the module level.
    - **Exception Testing:** Testing for expected exceptions **MUST** use `pytest.raises` as a context manager (e.g., `with pytest.raises(SpecificException):`).

## 11. Test Suite Analysis & Documentation

**11.1. Purpose:**
    - To maintain a continuous understanding of the test suite's health, coverage, and adherence to conventions.
    - To provide a quick reference for the status and quality of tests within different modules.

**11.2. Root Analysis Overview (`tests/test_analysis_overview.md`):**
    - **Maintenance:** This document **MUST** be kept up-to-date.
    - **Content:**
        - It **MUST** list all test files and their current analysis status (e.g., "Analyzed", "Needs Analysis", "Analyzed - Empty").
        - It **MUST** include links to per-directory detailed analysis documents.
    - **Updates:** This document **SHOULD** be updated whenever:
        - New test files are added.
        - Existing test files are significantly refactored or moved.
        - A new pass of test analysis is completed for any part of the suite.

**11.3. Per-Directory Analysis Documents (e.g., `tests/<module>/<module>_analysis.md`):**
    - **Maintenance:** These documents **SHOULD** be created or updated when a significant portion of tests within a directory/module is reviewed or refactored.
    - **Content:** Each document **SHOULD** summarize:
        - Overall conciseness and consistency of tests within that directory.
        - Alignment with these testing conventions.
        - Identification of any potentially contradictory expectations or areas for improvement specific to that module's tests.
    - **Trigger for Updates:**
        - After a focused review or refactoring effort on the tests within that directory.
        - When the `test_analysis_overview.md` indicates a need for a detailed review of a specific area.

**11.4. Responsibility for AI Agents:**
    - When an AI agent is tasked with adding new test files or performing significant refactoring of existing tests, it **MUST** update the `tests/test_analysis_overview.md` to reflect the new or changed files and their status.
    - If an AI agent performs an analysis pass on a directory as requested, it **MUST** contribute to or create the relevant per-directory analysis document.