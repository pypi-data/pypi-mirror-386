# seem_language_encoder.yaml
_target_: azureml.acft.image.components.olympus_biomed_parse.models.multimodal_models.seem.transformer_decoder.language_encoders.seem_language_encoder.SEEMLanguageEncoder
tokenizer:
  _target_: transformers.AutoTokenizer.from_pretrained
  pretrained_model_name_or_path: openai/clip-vit-base-patch32
tokenizer_type: clip
encoder_transformer:
  _target_: azureml.acft.image.components.olympus_biomed_parse.models.multimodal_models.seem.transformer_decoder.language_encoders.language_transformer.get_transformer_encoder
  config_encoder:
    CONTEXT_LENGTH: 77
    VOCAB_SIZE: 49408
    WIDTH: 512
    LAYERS: 12
    HEADS: 8
    AUTOGRESSIVE: true
    LOAD_PRETRAINED: false
  verbose: true
lang_projection:
  _target_: torch.nn.Parameter
  data:
    _target_: torch.randn
    size: 
      - 512 
      - 512
max_token_num: 77
# queue_operator:
#   test_queue:
#     _target_: torch.randn
#     size: 
#       - 10 
#       - 256
