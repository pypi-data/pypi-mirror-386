# generated by datamodel-codegen:
#   filename:  configuration/logStorageConfiguration.json
#   timestamp: 2025-10-22T04:03:21+00:00

from __future__ import annotations

from enum import Enum
from typing import Optional

from pydantic import ConfigDict, Field
from typing_extensions import Annotated

from metadata.ingestion.models.custom_pydantic import BaseModel

from ..security.credentials import awsCredentials


class Type(Enum):
    default = 'default'
    s3 = 's3'


class SseAlgorithm(Enum):
    AES256 = 'AES256'
    aws_kms = 'aws:kms'


class StorageClass(Enum):
    STANDARD = 'STANDARD'
    STANDARD_IA = 'STANDARD_IA'
    ONEZONE_IA = 'ONEZONE_IA'
    INTELLIGENT_TIERING = 'INTELLIGENT_TIERING'
    GLACIER = 'GLACIER'
    DEEP_ARCHIVE = 'DEEP_ARCHIVE'


class LogStorageConfiguration(BaseModel):
    model_config = ConfigDict(
        extra='forbid',
    )
    type: Annotated[Type, Field(description='Type of log storage implementation')]
    enabled: Annotated[
        Optional[bool],
        Field(False, description='Enable it for pipelines deployed in the server'),
    ]
    bucketName: Annotated[
        Optional[str],
        Field(
            None, description='S3 bucket name for storing logs (required for S3 type)'
        ),
    ]
    prefix: Annotated[
        Optional[str],
        Field('pipeline-logs', description='S3 key prefix for organizing logs'),
    ]
    awsConfig: Annotated[
        Optional[awsCredentials.AWSCredentials],
        Field(None, description='AWS credentials configuration'),
    ]
    enableServerSideEncryption: Annotated[
        Optional[bool],
        Field(True, description='Enable server-side encryption for S3 objects'),
    ]
    sseAlgorithm: Annotated[
        Optional[SseAlgorithm],
        Field(None, description='Server-side encryption algorithm (if applicable)'),
    ]
    kmsKeyId: Annotated[
        Optional[str],
        Field(
            None, description='KMS Key ID for server-side encryption (if applicable)'
        ),
    ]
    storageClass: Annotated[
        Optional[StorageClass],
        Field(StorageClass.STANDARD_IA, description='S3 storage class for log objects'),
    ]
    expirationDays: Annotated[
        Optional[int],
        Field(
            30,
            description='Number of days after which logs are automatically deleted (0 means no expiration)',
            ge=0,
        ),
    ]
    maxConcurrentStreams: Annotated[
        Optional[int],
        Field(
            100, description='Maximum number of concurrent log streams allowed', ge=1
        ),
    ]
    streamTimeoutMinutes: Annotated[
        Optional[int],
        Field(
            5,
            description='Timeout in minutes for idle log streams before automatic cleanup',
            ge=1,
        ),
    ]
    asyncBufferSizeMB: Annotated[
        Optional[int],
        Field(
            5, description='Size of async buffer in MB for batching log writes', ge=1
        ),
    ]
