import asyncio
import logging
from typing import Any, Dict, List, Optional

from openai import AsyncOpenAI
from openai.types.chat import ChatCompletion

logger = logging.getLogger(__name__)


class OpenAIClient:
    """–ö–ª–∏–µ–Ω—Ç –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å OpenAI API"""

    def __init__(
        self,
        api_key: str,
        model: str = "gpt-4",
        max_tokens: int = 1500,
        temperature: float = 0.7,
    ):
        self.client = AsyncOpenAI(api_key=api_key)
        self.model = model
        self.max_tokens = max_tokens
        self.temperature = temperature

        # –ù–∞—Å—Ç—Ä–æ–π–∫–∏ –¥–ª—è –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫
        self.max_retries = 3
        self.retry_delay = 1  # —Å–µ–∫—É–Ω–¥—ã

        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º, —è–≤–ª—è–µ—Ç—Å—è –ª–∏ —ç—Ç–æ GPT-5 –º–æ–¥–µ–ª—å—é
        self.is_gpt5 = "gpt-5" in model.lower()

        # –ü–æ–ª—É—á–∞–µ–º –ª–∏–º–∏—Ç—ã –¥–ª—è –º–æ–¥–µ–ª–∏
        self.model_limits = self._get_model_limits()

        # üÜï –î–ª—è –¥–∏–∞–≥–Ω–æ—Å—Ç–∏–∫–∏ –ø—É—Å—Ç—ã—Ö –æ—Ç–≤–µ—Ç–æ–≤
        self.last_completion_tokens = 0

        logger.info(
            f"OpenAI –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω —Å –º–æ–¥–µ–ª—å—é {model} (GPT-5: {self.is_gpt5}, –ª–∏–º–∏—Ç: {self.model_limits['total_context']} —Ç–æ–∫–µ–Ω–æ–≤)"
        )

    def _get_model_limits(self) -> Dict[str, int]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ª–∏–º–∏—Ç—ã –¥–ª—è –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–π –º–æ–¥–µ–ª–∏"""
        model_limits = {
            # GPT-3.5
            "gpt-3.5-turbo": 4096,
            "gpt-3.5-turbo-16k": 16385,
            # GPT-4
            "gpt-4": 8192,
            "gpt-4-32k": 32768,
            "gpt-4-turbo": 128000,
            "gpt-4-turbo-preview": 128000,
            "gpt-4o": 128000,
            "gpt-4o-mini": 128000,
            # GPT-5
            "gpt-5-mini": 128000,
            "gpt-5": 200000,
        }

        # –ü–æ–ª—É—á–∞–µ–º –ª–∏–º–∏—Ç –¥–ª—è —Ç–µ–∫—É—â–µ–π –º–æ–¥–µ–ª–∏ –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ–º –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ
        total_limit = model_limits.get(self.model, 8192)

        # –†–µ–∑–µ—Ä–≤–∏—Ä—É–µ–º –º–µ—Å—Ç–æ –¥–ª—è –æ—Ç–≤–µ—Ç–∞ –∏ –±—É—Ñ–µ—Ä–∞
        completion_reserve = min(
            self.max_tokens * 2, total_limit // 4
        )  # –†–µ–∑–µ—Ä–≤–∏—Ä—É–µ–º –º–µ—Å—Ç–æ –¥–ª—è –æ—Ç–≤–µ—Ç–∞
        buffer_reserve = 500  # –ë—É—Ñ–µ—Ä –¥–ª—è –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏

        return {
            "total_context": total_limit,
            "max_input_tokens": total_limit - completion_reserve - buffer_reserve,
            "completion_reserve": completion_reserve,
        }

    def _get_completion_params(
        self, max_tokens: Optional[int] = None, temperature: Optional[float] = None
    ) -> Dict[str, Any]:
        """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–æ–∑–¥–∞–Ω–∏—è completion –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–æ–¥–µ–ª–∏"""
        tokens = max_tokens or self.max_tokens
        temp = temperature or self.temperature

        params = {
            "model": self.model,
        }

        # üÜï –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –õ–û–ì–ò–ö–ê –î–õ–Ø GPT-5
        if self.is_gpt5:
            # üîß –ò–°–ü–û–õ–¨–ó–£–ï–ú –¢–û–ö–ï–ù–´ –ò–ó –ö–û–ù–§–ò–ì–ê (–∫–∞–∫ –Ω–∞—Å—Ç—Ä–æ–∏–ª –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å)
            params["max_completion_tokens"] = tokens

            # üîß –ë–´–°–¢–†–´–ï –û–¢–í–ï–¢–´ —Å –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º reasoning
            # –î–æ–ø—É—Å—Ç–∏–º—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è reasoning_effort:
            # "minimal" - —Å–∞–º—ã–µ –±—ã—Å—Ç—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã, –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ reasoning
            # "low" - –Ω–µ–º–Ω–æ–≥–æ –±–æ–ª—å—à–µ reasoning
            # "medium" - —Å–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ reasoning (–º–µ–¥–ª–µ–Ω–Ω–µ–µ)
            # "high" - –º–∞–∫—Å–∏–º–∞–ª—å–Ω–æ–µ reasoning (—Å–∞–º—ã–µ –º–µ–¥–ª–µ–Ω–Ω—ã–µ, –Ω–æ –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ)
            params["reasoning_effort"] = "minimal"  # –ë—ã—Å—Ç—Ä—ã–µ –æ—Ç–≤–µ—Ç—ã

            logger.debug(
                f"GPT-5 –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: max_completion_tokens={tokens}, reasoning_effort=minimal"
            )
        else:
            # –î–ª—è –æ—Å—Ç–∞–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            params["max_tokens"] = tokens
            params["temperature"] = temp

        return params

    async def get_completion(
        self,
        messages: List[Dict[str, str]],
        max_tokens: Optional[int] = None,
        temperature: Optional[float] = None,
        stream: bool = False,
    ) -> str:
        """–ü–æ–ª—É—á–∞–µ—Ç –æ—Ç–≤–µ—Ç –æ—Ç OpenAI API —Å –ø–æ–¥–¥–µ—Ä–∂–∫–æ–π GPT-5"""
        if not messages:
            raise ValueError("–°–ø–∏—Å–æ–∫ —Å–æ–æ–±—â–µ–Ω–∏–π –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")

        for attempt in range(self.max_retries):
            try:
                logger.debug(f"–û—Ç–ø—Ä–∞–≤–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ OpenAI (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1})")

                # –û–±—Ä–µ–∑–∞–µ–º —Å–æ–æ–±—â–µ–Ω–∏—è –µ—Å–ª–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π
                processed_messages = await self._prepare_messages(messages)

                # –ü–æ–ª—É—á–∞–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–æ–¥–µ–ª–∏
                params = self._get_completion_params(max_tokens, temperature)
                params["messages"] = processed_messages
                params["stream"] = stream

                logger.info(f"üöÄ –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ {self.model}")
                logger.info(f"üìù –°–æ–æ–±—â–µ–Ω–∏–π –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç–µ: {len(processed_messages)}")

                # –õ–æ–≥–∏—Ä—É–µ–º –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –º–æ–¥–µ–ª–∏
                if self.is_gpt5:
                    max_tokens_param = params.get("max_completion_tokens")
                    reasoning_effort = params.get("reasoning_effort")
                    logger.info(
                        f"‚öôÔ∏è GPT-5 –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: max_completion_tokens={max_tokens_param}, reasoning_effort={reasoning_effort}"
                    )
                else:
                    max_tokens_param = params.get("max_tokens")
                    temp_param = params.get("temperature")
                    logger.info(
                        f"‚öôÔ∏è –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã: max_tokens={max_tokens_param}, temp={temp_param}"
                    )

                response: ChatCompletion = await self.client.chat.completions.create(
                    **params
                )

                if stream:
                    return await self._handle_stream_response(response)
                else:
                    # üîß –ü–†–ê–í–ò–õ–¨–ù–´–ô –ü–ê–†–°–ò–ù–ì –î–õ–Ø GPT-5 (–º–æ–∂–µ—Ç –±—ã—Ç—å –Ω–µ—Å–∫–æ–ª—å–∫–æ choices)
                    if self.is_gpt5 and len(response.choices) > 1:
                        logger.info(
                            f"üìä GPT-5 –≤–µ—Ä–Ω—É–ª {len(response.choices)} choices, –∏—â–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç"
                        )

                        # –ò—â–µ–º choice —Å –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º (–Ω–µ reasoning summary)
                        content = None
                        finish_reason = None

                        for i, choice in enumerate(response.choices):
                            choice_content = choice.message.content
                            choice_finish = choice.finish_reason

                            logger.info(
                                f"   Choice {i}: content={len(choice_content) if choice_content else 0} chars, finish={choice_finish}"
                            )

                            # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—ã–π choice —Å —Ä–µ–∞–ª—å–Ω—ã–º –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º
                            if choice_content and choice_content.strip():
                                content = choice_content
                                finish_reason = choice_finish
                                logger.info(
                                    f"   ‚úÖ –ò—Å–ø–æ–ª—å–∑—É–µ–º choice {i} –∫–∞–∫ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç"
                                )
                                break

                        # –ï—Å–ª–∏ –Ω–µ –Ω–∞—à–ª–∏ –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∏ –≤ –æ–¥–Ω–æ–º choice
                        if not content:
                            logger.warning(
                                "   ‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç –Ω–∏ –≤ –æ–¥–Ω–æ–º choice"
                            )
                            content = ""
                            finish_reason = response.choices[0].finish_reason
                    else:
                        # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–ª—è –Ω–µ-GPT-5 –∏–ª–∏ –æ–¥–Ω–æ–≥–æ choice
                        finish_reason = response.choices[0].finish_reason
                        content = response.choices[0].message.content

                    logger.info("üìä OpenAI –æ—Ç–≤–µ—Ç –ø–æ–ª—É—á–µ–Ω:")
                    logger.info(f"   üèÅ Finish reason: {finish_reason}")
                    logger.info(f"   üî§ –¢–∏–ø content: {type(content)}")
                    logger.info(f"   üìè –î–ª–∏–Ω–∞: {len(content) if content else 0}")

                    # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤
                    if response.usage:
                        self.last_completion_tokens = response.usage.completion_tokens

                        logger.info("üí∞ –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤:")
                        logger.info(f"   üì• Prompt: {response.usage.prompt_tokens}")
                        logger.info(
                            f"   üì§ Completion: {response.usage.completion_tokens}"
                        )
                        logger.info(f"   üî¢ Total: {response.usage.total_tokens}")

                        # üÜï –õ–û–ì–ò–†–£–ï–ú REASONING TOKENS –î–õ–Ø GPT-5
                        if hasattr(response.usage, "reasoning_tokens"):
                            reasoning_tokens = getattr(
                                response.usage, "reasoning_tokens", 0
                            )
                            if reasoning_tokens > 0:
                                logger.info(f"   üí≠ Reasoning: {reasoning_tokens}")

                    # üîß –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –û–ë–†–ê–ë–û–¢–ö–ê GPT-5
                    if self.is_gpt5:
                        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –µ—Å—Ç—å –ª–∏ reasoning —Ç–æ–∫–µ–Ω—ã
                        reasoning_tokens = 0
                        if response.usage and hasattr(
                            response.usage, "reasoning_tokens"
                        ):
                            reasoning_tokens = getattr(
                                response.usage, "reasoning_tokens", 0
                            )

                        # –ï—Å–ª–∏ –µ—Å—Ç—å reasoning –Ω–æ –Ω–µ—Ç content, —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã
                        if reasoning_tokens > 0 and (
                            not content or not content.strip()
                        ):
                            logger.warning(
                                f"üîß GPT-5: reasoning_tokens={reasoning_tokens}, –Ω–æ content –ø—É—Å—Ç–æ–π"
                            )

                            # –ï—Å–ª–∏ —ç—Ç–æ –Ω–µ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞, –ø—Ä–æ–±—É–µ–º —Å –±–æ–ª—å—à–∏–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º —Ç–æ–∫–µ–Ω–æ–≤
                            if attempt < self.max_retries - 1:
                                current_tokens = max_tokens or self.max_tokens
                                increased_tokens = min(
                                    current_tokens * 2, 8000
                                )  # –£–¥–≤–∞–∏–≤–∞–µ–º, –º–∞–∫—Å–∏–º—É–º 8000

                                logger.info(
                                    f"üîÑ –ü—Ä–æ–±—É–µ–º —Å —É–≤–µ–ª–∏—á–µ–Ω–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏: {increased_tokens}"
                                )
                                return await self.get_completion(
                                    messages,
                                    max_tokens=increased_tokens,
                                    temperature=temperature,
                                    stream=stream,
                                )
                            else:
                                # –ü–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞ - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
                                return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –º–æ–¥–µ–ª—å –ø–æ—Ç—Ä–∞—Ç–∏–ª–∞ –≤—Ä–µ–º—è –Ω–∞ –∞–Ω–∞–ª–∏–∑, –Ω–æ –Ω–µ —Å–º–æ–≥–ª–∞ —Å—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –æ—Ç–≤–µ—Ç. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–µ—Ä–µ—Ñ–æ—Ä–º—É–ª–∏—Ä–æ–≤–∞—Ç—å –≤–æ–ø—Ä–æ—Å."

                    # –û–±—ã—á–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ finish_reason
                    if finish_reason == "length":
                        logger.warning("‚ö†Ô∏è –ú–æ–¥–µ–ª—å –¥–æ—Å—Ç–∏–≥–ª–∞ –ª–∏–º–∏—Ç–∞ —Ç–æ–∫–µ–Ω–æ–≤")

                        if content and content.strip():
                            # –ï—Å—Ç—å —á–∞—Å—Ç–∏—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –µ–≥–æ
                            return content
                        else:
                            # –ù–µ—Ç –∫–æ–Ω—Ç–µ–Ω—Ç–∞ - –∏–Ω—Ñ–æ—Ä–º–∏—Ä—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
                            return "–û—Ç–≤–µ—Ç —Å–ª–∏—à–∫–æ–º –¥–ª–∏–Ω–Ω—ã–π. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –∑–∞–¥–∞—Ç—å –±–æ–ª–µ–µ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π –≤–æ–ø—Ä–æ—Å."

                    elif finish_reason == "stop":
                        logger.info("‚úÖ –ù–æ—Ä–º–∞–ª—å–Ω–æ–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏–µ")

                    elif finish_reason == "content_filter":
                        logger.warning("üö´ –û—Ç–≤–µ—Ç –∑–∞–±–ª–æ–∫–∏—Ä–æ–≤–∞–Ω —Ñ–∏–ª—å—Ç—Ä–æ–º –∫–æ–Ω—Ç–µ–Ω—Ç–∞")
                        return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ –º–æ–≥—É –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ —ç—Ç–æ—Ç –≤–æ–ø—Ä–æ—Å."

                    # –î–µ—Ç–∞–ª—å–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–¥–µ—Ä–∂–∏–º–æ–≥–æ
                    if content:
                        logger.info(f"   üîç –ù–∞—á–∞–ª–æ (50 —Å–∏–º–≤–æ–ª–æ–≤): '{content[:50]}'")
                        logger.info(f"   üîç –ö–æ–Ω–µ—Ü (50 —Å–∏–º–≤–æ–ª–æ–≤): '{content[-50:]}'")
                    else:
                        logger.warning("   ‚ùå content is None –∏–ª–∏ –ø—É—Å—Ç–æ–π")

                    return content or ""

            except Exception as e:
                logger.warning(
                    f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞—â–µ–Ω–∏–∏ –∫ OpenAI (–ø–æ–ø—ã—Ç–∫–∞ {attempt + 1}): {e}"
                )

                if attempt == self.max_retries - 1:
                    logger.error(f"–í—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã. –ü–æ—Å–ª–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞: {e}")
                    raise

                await asyncio.sleep(self.retry_delay * (attempt + 1))

        raise Exception("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –æ—Ç–≤–µ—Ç –æ—Ç OpenAI")

    async def _prepare_messages(
        self, messages: List[Dict[str, str]]
    ) -> List[Dict[str, str]]:
        """
        –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ API
        –û–±—Ä–µ–∑–∞–µ—Ç –∫–æ–Ω—Ç–µ–∫—Å—Ç –µ—Å–ª–∏ –æ–Ω —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π
        """

        # –ë–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Ç–æ–∫–µ–Ω–æ–≤ –¥–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞
        def estimate_message_tokens(msg):
            content = msg.get("content", "")
            # –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: –ø—Ä–∏–º–µ—Ä–Ω–æ 2.5-3 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω
            return len(content) // 2.5

        total_estimated_tokens = sum(estimate_message_tokens(msg) for msg in messages)
        max_input_tokens = self.model_limits["max_input_tokens"]

        if total_estimated_tokens <= max_input_tokens:
            return messages

        logger.info(
            f"–ö–æ–Ω—Ç–µ–∫—Å—Ç —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π ({int(total_estimated_tokens)} —Ç–æ–∫–µ–Ω–æ–≤), –æ–±—Ä–µ–∑–∞–µ–º –¥–æ {max_input_tokens}"
        )

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–∏—Å—Ç–µ–º–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è
        system_messages = [msg for msg in messages if msg.get("role") == "system"]
        other_messages = [msg for msg in messages if msg.get("role") != "system"]

        # –†–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ–º —Ç–æ–∫–µ–Ω—ã —Å–∏—Å—Ç–µ–º–Ω—ã—Ö —Å–æ–æ–±—â–µ–Ω–∏–π
        system_tokens = sum(estimate_message_tokens(msg) for msg in system_messages)
        available_tokens = max_input_tokens - system_tokens

        if available_tokens <= 0:
            logger.warning("–°–∏—Å—Ç–µ–º–Ω—ã–µ —Å–æ–æ–±—â–µ–Ω–∏—è –∑–∞–Ω–∏–º–∞—é—Ç –≤–µ—Å—å –¥–æ—Å—Ç—É–ø–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç")
            return system_messages

        # –ë–µ—Ä–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è, –ø–æ–º–µ—â–∞—é—â–∏–µ—Å—è –≤ –¥–æ—Å—Ç—É–ø–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã
        current_tokens = 0
        trimmed_messages = []

        for msg in reversed(other_messages):
            msg_tokens = estimate_message_tokens(msg)
            if current_tokens + msg_tokens > available_tokens:
                break
            trimmed_messages.insert(0, msg)
            current_tokens += msg_tokens

        result_messages = system_messages + trimmed_messages
        logger.info(
            f"–ö–æ–Ω—Ç–µ–∫—Å—Ç –æ–±—Ä–µ–∑–∞–Ω –¥–æ {len(result_messages)} —Å–æ–æ–±—â–µ–Ω–∏–π (~{int(current_tokens + system_tokens)} —Ç–æ–∫–µ–Ω–æ–≤)"
        )

        return result_messages

    async def _handle_stream_response(self, response) -> str:
        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –ø–æ—Ç–æ–∫–æ–≤—ã–π –æ—Ç–≤–µ—Ç"""
        full_content = ""

        async for chunk in response:
            if chunk.choices[0].delta.content is not None:
                full_content += chunk.choices[0].delta.content

        return full_content

    async def analyze_sentiment(self, text: str) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –∏ –Ω–∞–º–µ—Ä–µ–Ω–∏—è —Å–æ–æ–±—â–µ–Ω–∏—è

        Args:
            text: –¢–µ–∫—Å—Ç –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞

        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∞–Ω–∞–ª–∏–∑–∞
        """
        analysis_prompt = f"""
        –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Å–ª–µ–¥—É—é—â–µ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è –∏ –æ–ø—Ä–µ–¥–µ–ª–∏:
        1. –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ (–ø–æ–∑–∏—Ç–∏–≤–Ω–æ–µ/–Ω–µ–π—Ç—Ä–∞–ª—å–Ω–æ–µ/–Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–µ)
        2. –£—Ä–æ–≤–µ–Ω—å –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏ (1-10)
        3. –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –ø–æ–∫—É–ø–∫–µ (1-10)
        4. –û—Å–Ω–æ–≤–Ω—ã–µ –≤–æ–∑—Ä–∞–∂–µ–Ω–∏—è –∏–ª–∏ –≤–æ–ø—Ä–æ—Å—ã
        5. –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è –æ—Ç–≤–µ—Ç–∞
        
        –°–æ–æ–±—â–µ–Ω–∏–µ: "{text}"
        
        –û—Ç–≤–µ—Ç—å –≤ —Ñ–æ—Ä–º–∞—Ç–µ JSON:
        {{
            "sentiment": "positive/neutral/negative",
            "interest_level": 1-10,
            "purchase_readiness": 1-10,
            "objections": ["—Å–ø–∏—Å–æ–∫ –≤–æ–∑—Ä–∞–∂–µ–Ω–∏–π"],
            "key_questions": ["–∫–ª—é—á–µ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã"],
            "response_strategy": "—Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–∞—è —Å—Ç—Ä–∞—Ç–µ–≥–∏—è"
        }}
        """

        try:
            # –î–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –Ω–∏–∑–∫—É—é —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
            temp = 0.3 if not self.is_gpt5 else None

            response = await self.get_completion(
                [
                    {
                        "role": "system",
                        "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –∞–Ω–∞–ª–∏–∑—É –Ω–∞–º–µ—Ä–µ–Ω–∏–π –∫–ª–∏–µ–Ω—Ç–æ–≤ –≤ –ø—Ä–æ–¥–∞–∂–∞—Ö.",
                    },
                    {"role": "user", "content": analysis_prompt},
                ],
                temperature=temp,
            )

            # –ü—ã—Ç–∞–µ–º—Å—è —Ä–∞—Å–ø–∞—Ä—Å–∏—Ç—å JSON
            import json

            return json.loads(response)

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∞–Ω–∞–ª–∏–∑–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è: {e}")
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –¥–µ—Ñ–æ–ª—Ç–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
            return {
                "sentiment": "neutral",
                "interest_level": 5,
                "purchase_readiness": 5,
                "objections": [],
                "key_questions": [],
                "response_strategy": "continue_conversation",
            }

    async def generate_follow_up(
        self, conversation_history: List[Dict[str, str]], analysis: Dict[str, Any]
    ) -> str:
        """
        –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞

        Args:
            conversation_history: –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
            analysis: –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è

        Returns:
            –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç
        """
        strategy_prompt = f"""
        –ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ —Å–æ–æ–±—â–µ–Ω–∏—è –∫–ª–∏–µ–Ω—Ç–∞:
        - –ù–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ: {analysis['sentiment']}
        - –£—Ä–æ–≤–µ–Ω—å –∑–∞–∏–Ω—Ç–µ—Ä–µ—Å–æ–≤–∞–Ω–Ω–æ—Å—Ç–∏: {analysis['interest_level']}/10
        - –ì–æ—Ç–æ–≤–Ω–æ—Å—Ç—å –∫ –ø–æ–∫—É–ø–∫–µ: {analysis['purchase_readiness']}/10
        - –í–æ–∑—Ä–∞–∂–µ–Ω–∏—è: {analysis['objections']}
        - –°—Ç—Ä–∞—Ç–µ–≥–∏—è: {analysis['response_strategy']}
        
        –°–≥–µ–Ω–µ—Ä–∏—Ä—É–π –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç, –∫–æ—Ç–æ—Ä—ã–π:
        1. –£—á–∏—Ç—ã–≤–∞–µ—Ç —Ç–µ–∫—É—â–µ–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ –∫–ª–∏–µ–Ω—Ç–∞
        2. –û—Ç–≤–µ—á–∞–µ—Ç –Ω–∞ –µ–≥–æ –∫–ª—é—á–µ–≤—ã–µ –≤–æ–ø—Ä–æ—Å—ã –∏ –≤–æ–∑—Ä–∞–∂–µ–Ω–∏—è
        3. –ú—è–≥–∫–æ –Ω–∞–ø—Ä–∞–≤–ª—è–µ—Ç –∫ —Å–ª–µ–¥—É—é—â–µ–º—É —ç—Ç–∞–ø—É –≤–æ—Ä–æ–Ω–∫–∏ –ø—Ä–æ–¥–∞–∂
        4. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –¥–æ–≤–µ—Ä–∏—Ç–µ–ª—å–Ω—ã–π —Ç–æ–Ω –æ–±—â–µ–Ω–∏—è
        """

        messages = conversation_history + [
            {"role": "system", "content": strategy_prompt}
        ]

        # –î–ª—è —Ç–≤–æ—Ä—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á –∏—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –≤—ã—Å–æ–∫—É—é —Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä—É –µ—Å–ª–∏ –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è
        temp = 0.8 if not self.is_gpt5 else None

        return await self.get_completion(messages, temperature=temp)

    async def check_api_health(self) -> bool:
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å OpenAI API"""
        try:
            params = self._get_completion_params(max_tokens=10)
            params["messages"] = [{"role": "user", "content": "–ü—Ä–∏–≤–µ—Ç"}]

            await self.client.chat.completions.create(**params)
            return True
        except Exception as e:
            logger.error(f"OpenAI API –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω: {e}")
            return False

    def estimate_tokens(self, text: str) -> int:
        """–ë–æ–ª–µ–µ —Ç–æ—á–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤ –≤ —Ç–µ–∫—Å—Ç–µ"""
        # –î–ª—è —Ä—É—Å—Å–∫–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: –ø—Ä–∏–º–µ—Ä–Ω–æ 2.5-3 —Å–∏–º–≤–æ–ª–∞ –Ω–∞ —Ç–æ–∫–µ–Ω
        return int(len(text) / 2.5)

    async def get_available_models(self) -> List[str]:
        """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            models = await self.client.models.list()
            return [model.id for model in models.data if "gpt" in model.id.lower()]
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: {e}")
            return []

    async def transcribe_audio(self, audio_file_path: str) -> str:
        """
        –†–∞—Å–ø–æ–∑–Ω–∞–µ—Ç –≥–æ–ª–æ—Å–æ–≤–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ —á–µ—Ä–µ–∑ Whisper API

        Args:
            audio_file_path: –ü—É—Ç—å –∫ –∞—É–¥–∏–æ —Ñ–∞–π–ª—É

        Returns:
            –†–∞—Å–ø–æ–∑–Ω–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
        """
        try:
            logger.info(f"üé§ –û—Ç–ø—Ä–∞–≤–∫–∞ –∞—É–¥–∏–æ –Ω–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏–µ: {audio_file_path}")

            with open(audio_file_path, "rb") as audio_file:
                transcript = await self.client.audio.transcriptions.create(
                    model="whisper-1", file=audio_file, language="ru"  # –†—É—Å—Å–∫–∏–π —è–∑—ã–∫
                )

            text = transcript.text
            logger.info(f"‚úÖ –†–∞—Å–ø–æ–∑–Ω–∞–Ω–æ {len(text)} —Å–∏–º–≤–æ–ª–æ–≤: '{text[:100]}...'")
            return text

        except Exception as e:
            logger.error(f"‚ùå –û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è –∞—É–¥–∏–æ: {e}")
            return ""
