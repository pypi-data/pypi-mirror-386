# Auto-generated from responses_api.py
# Model: Response

# generated by datamodel-codegen:
#   filename:  filtered_openapi.yaml
#   timestamp: 2025-10-18T15:02:20+00:00


from typing import List, Optional, Self, Union, cast

from pydantic import Field, PrivateAttr

from agentle.responses.definitions.function_tool_call import FunctionToolCall


# Model dependencies
from .conversation2 import Conversation2
from .incomplete_details import IncompleteDetails
from .input_item import InputItem
from .model_response_properties import ModelResponseProperties
from .object import Object
from .output_item import OutputItem
from .response_error import ResponseError
from .response_properties import ResponseProperties
from .response_usage import ResponseUsage
from .status10 import Status10


class Response[TextFormatT = None](ModelResponseProperties, ResponseProperties):
    id: str = Field(..., description="Unique identifier for this Response.\n")
    object: Object = Field(
        ...,
        description="The object type of this resource - always set to `response`.\n",
    )
    status: Optional[Status10] = Field(
        None,
        description="The status of the response generation. One of `completed`, `failed`,\n`in_progress`, `cancelled`, `queued`, or `incomplete`.\n",
    )
    created_at: float = Field(
        ...,
        description="Unix timestamp (in seconds) of when this Response was created.\n",
    )
    error: ResponseError
    incomplete_details: Optional[IncompleteDetails]
    output: List[OutputItem] = Field(
        ...,
        description="An array of content items generated by the model.\n\n- The length and order of items in the `output` array is dependent\n  on the model's response.\n- Rather than accessing the first item in the `output` array and\n  assuming it's an `assistant` message with the content generated by\n  the model, you might consider using the `output_text` property where\n  supported in SDKs.\n",
    )
    instructions: Optional[Union[str, List[InputItem]]]
    output_text: Optional[str] = None
    usage: Optional[ResponseUsage] = None
    parallel_tool_calls: bool = Field(
        ..., description="Whether to allow the model to run tool calls in parallel.\n"
    )
    conversation: Optional[Conversation2] = None
    _text_format: type[TextFormatT] | None = PrivateAttr(default=None)

    def set_text_format(self, text_format: type[TextFormatT] | None) -> Self:
        # Accept None for cases where callers pass through optional text_format
        if text_format is None:
            return self

        self._text_format = text_format
        return self

    @property
    def function_calls(self) -> list[FunctionToolCall]:
        _function_calls: list[FunctionToolCall] = []
        for output in self.output:
            if output.type == "function_call":
                _function_calls.append(output)

        return _function_calls

    @property
    def output_parsed(self) -> TextFormatT:
        if not self._text_format:
            return cast(TextFormatT, None)

        for output in self.output:
            if output.type == "message":
                for content in output.content:
                    if content.type == "output_text":
                        if not content.parsed:
                            raise ValueError("No parsed output available")
                        return cast(TextFormatT, content.parsed)

        # If a text_format was provided but we didn't find any parsed content,
        # raise a clear error rather than silently returning None
        raise ValueError("No parsed output available")
