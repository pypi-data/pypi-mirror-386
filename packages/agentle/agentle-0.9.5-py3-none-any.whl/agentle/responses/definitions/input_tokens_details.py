# Auto-generated from responses_api.py
# Model: InputTokensDetails

# generated by datamodel-codegen:
#   filename:  filtered_openapi.yaml
#   timestamp: 2025-10-18T15:02:20+00:00


from pydantic import BaseModel, Field


class InputTokensDetails(BaseModel):
    cached_tokens: int = Field(
        ...,
        description="The number of tokens that were retrieved from the cache. \n[More on prompt caching](https://platform.openai.com/docs/guides/prompt-caching).\n",
    )
