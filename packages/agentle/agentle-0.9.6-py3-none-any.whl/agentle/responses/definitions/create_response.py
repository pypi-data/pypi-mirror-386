# Auto-generated from responses_api.py
# Model: CreateResponse

# generated by datamodel-codegen:
#   filename:  filtered_openapi.yaml
#   timestamp: 2025-10-18T15:02:20+00:00

from __future__ import annotations

from typing import List, Optional, Self, Union, cast

from pydantic import BaseModel, Field

from agentle.responses.definitions.response_format_json_schema_schema import (
    ResponseFormatJsonSchemaSchema,
)
from agentle.responses.definitions.text import Text
from agentle.responses.definitions.text_response_format_json_schema import (
    TextResponseFormatJsonSchema,
)
from agentle.responses.definitions.verbosity import Verbosity
from agentle.responses.definitions.verbosity1 import Verbosity1
from agentle.responses.json_schema_extractor import (
    JsonSchemaConfig,
    JsonSchemaExtractor,
)

# Model dependencies
from .conversation_param import ConversationParam
from .create_model_response_properties import CreateModelResponseProperties
from .include_enum import IncludeEnum
from .input_item import InputItem
from .response_properties import ResponseProperties
from .response_stream_options import ResponseStreamOptions


class CreateResponse(CreateModelResponseProperties, ResponseProperties):
    input: Optional[Union[str, List[InputItem]]] = Field(
        None,
        description="Text, image, or file inputs to the model, used to generate a response.\n\nLearn more:\n- [Text inputs and outputs](https://platform.openai.com/docs/guides/text)\n- [Image inputs](https://platform.openai.com/docs/guides/images)\n- [File inputs](https://platform.openai.com/docs/guides/pdf-files)\n- [Conversation state](https://platform.openai.com/docs/guides/conversation-state)\n- [Function calling](https://platform.openai.com/docs/guides/function-calling)\n",
    )
    include: Optional[List[IncludeEnum]] = None
    parallel_tool_calls: Optional[bool] = None
    store: Optional[bool] = None
    instructions: Optional[str] = None
    stream: Optional[bool] = None
    stream_options: Optional[ResponseStreamOptions] = None
    conversation: Optional[Union[str, ConversationParam]] = None

    def set_text_format[TextFormatT: BaseModel](
        self, text_format: type[TextFormatT]
    ) -> Self:
        extractor = JsonSchemaExtractor(
            config=JsonSchemaConfig(
                ensure_additional_properties=True,
                include_descriptions=True,
                strict_mode=True,
                max_enum_values=1000,
                max_nesting_depth=10,
                make_all_required=True,
                dereference=True,
            )
        )

        schema_dict = extractor.extract(text_format)
        # Use only the inner JSON Schema object for the `schema` field
        inner_schema = schema_dict.get("schema", {})
        self.text = Text(
            format=TextResponseFormatJsonSchema(
                type="json_schema",
                description="JSON Output",
                name=text_format.__name__,
                schema=ResponseFormatJsonSchemaSchema(**inner_schema),
                strict=True,
            ),
            verbosity=Verbosity(Verbosity1.medium),
        )
        return self

    def encode(self) -> str:
        return self.model_dump_json()

    @classmethod
    def decode[T = None](
        cls, data: str, text_format: type[T] | None = None
    ) -> CreateResponse[T]:
        return cast(CreateResponse[T], cls.model_validate_json(data))
