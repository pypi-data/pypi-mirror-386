"""
MCPå·¥å…·å¤„ç†å‡½æ•°
åŒ…å«æ‰€æœ‰å·¥å…·çš„ä¸šåŠ¡é€»è¾‘å’Œé”™è¯¯å¤„ç†
"""

from typing import Any, Dict, List
from mcp import types
import json

from .errors import (
    MCPError,
    validate_data_id,
    validate_factor_name,
    validate_required_columns,
    validate_data_length,
    validate_window_size,
    validate_period,
    validate_file_path
)
from .utils import serialize_response, convert_to_serializable

from quantanalyzer.data import DataLoader
from quantanalyzer.data.processor import ProcessInf, CSZFillna, CSZScoreNorm, ProcessorChain
from quantanalyzer.factor import FactorLibrary, FactorEvaluator, Alpha158Generator


# å…¨å±€å­˜å‚¨
data_store = {}
factor_store = {}
model_store = {}
processor_store = {}


async def handle_preprocess_data(args: Dict[str, Any]) -> List[types.TextContent]:
    """æ•°æ®é¢„å¤„ç†å’Œæ¸…æ´—"""
    file_path = args["file_path"]
    data_id = args["data_id"]
    auto_clean = args.get("auto_clean", True)  # é»˜è®¤å¼€å¯è‡ªåŠ¨æ¸…æ´—
    export_path = args.get("export_path", None)  # å¯¼å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰
    
    # éªŒè¯æ–‡ä»¶è·¯å¾„
    error = validate_file_path(file_path)
    if error:
        return [types.TextContent(type="text", text=error)]
    
    try:
        loader = DataLoader()
        data = loader.load_from_csv(file_path)
        
        # éªŒè¯å¿…éœ€åˆ—
        required_cols = ['open', 'high', 'low', 'close', 'volume']
        error = validate_required_columns(data, required_cols, data_id)
        if error:
            return [types.TextContent(type="text", text=error)]
        
        # éªŒè¯æ•°æ®é‡
        error = validate_data_length(data, min_length=100, data_id=data_id)
        if error:
            return [types.TextContent(type="text", text=error)]
        
        # è®°å½•åŸå§‹æ•°æ®è´¨é‡
        original_null_count = int(data.isna().sum().sum())
        original_null_rate = original_null_count / (data.shape[0] * data.shape[1]) if data.shape[0] * data.shape[1] > 0 else 0
        
        # è‡ªåŠ¨æ•°æ®æ¸…æ´—
        cleaned = False
        cleaning_methods = []
        if auto_clean:
            try:
                # æ£€æŸ¥è‚¡ç¥¨æ•°é‡
                symbol_count = len(data.index.get_level_values(1).unique())
                
                # æŒ‰ç…§Qlibæ ‡å‡†æµç¨‹æ¸…æ´—æ•°æ®
                if symbol_count > 1:
                    # å¤šè‚¡ç¥¨ï¼šProcessInf + CSZFillna + CSZScoreNorm
                    processor_chain = ProcessorChain([
                        ProcessInf(),          # å¤„ç†æ— ç©·å€¼ï¼ˆç”¨æˆªé¢å‡å€¼æ›¿æ¢ï¼‰
                        CSZFillna(),           # æˆªé¢å¡«å……ç¼ºå¤±å€¼ï¼ˆç”¨æˆªé¢å‡å€¼ï¼‰
                        CSZScoreNorm()         # æˆªé¢Z-scoreæ ‡å‡†åŒ–
                    ])
                    data = processor_chain.fit_transform(data.copy())
                    
                    cleaning_methods = [
                        "ProcessInf() - ç”¨æˆªé¢å‡å€¼æ›¿æ¢inf/-inf",
                        "CSZFillna() - ç”¨æˆªé¢å‡å€¼å¡«å……NaN",
                        "CSZScoreNorm() - æˆªé¢Z-scoreæ ‡å‡†åŒ–"
                    ]
                else:
                    # å•è‚¡ç¥¨ï¼šProcessInf + CSZFillnaï¼ˆä¸è¿›è¡Œæ ‡å‡†åŒ–ï¼‰
                    processor_chain = ProcessorChain([
                        ProcessInf(),          # å¤„ç†æ— ç©·å€¼
                        CSZFillna()            # å¡«å……ç¼ºå¤±å€¼ï¼ˆå•è‚¡ç¥¨æ—¶é€€åŒ–ä¸ºç®€å•å¡«å……ï¼‰
                    ])
                    data = processor_chain.fit_transform(data.copy())
                    
                    cleaning_methods = [
                        "ProcessInf() - å¤„ç†inf/-infå€¼",
                        "CSZFillna() - å¡«å……NaNå€¼"
                    ]
                
                cleaned = True
            except Exception as e:
                # å¦‚æœæ¸…æ´—å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®å¹¶è®°å½•è­¦å‘Š
                cleaned = False
                cleaning_methods = []
        
        data_store[data_id] = data
        
        # è®¡ç®—æ¸…æ´—åçš„æ•°æ®è´¨é‡
        current_null_count = int(data.isna().sum().sum())
        current_null_rate = current_null_count / (data.shape[0] * data.shape[1]) if data.shape[0] * data.shape[1] > 0 else 0
        
        # å¯¼å‡ºæ¸…æ´—åçš„æ•°æ®åˆ°æœ¬åœ°
        export_info = None
        if export_path or cleaned:  # å¦‚æœæŒ‡å®šå¯¼å‡ºè·¯å¾„æˆ–è¿›è¡Œäº†æ¸…æ´—ï¼Œåˆ™å¯¼å‡º
            import os
            from pathlib import Path
            from datetime import datetime
            
            # å¦‚æœæ²¡æœ‰æŒ‡å®šå¯¼å‡ºè·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
            if not export_path:
                # é»˜è®¤ä¿å­˜åˆ°é¡¹ç›®æ ¹ç›®å½•çš„ exports æ–‡ä»¶å¤¹
                exports_dir = Path("exports")
                exports_dir.mkdir(exist_ok=True)
                
                # ç”Ÿæˆæ–‡ä»¶åï¼šdata_id_timestamp.csv
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                export_filename = f"{data_id}_cleaned_{timestamp}.csv"
                export_path = exports_dir / export_filename
            else:
                export_path = Path(export_path)
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                export_path.parent.mkdir(parents=True, exist_ok=True)
            
            # å¯¼å‡ºæ•°æ®
            try:
                data.to_csv(export_path, encoding='utf-8')
                export_info = {
                    "exported": True,
                    "path": str(export_path.absolute()),
                    "size_mb": round(os.path.getsize(export_path) / (1024 * 1024), 2)
                }
            except Exception as e:
                export_info = {
                    "exported": False,
                    "error": f"å¯¼å‡ºå¤±è´¥: {str(e)}"
                }
        
        # ä¼˜åŒ–çš„å“åº”æ ¼å¼
        result = {
            "status": "success",
            "message": f"âœ… æ•°æ®å·²æˆåŠŸé¢„å¤„ç†ä¸º '{data_id}'" + (f" (å·²è‡ªåŠ¨æ¸…æ´—)" if cleaned else "") + (f" å¹¶å¯¼å‡ºåˆ°æœ¬åœ°" if export_info and export_info.get('exported') else ""),
            "summary": {
                "data_id": data_id,
                "shape": {"rows": data.shape[0], "columns": data.shape[1]},
                "columns": list(data.columns),
                "date_range": {
                    "start": str(data.index.get_level_values(0).min()),
                    "end": str(data.index.get_level_values(0).max())
                },
                "symbol_count": len(data.index.get_level_values(1).unique()),
                "total_records": len(data)
            },
            "preview": {
                "head_3": data.head(3).to_dict('records')[:3],
                "tail_3": data.tail(3).to_dict('records')[:3]
            },
            "data_quality": {
                "auto_cleaned": cleaned,
                "original_missing_values": original_null_count if cleaned else current_null_count,
                "original_missing_rate": f"{original_null_rate * 100:.2f}%" if cleaned else f"{current_null_rate * 100:.2f}%",
                "current_missing_values": current_null_count,
                "current_missing_rate": f"{current_null_rate * 100:.2f}%",
                "quality_score": "ä¼˜ç§€" if current_null_rate < 0.01 else "è‰¯å¥½" if current_null_rate < 0.05 else "éœ€è¦æ¸…æ´—",
                "cleaning_applied": cleaning_methods
            },
            "next_steps": [
                f"ä½¿ç”¨ generate_alpha158 ç”Ÿæˆå› å­: result_id='alpha158_{data_id}'",
                f"æˆ–ä½¿ç”¨ calculate_factor è®¡ç®—å•ä¸ªå› å­",
                f"ä½¿ç”¨ list_factors æŸ¥çœ‹å·²åŠ è½½æ•°æ®: æ— éœ€å‚æ•°"
            ]
        }
        
        # æ·»åŠ å¯¼å‡ºä¿¡æ¯
        if export_info:
            result["export_info"] = export_info
        
        result = convert_to_serializable(result)
        return [types.TextContent(type="text", text=serialize_response(result))]
        
    except FileNotFoundError as e:
        return [types.TextContent(
            type="text",
            text=MCPError.format_error(
                error_code=MCPError.FILE_ERROR,
                message=f"æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}",
                details={"file_path": file_path, "error": str(e)},
                suggestions=[
                    "æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®",
                    "ç¡®è®¤æ–‡ä»¶æ˜¯å¦å­˜åœ¨",
                    "ä½¿ç”¨ç»å¯¹è·¯å¾„"
                ]
            )
        )]
    except Exception as e:
        return [types.TextContent(
            type="text",
            text=MCPError.format_error(
                error_code=MCPError.COMPUTATION_ERROR,
                message=f"æ•°æ®åŠ è½½å¤±è´¥: {str(e)}",
                details={
                    "file_path": file_path,
                    "exception_type": type(e).__name__,
                    "exception_message": str(e)
                },
                suggestions=[
                    "æ£€æŸ¥CSVæ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®",
                    "ç¡®è®¤æ–‡ä»¶ç¼–ç ä¸ºUTF-8",
                    "éªŒè¯æ–‡ä»¶æ˜¯å¦åŒ…å«å¿…éœ€çš„åˆ—"
                ]
            )
        )]


async def handle_calculate_factor(args: Dict[str, Any]) -> List[types.TextContent]:
    """è®¡ç®—å› å­"""
    data_id = args["data_id"]
    factor_name = args["factor_name"]
    factor_type = args["factor_type"]
    period = args.get("period", 20)
    
    # éªŒè¯æ•°æ®ID
    error = validate_data_id(data_id, data_store)
    if error:
        return [types.TextContent(type="text", text=error)]
    
    # éªŒè¯periodå‚æ•°
    error = validate_period(period)
    if error:
        return [types.TextContent(type="text", text=error)]
    
    try:
        data = data_store[data_id]
        library = FactorLibrary()
        factor_func = getattr(library, factor_type)
        factor_values = factor_func(data, period)
        
        factor_store[factor_name] = factor_values
        
        # ä¼˜åŒ–çš„å“åº”æ ¼å¼
        null_count = int(factor_values.isna().sum()) if hasattr(factor_values, 'isna') else 0
        null_rate = null_count / len(factor_values) if len(factor_values) > 0 else 0
        
        # å…ˆè®¡ç®—è´¨é‡åˆ†æ•°ï¼Œé¿å…åœ¨æ„å»ºresultæ—¶å¼•ç”¨è‡ªèº«
        quality_score = "ä¼˜ç§€" if null_rate < 0.01 else "è‰¯å¥½" if null_rate < 0.05 else "éœ€è¦æ¸…æ´—"
        
        result = {
            "status": "success",
            "message": f"âœ… å› å­ '{factor_name}' è®¡ç®—å®Œæˆ",
            "factor_info": {
                "factor_name": factor_name,
                "factor_type": factor_type,
                "period": period,
                "data_rows": len(factor_values),
                "valid_values": len(factor_values) - null_count
            },
            "data_quality": {
                "null_count": null_count,
                "null_rate": f"{null_rate * 100:.2f}%",
                "quality_score": quality_score
            },
            "next_steps": [
                {
                    "step": 1,
                    "action": "è¯„ä¼°å› å­æœ‰æ•ˆæ€§",
                    "tool": "evaluate_factor_ic",
                    "params_example": {
                        "factor_name": factor_name,
                        "data_id": data_id,
                        "method": "spearman"
                    },
                    "reason": "åˆ¤æ–­å› å­æ˜¯å¦æœ‰é¢„æµ‹èƒ½åŠ›"
                },
                {
                    "step": 2,
                    "action": "å¦‚æœICæœ‰æ•ˆï¼Œå¯ç”Ÿæˆæ›´å¤šå› å­æˆ–ç›´æ¥è®­ç»ƒæ¨¡å‹",
                    "tools": ["generate_alpha158", "train_lstm_model"]
                }
            ],
            "tips": [
                f"ğŸ’¡ å› å­ç±»å‹: {factor_type}ï¼Œå‘¨æœŸ: {period}å¤©",
                f"ğŸ’¡ æ•°æ®è´¨é‡: {quality_score}",
                "ğŸ’¡ å»ºè®®å…ˆè¯„ä¼°ICå†å†³å®šæ˜¯å¦ä½¿ç”¨æ­¤å› å­"
            ]
        }
        
        return [types.TextContent(type="text", text=serialize_response(result))]
        
    except AttributeError:
        return [types.TextContent(
            type="text",
            text=MCPError.format_error(
                error_code=MCPError.INVALID_PARAMETER,
                message=f"ä¸æ”¯æŒçš„å› å­ç±»å‹: {factor_type}",
                details={
                    "factor_type": factor_type,
                    "supported_types": ["momentum", "volatility", "volume_ratio", "rsi", "macd", "bollinger_bands"]
                },
                suggestions=[
                    "ä½¿ç”¨æ”¯æŒçš„å› å­ç±»å‹ä¹‹ä¸€",
                    "æ£€æŸ¥factor_typeå‚æ•°æ‹¼å†™"
                ]
            )
        )]
    except Exception as e:
        return [types.TextContent(
            type="text",
            text=MCPError.format_error(
                error_code=MCPError.COMPUTATION_ERROR,
                message=f"å› å­è®¡ç®—å¤±è´¥: {str(e)}",
                details={
                    "factor_name": factor_name,
                    "factor_type": factor_type,
                    "period": period,
                    "exception_type": type(e).__name__
                },
                suggestions=[
                    "æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦æœ‰NaNæˆ–Infå€¼",
                    "ç¡®è®¤periodå‚æ•°åˆç†",
                    "å°è¯•ä½¿ç”¨æ›´å°çš„periodå€¼"
                ]
            )
        )]


async def handle_generate_alpha158(args: Dict[str, Any]) -> List[types.TextContent]:
    """ç”ŸæˆAlpha158å› å­"""
    data_id = args["data_id"]
    result_id = args["result_id"]
    kbar = args.get("kbar", True)
    price = args.get("price", True)
    volume = args.get("volume", True)
    rolling = args.get("rolling", True)
    rolling_windows = args.get("rolling_windows", [5, 10, 20, 30, 60])
    
    # éªŒè¯æ•°æ®ID
    error = validate_data_id(data_id, data_store)
    if error:
        return [types.TextContent(type="text", text=error)]
    
    # éªŒè¯rolling_windowså‚æ•°
    if rolling:
        error = validate_window_size(rolling_windows)
        if error:
            return [types.TextContent(type="text", text=error)]
    
    try:
        data = data_store[data_id]
        
        # éªŒè¯å¿…éœ€åˆ—
        required_cols = ['open', 'high', 'low', 'close', 'volume']
        error = validate_required_columns(data, required_cols, data_id)
        if error:
            return [types.TextContent(type="text", text=error)]
        
        # éªŒè¯æ•°æ®é‡
        min_required = max(rolling_windows) if rolling and rolling_windows else 100
        error = validate_data_length(data, min_length=min_required, data_id=data_id)
        if error:
            return [types.TextContent(type="text", text=error)]
        
        generator = Alpha158Generator(data)
        
        alpha158 = generator.generate_all(
            kbar=kbar,
            price=price,
            volume=volume,
            rolling=rolling,
            rolling_windows=rolling_windows
        )
        
        factor_store[result_id] = alpha158
        
        # ä¼˜åŒ–çš„å“åº”æ ¼å¼
        null_count = int(alpha158.isna().sum().sum())
        total_values = alpha158.shape[0] * alpha158.shape[1]
        null_rate = null_count / total_values if total_values > 0 else 0
        
        # å…ˆè®¡ç®—è´¨é‡åˆ†æ•°ï¼Œé¿å…åœ¨æ„å»ºresultæ—¶å¼•ç”¨è‡ªèº«
        quality_score = "ä¼˜ç§€" if null_rate < 0.01 else "è‰¯å¥½" if null_rate < 0.05 else "éœ€è¦æ¸…æ´—"
        
        result = {
            "status": "success",
            "message": f"âœ… Alpha158å› å­å·²ç”Ÿæˆå¹¶å­˜å‚¨ä¸º '{result_id}'",
            "factor_info": {
                "factor_id": result_id,
                "total_factors": len(alpha158.columns),
                "shape": list(alpha158.shape),
                "categories": {
                    "kbar": 9 if kbar else 0,
                    "price": 5 if price else 0,
                    "volume": 5 if volume else 0,
                    "rolling": len(alpha158.columns) - (9 if kbar else 0) - (5 if price else 0) - (5 if volume else 0)
                }
            },
            "data_quality": {
                "null_count": null_count,
                "null_rate": f"{null_rate * 100:.2f}%",
                "quality_score": quality_score,
                "recommendation": "æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå¯ç›´æ¥ç”¨äºæ¨¡å‹è®­ç»ƒ" if null_rate < 0.01 else "å»ºè®®ä½¿ç”¨ apply_processor_chain è¿›è¡Œæ•°æ®æ¸…æ´—"
            },
            "next_steps": [
                {
                    "step": 1,
                    "action": "æ•°æ®é¢„å¤„ç†ï¼ˆå»ºè®®ï¼‰" if null_rate > 0 else "æ•°æ®é¢„å¤„ç†ï¼ˆå¯é€‰ï¼‰",
                    "tool": "apply_processor_chain",
                    "reason": "æ¸…æ´—ç¼ºå¤±å€¼å¹¶æ ‡å‡†åŒ–" if null_rate > 0 else "æ ‡å‡†åŒ–æ•°æ®æå‡æ¨¡å‹æ•ˆæœ"
                },
                {
                    "step": 2,
                    "action": "è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹",
                    "tools": ["train_lstm_model", "train_gru_model", "train_transformer_model"],
                    "params_example": {
                        "data_id": result_id,
                        "model_id": f"model_{result_id}"
                    }
                },
                {
                    "step": 3,
                    "action": "å› å­è¯„ä¼°ï¼ˆå¯é€‰ï¼‰",
                    "tool": "evaluate_factor_ic",
                    "params_example": {
                        "factor_name": result_id,
                        "data_id": data_id
                    }
                }
            ],
            "tips": [
                f"ğŸ’¡ å› å­æ•°é‡: {len(alpha158.columns)}ä¸ªï¼Œå»ºè®®ä½¿ç”¨LSTMæˆ–Transformeræ¨¡å‹",
                f"ğŸ’¡ æ•°æ®è´¨é‡: {quality_score}",
                "ğŸ’¡ å¦‚æœæ•°æ®é‡ä¸è¶³1000æ¡ï¼Œå»ºè®®ä½¿ç”¨æ›´å°çš„rolling_windows"
            ]
        }
        
        return [types.TextContent(type="text", text=serialize_response(result))]
        
    except Exception as e:
        return [types.TextContent(
            type="text",
            text=MCPError.format_error(
                error_code=MCPError.COMPUTATION_ERROR,
                message=f"Alpha158å› å­è®¡ç®—å¤±è´¥: {str(e)}",
                details={
                    "data_id": data_id,
                    "result_id": result_id,
                    "exception_type": type(e).__name__
                },
                suggestions=[
                    "æ£€æŸ¥æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®",
                    "ç¡®è®¤æ•°æ®ä¸­æ²¡æœ‰NaNæˆ–Infå€¼",
                    "å°è¯•å‡å°‘rolling_windowså‚æ•°",
                    "ç¡®ä¿æ•°æ®é‡è¶³å¤Ÿï¼ˆå»ºè®®1000+æ¡ï¼‰"
                ]
            )
        )]


async def handle_evaluate_factor_ic(args: Dict[str, Any]) -> List[types.TextContent]:
    """è¯„ä¼°å› å­IC"""


async def handle_quick_start_lstm(args: Dict[str, Any]) -> List[types.TextContent]:
    """å¿«é€Ÿå¯åŠ¨LSTMå·¥ä½œæµ"""
    data_file = args["data_file"]
    project = args["project_name"]
    model_config = args.get("model_config", {})
    
    workflow_results = {
        "project_name": project,
        "steps_completed": [],
        "generated_ids": {}
    }
    
    try:
        # æ­¥éª¤1: é¢„å¤„ç†æ•°æ®
        # printè¯­å¥ç§»é™¤ä»¥é¿å…Windowsç¯å¢ƒä¸‹çš„GBKç¼–ç é”™è¯¯
        data_result = await handle_preprocess_data({
            "file_path": data_file,
            "data_id": f"{project}_data"
        })
        workflow_results["steps_completed"].append("preprocess_data")
        workflow_results["generated_ids"]["data_id"] = f"{project}_data"
        
        # æ­¥éª¤2: ç”ŸæˆAlpha158å› å­
        factor_result = await handle_generate_alpha158({
            "data_id": f"{project}_data",
            "result_id": f"{project}_alpha158"
        })
        workflow_results["steps_completed"].append("generate_alpha158")
        workflow_results["generated_ids"]["factor_id"] = f"{project}_alpha158"
        
        # æ­¥éª¤3: æ•°æ®é¢„å¤„ç†
        # æ³¨æ„ï¼šè¿™é‡Œç®€åŒ–äº†ï¼Œå®é™…åº”è¯¥æœ‰apply_processorå‡½æ•°
        # æš‚æ—¶è·³è¿‡é¢„å¤„ç†æ­¥éª¤
        workflow_results["steps_completed"].append("preprocessing_skipped")
        workflow_results["generated_ids"]["processed_id"] = f"{project}_alpha158"
        
        # æ­¥éª¤4: è®­ç»ƒLSTMæ¨¡å‹
        # æ³¨æ„ï¼šè¿™é‡Œéœ€è¦train_lstm_modelå‡½æ•°
        # æš‚æ—¶è¿”å›å ä½ç¬¦
        workflow_results["steps_completed"].append("model_training_placeholder")
        workflow_results["generated_ids"]["model_id"] = f"{project}_lstm"
        
        # è¿”å›ç»¼åˆç»“æœ
        result = {
            "status": "success",
            "message": f"ğŸ‰ LSTMå·¥ä½œæµå®Œæˆï¼é¡¹ç›®: {project}",
            "workflow_summary": {
                "project_name": project,
                "steps_completed": len(workflow_results["steps_completed"]),
                "total_time": "æ ¹æ®æ•°æ®é‡è€Œå®š",
                "generated_ids": workflow_results["generated_ids"]
            },
            "next_steps": [
                {
                    "action": "æ¨¡å‹é¢„æµ‹",
                    "tool": "predict_with_model",
                    "params": {
                        "model_id": f"{project}_lstm",
                        "data_id": f"{project}_alpha158",
                        "result_id": f"{project}_predictions"
                    }
                },
                {
                    "action": "è¯„ä¼°æ•ˆæœ",
                    "tool": "evaluate_factor_ic",
                    "params": {
                        "factor_name": f"{project}_predictions",
                        "data_id": f"{project}_data"
                    }
                }
            ],
            "tips": [
                f"ğŸ’¡ æ‰€æœ‰ç”Ÿæˆçš„IDéƒ½ä»¥ '{project}_' ä¸ºå‰ç¼€",
                "ğŸ’¡ ä½¿ç”¨ list_factors æŸ¥çœ‹æ‰€æœ‰ç”Ÿæˆçš„æ•°æ®å’Œå› å­",
                "ğŸ’¡ ä½¿ç”¨ predict_with_model è¿›è¡Œé¢„æµ‹"
            ]
        }
        
        return [types.TextContent(type="text", text=serialize_response(result))]
        
    except Exception as e:
        return [types.TextContent(
            type="text",
            text=MCPError.format_error(
                error_code=MCPError.COMPUTATION_ERROR,
                message=f"LSTMå·¥ä½œæµå¤±è´¥: {str(e)}",
                details={
                    "project_name": project,
                    "failed_at_step": len(workflow_results["steps_completed"]) + 1,
                    "completed_steps": workflow_results["steps_completed"]
                },
                suggestions=[
                    "æ£€æŸ¥æ•°æ®æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®",
                    "ç¡®è®¤æ•°æ®æ ¼å¼ç¬¦åˆè¦æ±‚",
                    "æŸ¥çœ‹å·²å®Œæˆçš„æ­¥éª¤ï¼Œä»å¤±è´¥å¤„ç»§ç»­",
                    f"å·²ç”Ÿæˆçš„ID: {workflow_results['generated_ids']}"
                ]
            )
        )]
    factor_name = args["factor_name"]
    data_id = args["data_id"]
    method = args.get("method", "spearman")
    
    # éªŒè¯å› å­åç§°
    error = validate_factor_name(factor_name, factor_store)
    if error:
        return [types.TextContent(type="text", text=error)]
    
    # éªŒè¯æ•°æ®ID
    error = validate_data_id(data_id, data_store)
    if error:
        return [types.TextContent(type="text", text=error)]
    
    try:
        factor_data = factor_store[factor_name]
        price_data = data_store[data_id]["close"]
        
        returns = price_data.groupby(level=1).pct_change().shift(-1)
        aligned_factor = factor_data.dropna()
        aligned_returns = returns.reindex(aligned_factor.index)
        
        evaluator = FactorEvaluator(aligned_factor, aligned_returns)
        ic_result = evaluator.calculate_ic(method=method)
        
        # ä¼˜åŒ–çš„å“åº”æ ¼å¼
        ic_mean = ic_result.get('ic_mean', 0)
        ic_std = ic_result.get('ic_std', 0)
        icir = ic_result.get('icir', 0)
        
        # åˆ¤æ–­å› å­è´¨é‡
        abs_ic = abs(ic_mean)
        if abs_ic > 0.10:
            quality = "éå¸¸å¼º"
            recommendation = "å¼ºçƒˆæ¨èä½¿ç”¨æ­¤å› å­"
        elif abs_ic > 0.08:
            quality = "å¼º"
            recommendation = "æ¨èä½¿ç”¨æ­¤å› å­"
        elif abs_ic > 0.05:
            quality = "è¾ƒå¼º"
            recommendation = "å¯ä»¥ä½¿ç”¨æ­¤å› å­"
        elif abs_ic > 0.03:
            quality = "æœ‰æ•ˆ"
            recommendation = "è°¨æ…ä½¿ç”¨ï¼Œå»ºè®®ä¸å…¶ä»–å› å­ç»„åˆ"
        else:
            quality = "æ— æ•ˆ"
            recommendation = "ä¸æ¨èä½¿ç”¨ï¼Œé¢„æµ‹èƒ½åŠ›ä¸è¶³"
        
        result = {
            "status": "success",
            "message": f"âœ… å› å­ '{factor_name}' ICè¯„ä¼°å®Œæˆ",
            "ic_metrics": ic_result,
            "factor_quality": {
                "rating": quality,
                "ic_mean": f"{ic_mean:.4f}",
                "icir": f"{icir:.4f}",
                "recommendation": recommendation
            },
            "interpretation": {
                "direction": "æ­£å‘å› å­ï¼ˆå› å­å€¼è¶Šå¤§ï¼Œæ”¶ç›Šè¶Šé«˜ï¼‰" if ic_mean > 0 else "åå‘å› å­ï¼ˆå› å­å€¼è¶Šå¤§ï¼Œæ”¶ç›Šè¶Šä½ï¼‰",
                "stability": "ç¨³å®š" if icir > 1.0 else "è¾ƒç¨³å®š" if icir > 0.5 else "ä¸ç¨³å®š",
                "predictive_power": f"æ¯å•ä½å› å­å˜åŒ–å¯¹åº”çº¦{abs(ic_mean)*100:.2f}%çš„æ”¶ç›Šç‡å˜åŒ–"
            },
            "next_steps": [
                {
                    "step": 1,
                    "action": "ä½¿ç”¨æœ‰æ•ˆå› å­" if abs_ic > 0.03 else "å°è¯•å…¶ä»–å› å­",
                    "reason": recommendation
                },
                {
                    "step": 2,
                    "action": "ç”Ÿæˆæ›´å¤šå› å­æˆ–è®­ç»ƒæ¨¡å‹",
                    "tools": ["generate_alpha158", "train_lstm_model"] if abs_ic > 0.05 else ["calculate_factor"],
                    "reason": "å› å­æœ‰æ•ˆï¼Œå¯ç»§ç»­å»ºæ¨¡" if abs_ic > 0.05 else "å½“å‰å› å­æ•ˆæœä¸ä½³ï¼Œå»ºè®®å°è¯•å…¶ä»–å› å­"
                }
            ],
            "tips": [
                f"ğŸ’¡ ICå‡å€¼: {ic_mean:.4f} ({'å¼º' if abs_ic > 0.08 else 'è¾ƒå¼º' if abs_ic > 0.05 else 'å¼±'})",
                f"ğŸ’¡ ICIR: {icir:.4f} ({'ç¨³å®š' if icir > 1.0 else 'è¾ƒç¨³å®š' if icir > 0.5 else 'ä¸ç¨³å®š'})",
                f"ğŸ’¡ {quality}å› å­ï¼Œ{recommendation}"
            ]
        }
        
        return [types.TextContent(type="text", text=serialize_response(result))]
        
    except KeyError as e:
        return [types.TextContent(
            type="text",
            text=MCPError.format_error(
                error_code=MCPError.INVALID_PARAMETER,
                message=f"æ•°æ®ç¼ºå°‘closeåˆ—: {str(e)}",
                details={"data_id": data_id, "missing_column": "close"},
                suggestions=[
                    "ç¡®ä¿æ•°æ®åŒ…å«closeåˆ—",
                    "æ£€æŸ¥æ•°æ®åŠ è½½æ˜¯å¦æ­£ç¡®"
                ]
            )
        )]
    except Exception as e:
        return [types.TextContent(
            type="text",
            text=MCPError.format_error(
                error_code=MCPError.COMPUTATION_ERROR,
                message=f"ICè¯„ä¼°å¤±è´¥: {str(e)}",
                details={
                    "factor_name": factor_name,
                    "data_id": data_id,
                    "method": method,
                    "exception_type": type(e).__name__
                },
                suggestions=[
                    "æ£€æŸ¥å› å­å’Œæ•°æ®çš„æ—¶é—´èŒƒå›´æ˜¯å¦åŒ¹é…",
                    "ç¡®è®¤æ•°æ®ä¸­æœ‰è¶³å¤Ÿçš„æ ·æœ¬",
                    "å°è¯•ä½¿ç”¨ä¸åŒçš„methodå‚æ•°"
                ]
            )
        )]


async def handle_list_factors(args: Dict[str, Any]) -> List[types.TextContent]:
    """åˆ—å‡ºæ‰€æœ‰å› å­"""
    factors_info = {}
    for factor_id, factor in factor_store.items():
        factors_info[factor_id] = {
            "type": str(type(factor).__name__),
            "shape": list(factor.shape) if hasattr(factor, 'shape') else None
        }
    
    result = {
        "data_count": len(data_store),
        "data_ids": list(data_store.keys()),
        "factor_count": len(factor_store),
        "factors": factors_info
    }
    
    return [types.TextContent(
        type="text",
        text=json.dumps(result, ensure_ascii=False, indent=2)
    )]