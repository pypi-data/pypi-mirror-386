"""
MCPå·¥å…·å¤„ç†å‡½æ•°
åŒ…å«æ‰€æœ‰å·¥å…·çš„ä¸šåŠ¡é€»è¾‘å’Œé”™è¯¯å¤„ç†
"""

from typing import Any, Dict, List
from mcp import types
import json
import pandas as pd

from .errors import (
    MCPError,
    validate_data_id,
    validate_factor_name,
    validate_required_columns,
    validate_data_length,
    validate_window_size,
    validate_period,
    validate_file_path
)
from .utils import serialize_response, convert_to_serializable

from quantanalyzer.data import DataLoader
from quantanalyzer.data.processor import (
    ProcessInf, CSZFillna, CSZScoreNorm, ZScoreNorm,
    RobustZScoreNorm, CSRankNorm, MinMaxNorm, ProcessorChain
)
from quantanalyzer.factor import FactorLibrary, FactorEvaluator, Alpha158Generator


# å…¨å±€å­˜å‚¨
data_store = {}
factor_store = {}
model_store = {}
processor_store = {}


async def handle_preprocess_data(args: Dict[str, Any]) -> List[types.TextContent]:
    """æ•°æ®é¢„å¤„ç†å’Œæ¸…æ´—"""
    file_path = args["file_path"]
    data_id = args["data_id"]
    auto_clean = args.get("auto_clean", True)  # é»˜è®¤å¼€å¯è‡ªåŠ¨æ¸…æ´—
    export_path = args.get("export_path", None)  # å¯¼å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰
    
    # éªŒè¯æ–‡ä»¶è·¯å¾„
    error = validate_file_path(file_path)
    if error:
        return [types.TextContent(type="text", text=error)]
    
    try:
        loader = DataLoader()
        data = loader.load_from_csv(file_path)
        
        # éªŒè¯å¿…éœ€åˆ—
        required_cols = ['open', 'high', 'low', 'close', 'volume']
        error = validate_required_columns(data, required_cols, data_id)
        if error:
            return [types.TextContent(type="text", text=error)]
        
        # éªŒè¯æ•°æ®é‡
        error = validate_data_length(data, min_length=100, data_id=data_id)
        if error:
            return [types.TextContent(type="text", text=error)]
        
        # è®°å½•åŸå§‹æ•°æ®è´¨é‡
        original_null_count = int(data.isna().sum().sum())
        original_null_rate = original_null_count / (data.shape[0] * data.shape[1]) if data.shape[0] * data.shape[1] > 0 else 0
        
        # è‡ªåŠ¨æ•°æ®æ¸…æ´—
        cleaned = False
        cleaning_methods = []
        if auto_clean:
            try:
                # æ£€æŸ¥è‚¡ç¥¨æ•°é‡
                symbol_count = len(data.index.get_level_values(1).unique())
                
                # æŒ‰ç…§Qlibæ ‡å‡†æµç¨‹æ¸…æ´—æ•°æ®
                # æ³¨æ„ï¼šè¿™é‡Œåªæ¸…æ´—å¼‚å¸¸å€¼å’Œç¼ºå¤±å€¼ï¼Œä¸è¿›è¡Œæ ‡å‡†åŒ–
                # æ ‡å‡†åŒ–åº”è¯¥åœ¨å› å­ç”Ÿæˆåè¿›è¡Œï¼ˆå¯¹å› å­æ ‡å‡†åŒ–ï¼Œè€ŒéåŸå§‹OHLCVæ•°æ®ï¼‰
                processor_chain = ProcessorChain([
                    ProcessInf(),          # å¤„ç†æ— ç©·å€¼ï¼ˆç”¨æˆªé¢å‡å€¼æ›¿æ¢ï¼‰
                    CSZFillna(),           # æˆªé¢å¡«å……ç¼ºå¤±å€¼ï¼ˆç”¨æˆªé¢å‡å€¼ï¼‰
                ])
                data = processor_chain.fit_transform(data.copy())
                
                cleaning_methods = [
                    "ProcessInf() - ç”¨æˆªé¢å‡å€¼æ›¿æ¢inf/-inf",
                    "CSZFillna() - ç”¨æˆªé¢å‡å€¼å¡«å……NaN"
                ]
                
                # å¤šè‚¡ç¥¨åœºæ™¯çš„æç¤ºä¿¡æ¯
                if symbol_count > 1:
                    cleaning_methods.append(
                        "âš ï¸ æç¤ºï¼šåŸå§‹æ•°æ®æœªæ ‡å‡†åŒ–ï¼Œå»ºè®®åœ¨å› å­ç”Ÿæˆåä½¿ç”¨CSZScoreNormæ ‡å‡†åŒ–å› å­"
                    )
                
                cleaned = True
            except Exception as e:
                # å¦‚æœæ¸…æ´—å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹æ•°æ®å¹¶è®°å½•è­¦å‘Š
                cleaned = False
                cleaning_methods = []
        
        data_store[data_id] = data
        
        # è®¡ç®—æ¸…æ´—åçš„æ•°æ®è´¨é‡
        current_null_count = int(data.isna().sum().sum())
        current_null_rate = current_null_count / (data.shape[0] * data.shape[1]) if data.shape[0] * data.shape[1] > 0 else 0
        
        # å¯¼å‡ºæ¸…æ´—åçš„æ•°æ®åˆ°æœ¬åœ°
        export_info = None
        if export_path or cleaned:  # å¦‚æœæŒ‡å®šå¯¼å‡ºè·¯å¾„æˆ–è¿›è¡Œäº†æ¸…æ´—ï¼Œåˆ™å¯¼å‡º
            import os
            from pathlib import Path
            from datetime import datetime
            
            # å¦‚æœæ²¡æœ‰æŒ‡å®šå¯¼å‡ºè·¯å¾„ï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
            if not export_path:
                # é»˜è®¤ä¿å­˜åˆ°é¡¹ç›®æ ¹ç›®å½•çš„ exports æ–‡ä»¶å¤¹
                exports_dir = Path("exports")
                exports_dir.mkdir(exist_ok=True)
                
                # ç”Ÿæˆæ–‡ä»¶åï¼šdata_id_timestamp.csv
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                export_filename = f"{data_id}_cleaned_{timestamp}.csv"
                export_path = exports_dir / export_filename
            else:
                export_path = Path(export_path)
                # ç¡®ä¿ç›®å½•å­˜åœ¨
                export_path.parent.mkdir(parents=True, exist_ok=True)
            
            # å¯¼å‡ºæ•°æ®
            try:
                data.to_csv(export_path, encoding='utf-8')
                export_info = {
                    "exported": True,
                    "path": str(export_path.absolute()),
                    "size_mb": round(os.path.getsize(export_path) / (1024 * 1024), 2)
                }
            except Exception as e:
                export_info = {
                    "exported": False,
                    "error": f"å¯¼å‡ºå¤±è´¥: {str(e)}"
                }
        
        # ä¼˜åŒ–çš„å“åº”æ ¼å¼
        result = {
            "status": "success",
            "message": f"âœ… æ•°æ®å·²æˆåŠŸé¢„å¤„ç†ä¸º '{data_id}'" + (f" (å·²è‡ªåŠ¨æ¸…æ´—)" if cleaned else "") + (f" å¹¶å¯¼å‡ºåˆ°æœ¬åœ°" if export_info and export_info.get('exported') else ""),
            "summary": {
                "data_id": data_id,
                "shape": {"rows": data.shape[0], "columns": data.shape[1]},
                "columns": list(data.columns),
                "date_range": {
                    "start": str(data.index.get_level_values(0).min()),
                    "end": str(data.index.get_level_values(0).max())
                },
                "symbol_count": len(data.index.get_level_values(1).unique()),
                "total_records": len(data)
            },
            "preview": {
                "head_3": data.head(3).to_dict('records')[:3],
                "tail_3": data.tail(3).to_dict('records')[:3]
            },
            "data_quality": {
                "auto_cleaned": cleaned,
                "original_missing_values": original_null_count if cleaned else current_null_count,
                "original_missing_rate": f"{original_null_rate * 100:.2f}%" if cleaned else f"{current_null_rate * 100:.2f}%",
                "current_missing_values": current_null_count,
                "current_missing_rate": f"{current_null_rate * 100:.2f}%",
                "quality_score": "ä¼˜ç§€" if current_null_rate < 0.01 else "è‰¯å¥½" if current_null_rate < 0.05 else "éœ€è¦æ¸…æ´—",
                "cleaning_applied": cleaning_methods
            },
            "next_steps": [
                f"ä½¿ç”¨ generate_alpha158 ç”Ÿæˆå› å­: result_id='alpha158_{data_id}'",
                f"æˆ–ä½¿ç”¨ calculate_factor è®¡ç®—å•ä¸ªå› å­",
                f"ä½¿ç”¨ list_factors æŸ¥çœ‹å·²åŠ è½½æ•°æ®: æ— éœ€å‚æ•°"
            ]
        }
        
        # æ·»åŠ å¯¼å‡ºä¿¡æ¯
        if export_info:
            result["export_info"] = export_info
        
        result = convert_to_serializable(result)
        return [types.TextContent(type="text", text=serialize_response(result))]
        
    except FileNotFoundError as e:
        return [types.TextContent(
            type="text",
            text=MCPError.format_error(
                error_code=MCPError.FILE_ERROR,
                message=f"æ–‡ä»¶æœªæ‰¾åˆ°: {file_path}",
                details={"file_path": file_path, "error": str(e)},
                suggestions=[
                    "æ£€æŸ¥æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®",
                    "ç¡®è®¤æ–‡ä»¶æ˜¯å¦å­˜åœ¨",
                    "ä½¿ç”¨ç»å¯¹è·¯å¾„"
                ]
            )
        )]
    except Exception as e:
        return [types.TextContent(
            type="text",
            text=MCPError.format_error(
                error_code=MCPError.COMPUTATION_ERROR,
                message=f"æ•°æ®åŠ è½½å¤±è´¥: {str(e)}",
                details={
                    "file_path": file_path,
                    "exception_type": type(e).__name__,
                    "exception_message": str(e)
                },
                suggestions=[
                    "æ£€æŸ¥CSVæ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡®",
                    "ç¡®è®¤æ–‡ä»¶ç¼–ç ä¸ºUTF-8",
                    "éªŒè¯æ–‡ä»¶æ˜¯å¦åŒ…å«å¿…éœ€çš„åˆ—"
                ]
            )
        )]


async def handle_calculate_factor(args: Dict[str, Any]) -> List[types.TextContent]:
    """è®¡ç®—å› å­"""
    data_id = args["data_id"]
    factor_name = args["factor_name"]
    factor_type = args["factor_type"]
    period = args.get("period", 20)
    
    # éªŒè¯æ•°æ®ID
    error = validate_data_id(data_id, data_store)
    if error:
        return [types.TextContent(type="text", text=error)]
    
    # éªŒè¯periodå‚æ•°
    error = validate_period(period)
    if error:
        return [types.TextContent(type="text", text=error)]
    
    try:
        data = data_store[data_id]
        library = FactorLibrary()
        factor_func = getattr(library, factor_type)
        factor_values = factor_func(data, period)
        
        factor_store[factor_name] = factor_values
        
        # ä¼˜åŒ–çš„å“åº”æ ¼å¼
        null_count = int(factor_values.isna().sum()) if hasattr(factor_values, 'isna') else 0
        null_rate = null_count / len(factor_values) if len(factor_values) > 0 else 0
        
        # å…ˆè®¡ç®—è´¨é‡åˆ†æ•°ï¼Œé¿å…åœ¨æ„å»ºresultæ—¶å¼•ç”¨è‡ªèº«
        quality_score = "ä¼˜ç§€" if null_rate < 0.01 else "è‰¯å¥½" if null_rate < 0.05 else "éœ€è¦æ¸…æ´—"
        
        result = {
            "status": "success",
            "message": f"âœ… å› å­ '{factor_name}' è®¡ç®—å®Œæˆ",
            "factor_info": {
                "factor_name": factor_name,
                "factor_type": factor_type,
                "period": period,
                "data_rows": len(factor_values),
                "valid_values": len(factor_values) - null_count
            },
            "data_quality": {
                "null_count": null_count,
                "null_rate": f"{null_rate * 100:.2f}%",
                "quality_score": quality_score
            },
            "next_steps": [
                {
                    "step": 1,
                    "action": "è¯„ä¼°å› å­æœ‰æ•ˆæ€§",
                    "tool": "evaluate_factor_ic",
                    "params_example": {
                        "factor_name": factor_name,
                        "data_id": data_id,
                        "method": "spearman"
                    },
                    "reason": "åˆ¤æ–­å› å­æ˜¯å¦æœ‰é¢„æµ‹èƒ½åŠ›"
                },
                {
                    "step": 2,
                    "action": "å¦‚æœICæœ‰æ•ˆï¼Œå¯ç”Ÿæˆæ›´å¤šå› å­æˆ–ç›´æ¥è®­ç»ƒæ¨¡å‹",
                    "tools": ["generate_alpha158", "train_lstm_model"]
                }
            ],
            "tips": [
                f"ğŸ’¡ å› å­ç±»å‹: {factor_type}ï¼Œå‘¨æœŸ: {period}å¤©",
                f"ğŸ’¡ æ•°æ®è´¨é‡: {quality_score}",
                "ğŸ’¡ å»ºè®®å…ˆè¯„ä¼°ICå†å†³å®šæ˜¯å¦ä½¿ç”¨æ­¤å› å­"
            ]
        }
        
        return [types.TextContent(type="text", text=serialize_response(result))]
        
    except AttributeError:
        return [types.TextContent(
            type="text",
            text=MCPError.format_error(
                error_code=MCPError.INVALID_PARAMETER,
                message=f"ä¸æ”¯æŒçš„å› å­ç±»å‹: {factor_type}",
                details={
                    "factor_type": factor_type,
                    "supported_types": ["momentum", "volatility", "volume_ratio", "rsi", "macd", "bollinger_bands"]
                },
                suggestions=[
                    "ä½¿ç”¨æ”¯æŒçš„å› å­ç±»å‹ä¹‹ä¸€",
                    "æ£€æŸ¥factor_typeå‚æ•°æ‹¼å†™"
                ]
            )
        )]
    except Exception as e:
        return [types.TextContent(
            type="text",
            text=MCPError.format_error(
                error_code=MCPError.COMPUTATION_ERROR,
                message=f"å› å­è®¡ç®—å¤±è´¥: {str(e)}",
                details={
                    "factor_name": factor_name,
                    "factor_type": factor_type,
                    "period": period,
                    "exception_type": type(e).__name__
                },
                suggestions=[
                    "æ£€æŸ¥æ•°æ®ä¸­æ˜¯å¦æœ‰NaNæˆ–Infå€¼",
                    "ç¡®è®¤periodå‚æ•°åˆç†",
                    "å°è¯•ä½¿ç”¨æ›´å°çš„periodå€¼"
                ]
            )
        )]


async def handle_generate_alpha158(args: Dict[str, Any]) -> List[types.TextContent]:
    """ç”ŸæˆAlpha158å› å­"""
    data_id = args["data_id"]
    result_id = args["result_id"]
    kbar = args.get("kbar", True)
    price = args.get("price", True)
    volume = args.get("volume", True)
    rolling = args.get("rolling", True)
    rolling_windows = args.get("rolling_windows", [5, 10, 20, 30, 60])
    export_path = args.get("export_path", None)  # å¯¼å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰
    export_format = args.get("export_format", "csv")  # csv æˆ– json
    
    # éªŒè¯æ•°æ®ID
    error = validate_data_id(data_id, data_store)
    if error:
        return [types.TextContent(type="text", text=error)]
    
    # éªŒè¯rolling_windowså‚æ•°
    if rolling:
        error = validate_window_size(rolling_windows)
        if error:
            return [types.TextContent(type="text", text=error)]
    
    try:
        data = data_store[data_id]
        
        # éªŒè¯å¿…éœ€åˆ—
        required_cols = ['open', 'high', 'low', 'close', 'volume']
        error = validate_required_columns(data, required_cols, data_id)
        if error:
            return [types.TextContent(type="text", text=error)]
        
        # éªŒè¯æ•°æ®é‡
        min_required = max(rolling_windows) if rolling and rolling_windows else 100
        error = validate_data_length(data, min_length=min_required, data_id=data_id)
        if error:
            return [types.TextContent(type="text", text=error)]
        
        generator = Alpha158Generator(data)
        
        alpha158 = generator.generate_all(
            kbar=kbar,
            price=price,
            volume=volume,
            rolling=rolling,
            rolling_windows=rolling_windows
        )
        
        factor_store[result_id] = alpha158
        
        # å¯¼å‡ºAlpha158å› å­æ•°æ®
        export_info = None
        if export_path:
            import os
            from pathlib import Path
            from datetime import datetime
            
            export_path_obj = Path(export_path)
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            export_path_obj.parent.mkdir(parents=True, exist_ok=True)
            
            try:
                if export_format.lower() == "csv":
                    alpha158.to_csv(export_path, encoding='utf-8')
                elif export_format.lower() == "json":
                    alpha158.to_json(export_path, orient='records', indent=2)
                else:
                    export_format = "csv"  # é»˜è®¤ä½¿ç”¨CSV
                    alpha158.to_csv(export_path, encoding='utf-8')
                
                file_size = os.path.getsize(export_path) / (1024 * 1024)
                export_info = {
                    "exported": True,
                    "path": str(export_path_obj.absolute()),
                    "format": export_format,
                    "size_mb": round(file_size, 2),
                    "factor_count": len(alpha158.columns),
                    "row_count": len(alpha158)
                }
            except Exception as e:
                export_info = {
                    "exported": False,
                    "error": f"å¯¼å‡ºå¤±è´¥: {str(e)}"
                }
        
        # ä¼˜åŒ–çš„å“åº”æ ¼å¼
        null_count = int(alpha158.isna().sum().sum())
        total_values = alpha158.shape[0] * alpha158.shape[1]
        null_rate = null_count / total_values if total_values > 0 else 0
        
        # å…ˆè®¡ç®—è´¨é‡åˆ†æ•°ï¼Œé¿å…åœ¨æ„å»ºresultæ—¶å¼•ç”¨è‡ªèº«
        quality_score = "ä¼˜ç§€" if null_rate < 0.01 else "è‰¯å¥½" if null_rate < 0.05 else "éœ€è¦æ¸…æ´—"
        
        result = {
            "status": "success",
            "message": f"âœ… Alpha158å› å­å·²ç”Ÿæˆå¹¶å­˜å‚¨ä¸º '{result_id}'",
            "factor_info": {
                "factor_id": result_id,
                "total_factors": len(alpha158.columns),
                "shape": list(alpha158.shape),
                "categories": {
                    "kbar": 9 if kbar else 0,
                    "price": 5 if price else 0,
                    "volume": 5 if volume else 0,
                    "rolling": len(alpha158.columns) - (9 if kbar else 0) - (5 if price else 0) - (5 if volume else 0)
                }
            },
            "data_quality": {
                "null_count": null_count,
                "null_rate": f"{null_rate * 100:.2f}%",
                "quality_score": quality_score,
                "recommendation": "æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå¯ç›´æ¥ç”¨äºæ¨¡å‹è®­ç»ƒ" if null_rate < 0.01 else "å»ºè®®ä½¿ç”¨ apply_processor_chain è¿›è¡Œæ•°æ®æ¸…æ´—"
            },
            "next_steps": [
                {
                    "step": 1,
                    "action": "æ•°æ®é¢„å¤„ç†ï¼ˆå»ºè®®ï¼‰" if null_rate > 0 else "æ•°æ®é¢„å¤„ç†ï¼ˆå¯é€‰ï¼‰",
                    "tool": "apply_processor_chain",
                    "reason": "æ¸…æ´—ç¼ºå¤±å€¼å¹¶æ ‡å‡†åŒ–" if null_rate > 0 else "æ ‡å‡†åŒ–æ•°æ®æå‡æ¨¡å‹æ•ˆæœ"
                },
                {
                    "step": 2,
                    "action": "è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹",
                    "tools": ["train_lstm_model", "train_gru_model", "train_transformer_model"],
                    "params_example": {
                        "data_id": result_id,
                        "model_id": f"model_{result_id}"
                    }
                },
                {
                    "step": 3,
                    "action": "å› å­è¯„ä¼°ï¼ˆå¯é€‰ï¼‰",
                    "tool": "evaluate_factor_ic",
                    "params_example": {
                        "factor_name": result_id,
                        "data_id": data_id
                    }
                }
            ],
            "tips": [
                f"ğŸ’¡ å› å­æ•°é‡: {len(alpha158.columns)}ä¸ªï¼Œå»ºè®®ä½¿ç”¨LSTMæˆ–Transformeræ¨¡å‹",
                f"ğŸ’¡ æ•°æ®è´¨é‡: {quality_score}",
                "ğŸ’¡ å¦‚æœæ•°æ®é‡ä¸è¶³1000æ¡ï¼Œå»ºè®®ä½¿ç”¨æ›´å°çš„rolling_windows",
                f"ğŸ’¡ ä½¿ç”¨export_pathå‚æ•°å¯¼å‡ºå› å­æ•°æ®ä¾¿äºæŸ¥çœ‹" if not export_path else f"ğŸ’¡ å› å­æ•°æ®å·²å¯¼å‡ºåˆ°: {export_path}"
            ]
        }
        
        # æ·»åŠ å¯¼å‡ºä¿¡æ¯
        if export_info:
            result["export_info"] = export_info
        
        return [types.TextContent(type="text", text=serialize_response(result))]
        
    except Exception as e:
        return [types.TextContent(
            type="text",
            text=MCPError.format_error(
                error_code=MCPError.COMPUTATION_ERROR,
                message=f"Alpha158å› å­è®¡ç®—å¤±è´¥: {str(e)}",
                details={
                    "data_id": data_id,
                    "result_id": result_id,
                    "exception_type": type(e).__name__
                },
                suggestions=[
                    "æ£€æŸ¥æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®",
                    "ç¡®è®¤æ•°æ®ä¸­æ²¡æœ‰NaNæˆ–Infå€¼",
                    "å°è¯•å‡å°‘rolling_windowså‚æ•°",
                    "ç¡®ä¿æ•°æ®é‡è¶³å¤Ÿï¼ˆå»ºè®®1000+æ¡ï¼‰"
                ]
            )
        )]


async def handle_evaluate_factor_ic(args: Dict[str, Any]) -> List[types.TextContent]:
    """è¯„ä¼°å› å­IC"""
    factor_name = args["factor_name"]
    data_id = args["data_id"]
    method = args.get("method", "spearman")
    report_path = args.get("report_path", None)  # è¯„ä¼°æŠ¥å‘Šä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    
    # éªŒè¯å› å­åç§°
    error = validate_factor_name(factor_name, factor_store)
    if error:
        return [types.TextContent(type="text", text=error)]
    
    # éªŒè¯æ•°æ®ID
    error = validate_data_id(data_id, data_store)
    if error:
        return [types.TextContent(type="text", text=error)]
    
    try:
        factor_data = factor_store[factor_name]
        price_data = data_store[data_id]["close"]
        
        # ç¡®ä¿price_dataæ˜¯Seriesè€Œä¸æ˜¯DataFrame
        if isinstance(price_data, pd.DataFrame):
            # å¦‚æœæ˜¯DataFrameï¼Œåªå–ç¬¬ä¸€åˆ—
            price_data = price_data.iloc[:, 0]
        
        # è®¡ç®—æ”¶ç›Šç‡æ—¶æ˜ç¡®æŒ‡å®šä¸éœ€è¦axisï¼ˆå› ä¸ºæ˜¯Seriesï¼‰
        returns = price_data.groupby(level=1).pct_change().shift(-1)
        
        # å¤„ç†å› å­æ•°æ®
        if isinstance(factor_data, pd.DataFrame):
            # å¦‚æœå› å­æ˜¯DataFrameï¼ˆAlpha158ï¼‰ï¼Œéœ€è¦é€åˆ—è®¡ç®—IC
            # æš‚æ—¶åªä½¿ç”¨ç¬¬ä¸€ä¸ªå› å­åˆ—
            factor_data = factor_data.iloc[:, 0]
        
        aligned_factor = factor_data.dropna()
        aligned_returns = returns.reindex(aligned_factor.index)
        
        evaluator = FactorEvaluator(aligned_factor, aligned_returns)
        ic_result = evaluator.calculate_ic(method=method)
        
        # ä¼˜åŒ–çš„å“åº”æ ¼å¼
        ic_mean = ic_result.get('ic_mean', 0)
        ic_std = ic_result.get('ic_std', 0)
        icir = ic_result.get('icir', 0)
        ic_positive_ratio = ic_result.get('ic_positive_ratio', 0)
        
        # åˆ¤æ–­å› å­è´¨é‡
        abs_ic = abs(ic_mean)
        if abs_ic > 0.10:
            quality = "éå¸¸å¼º"
            recommendation = "å¼ºçƒˆæ¨èä½¿ç”¨æ­¤å› å­"
        elif abs_ic > 0.08:
            quality = "å¼º"
            recommendation = "æ¨èä½¿ç”¨æ­¤å› å­"
        elif abs_ic > 0.05:
            quality = "è¾ƒå¼º"
            recommendation = "å¯ä»¥ä½¿ç”¨æ­¤å› å­"
        elif abs_ic > 0.03:
            quality = "æœ‰æ•ˆ"
            recommendation = "è°¨æ…ä½¿ç”¨ï¼Œå»ºè®®ä¸å…¶ä»–å› å­ç»„åˆ"
        else:
            quality = "æ— æ•ˆ"
            recommendation = "ä¸æ¨èä½¿ç”¨ï¼Œé¢„æµ‹èƒ½åŠ›ä¸è¶³"
        
        # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š
        report_info = None
        if report_path:
            import os
            from pathlib import Path
            from datetime import datetime
            
            report_path_obj = Path(report_path)
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            report_path_obj.parent.mkdir(parents=True, exist_ok=True)
            
            try:
                # ç”ŸæˆMarkdownæ ¼å¼çš„è¯„ä¼°æŠ¥å‘Š
                report_content = f"""# å› å­è¯„ä¼°æŠ¥å‘Š

## åŸºæœ¬ä¿¡æ¯
- **å› å­åç§°**: {factor_name}
- **æ•°æ®æº**: {data_id}
- **è¯„ä¼°æ–¹æ³•**: {method}
- **è¯„ä¼°æ—¶é—´**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}

## ICæŒ‡æ ‡
- **ICå‡å€¼**: {ic_mean:.4f}
- **ICæ ‡å‡†å·®**: {ic_std:.4f}
- **ICIR (ä¿¡æ¯æ¯”ç‡)**: {icir:.4f}
- **ICæ­£å€¼å æ¯”**: {ic_positive_ratio*100:.2f}%

## å› å­è´¨é‡è¯„çº§
- **è´¨é‡ç­‰çº§**: {quality}
- **æ¨èå»ºè®®**: {recommendation}

## æŒ‡æ ‡è§£è¯»
### ICå‡å€¼ ({ic_mean:.4f})
- å› å­ä¸æœªæ¥æ”¶ç›Šç‡çš„ç›¸å…³æ€§å‡å€¼
- ç»å¯¹å€¼è¶Šå¤§ï¼Œé¢„æµ‹èƒ½åŠ›è¶Šå¼º
- å½“å‰æ°´å¹³: {'ä¼˜ç§€' if abs_ic > 0.08 else 'è‰¯å¥½' if abs_ic > 0.05 else 'ä¸€èˆ¬' if abs_ic > 0.03 else 'è¾ƒå·®'}

### ICIR ({icir:.4f})
- ICå‡å€¼ä¸ICæ ‡å‡†å·®çš„æ¯”å€¼ï¼Œè¡¡é‡å› å­ç¨³å®šæ€§
- å¤§äº1.0ä¸ºç¨³å®šï¼Œ0.5-1.0ä¸ºè¾ƒç¨³å®šï¼Œå°äº0.5ä¸ºä¸ç¨³å®š
- å½“å‰ç¨³å®šæ€§: {'ç¨³å®š' if icir > 1.0 else 'è¾ƒç¨³å®š' if icir > 0.5 else 'ä¸ç¨³å®š'}

### ICæ­£å€¼å æ¯” ({ic_positive_ratio*100:.2f}%)
- ICå€¼ä¸ºæ­£çš„æ—¶é—´æ®µå æ¯”
- å¤§äº60%è¯´æ˜å› å­æ–¹å‘æ€§è¾ƒå¥½
- å½“å‰æ–¹å‘æ€§: {'ä¼˜ç§€' if ic_positive_ratio > 0.6 else 'è‰¯å¥½' if ic_positive_ratio > 0.5 else 'ä¸€èˆ¬'}

## é¢„æµ‹æ–¹å‘
- **å› å­ç±»å‹**: {'æ­£å‘å› å­ï¼ˆå› å­å€¼è¶Šå¤§ï¼Œæ”¶ç›Šè¶Šé«˜ï¼‰' if ic_mean > 0 else 'åå‘å› å­ï¼ˆå› å­å€¼è¶Šå¤§ï¼Œæ”¶ç›Šè¶Šä½ï¼‰'}
- **é¢„æµ‹èƒ½åŠ›**: æ¯å•ä½å› å­å˜åŒ–å¯¹åº”çº¦{abs(ic_mean)*100:.2f}%çš„æ”¶ç›Šç‡å˜åŒ–

## ä½¿ç”¨å»ºè®®
{recommendation}

### åç»­æ­¥éª¤
"""
                if abs_ic > 0.03:
                    report_content += """
1. âœ… å› å­æœ‰æ•ˆï¼Œå¯ä»¥ä½¿ç”¨
2. å»ºè®®ä¸å…¶ä»–å› å­ç»„åˆä½¿ç”¨
3. å¯ä»¥è¿›è¡Œæ¨¡å‹è®­ç»ƒï¼šä½¿ç”¨train_lstm_modelç­‰å·¥å…·
4. å®šæœŸç›‘æ§å› å­ICçš„å˜åŒ–
"""
                else:
                    report_content += """
1. âš ï¸ å› å­é¢„æµ‹èƒ½åŠ›ä¸è¶³
2. å»ºè®®å°è¯•å…¶ä»–å› å­ç±»å‹
3. æˆ–è°ƒæ•´å› å­å‚æ•°ï¼ˆå¦‚periodï¼‰
4. å¯ä½¿ç”¨calculate_factorç”Ÿæˆå…¶ä»–å› å­
"""
                
                # ä¿å­˜æŠ¥å‘Š
                with open(report_path, 'w', encoding='utf-8') as f:
                    f.write(report_content)
                
                file_size = os.path.getsize(report_path) / 1024  # KB
                report_info = {
                    "generated": True,
                    "path": str(report_path_obj.absolute()),
                    "size_kb": round(file_size, 2),
                    "format": "markdown"
                }
            except Exception as e:
                report_info = {
                    "generated": False,
                    "error": f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {str(e)}"
                }
        
        result = {
            "status": "success",
            "message": f"âœ… å› å­ '{factor_name}' ICè¯„ä¼°å®Œæˆ",
            "ic_metrics": {
                "ic_mean": ic_mean,
                "ic_std": ic_std,
                "icir": icir,
                "ic_positive_ratio": ic_positive_ratio
            },
            "factor_quality": {
                "rating": quality,
                "ic_mean": f"{ic_mean:.4f}",
                "icir": f"{icir:.4f}",
                "recommendation": recommendation
            },
            "interpretation": {
                "direction": "æ­£å‘å› å­ï¼ˆå› å­å€¼è¶Šå¤§ï¼Œæ”¶ç›Šè¶Šé«˜ï¼‰" if ic_mean > 0 else "åå‘å› å­ï¼ˆå› å­å€¼è¶Šå¤§ï¼Œæ”¶ç›Šè¶Šä½ï¼‰",
                "stability": "ç¨³å®š" if icir > 1.0 else "è¾ƒç¨³å®š" if icir > 0.5 else "ä¸ç¨³å®š",
                "predictive_power": f"æ¯å•ä½å› å­å˜åŒ–å¯¹åº”çº¦{abs(ic_mean)*100:.2f}%çš„æ”¶ç›Šç‡å˜åŒ–"
            },
            "next_steps": [
                {
                    "step": 1,
                    "action": "ä½¿ç”¨æœ‰æ•ˆå› å­" if abs_ic > 0.03 else "å°è¯•å…¶ä»–å› å­",
                    "reason": recommendation
                },
                {
                    "step": 2,
                    "action": "ç”Ÿæˆæ›´å¤šå› å­æˆ–è®­ç»ƒæ¨¡å‹",
                    "tools": ["generate_alpha158", "train_lstm_model"] if abs_ic > 0.05 else ["calculate_factor"],
                    "reason": "å› å­æœ‰æ•ˆï¼Œå¯ç»§ç»­å»ºæ¨¡" if abs_ic > 0.05 else "å½“å‰å› å­æ•ˆæœä¸ä½³ï¼Œå»ºè®®å°è¯•å…¶ä»–å› å­"
                }
            ],
            "tips": [
                f"ğŸ’¡ ICå‡å€¼: {ic_mean:.4f} ({'å¼º' if abs_ic > 0.08 else 'è¾ƒå¼º' if abs_ic > 0.05 else 'å¼±'})",
                f"ğŸ’¡ ICIR: {icir:.4f} ({'ç¨³å®š' if icir > 1.0 else 'è¾ƒç¨³å®š' if icir > 0.5 else 'ä¸ç¨³å®š'})",
                f"ğŸ’¡ {quality}å› å­ï¼Œ{recommendation}",
                f"ğŸ’¡ ä½¿ç”¨report_pathå‚æ•°ç”Ÿæˆè¯¦ç»†è¯„ä¼°æŠ¥å‘Š" if not report_path else f"ğŸ’¡ è¯„ä¼°æŠ¥å‘Šå·²ç”Ÿæˆ: {report_path}"
            ]
        }
        
        # æ·»åŠ æŠ¥å‘Šä¿¡æ¯
        if report_info:
            result["report_info"] = report_info
        
        return [types.TextContent(type="text", text=serialize_response(result))]
        
    except KeyError as e:
        return [types.TextContent(
            type="text",
            text=MCPError.format_error(
                error_code=MCPError.INVALID_PARAMETER,
                message=f"æ•°æ®ç¼ºå°‘closeåˆ—: {str(e)}",
                details={"data_id": data_id, "missing_column": "close"},
                suggestions=[
                    "ç¡®ä¿æ•°æ®åŒ…å«closeåˆ—",
                    "æ£€æŸ¥æ•°æ®åŠ è½½æ˜¯å¦æ­£ç¡®"
                ]
            )
        )]
    except Exception as e:
        return [types.TextContent(
            type="text",
            text=MCPError.format_error(
                error_code=MCPError.COMPUTATION_ERROR,
                message=f"ICè¯„ä¼°å¤±è´¥: {str(e)}",
                details={
                    "factor_name": factor_name,
                    "data_id": data_id,
                    "method": method,
                    "exception_type": type(e).__name__
                },
                suggestions=[
                    "æ£€æŸ¥å› å­å’Œæ•°æ®çš„æ—¶é—´èŒƒå›´æ˜¯å¦åŒ¹é…",
                    "ç¡®è®¤æ•°æ®ä¸­æœ‰è¶³å¤Ÿçš„æ ·æœ¬",
                    "å°è¯•ä½¿ç”¨ä¸åŒçš„methodå‚æ•°"
                ]
            )
        )]


async def handle_apply_processor_chain(args: Dict[str, Any]) -> List[types.TextContent]:
    """åº”ç”¨æ•°æ®å¤„ç†å™¨é“¾"""
    data_id = args["data_id"]
    result_id = args["result_id"]
    processors_config = args["processors"]
    export_path = args.get("export_path", None)
    export_format = args.get("export_format", "csv")  # csv æˆ– json
    
    # éªŒè¯æ•°æ®ID
    error = validate_data_id(data_id, factor_store)  # é€šå¸¸å¤„ç†å› å­æ•°æ®
    if error:
        # å¦‚æœä¸åœ¨factor_storeï¼Œæ£€æŸ¥data_store
        error = validate_data_id(data_id, data_store)
        if error:
            return [types.TextContent(type="text", text=error)]
        data = data_store[data_id].copy()
    else:
        data = factor_store[data_id].copy()
    
    try:
        # ğŸ†• æ™ºèƒ½æ£€æµ‹æ•°æ®ç±»å‹ï¼šå•å•†å“ vs å¤šå•†å“
        symbol_count = 1
        if isinstance(data.index, pd.MultiIndex):
            symbol_count = len(data.index.get_level_values(1).unique())
        elif 'symbol' in data.columns:
            symbol_count = data['symbol'].nunique()
        
        data_type = "å•å•†å“" if symbol_count == 1 else f"å¤šå•†å“({symbol_count}ä¸ª)"
        auto_adjusted = []  # è®°å½•è‡ªåŠ¨è°ƒæ•´çš„å¤„ç†å™¨
        
        # æ„å»ºå¤„ç†å™¨é“¾
        processors = []
        for proc_config in processors_config:
            proc_name = proc_config["name"]
            proc_params = proc_config.get("params", {})
            
            # ğŸ†• æ™ºèƒ½å¤„ç†CSZScoreNormï¼šå•å•†å“è‡ªåŠ¨åˆ‡æ¢ä¸ºZScoreNorm
            original_proc_name = proc_name
            if proc_name == "CSZScoreNorm" and symbol_count == 1:
                proc_name = "ZScoreNorm"
                auto_adjusted.append({
                    "original": "CSZScoreNorm",
                    "adjusted_to": "ZScoreNorm",
                    "reason": f"æ£€æµ‹åˆ°å•å•†å“æ•°æ®ï¼ŒCSZScoreNormä¼šå¯¼è‡´100% NaNï¼Œè‡ªåŠ¨åˆ‡æ¢ä¸ºZScoreNorm"
                })
            
            # æ ¹æ®åç§°åˆ›å»ºå¤„ç†å™¨
            if proc_name == "CSZScoreNorm":
                proc = CSZScoreNorm(**proc_params)
            elif proc_name == "ZScoreNorm":
                from quantanalyzer.data.processor import ZScoreNorm
                proc = ZScoreNorm(**proc_params)
            elif proc_name == "CSZFillna":
                proc = CSZFillna(**proc_params)
            elif proc_name == "ProcessInf":
                proc = ProcessInf()
            elif proc_name == "ZScoreNorm":
                proc = ZScoreNorm(**proc_params)
            elif proc_name == "RobustZScoreNorm":
                proc = RobustZScoreNorm(**proc_params)
            elif proc_name == "CSRankNorm":
                proc = CSRankNorm(**proc_params)
            elif proc_name == "MinMaxNorm":
                proc = MinMaxNorm(**proc_params)
            else:
                return [types.TextContent(
                    type="text",
                    text=MCPError.format_error(
                        error_code=MCPError.INVALID_PARAMETER,
                        message=f"ä¸æ”¯æŒçš„å¤„ç†å™¨: {proc_name}",
                        details={"processor": proc_name},
                        suggestions=[
                            "æ”¯æŒçš„å¤„ç†å™¨: CSZScoreNorm, CSZFillna, ProcessInf, ZScoreNorm, RobustZScoreNorm, CSRankNorm, MinMaxNorm"
                        ]
                    )
                )]
            
            processors.append(proc)
        
        # åº”ç”¨å¤„ç†å™¨é“¾
        chain = ProcessorChain(processors)
        processed_data = chain.fit_transform(data)
        
        # ä¿å­˜å¤„ç†åçš„æ•°æ®
        factor_store[result_id] = processed_data
        
        # å¯¼å‡ºå¤„ç†åçš„æ•°æ®
        export_info = None
        if export_path:
            import os
            from pathlib import Path
            from datetime import datetime
            
            export_path_obj = Path(export_path)
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            export_path_obj.parent.mkdir(parents=True, exist_ok=True)
            
            try:
                if export_format.lower() == "csv":
                    processed_data.to_csv(export_path, encoding='utf-8')
                elif export_format.lower() == "json":
                    processed_data.to_json(export_path, orient='records', indent=2)
                else:
                    export_format = "csv"  # é»˜è®¤ä½¿ç”¨CSV
                    processed_data.to_csv(export_path, encoding='utf-8')
                
                file_size = os.path.getsize(export_path) / (1024 * 1024)
                export_info = {
                    "exported": True,
                    "path": str(export_path_obj.absolute()),
                    "format": export_format,
                    "size_mb": round(file_size, 2)
                }
            except Exception as e:
                export_info = {
                    "exported": False,
                    "error": f"å¯¼å‡ºå¤±è´¥: {str(e)}"
                }
        
        # è®¡ç®—æ•°æ®è´¨é‡
        null_count = int(processed_data.isna().sum().sum())
        total_values = processed_data.shape[0] * processed_data.shape[1]
        null_rate = null_count / total_values if total_values > 0 else 0
        
        result = {
            "status": "success",
            "message": f"âœ… æ•°æ®å¤„ç†å®Œæˆå¹¶å­˜å‚¨ä¸º '{result_id}'" + (f" (æ™ºèƒ½ä¼˜åŒ–: {len(auto_adjusted)}ä¸ªå¤„ç†å™¨)" if auto_adjusted else ""),
            "data_info": {
                "data_type": data_type,
                "symbol_count": symbol_count
            },
            "processing_info": {
                "input_id": data_id,
                "output_id": result_id,
                "processors_requested": [p["name"] for p in processors_config],
                "processors_applied": [p.name if hasattr(p, 'name') else type(p).__name__ for p in processors],
                "shape": list(processed_data.shape),
                "auto_adjustments": auto_adjusted if auto_adjusted else None
            },
            "data_quality": {
                "null_count": null_count,
                "null_rate": f"{null_rate * 100:.2f}%",
                "quality_score": "ä¼˜ç§€" if null_rate < 0.01 else "è‰¯å¥½" if null_rate < 0.05 else "ä¸€èˆ¬"
            },
            "next_steps": [
                {
                    "step": 1,
                    "action": "è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹",
                    "tools": ["train_lstm_model", "train_gru_model", "train_transformer_model"],
                    "params_example": {
                        "data_id": result_id,
                        "model_id": f"model_{result_id}"
                    }
                }
            ]
        }
        
        return [types.TextContent(type="text", text=serialize_response(result))]
        
    except Exception as e:
        return [types.TextContent(
            type="text",
            text=MCPError.format_error(
                error_code=MCPError.COMPUTATION_ERROR,
                message=f"æ•°æ®å¤„ç†å¤±è´¥: {str(e)}",
                details={
                    "data_id": data_id,
                    "processors": processors_config,
                    "exception_type": type(e).__name__
                },
                suggestions=[
                    "æ£€æŸ¥å¤„ç†å™¨å‚æ•°æ˜¯å¦æ­£ç¡®",
                    "ç¡®è®¤æ•°æ®æ ¼å¼ç¬¦åˆè¦æ±‚",
                    "å°è¯•å‡å°‘å¤„ç†å™¨æ•°é‡"
                ]
            )
        )]


async def handle_merge_factor_data(args: Dict[str, Any]) -> List[types.TextContent]:
    """
    åˆå¹¶å› å­æ•°æ®å’Œä»·æ ¼æ•°æ®
    å°†Alpha158ç­‰å› å­æ•°æ®ä¸åŸå§‹ä»·æ ¼æ•°æ®åˆå¹¶ï¼Œä»¥ä¾¿ç”¨äºæ¨¡å‹è®­ç»ƒ
    """
    factor_data_id = args["factor_data_id"]
    price_data_id = args["price_data_id"]
    result_id = args["result_id"]
    export_path = args.get("export_path", None)
    export_format = args.get("export_format", "csv")
    
    # éªŒè¯å› å­æ•°æ®ID
    error = validate_data_id(factor_data_id, factor_store)
    if error:
        return [types.TextContent(type="text", text=error)]
    
    # éªŒè¯ä»·æ ¼æ•°æ®ID
    error = validate_data_id(price_data_id, data_store)
    if error:
        return [types.TextContent(type="text", text=error)]
    
    try:
        factor_data = factor_store[factor_data_id]
        price_data = data_store[price_data_id]
        
        # ç¡®ä¿ç´¢å¼•å¯¹é½
        if not factor_data.index.equals(price_data.index):
            # ä½¿ç”¨å†…è¿æ¥ç¡®ä¿ç´¢å¼•åŒ¹é…
            common_index = factor_data.index.intersection(price_data.index)
            if len(common_index) == 0:
                return [types.TextContent(
                    type="text",
                    text=MCPError.format_error(
                        error_code=MCPError.INVALID_PARAMETER,
                        message="å› å­æ•°æ®å’Œä»·æ ¼æ•°æ®çš„ç´¢å¼•æ²¡æœ‰é‡å ",
                        details={
                            "factor_data_id": factor_data_id,
                            "price_data_id": price_data_id,
                            "factor_index_range": f"{factor_data.index[0]} ~ {factor_data.index[-1]}",
                            "price_index_range": f"{price_data.index[0]} ~ {price_data.index[-1]}"
                        },
                        suggestions=[
                            "ç¡®è®¤å› å­æ•°æ®å’Œä»·æ ¼æ•°æ®æ¥æºäºåŒä¸€ä¸ªåŸå§‹æ•°æ®",
                            "æ£€æŸ¥æ•°æ®çš„æ—¶é—´èŒƒå›´æ˜¯å¦åŒ¹é…"
                        ]
                    )
                )]
            
            factor_data = factor_data.loc[common_index]
            price_data = price_data.loc[common_index]
        
        # åˆå¹¶æ•°æ®ï¼šå› å­åˆ— + closeåˆ—ï¼ˆç”¨äºç”Ÿæˆæ ‡ç­¾ï¼‰
        # åªå–ä»·æ ¼æ•°æ®ä¸­çš„closeåˆ—
        merged_data = pd.concat([
            factor_data,
            price_data[['close']]
        ], axis=1)
        
        # ä¿å­˜åˆå¹¶åçš„æ•°æ®
        factor_store[result_id] = merged_data
        
        # å¯¼å‡ºåˆå¹¶åçš„æ•°æ®
        export_info = None
        if export_path:
            import os
            from pathlib import Path
            
            export_path_obj = Path(export_path)
            export_path_obj.parent.mkdir(parents=True, exist_ok=True)
            
            try:
                if export_format.lower() == "csv":
                    merged_data.to_csv(export_path, encoding='utf-8')
                elif export_format.lower() == "json":
                    merged_data.to_json(export_path, orient='records', indent=2)
                else:
                    export_format = "csv"
                    merged_data.to_csv(export_path, encoding='utf-8')
                
                file_size = os.path.getsize(export_path) / (1024 * 1024)
                export_info = {
                    "exported": True,
                    "path": str(export_path_obj.absolute()),
                    "format": export_format,
                    "size_mb": round(file_size, 2)
                }
            except Exception as e:
                export_info = {
                    "exported": False,
                    "error": f"å¯¼å‡ºå¤±è´¥: {str(e)}"
                }
        
        # æ„å»ºå“åº”
        result = {
            "status": "success",
            "message": f"âœ… æ•°æ®åˆå¹¶å®Œæˆå¹¶å­˜å‚¨ä¸º '{result_id}'",
            "merge_info": {
                "result_id": result_id,
                "factor_data_id": factor_data_id,
                "price_data_id": price_data_id,
                "factor_count": len(factor_data.columns),
                "total_columns": len(merged_data.columns),
                "row_count": len(merged_data),
                "columns": list(merged_data.columns)
            },
            "data_quality": {
                "null_count": int(merged_data.isna().sum().sum()),
                "null_rate": f"{(merged_data.isna().sum().sum() / (merged_data.shape[0] * merged_data.shape[1]) * 100):.2f}%"
            },
            "next_steps": [
                {
                    "step": 1,
                    "action": "è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹",
                    "tool": "train_ml_model",
                    "params_example": {
                        "data_id": result_id,
                        "model_id": f"model_{result_id}",
                        "train_start": "2023-01-01",
                        "train_end": "2023-06-30",
                        "test_start": "2023-07-01",
                        "test_end": "2023-12-31"
                    },
                    "reason": "åˆå¹¶åçš„æ•°æ®åŒ…å«å› å­å’Œcloseåˆ—ï¼Œå¯ä»¥ç›´æ¥ç”¨äºè®­ç»ƒ"
                }
            ],
            "tips": [
                f"ğŸ’¡ åˆå¹¶äº†{len(factor_data.columns)}ä¸ªå› å­ + 1ä¸ªcloseåˆ—",
                f"ğŸ’¡ æ•°æ®è¡Œæ•°: {len(merged_data)}",
                f"ğŸ’¡ æ•°æ®å·²å‡†å¤‡å¥½ç”¨äºtrain_ml_modelè®­ç»ƒ",
                f"ğŸ’¡ ä½¿ç”¨export_pathå‚æ•°å¯å¯¼å‡ºåˆå¹¶æ•°æ®" if not export_path else f"ğŸ’¡ æ•°æ®å·²å¯¼å‡ºåˆ°: {export_path}"
            ]
        }
        
        if export_info:
            result["export_info"] = export_info
        
        return [types.TextContent(type="text", text=serialize_response(result))]
        
    except Exception as e:
        return [types.TextContent(
            type="text",
            text=MCPError.format_error(
                error_code=MCPError.COMPUTATION_ERROR,
                message=f"æ•°æ®åˆå¹¶å¤±è´¥: {str(e)}",
                details={
                    "factor_data_id": factor_data_id,
                    "price_data_id": price_data_id,
                    "exception_type": type(e).__name__
                },
                suggestions=[
                    "ç¡®è®¤å› å­æ•°æ®å’Œä»·æ ¼æ•°æ®çš„ç´¢å¼•æ ¼å¼ä¸€è‡´",
                    "æ£€æŸ¥ä¸¤ä¸ªæ•°æ®é›†çš„æ—¶é—´èŒƒå›´æ˜¯å¦é‡å ",
                    "å°è¯•ä½¿ç”¨list_factorsæŸ¥çœ‹æ•°æ®è¯¦æƒ…"
                ]
            )
        )]


async def handle_list_factors(args: Dict[str, Any]) -> List[types.TextContent]:
    """åˆ—å‡ºæ‰€æœ‰å› å­"""
    factors_info = {}
    for factor_id, factor in factor_store.items():
        factors_info[factor_id] = {
            "type": str(type(factor).__name__),
            "shape": list(factor.shape) if hasattr(factor, 'shape') else None
        }
    
    result = {
        "data_count": len(data_store),
        "data_ids": list(data_store.keys()),
        "factor_count": len(factor_store),
        "factors": factors_info
    }
    
    return [types.TextContent(
        type="text",
        text=json.dumps(result, ensure_ascii=False, indent=2)
    )]


async def handle_train_ml_model(args: Dict[str, Any]) -> List[types.TextContent]:
    """è®­ç»ƒæœºå™¨å­¦ä¹ æ¨¡å‹ï¼ˆLightGBM/XGBoost/sklearnï¼‰"""
    data_id = args["data_id"]
    model_id = args["model_id"]
    model_type = args.get("model_type", "lightgbm")
    train_start = args["train_start"]
    train_end = args["train_end"]
    test_start = args["test_start"]
    test_end = args["test_end"]
    params = args.get("params", None)
    
    # éªŒè¯æ•°æ®ID
    error = validate_data_id(data_id, factor_store)
    if error:
        error = validate_data_id(data_id, data_store)
        if error:
            return [types.TextContent(type="text", text=error)]
        data = data_store[data_id]
    else:
        data = factor_store[data_id]
    
    try:
        from quantanalyzer.model.trainer import ModelTrainer
        
        # åˆ›å»ºæ ‡ç­¾ï¼ˆä¸‹ä¸€æ—¥æ”¶ç›Šç‡ï¼‰
        if 'close' in data.columns:
            labels = data['close'].groupby(level=1).pct_change().shift(-1)
        else:
            # å¦‚æœæ˜¯å› å­æ•°æ®ï¼Œå°è¯•ä»åŸå§‹æ•°æ®è·å–æ ‡ç­¾
            return [types.TextContent(
                type="text",
                text=MCPError.format_error(
                    error_code=MCPError.INVALID_PARAMETER,
                    message="å› å­æ•°æ®éœ€è¦æä¾›å¯¹åº”çš„ä»·æ ¼æ•°æ®æ¥ç”Ÿæˆæ ‡ç­¾",
                    suggestions=[
                        "ä½¿ç”¨åŸå§‹æ•°æ®IDï¼ˆåŒ…å«closeåˆ—ï¼‰",
                        "æˆ–å…ˆä½¿ç”¨preprocess_dataåŠ è½½ä»·æ ¼æ•°æ®"
                    ]
                )
            )]
        
        # åˆ›å»ºè®­ç»ƒå™¨
        trainer = ModelTrainer(model_type=model_type)
        
        # å‡†å¤‡æ•°æ®é›†
        X_train, y_train, X_test, y_test = trainer.prepare_dataset(
            data, labels,
            train_start, train_end,
            test_start, test_end
        )
        
        # è®­ç»ƒæ¨¡å‹
        trainer.train(X_train, y_train, X_test, y_test, params)
        
        # é¢„æµ‹
        train_pred = trainer.predict(X_train)
        test_pred = trainer.predict(X_test)
        
        # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
        from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
        import numpy as np
        
        train_mse = mean_squared_error(y_train, train_pred)
        train_mae = mean_absolute_error(y_train, train_pred)
        train_r2 = r2_score(y_train, train_pred)
        
        test_mse = mean_squared_error(y_test, test_pred)
        test_mae = mean_absolute_error(y_test, test_pred)
        test_r2 = r2_score(y_test, test_pred)
        
        # è®¡ç®—IC (Information Coefficient)
        train_ic = np.corrcoef(y_train, train_pred)[0, 1]
        test_ic = np.corrcoef(y_test, test_pred)[0, 1]
        
        # ä¿å­˜æ¨¡å‹
        model_store[model_id] = {
            'trainer': trainer,
            'model_type': model_type,
            'train_samples': len(X_train),
            'test_samples': len(X_test),
            'feature_count': X_train.shape[1]
        }
        
        # æ„å»ºå“åº”
        result = {
            "status": "success",
            "message": f"âœ… {model_type.upper()}æ¨¡å‹è®­ç»ƒå®Œæˆå¹¶å­˜å‚¨ä¸º '{model_id}'",
            "model_info": {
                "model_id": model_id,
                "model_type": model_type,
                "train_period": f"{train_start} ~ {train_end}",
                "test_period": f"{test_start} ~ {test_end}",
                "train_samples": len(X_train),
                "test_samples": len(X_test),
                "feature_count": X_train.shape[1]
            },
            "performance": {
                "train": {
                    "mse": float(train_mse),
                    "mae": float(train_mae),
                    "r2": float(train_r2),
                    "ic": float(train_ic)
                },
                "test": {
                    "mse": float(test_mse),
                    "mae": float(test_mae),
                    "r2": float(test_r2),
                    "ic": float(test_ic)
                }
            },
            "feature_importance": {
                "top_10": trainer.feature_importance.head(10).to_dict() if trainer.feature_importance is not None else None
            },
            "next_steps": [
                {
                    "step": 1,
                    "action": "ä½¿ç”¨æ¨¡å‹é¢„æµ‹",
                    "tool": "predict_ml_model",
                    "params_example": {
                        "model_id": model_id,
                        "data_id": data_id
                    }
                }
            ],
            "tips": [
                f"ğŸ’¡ æ¨¡å‹ç±»å‹: {model_type}",
                f"ğŸ’¡ è®­ç»ƒé›†RÂ²: {train_r2:.4f}, æµ‹è¯•é›†RÂ²: {test_r2:.4f}",
                f"ğŸ’¡ æµ‹è¯•é›†IC: {test_ic:.4f} ({'æœ‰æ•ˆ' if abs(test_ic) > 0.03 else 'è¾ƒå¼±'})",
                "ğŸ’¡ ä½¿ç”¨predict_ml_modelå·¥å…·è¿›è¡Œé¢„æµ‹"
            ]
        }
        
        return [types.TextContent(type="text", text=serialize_response(result))]
        
    except ImportError as e:
        return [types.TextContent(
            type="text",
            text=MCPError.format_error(
                error_code=MCPError.COMPUTATION_ERROR,
                message=f"ç¼ºå°‘å¿…éœ€çš„åº“: {str(e)}",
                suggestions=[
                    f"å®‰è£…{model_type}: pip install {model_type}",
                    "æˆ–ä½¿ç”¨å…¶ä»–æ¨¡å‹ç±»å‹"
                ]
            )
        )]
    except Exception as e:
        return [types.TextContent(
            type="text",
            text=MCPError.format_error(
                error_code=MCPError.COMPUTATION_ERROR,
                message=f"æ¨¡å‹è®­ç»ƒå¤±è´¥: {str(e)}",
                details={
                    "model_type": model_type,
                    "exception_type": type(e).__name__
                },
                suggestions=[
                    "æ£€æŸ¥æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®",
                    "ç¡®è®¤æ—¶é—´èŒƒå›´å‚æ•°æœ‰æ•ˆ",
                    "å°è¯•è°ƒæ•´æ¨¡å‹å‚æ•°"
                ]
            )
        )]


async def handle_predict_ml_model(args: Dict[str, Any]) -> List[types.TextContent]:
    """ä½¿ç”¨è®­ç»ƒå¥½çš„æœºå™¨å­¦ä¹ æ¨¡å‹è¿›è¡Œé¢„æµ‹"""
    model_id = args["model_id"]
    data_id = args["data_id"]
    export_path = args.get("export_path", None)
    
    # éªŒè¯æ¨¡å‹ID
    if model_id not in model_store:
        return [types.TextContent(
            type="text",
            text=MCPError.format_error(
                error_code=MCPError.INVALID_PARAMETER,
                message=f"æ¨¡å‹IDä¸å­˜åœ¨: {model_id}",
                suggestions=[
                    "ä½¿ç”¨train_ml_modelè®­ç»ƒæ¨¡å‹",
                    "æ£€æŸ¥model_idæ˜¯å¦æ­£ç¡®"
                ]
            )
        )]
    
    # éªŒè¯æ•°æ®ID
    error = validate_data_id(data_id, factor_store)
    if error:
        error = validate_data_id(data_id, data_store)
        if error:
            return [types.TextContent(type="text", text=error)]
        data = data_store[data_id]
    else:
        data = factor_store[data_id]
    
    try:
        model_info = model_store[model_id]
        trainer = model_info['trainer']
        
        # é¢„æµ‹
        predictions = trainer.predict(data)
        
        # å¯¼å‡ºé¢„æµ‹ç»“æœ
        export_info = None
        if export_path:
            import os
            from pathlib import Path
            
            export_path_obj = Path(export_path)
            export_path_obj.parent.mkdir(parents=True, exist_ok=True)
            
            try:
                predictions.to_csv(export_path, encoding='utf-8')
                file_size = os.path.getsize(export_path) / (1024 * 1024)
                export_info = {
                    "exported": True,
                    "path": str(export_path_obj.absolute()),
                    "size_mb": round(file_size, 2),
                    "prediction_count": len(predictions)
                }
            except Exception as e:
                export_info = {
                    "exported": False,
                    "error": f"å¯¼å‡ºå¤±è´¥: {str(e)}"
                }
        
        # æ„å»ºå“åº”
        result = {
            "status": "success",
            "message": f"âœ… ä½¿ç”¨æ¨¡å‹ '{model_id}' é¢„æµ‹å®Œæˆ",
            "prediction_info": {
                "model_id": model_id,
                "model_type": model_info['model_type'],
                "data_id": data_id,
                "prediction_count": len(predictions),
                "statistics": {
                    "mean": float(predictions.mean()),
                    "std": float(predictions.std()),
                    "min": float(predictions.min()),
                    "max": float(predictions.max())
                }
            },
            "preview": {
                "head_5": predictions.head(5).to_dict(),
                "tail_5": predictions.tail(5).to_dict()
            },
            "tips": [
                f"ğŸ’¡ é¢„æµ‹æ•°é‡: {len(predictions)}æ¡",
                f"ğŸ’¡ é¢„æµ‹å‡å€¼: {predictions.mean():.6f}",
                "ğŸ’¡ é¢„æµ‹ç»“æœå·²ä¿å­˜åœ¨å†…å­˜ä¸­" if not export_path else f"ğŸ’¡ å·²å¯¼å‡ºåˆ°: {export_path}"
            ]
        }
        
        if export_info:
            result["export_info"] = export_info
        
        return [types.TextContent(type="text", text=serialize_response(result))]
        
    except Exception as e:
        return [types.TextContent(
            type="text",
            text=MCPError.format_error(
                error_code=MCPError.COMPUTATION_ERROR,
                message=f"é¢„æµ‹å¤±è´¥: {str(e)}",
                details={
                    "model_id": model_id,
                    "data_id": data_id,
                    "exception_type": type(e).__name__
                },
                suggestions=[
                    "æ£€æŸ¥æ•°æ®æ ¼å¼æ˜¯å¦ä¸è®­ç»ƒæ—¶ä¸€è‡´",
                    "ç¡®è®¤æ¨¡å‹å·²æ­£ç¡®è®­ç»ƒ"
                ]
            )
        )]