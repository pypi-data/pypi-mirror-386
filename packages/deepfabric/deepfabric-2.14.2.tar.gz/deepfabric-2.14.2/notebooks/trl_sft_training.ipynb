{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TRL SFT Tool Calling Training - Complete Tutorial\n",
    "\n",
    "This notebook demonstrates the end-to-end workflow for:\n",
    "1. Generating a tool-calling dataset with DeepFabric\n",
    "2. Formatting it for HuggingFace TRL SFTTrainer\n",
    "3. Loading and training a model with tool calling capabilities\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Install required packages:\n",
    "```bash\n",
    "pip install deepfabric transformers trl datasets peft accelerate\n",
    "```\n",
    "\n",
    "## References\n",
    "- [HuggingFace TRL SFTTrainer Tool Calling](https://huggingface.co/docs/trl/en/sft_trainer#tool-calling-with-sft)\n",
    "- [Fine-tuning for Tool Calling](https://www.stephendiehl.com/posts/fine_tuning_tools/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Generate Tool-Calling Dataset with DeepFabric\n",
    "\n",
    "First, we'll use DeepFabric to generate a dataset with tool-calling examples that include:\n",
    "- Realistic tool usage scenarios\n",
    "- Step-by-step reasoning\n",
    "- Proper tool parameter construction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import os\n",
    "\n",
    "from deepfabric.config import DeepFabricConfig\n",
    "from deepfabric.dataset import Dataset\n",
    "from deepfabric.generator import DataSetGenerator\n",
    "from deepfabric.tree import Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Dataset Generation\n",
    "\n",
    "Define the configuration for generating our tool-calling dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"dataset_system_prompt\": \"\"\"You are a helpful AI assistant with access to various tools.\n",
    "Use tools when needed to answer questions accurately.\"\"\",\n",
    "\n",
    "    \"topic_tree\": {\n",
    "        \"topic_prompt\": \"Real-world tasks requiring AI assistant tool usage\",\n",
    "        \"provider\": \"openai\",\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"temperature\": 0.7,\n",
    "        \"depth\": 2,\n",
    "        \"degree\": 3,\n",
    "    },\n",
    "\n",
    "    \"data_engine\": {\n",
    "        \"generation_system_prompt\": \"\"\"Generate realistic AI assistant scenarios with tool usage.\n",
    "Show clear reasoning about tool selection and parameter construction.\"\"\",\n",
    "        \"provider\": \"openai\",\n",
    "        \"model\": \"gpt-4o-mini\",\n",
    "        \"temperature\": 0.8,\n",
    "        \"conversation_type\": \"agent_cot_tools\",\n",
    "        \"available_tools\": [\"web_search\", \"calculator\", \"get_weather\"],\n",
    "        \"max_tools_per_query\": 2,\n",
    "    },\n",
    "\n",
    "    \"dataset\": {\n",
    "        \"creation\": {\n",
    "            \"num_steps\": 5,\n",
    "            \"batch_size\": 2,\n",
    "            \"sys_msg\": True\n",
    "        },\n",
    "        \"save_as\": \"trl_raw.jsonl\",\n",
    "        \"formatters\": [\n",
    "            {\n",
    "                \"name\": \"trl_sft\",\n",
    "                \"template\": \"builtin://trl_sft_tools\",\n",
    "                \"output\": \"trl_formatted.jsonl\",\n",
    "                \"config\": {\n",
    "                    \"include_system_prompt\": True,\n",
    "                    \"validate_tool_schemas\": True,\n",
    "                    \"remove_available_tools_field\": False,\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"📊 Configuration:\")\n",
    "print(f\"  - Topic depth: {config['topic_tree']['depth']}\")\n",
    "print(f\"  - Topic degree: {config['topic_tree']['degree']}\")\n",
    "print(f\"  - Samples: {config['dataset']['creation']['num_steps'] * config['dataset']['creation']['batch_size']}\")\n",
    "print(f\"  - Conversation type: {config['data_engine']['conversation_type']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Topic Tree\n",
    "\n",
    "Create diverse topic paths for our dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating topic tree...\")\n",
    "\n",
    "df_config = DeepFabricConfig(**config)\n",
    "tree_params = df_config.get_topic_tree_params()\n",
    "tree = Tree(**tree_params)\n",
    "\n",
    "async def _build_tree():\n",
    "    async for _ in tree.build_async():\n",
    "        pass\n",
    "\n",
    "await _build_tree()\n",
    "print(f\"✓ Generated {len(tree.tree_paths)} topic paths\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Agent Tool-Calling Samples\n",
    "\n",
    "Create the actual training examples with tool usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating agent tool-calling samples...\")\n",
    "\n",
    "engine_params = df_config.get_engine_params()\n",
    "generator = DataSetGenerator(**engine_params)\n",
    "\n",
    "dataset = generator.create_data(\n",
    "    num_steps=df_config.dataset.creation.num_steps,\n",
    "    batch_size=df_config.dataset.creation.batch_size,\n",
    "    sys_msg=df_config.dataset.creation.sys_msg,\n",
    "    topic_model=tree,\n",
    ")\n",
    "\n",
    "print(f\"✓ Generated {len(dataset)} samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save Raw Dataset\n",
    "\n",
    "Save the unformatted dataset for reference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"💾 Saving dataset...\")\n",
    "\n",
    "dataset.save(\"trl_raw.jsonl\")\n",
    "print(\"✓ Saved raw dataset to trl_raw.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply TRL SFT Formatter\n",
    "\n",
    "Convert the dataset to TRL-compatible format with OpenAI function calling schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🔧 Applying TRL SFT formatter...\")\n",
    "\n",
    "formatter_configs = df_config.get_formatter_configs()\n",
    "formatted_datasets = dataset.apply_formatters(formatter_configs)\n",
    "formatted_dataset = formatted_datasets[\"trl_sft\"]\n",
    "\n",
    "print(f\"✓ Formatted {len(formatted_dataset)} samples for TRL\")\n",
    "print(\"✓ Saved formatted dataset to trl_formatted.jsonl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect Example\n",
    "\n",
    "Let's look at what the formatted data looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(formatted_dataset) > 0:\n",
    "    print(\"📋 Example formatted sample:\")\n",
    "    example = formatted_dataset[0]\n",
    "    \n",
    "    print(f\"  - Messages: {len(example.get('messages', []))} message(s)\")\n",
    "    print(f\"  - Tools: {len(example.get('tools', []))} tool(s)\")\n",
    "    \n",
    "    if \"tools\" in example and example[\"tools\"]:\n",
    "        print(f\"\\n  First tool schema:\")\n",
    "        tool = example[\"tools\"][0]\n",
    "        print(f\"    - Name: {tool['function']['name']}\")\n",
    "        print(f\"    - Description: {tool['function']['description']}\")\n",
    "        print(f\"    - Parameters: {list(tool['function']['parameters']['properties'].keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Load Dataset for Training\n",
    "\n",
    "Convert the formatted dataset to HuggingFace Dataset format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset as HFDataset\n",
    "\n",
    "print(\"Loading dataset for training...\")\n",
    "\n",
    "# Load samples from file\n",
    "samples = []\n",
    "with open(\"trl_formatted.jsonl\") as f:\n",
    "    for line in f:\n",
    "        samples.append(json.loads(line))\n",
    "\n",
    "# Convert to HuggingFace Dataset\n",
    "hf_dataset = HFDataset.from_list(samples)\n",
    "\n",
    "print(f\"✓ Loaded {len(hf_dataset)} samples\")\n",
    "print(f\"  - Features: {list(hf_dataset.features.keys())}\")\n",
    "\n",
    "# Verify format\n",
    "first_sample = hf_dataset[0]\n",
    "print(\"\\n✓ Sample validation:\")\n",
    "print(f\"  - Has 'messages': {'messages' in first_sample}\")\n",
    "print(f\"  - Has 'tools': {'tools' in first_sample}\")\n",
    "if \"tools\" in first_sample:\n",
    "    print(f\"  - Number of tools: {len(first_sample['tools'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Setup TRL SFTTrainer\n",
    "\n",
    "Configure the model, tokenizer, and training arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from trl import SFTConfig, SFTTrainer\n",
    "\n",
    "print(\"Setting up training components...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Model\n",
    "\n",
    "Choose a model appropriate for your use case:\n",
    "- **For testing**: `Qwen/Qwen2.5-0.5B-Instruct` (small, fast)\n",
    "- **For production**: `Qwen/Qwen2.5-7B-Instruct` or `meta-llama/Llama-3.1-8B-Instruct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"  # Small model for testing\n",
    "\n",
    "print(f\"Model: {model_name}\")\n",
    "print(\"\\nLoading tokenizer and model...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n",
    "\n",
    "print(\"✓ Model loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure LoRA\n",
    "\n",
    "LoRA (Low-Rank Adaptation) allows efficient fine-tuning by only updating a small number of parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "\n",
    "print(\"LoRA Configuration:\")\n",
    "print(f\"  - r: {peft_config.r}\")\n",
    "print(f\"  - alpha: {peft_config.lora_alpha}\")\n",
    "print(f\"  - target modules: {peft_config.target_modules}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Configuration\n",
    "\n",
    "Set up training hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = SFTConfig(\n",
    "    output_dir=\"./trl_tool_calling_model\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    learning_rate=2e-4,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    warmup_steps=50,\n",
    "    max_seq_length=2048,\n",
    "    # Tool calling specific\n",
    "    dataset_text_field=None,  # We'll handle formatting\n",
    "    packing=False,\n",
    ")\n",
    "\n",
    "print(\"Training Configuration:\")\n",
    "print(f\"  - Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  - Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  - Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"  - Max sequence length: {training_args.max_seq_length}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Trainer\n",
    "\n",
    "Create the SFTTrainer with all components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Initializing SFTTrainer...\")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=hf_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "print(\"✓ Trainer initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Train the Model\n",
    "\n",
    "⚠️ **Note**: This is a demonstration with a small dataset. For production:\n",
    "- Use a larger dataset (1000+ samples)\n",
    "- Use a larger model (7B+ parameters)\n",
    "- Train for more epochs with validation\n",
    "- Monitor metrics and adjust hyperparameters\n",
    "\n",
    "Set `RUN_TRAINING=true` in the environment to actually run training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_TRAINING = os.environ.get(\"RUN_TRAINING\", \"false\").lower() == \"true\"\n",
    "\n",
    "if RUN_TRAINING:\n",
    "    print(\"🏋️  Starting training...\")\n",
    "\n",
    "    try:\n",
    "        trainer.train()\n",
    "        print(\"\\n✓ Training completed!\")\n",
    "\n",
    "        # Save the model\n",
    "        trainer.save_model(\"./trl_tool_calling_model/final\")\n",
    "        print(\"✓ Model saved to ./trl_tool_calling_model/final\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Training failed: {e}\")\n",
    "        print(\"This is expected in a demo environment without GPU/proper setup\")\n",
    "else:\n",
    "    print(\"Skipping training (set RUN_TRAINING=true to train)\")\n",
    "    print(\"Training requires: GPU, sufficient memory, and time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Using the Trained Model\n",
    "\n",
    "After training, you can use the model for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is example code - uncomment to use after training\n",
    "\"\"\"\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load the trained model\n",
    "base_model = \"Qwen/Qwen2.5-0.5B-Instruct\"\n",
    "adapter_path = \"./trl_tool_calling_model/final\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
    "base = AutoModelForCausalLM.from_pretrained(base_model)\n",
    "model = PeftModel.from_pretrained(base, adapter_path)\n",
    "\n",
    "# Example inference with tools\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather\",\n",
    "            \"description\": \"Get weather for a location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\": \"string\", \"description\": \"City name\"}\n",
    "                },\n",
    "                \"required\": [\"location\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant with tools.\"},\n",
    "    {\"role\": \"user\", \"content\": \"What's the weather in Paris?\"}\n",
    "]\n",
    "\n",
    "# Format with chat template and generate\n",
    "input_text = tokenizer.apply_chat_template(messages, tools=tools, tokenize=False)\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = model.generate(**inputs, max_new_tokens=256)\n",
    "response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(response)\n",
    "\"\"\"\n",
    "print(\"📖 See the cell above for inference example code\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "You've completed the full workflow:\n",
    "\n",
    "1. ✅ Generated a tool-calling dataset with DeepFabric\n",
    "2. ✅ Formatted it for TRL SFTTrainer\n",
    "3. ✅ Configured training with LoRA\n",
    "4. ⏸️ (Optional) Trained the model\n",
    "5. 📖 Learned how to use the trained model\n",
    "\n",
    "### Next Steps\n",
    "\n",
    "1. Review the generated datasets (`trl_raw.jsonl`, `trl_formatted.jsonl`)\n",
    "2. Adjust configuration for your use case\n",
    "3. Scale up dataset size and model size\n",
    "4. Add evaluation and validation splits\n",
    "5. Monitor training metrics and adjust hyperparameters\n",
    "\n",
    "### Files Created\n",
    "\n",
    "- `trl_raw.jsonl` - Raw DeepFabric dataset\n",
    "- `trl_formatted.jsonl` - TRL-formatted dataset with OpenAI schema\n",
    "- `./trl_tool_calling_model/` - Training checkpoints (if training was run)\n",
    "- `./trl_tool_calling_model/final/` - Final trained model (if training was run)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
