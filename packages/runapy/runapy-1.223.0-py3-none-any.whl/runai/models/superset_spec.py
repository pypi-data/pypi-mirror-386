# coding: utf-8

"""
NVIDIA Run:ai

# Introduction  The NVIDIA Run:ai Control-Plane API reference is a guide that provides an easy-to-use programming interface for adding various tasks to your application, including workload submission, resource management, and administrative operations.  NVIDIA Run:ai APIs are accessed using *bearer tokens*. To obtain a token, you need to create an **Application** through the NVIDIA Run:ai user interface. To create an application, in your UI, go to `Settings & Tools`, `Application` and create a new Application.  After you have created a new application, you will need to assign it access rules. To assign access rules to the application, see [Create access rules](https://run-ai-docs.nvidia.com/saas/infrastructure-setup/authentication/accessrules#create-or-delete-rules). Make sure you assign the correct rules to your application. Use the [Roles](https://run-ai-docs.nvidia.com/saas/infrastructure-setup/authentication/roles) to assign the correct access rules.  To get your access token, follow the instructions in [Request a token](https://run-ai-docs.nvidia.com/saas/reference/api/rest-auth/#request-an-api-token).

The version of the OpenAPI document: latest
Generated by OpenAPI Generator (https://openapi-generator.tech)

Do not edit the class manually.
"""  # noqa: E501


from __future__ import annotations
import pprint
import re  # noqa: F401
import json

from pydantic import (
    BaseModel,
    ConfigDict,
    Field,
    StrictBool,
    StrictInt,
    field_validator,
)
from typing import Any, ClassVar, Dict, List, Optional
from typing_extensions import Annotated
from runai.models.annotation import Annotation
from runai.models.auto_scaling import AutoScaling
from runai.models.clean_pod_policy import CleanPodPolicy
from runai.models.distributed_framework import DistributedFramework
from runai.models.distributed_inference_leader_worker_spec1 import (
    DistributedInferenceLeaderWorkerSpec1,
)
from runai.models.distributed_inference_startup_policy import (
    DistributedInferenceStartupPolicy,
)
from runai.models.environment_variable import EnvironmentVariable
from runai.models.exposed_url import ExposedUrl
from runai.models.image_pull_policy import ImagePullPolicy
from runai.models.image_pull_secret import ImagePullSecret
from runai.models.label import Label
from runai.models.mpi_launcher_creation_policy import MpiLauncherCreationPolicy
from runai.models.node_affinity_required import NodeAffinityRequired
from runai.models.pod_affinity import PodAffinity
from runai.models.port import Port
from runai.models.probes import Probes
from runai.models.related_url import RelatedUrl
from runai.models.serving_configuration import ServingConfiguration
from runai.models.superset_serving_port import SupersetServingPort
from runai.models.superset_spec_all_of_compute import SupersetSpecAllOfCompute
from runai.models.superset_spec_all_of_security import SupersetSpecAllOfSecurity
from runai.models.superset_spec_all_of_storage import SupersetSpecAllOfStorage
from runai.models.toleration import Toleration
from typing import Optional, Set
from typing_extensions import Self


class SupersetSpec(BaseModel):
    """
    Pydantic class model representing SupersetSpec.

    Parameters:
        ```python
        leader: Optional[DistributedInferenceLeaderWorkerSpec1]
        worker: Optional[DistributedInferenceLeaderWorkerSpec1]
        category: Optional[str]
        node_pools: Optional[List[str]]
        priority_class: Optional[str]
        restart_policy: Optional[str]
        serving_port: Optional[SupersetServingPort]
        startup_policy: Optional[DistributedInferenceStartupPolicy]
        workers: Optional[int]
        replicas: Optional[int]
        autoscaling: Optional[AutoScaling]
        serving_configuration: Optional[ServingConfiguration]
        annotations: Optional[List[Annotation]]
        args: Optional[str]
        auto_deletion_time_after_completion_seconds: Optional[int]
        backoff_limit: Optional[int]
        clean_pod_policy: Optional[CleanPodPolicy]
        command: Optional[str]
        completions: Optional[int]
        compute: Optional[SupersetSpecAllOfCompute]
        create_home_dir: Optional[bool]
        distributed_framework: Optional[DistributedFramework]
        environment_variables: Optional[List[EnvironmentVariable]]
        exposed_urls: Optional[List[ExposedUrl]]
        image: Optional[str]
        image_pull_policy: Optional[ImagePullPolicy]
        image_pull_secrets: Optional[List[ImagePullSecret]]
        labels: Optional[List[Label]]
        max_replicas: Optional[int]
        min_replicas: Optional[int]
        mpi_launcher_creation_policy: Optional[MpiLauncherCreationPolicy]
        node_affinity_required: Optional[NodeAffinityRequired]
        node_type: Optional[str]
        num_workers: Optional[int]
        parallelism: Optional[int]
        pod_affinity: Optional[PodAffinity]
        ports: Optional[List[Port]]
        probes: Optional[Probes]
        related_urls: Optional[List[RelatedUrl]]
        security: Optional[SupersetSpecAllOfSecurity]
        slots_per_worker: Optional[int]
        ssh_auth_mount_path: Optional[str]
        stdin: Optional[bool]
        storage: Optional[SupersetSpecAllOfStorage]
        terminate_after_preemption: Optional[bool]
        termination_grace_period_seconds: Optional[int]
        tolerations: Optional[List[Toleration]]
        tty: Optional[bool]
        working_dir: Optional[str]
        ```
        leader: Defines the pod specification for the leader. Must always be provided, regardless of the number of workers.
        worker: Defines the pod specification for the workers. Required only if the number of workers is greater than 0.
        category: Specify the workload category assigned to the workload. Categories are used to classify and monitor different types of workloads within the NVIDIA Run:ai platform.
        node_pools: A prioritized list of node pools for the scheduler to run the workload on. The scheduler will always try to use the first node pool before moving to the next one if the first is not available.
        priority_class: Specifies the priority class for the workload.  The default value depends on the workload type:  high for workspace workloads, low for training/distributed training workloads, and very-high for inference workloads. You can change it to any of the following valid values to adjust the workload&#39;s scheduling behavior: very-low, low, medium-low, medium, medium-high, high, very-high.
        restart_policy: See model str for more information.
        serving_port: See model SupersetServingPort for more information.
        startup_policy: See model DistributedInferenceStartupPolicy for more information. - Default: DistributedInferenceStartupPolicy.LEADERCREATED
        workers: Specifies the number of worker nodes to run. If set to 0, only the leader node will run, and no worker pods will be created. In this case, worker spec is not required. - Default: 0
        replicas: Specifies the number of leader-worker sets to deploy. Each replica represents a group consisting of one leader pod and multiple worker pods.  For example, setting replicas: 3 will create 3 independent groups, each with its own leader and corresponding set of workers.  - Default: 1
        autoscaling: See model AutoScaling for more information.
        serving_configuration: See model ServingConfiguration for more information.
        annotations: Set of annotations to populate into the container running the workload.
        args: Arguments to the command that the container running the workload executes.
        auto_deletion_time_after_completion_seconds: Specifies the duration after which a finished workload (completed or failed) will be automatically deleted. The default is 30 days. Log retention is managed separately.
        backoff_limit: Specifies the number of retries before marking a workload as failed (not applicable to Inference workloads). The default value is 6.
        clean_pod_policy: See model CleanPodPolicy for more information.
        command: A command to the server as the entry point of the container running the workload.
        completions: Used with Hyperparameter Optimization. Specifies the number of successful pods the job should reach to be completed. The Job will be marked as successful once the specified amount of pods has been reached (applicable to standard training only).
        compute: See model SupersetSpecAllOfCompute for more information.
        create_home_dir: When set to &#x60;true&#x60;, creates a home directory for the container.
        distributed_framework: See model DistributedFramework for more information.
        environment_variables: Set of environment variables to populate into the container running the workload.
        exposed_urls: Set of container ports that the workload exposes via URLs.
        image: Docker image name. For more information, see [Images](https://kubernetes.io/docs/concepts/containers/images). The image name is mandatory for creating a workload.
        image_pull_policy: See model ImagePullPolicy for more information.
        image_pull_secrets: A list of references to Kubernetes secrets in the same namespace used for pulling container images.
        labels: Set of labels to populate into the container running the workload.
        max_replicas: the upper limit for the number of worker pods that can be set by the autoscaler. Cannot be smaller than MinReplicas. (applicable only for PyTorch)
        min_replicas: the lower limit for the number of worker pods to which the training job can scale down. (applicable only for PyTorch)
        mpi_launcher_creation_policy: See model MpiLauncherCreationPolicy for more information.
        node_affinity_required: See model NodeAffinityRequired for more information.
        node_type: Nodes (machines), or a group of nodes on which the workload will run. To use this feature, your Administrator will need to label nodes. For more information, see [Group Nodes](https://docs.run.ai/latest/admin/researcher-setup/limit-to-node-group). When using this flag with with Project-based affinity, it refines the list of allowable node groups set in the Project. For more information, see [Projects](https://docshub.run.ai/guides/platform-management/aiinitiatives/organization/projects).
        num_workers: the number of workers that will be allocated for running the workload.
        parallelism: Used with Hyperparameter Optimization. Specifies the maximum number of pods the workload should run at any given time (applicable to standard training only).
        pod_affinity: See model PodAffinity for more information.
        ports: Set of container ports that the workload exposes.
        probes: See model Probes for more information.
        related_urls: Set of URLs that are related to the workload.
        security: See model SupersetSpecAllOfSecurity for more information.
        slots_per_worker: Specifies the number of slots per worker used in hostfile. Defaults to 1. (applicable only for MPI) - Default: 1
        ssh_auth_mount_path: Specifies the directory where SSH keys are mounted. (applicable only for MPI) - Default: &#39;/root/.ssh&#39;
        stdin: Whether this container should allocate a buffer for stdin in the container runtime. If this is not set, reads from stdin in the container will always result in EOF
        storage: See model SupersetSpecAllOfStorage for more information.
        terminate_after_preemption: Indicates if the job should be terminated by the system after it has been preempted.
        termination_grace_period_seconds: Duration in seconds the pod needs to terminate gracefully upon probe failure. The grace period is the duration in seconds after the processes running in the pod are sent a termination signal and the time when the processes are forcibly halted with a kill signal. Set this value longer than the expected cleanup time for your process. Value must be non-negative integer. The value zero indicates stop immediately via the kill signal (no opportunity to shut down).
        tolerations: Set of tolerations to apply to the workload.
        tty: Whether this container should allocate a TTY for itself, also requires &#39;stdin&#39; to be true.
        working_dir: Container&#39;s working directory. If not specified, the container runtime default will be used. This may be configured in the container image.
    Example:
        ```python
        SupersetSpec(
            leader=runai.models.distributed_inference_leader_worker_spec1.DistributedInferenceLeaderWorkerSpec1(
                    annotations = [
                        runai.models.annotation.Annotation(
                            name = 'billing',
                            value = 'my-billing-unit',
                            exclude = False, )
                        ],
                    args = '-x my-script.py',
                    command = 'python',
                    compute = runai.models.superset_spec_all_of_compute.SupersetSpec_allOf_compute(
                        cpu_core_limit = 2,
                        cpu_core_request = 0.5,
                        cpu_memory_limit = '30M',
                        cpu_memory_request = '20M',
                        extended_resources = [
                            runai.models.extended_resource.ExtendedResource(
                                resource = 'hardware-vendor.example/foo',
                                quantity = '2',
                                exclude = False, )
                            ],
                        gpu_devices_request = 1,
                        gpu_memory_limit = '10M',
                        gpu_memory_request = '10M',
                        gpu_portion_limit = 0.5,
                        gpu_portion_request = 0.5,
                        gpu_request_type = 'portion',
                        large_shm_request = False,
                        mig_profile = null, ),
                    create_home_dir = True,
                    environment_variables = [
                        runai.models.environment_variable.EnvironmentVariable(
                            name = 'HOME',
                            value = '/home/my-folder',
                            secret = runai.models.environment_variable_secret.EnvironmentVariableSecret(
                                name = 'postgress_secret',
                                key = 'POSTGRES_PASSWORD', ),
                            config_map = runai.models.environment_variable_config_map.EnvironmentVariableConfigMap(
                                name = 'my-config-map',
                                key = 'MY_POSTGRES_SCHEMA', ),
                            pod_field_ref = runai.models.environment_variable_pod_field_reference.EnvironmentVariablePodFieldReference(
                                path = 'metadata.name', ),
                            exclude = False,
                            description = 'Home directory of the user.', )
                        ],
                    image = 'python:3.8',
                    image_pull_policy = 'Always',
                    image_pull_secrets = [
                        runai.models.image_pull_secret.ImagePullSecret(
                            name = 'w1c2v7s6djuy1zmetozkhdomha1bae37b8ocvx8o53ow2eg7p6qw9qklp6l4y010fogx',
                            user_credential = True,
                            exclude = False, )
                        ],
                    labels = [
                        runai.models.label.Label(
                            name = 'stage',
                            value = 'initial-research',
                            exclude = False, )
                        ],
                    node_affinity_required = runai.models.node_affinity_required.NodeAffinityRequired(
                        node_selector_terms = [
                            runai.models.node_selector_term.NodeSelectorTerm(
                                match_expressions = [
                                    runai.models.match_expression.MatchExpression(
                                        key = 'jUR,rZ#UM/?R,Fp^l6$ARj',
                                        operator = 'In',
                                        values = [
                                            'jUR,rZ#UM/?R,Fp^l6$ARj'
                                            ], )
                                    ], )
                            ], ),
                    node_type = 'my-node-type',
                    pod_affinity = runai.models.pod_affinity.PodAffinity(
                        type = 'Required',
                        key = 'jUR,rZ#UM/?R,Fp^l6$ARj', ),
                    probes = runai.models.probes.Probes(
                        readiness = runai.models.probe.Probe(
                            initial_delay_seconds = 0,
                            period_seconds = 1,
                            timeout_seconds = 1,
                            success_threshold = 1,
                            failure_threshold = 1,
                            handler = runai.models.probe_handler.ProbeHandler(
                                http_get = runai.models.probe_handler_http_get.ProbeHandler_httpGet(
                                    path = '/',
                                    port = 1,
                                    host = 'example.com',
                                    scheme = 'HTTP', ), ), ), ),
                    security = runai.models.inference_policy_defaults_v2_all_of_security.InferencePolicyDefaultsV2_allOf_security(
                        capabilities = ["CHOWN","KILL"],
                        read_only_root_filesystem = False,
                        run_as_gid = 30,
                        run_as_non_root = True,
                        run_as_uid = 500,
                        seccomp_profile_type = 'RuntimeDefault',
                        supplemental_groups = '2,3,5,8',
                        uid_gid_source = 'fromTheImage', ),
                    storage = runai.models.distributed_inference_leader_worker_spec1_storage.DistributedInferenceLeaderWorkerSpec1_storage(
                        config_map_volume = [
                            runai.models.config_map_instance.ConfigMapInstance()
                            ],
                        empty_dir_volume = [
                            runai.models.empty_dir_instance.EmptyDirInstance()
                            ],
                        pvc = [
                            runai.models.pvc_instance.PvcInstance()
                            ],
                        secret_volume = [
                            runai.models.secret_instance2.SecretInstance2()
                            ], ),
                    tolerations = [
                        runai.models.toleration.Toleration(
                            name = 'jUR,rZ#UM/?R,Fp^l6$ARj0',
                            key = 'jUR,rZ#UM/?R,Fp^l6$ARj',
                            value = 'jUR,rZ#UM/?R,Fp^l6$ARj',
                            effect = 'NoSchedule',
                            seconds = 1,
                            exclude = False, )
                        ],
                    working_dir = '/home/myfolder', ),
                        worker=runai.models.distributed_inference_leader_worker_spec1.DistributedInferenceLeaderWorkerSpec1(
                    annotations = [
                        runai.models.annotation.Annotation(
                            name = 'billing',
                            value = 'my-billing-unit',
                            exclude = False, )
                        ],
                    args = '-x my-script.py',
                    command = 'python',
                    compute = runai.models.superset_spec_all_of_compute.SupersetSpec_allOf_compute(
                        cpu_core_limit = 2,
                        cpu_core_request = 0.5,
                        cpu_memory_limit = '30M',
                        cpu_memory_request = '20M',
                        extended_resources = [
                            runai.models.extended_resource.ExtendedResource(
                                resource = 'hardware-vendor.example/foo',
                                quantity = '2',
                                exclude = False, )
                            ],
                        gpu_devices_request = 1,
                        gpu_memory_limit = '10M',
                        gpu_memory_request = '10M',
                        gpu_portion_limit = 0.5,
                        gpu_portion_request = 0.5,
                        gpu_request_type = 'portion',
                        large_shm_request = False,
                        mig_profile = null, ),
                    create_home_dir = True,
                    environment_variables = [
                        runai.models.environment_variable.EnvironmentVariable(
                            name = 'HOME',
                            value = '/home/my-folder',
                            secret = runai.models.environment_variable_secret.EnvironmentVariableSecret(
                                name = 'postgress_secret',
                                key = 'POSTGRES_PASSWORD', ),
                            config_map = runai.models.environment_variable_config_map.EnvironmentVariableConfigMap(
                                name = 'my-config-map',
                                key = 'MY_POSTGRES_SCHEMA', ),
                            pod_field_ref = runai.models.environment_variable_pod_field_reference.EnvironmentVariablePodFieldReference(
                                path = 'metadata.name', ),
                            exclude = False,
                            description = 'Home directory of the user.', )
                        ],
                    image = 'python:3.8',
                    image_pull_policy = 'Always',
                    image_pull_secrets = [
                        runai.models.image_pull_secret.ImagePullSecret(
                            name = 'w1c2v7s6djuy1zmetozkhdomha1bae37b8ocvx8o53ow2eg7p6qw9qklp6l4y010fogx',
                            user_credential = True,
                            exclude = False, )
                        ],
                    labels = [
                        runai.models.label.Label(
                            name = 'stage',
                            value = 'initial-research',
                            exclude = False, )
                        ],
                    node_affinity_required = runai.models.node_affinity_required.NodeAffinityRequired(
                        node_selector_terms = [
                            runai.models.node_selector_term.NodeSelectorTerm(
                                match_expressions = [
                                    runai.models.match_expression.MatchExpression(
                                        key = 'jUR,rZ#UM/?R,Fp^l6$ARj',
                                        operator = 'In',
                                        values = [
                                            'jUR,rZ#UM/?R,Fp^l6$ARj'
                                            ], )
                                    ], )
                            ], ),
                    node_type = 'my-node-type',
                    pod_affinity = runai.models.pod_affinity.PodAffinity(
                        type = 'Required',
                        key = 'jUR,rZ#UM/?R,Fp^l6$ARj', ),
                    probes = runai.models.probes.Probes(
                        readiness = runai.models.probe.Probe(
                            initial_delay_seconds = 0,
                            period_seconds = 1,
                            timeout_seconds = 1,
                            success_threshold = 1,
                            failure_threshold = 1,
                            handler = runai.models.probe_handler.ProbeHandler(
                                http_get = runai.models.probe_handler_http_get.ProbeHandler_httpGet(
                                    path = '/',
                                    port = 1,
                                    host = 'example.com',
                                    scheme = 'HTTP', ), ), ), ),
                    security = runai.models.inference_policy_defaults_v2_all_of_security.InferencePolicyDefaultsV2_allOf_security(
                        capabilities = ["CHOWN","KILL"],
                        read_only_root_filesystem = False,
                        run_as_gid = 30,
                        run_as_non_root = True,
                        run_as_uid = 500,
                        seccomp_profile_type = 'RuntimeDefault',
                        supplemental_groups = '2,3,5,8',
                        uid_gid_source = 'fromTheImage', ),
                    storage = runai.models.distributed_inference_leader_worker_spec1_storage.DistributedInferenceLeaderWorkerSpec1_storage(
                        config_map_volume = [
                            runai.models.config_map_instance.ConfigMapInstance()
                            ],
                        empty_dir_volume = [
                            runai.models.empty_dir_instance.EmptyDirInstance()
                            ],
                        pvc = [
                            runai.models.pvc_instance.PvcInstance()
                            ],
                        secret_volume = [
                            runai.models.secret_instance2.SecretInstance2()
                            ], ),
                    tolerations = [
                        runai.models.toleration.Toleration(
                            name = 'jUR,rZ#UM/?R,Fp^l6$ARj0',
                            key = 'jUR,rZ#UM/?R,Fp^l6$ARj',
                            value = 'jUR,rZ#UM/?R,Fp^l6$ARj',
                            effect = 'NoSchedule',
                            seconds = 1,
                            exclude = False, )
                        ],
                    working_dir = '/home/myfolder', ),
                        category='jUR,rZ#UM/?R,Fp^l6$ARj',
                        node_pools=["my-node-pool-a","my-node-pool-b"],
                        priority_class='jUR,rZ#UM/?R,Fp^l6$ARj',
                        restart_policy='jUR,rZ#UM/?R,Fp^l6$ARj0',
                        serving_port=runai.models.superset_serving_port.SupersetServingPort(
                    port = 56,
                    container = 56,
                    protocol = 'jUR,rZ#UM/?R,Fp^l6$ARj',
                    authorization_type = 'jUR,rZ#UM/?R,Fp^l6$ARj',
                    authorized_users = [
                        'jUR,rZ#UM/?R,Fp^l6$ARj'
                        ],
                    authorized_groups = [
                        'jUR,rZ#UM/?R,Fp^l6$ARj'
                        ],
                    cluster_local_access_only = True,
                    expose_externally = True,
                    exposed_url = 'jUR,rZ#UM/?R,Fp^l6$ARj', ),
                        startup_policy='LeaderCreated',
                        workers=4,
                        replicas=2,
                        autoscaling=runai.models.auto_scaling.AutoScaling(),
                        serving_configuration=runai.models.serving_configuration.ServingConfiguration(
                    initialization_timeout_seconds = 1,
                    request_timeout_seconds = 1, ),
                        annotations=[
                    runai.models.annotation.Annotation(
                        name = 'billing',
                        value = 'my-billing-unit',
                        exclude = False, )
                    ],
                        args='-x my-script.py',
                        auto_deletion_time_after_completion_seconds=15,
                        backoff_limit=3,
                        clean_pod_policy='None',
                        command='python',
                        completions=1,
                        compute=runai.models.superset_spec_all_of_compute.SupersetSpec_allOf_compute(
                    cpu_core_limit = 2,
                    cpu_core_request = 0.5,
                    cpu_memory_limit = '30M',
                    cpu_memory_request = '20M',
                    extended_resources = [
                        runai.models.extended_resource.ExtendedResource(
                            resource = 'hardware-vendor.example/foo',
                            quantity = '2',
                            exclude = False, )
                        ],
                    gpu_devices_request = 1,
                    gpu_memory_limit = '10M',
                    gpu_memory_request = '10M',
                    gpu_portion_limit = 0.5,
                    gpu_portion_request = 0.5,
                    gpu_request_type = 'portion',
                    large_shm_request = False,
                    mig_profile = null, ),
                        create_home_dir=True,
                        distributed_framework='MPI',
                        environment_variables=[
                    runai.models.environment_variable.EnvironmentVariable(
                        name = 'HOME',
                        value = '/home/my-folder',
                        secret = runai.models.environment_variable_secret.EnvironmentVariableSecret(
                            name = 'postgress_secret',
                            key = 'POSTGRES_PASSWORD', ),
                        config_map = runai.models.environment_variable_config_map.EnvironmentVariableConfigMap(
                            name = 'my-config-map',
                            key = 'MY_POSTGRES_SCHEMA', ),
                        pod_field_ref = runai.models.environment_variable_pod_field_reference.EnvironmentVariablePodFieldReference(
                            path = 'metadata.name', ),
                        exclude = False,
                        description = 'Home directory of the user.', )
                    ],
                        exposed_urls=[
                    runai.models.exposed_url.ExposedUrl(
                        container = 8080,
                        url = 'https://my-url.com',
                        authorized_users = ["user-a","user-b"],
                        authorized_groups = ["group-a","group-b"],
                        tool_type = 'jupyter',
                        tool_name = 'my-pytorch',
                        name = 'url-instance-a',
                        exclude = False, )
                    ],
                        image='python:3.8',
                        image_pull_policy='Always',
                        image_pull_secrets=[
                    runai.models.image_pull_secret.ImagePullSecret(
                        name = 'w1c2v7s6djuy1zmetozkhdomha1bae37b8ocvx8o53ow2eg7p6qw9qklp6l4y010fogx',
                        user_credential = True,
                        exclude = False, )
                    ],
                        labels=[
                    runai.models.label.Label(
                        name = 'stage',
                        value = 'initial-research',
                        exclude = False, )
                    ],
                        max_replicas=56,
                        min_replicas=56,
                        mpi_launcher_creation_policy='AtStartup',
                        node_affinity_required=runai.models.node_affinity_required.NodeAffinityRequired(
                    node_selector_terms = [
                        runai.models.node_selector_term.NodeSelectorTerm(
                            match_expressions = [
                                runai.models.match_expression.MatchExpression(
                                    key = 'jUR,rZ#UM/?R,Fp^l6$ARj',
                                    operator = 'In',
                                    values = [
                                        'jUR,rZ#UM/?R,Fp^l6$ARj'
                                        ], )
                                ], )
                        ], ),
                        node_type='my-node-type',
                        num_workers=1,
                        parallelism=1,
                        pod_affinity=runai.models.pod_affinity.PodAffinity(
                    type = 'Required',
                    key = 'jUR,rZ#UM/?R,Fp^l6$ARj', ),
                        ports=[
                    runai.models.port.Port(
                        container = 8080,
                        service_type = 'LoadBalancer',
                        external = 30080,
                        tool_type = 'pytorch',
                        tool_name = 'my-pytorch',
                        name = 'port-instance-a',
                        exclude = False, )
                    ],
                        probes=runai.models.probes.Probes(
                    readiness = runai.models.probe.Probe(
                        initial_delay_seconds = 0,
                        period_seconds = 1,
                        timeout_seconds = 1,
                        success_threshold = 1,
                        failure_threshold = 1,
                        handler = runai.models.probe_handler.ProbeHandler(
                            http_get = runai.models.probe_handler_http_get.ProbeHandler_httpGet(
                                path = '/',
                                port = 1,
                                host = 'example.com',
                                scheme = 'HTTP', ), ), ), ),
                        related_urls=[
                    runai.models.related_url.RelatedUrl(
                        url = 'https://my-url.com',
                        type = 'wandb',
                        name = 'url-instance-a',
                        exclude = False, )
                    ],
                        security=runai.models.superset_spec_all_of_security.SupersetSpec_allOf_security(
                    allow_privilege_escalation = False,
                    capabilities = ["CHOWN","KILL"],
                    host_ipc = False,
                    host_network = False,
                    read_only_root_filesystem = False,
                    run_as_gid = 30,
                    run_as_non_root = True,
                    run_as_uid = 500,
                    seccomp_profile_type = 'RuntimeDefault',
                    supplemental_groups = '2,3,5,8',
                    uid_gid_source = 'fromTheImage', ),
                        slots_per_worker=1,
                        ssh_auth_mount_path='/root/.ssh',
                        stdin=True,
                        storage=runai.models.superset_spec_all_of_storage.SupersetSpec_allOf_storage(
                    config_map_volume = [
                        runai.models.config_map_instance.ConfigMapInstance()
                        ],
                    data_volume = [
                        runai.models.data_volume_instance.DataVolumeInstance()
                        ],
                    empty_dir_volume = [
                        runai.models.empty_dir_instance.EmptyDirInstance()
                        ],
                    git = [
                        runai.models.git_instance.GitInstance()
                        ],
                    host_path = [
                        runai.models.host_path_instance.HostPathInstance()
                        ],
                    nfs = [
                        runai.models.nfs_instance.NfsInstance()
                        ],
                    pvc = [
                        runai.models.pvc_instance.PvcInstance()
                        ],
                    s3 = [
                        runai.models.s3_instance.S3Instance()
                        ],
                    secret_volume = [
                        runai.models.secret_instance2.SecretInstance2()
                        ], ),
                        terminate_after_preemption=False,
                        termination_grace_period_seconds=20,
                        tolerations=[
                    runai.models.toleration.Toleration(
                        name = 'jUR,rZ#UM/?R,Fp^l6$ARj0',
                        operator = 'Equal',
                        key = 'jUR,rZ#UM/?R,Fp^l6$ARj',
                        value = 'jUR,rZ#UM/?R,Fp^l6$ARj',
                        effect = 'NoSchedule',
                        seconds = 1,
                        exclude = False, )
                    ],
                        tty=True,
                        working_dir='/home/myfolder'
        )
        ```
    """  # noqa: E501

    leader: Optional[DistributedInferenceLeaderWorkerSpec1] = Field(
        default=None,
        description="Defines the pod specification for the leader. Must always be provided, regardless of the number of workers.",
    )
    worker: Optional[DistributedInferenceLeaderWorkerSpec1] = Field(
        default=None,
        description="Defines the pod specification for the workers. Required only if the number of workers is greater than 0.",
    )
    category: Optional[Annotated[str, Field(strict=True)]] = Field(
        default=None,
        description="Specify the workload category assigned to the workload. Categories are used to classify and monitor different types of workloads within the NVIDIA Run:ai platform.",
    )
    node_pools: Optional[List[Annotated[str, Field(strict=True)]]] = Field(
        default=None,
        description="A prioritized list of node pools for the scheduler to run the workload on. The scheduler will always try to use the first node pool before moving to the next one if the first is not available.",
        alias="nodePools",
    )
    priority_class: Optional[Annotated[str, Field(strict=True)]] = Field(
        default=None,
        description="Specifies the priority class for the workload.  The default value depends on the workload type:  high for workspace workloads, low for training/distributed training workloads, and very-high for inference workloads. You can change it to any of the following valid values to adjust the workload's scheduling behavior: very-low, low, medium-low, medium, medium-high, high, very-high.",
        alias="priorityClass",
    )
    restart_policy: Optional[Annotated[str, Field(min_length=1, strict=True)]] = Field(
        default=None, alias="restartPolicy"
    )
    serving_port: Optional[SupersetServingPort] = Field(
        default=None, alias="servingPort"
    )
    startup_policy: Optional[DistributedInferenceStartupPolicy] = Field(
        default=DistributedInferenceStartupPolicy.LEADERCREATED, alias="startupPolicy"
    )
    workers: Optional[Annotated[int, Field(le=1000, strict=True, ge=0)]] = Field(
        default=0,
        description="Specifies the number of worker nodes to run. If set to 0, only the leader node will run, and no worker pods will be created. In this case, worker spec is not required.",
    )
    replicas: Optional[Annotated[int, Field(le=1000, strict=True, ge=0)]] = Field(
        default=1,
        description="Specifies the number of leader-worker sets to deploy. Each replica represents a group consisting of one leader pod and multiple worker pods.  For example, setting replicas: 3 will create 3 independent groups, each with its own leader and corresponding set of workers. ",
    )
    autoscaling: Optional[AutoScaling] = None
    serving_configuration: Optional[ServingConfiguration] = Field(
        default=None, alias="servingConfiguration"
    )
    annotations: Optional[List[Optional[Annotation]]] = Field(
        default=None,
        description="Set of annotations to populate into the container running the workload.",
    )
    args: Optional[Annotated[str, Field(min_length=1, strict=True)]] = Field(
        default=None,
        description="Arguments to the command that the container running the workload executes.",
    )
    auto_deletion_time_after_completion_seconds: Optional[StrictInt] = Field(
        default=None,
        description="Specifies the duration after which a finished workload (completed or failed) will be automatically deleted. The default is 30 days. Log retention is managed separately.",
        alias="autoDeletionTimeAfterCompletionSeconds",
    )
    backoff_limit: Optional[StrictInt] = Field(
        default=None,
        description="Specifies the number of retries before marking a workload as failed (not applicable to Inference workloads). The default value is 6.",
        alias="backoffLimit",
    )
    clean_pod_policy: Optional[CleanPodPolicy] = Field(
        default=None, alias="cleanPodPolicy"
    )
    command: Optional[Annotated[str, Field(min_length=1, strict=True)]] = Field(
        default=None,
        description="A command to the server as the entry point of the container running the workload.",
    )
    completions: Optional[StrictInt] = Field(
        default=None,
        description="Used with Hyperparameter Optimization. Specifies the number of successful pods the job should reach to be completed. The Job will be marked as successful once the specified amount of pods has been reached (applicable to standard training only).",
    )
    compute: Optional[SupersetSpecAllOfCompute] = None
    create_home_dir: Optional[StrictBool] = Field(
        default=None,
        description="When set to `true`, creates a home directory for the container.",
        alias="createHomeDir",
    )
    distributed_framework: Optional[DistributedFramework] = Field(
        default=None, alias="distributedFramework"
    )
    environment_variables: Optional[List[Optional[EnvironmentVariable]]] = Field(
        default=None,
        description="Set of environment variables to populate into the container running the workload.",
        alias="environmentVariables",
    )
    exposed_urls: Optional[List[Optional[ExposedUrl]]] = Field(
        default=None,
        description="Set of container ports that the workload exposes via URLs.",
        alias="exposedUrls",
    )
    image: Optional[Annotated[str, Field(min_length=1, strict=True)]] = Field(
        default=None,
        description="Docker image name. For more information, see [Images](https://kubernetes.io/docs/concepts/containers/images). The image name is mandatory for creating a workload.",
    )
    image_pull_policy: Optional[ImagePullPolicy] = Field(
        default=None, alias="imagePullPolicy"
    )
    image_pull_secrets: Optional[List[Optional[ImagePullSecret]]] = Field(
        default=None,
        description="A list of references to Kubernetes secrets in the same namespace used for pulling container images.",
        alias="imagePullSecrets",
    )
    labels: Optional[List[Optional[Label]]] = Field(
        default=None,
        description="Set of labels to populate into the container running the workload.",
    )
    max_replicas: Optional[StrictInt] = Field(
        default=None,
        description="the upper limit for the number of worker pods that can be set by the autoscaler. Cannot be smaller than MinReplicas. (applicable only for PyTorch)",
        alias="maxReplicas",
    )
    min_replicas: Optional[StrictInt] = Field(
        default=None,
        description="the lower limit for the number of worker pods to which the training job can scale down. (applicable only for PyTorch)",
        alias="minReplicas",
    )
    mpi_launcher_creation_policy: Optional[MpiLauncherCreationPolicy] = Field(
        default=None, alias="mpiLauncherCreationPolicy"
    )
    node_affinity_required: Optional[NodeAffinityRequired] = Field(
        default=None, alias="nodeAffinityRequired"
    )
    node_type: Optional[Annotated[str, Field(min_length=1, strict=True)]] = Field(
        default=None,
        description="Nodes (machines), or a group of nodes on which the workload will run. To use this feature, your Administrator will need to label nodes. For more information, see [Group Nodes](https://docs.run.ai/latest/admin/researcher-setup/limit-to-node-group). When using this flag with with Project-based affinity, it refines the list of allowable node groups set in the Project. For more information, see [Projects](https://docshub.run.ai/guides/platform-management/aiinitiatives/organization/projects).",
        alias="nodeType",
    )
    num_workers: Optional[Annotated[int, Field(strict=True, ge=1)]] = Field(
        default=None,
        description="the number of workers that will be allocated for running the workload.",
        alias="numWorkers",
    )
    parallelism: Optional[StrictInt] = Field(
        default=None,
        description="Used with Hyperparameter Optimization. Specifies the maximum number of pods the workload should run at any given time (applicable to standard training only).",
    )
    pod_affinity: Optional[PodAffinity] = Field(default=None, alias="podAffinity")
    ports: Optional[List[Optional[Port]]] = Field(
        default=None, description="Set of container ports that the workload exposes."
    )
    probes: Optional[Probes] = None
    related_urls: Optional[List[Optional[RelatedUrl]]] = Field(
        default=None,
        description="Set of URLs that are related to the workload.",
        alias="relatedUrls",
    )
    security: Optional[SupersetSpecAllOfSecurity] = None
    slots_per_worker: Optional[Annotated[int, Field(strict=True, ge=1)]] = Field(
        default=1,
        description="Specifies the number of slots per worker used in hostfile. Defaults to 1. (applicable only for MPI)",
        alias="slotsPerWorker",
    )
    ssh_auth_mount_path: Optional[Annotated[str, Field(strict=True)]] = Field(
        default="/root/.ssh",
        description="Specifies the directory where SSH keys are mounted. (applicable only for MPI)",
        alias="sshAuthMountPath",
    )
    stdin: Optional[StrictBool] = Field(
        default=None,
        description="Whether this container should allocate a buffer for stdin in the container runtime. If this is not set, reads from stdin in the container will always result in EOF",
    )
    storage: Optional[SupersetSpecAllOfStorage] = None
    terminate_after_preemption: Optional[StrictBool] = Field(
        default=None,
        description="Indicates if the job should be terminated by the system after it has been preempted.",
        alias="terminateAfterPreemption",
    )
    termination_grace_period_seconds: Optional[
        Annotated[int, Field(strict=True, ge=0)]
    ] = Field(
        default=None,
        description="Duration in seconds the pod needs to terminate gracefully upon probe failure. The grace period is the duration in seconds after the processes running in the pod are sent a termination signal and the time when the processes are forcibly halted with a kill signal. Set this value longer than the expected cleanup time for your process. Value must be non-negative integer. The value zero indicates stop immediately via the kill signal (no opportunity to shut down).",
        alias="terminationGracePeriodSeconds",
    )
    tolerations: Optional[List[Optional[Toleration]]] = Field(
        default=None, description="Set of tolerations to apply to the workload."
    )
    tty: Optional[StrictBool] = Field(
        default=None,
        description="Whether this container should allocate a TTY for itself, also requires 'stdin' to be true.",
    )
    working_dir: Optional[Annotated[str, Field(min_length=1, strict=True)]] = Field(
        default=None,
        description="Container's working directory. If not specified, the container runtime default will be used. This may be configured in the container image.",
        alias="workingDir",
    )
    __properties: ClassVar[List[str]] = [
        "leader",
        "worker",
        "category",
        "nodePools",
        "priorityClass",
        "restartPolicy",
        "servingPort",
        "startupPolicy",
        "workers",
        "replicas",
        "autoscaling",
        "servingConfiguration",
        "annotations",
        "args",
        "autoDeletionTimeAfterCompletionSeconds",
        "backoffLimit",
        "cleanPodPolicy",
        "command",
        "completions",
        "compute",
        "createHomeDir",
        "distributedFramework",
        "environmentVariables",
        "exposedUrls",
        "image",
        "imagePullPolicy",
        "imagePullSecrets",
        "labels",
        "maxReplicas",
        "minReplicas",
        "mpiLauncherCreationPolicy",
        "nodeAffinityRequired",
        "nodeType",
        "numWorkers",
        "parallelism",
        "podAffinity",
        "ports",
        "probes",
        "relatedUrls",
        "security",
        "slotsPerWorker",
        "sshAuthMountPath",
        "stdin",
        "storage",
        "terminateAfterPreemption",
        "terminationGracePeriodSeconds",
        "tolerations",
        "tty",
        "workingDir",
    ]

    @field_validator("category")
    def category_validate_regular_expression(cls, value):
        """Validates the regular expression"""
        if value is None:
            return value

        if not re.match(r".*", value):
            raise ValueError(r"must validate the regular expression /.*/")
        return value

    @field_validator("priority_class")
    def priority_class_validate_regular_expression(cls, value):
        """Validates the regular expression"""
        if value is None:
            return value

        if not re.match(r".*", value):
            raise ValueError(r"must validate the regular expression /.*/")
        return value

    @field_validator("restart_policy")
    def restart_policy_validate_regular_expression(cls, value):
        """Validates the regular expression"""
        if value is None:
            return value

        if not re.match(r".*", value):
            raise ValueError(r"must validate the regular expression /.*/")
        return value

    @field_validator("args")
    def args_validate_regular_expression(cls, value):
        """Validates the regular expression"""
        if value is None:
            return value

        if not re.match(r".*", value):
            raise ValueError(r"must validate the regular expression /.*/")
        return value

    @field_validator("command")
    def command_validate_regular_expression(cls, value):
        """Validates the regular expression"""
        if value is None:
            return value

        if not re.match(r".*", value):
            raise ValueError(r"must validate the regular expression /.*/")
        return value

    @field_validator("image")
    def image_validate_regular_expression(cls, value):
        """Validates the regular expression"""
        if value is None:
            return value

        if not re.match(r".*", value):
            raise ValueError(r"must validate the regular expression /.*/")
        return value

    @field_validator("node_type")
    def node_type_validate_regular_expression(cls, value):
        """Validates the regular expression"""
        if value is None:
            return value

        if not re.match(r".*", value):
            raise ValueError(r"must validate the regular expression /.*/")
        return value

    @field_validator("ssh_auth_mount_path")
    def ssh_auth_mount_path_validate_regular_expression(cls, value):
        """Validates the regular expression"""
        if value is None:
            return value

        if not re.match(r".*", value):
            raise ValueError(r"must validate the regular expression /.*/")
        return value

    @field_validator("working_dir")
    def working_dir_validate_regular_expression(cls, value):
        """Validates the regular expression"""
        if value is None:
            return value

        if not re.match(r".*", value):
            raise ValueError(r"must validate the regular expression /.*/")
        return value

    model_config = ConfigDict(
        populate_by_name=True,
        validate_assignment=True,
        protected_namespaces=(),
    )

    def to_str(self) -> str:
        """Returns the string representation of the model using alias"""
        return pprint.pformat(self.model_dump(by_alias=True))

    def to_json(self) -> str:
        """Returns the JSON representation of the model using alias"""
        # TODO: pydantic v2: use .model_dump_json(by_alias=True, exclude_unset=True) instead
        return json.dumps(self.to_dict())

    @classmethod
    def from_json(cls, json_str: str) -> Optional[Self]:
        """Create an instance of SupersetSpec from a JSON string"""
        return cls.from_dict(json.loads(json_str))

    def to_dict(self) -> Dict[str, Any]:
        """Return the dictionary representation of the model using alias.

        This has the following differences from calling pydantic's
        `self.model_dump(by_alias=True)`:

        * `None` is only added to the output dict for nullable fields that
          were set at model initialization. Other fields with value `None`
          are ignored.
        """
        excluded_fields: Set[str] = set([])

        _dict = self.model_dump(
            by_alias=True,
            exclude=excluded_fields,
            exclude_none=True,
        )
        # override the default output from pydantic by calling `to_dict()` of leader
        if self.leader:
            _dict["leader"] = self.leader.to_dict()
        # override the default output from pydantic by calling `to_dict()` of worker
        if self.worker:
            _dict["worker"] = self.worker.to_dict()
        # override the default output from pydantic by calling `to_dict()` of serving_port
        if self.serving_port:
            _dict["servingPort"] = self.serving_port.to_dict()
        # override the default output from pydantic by calling `to_dict()` of autoscaling
        if self.autoscaling:
            _dict["autoscaling"] = self.autoscaling.to_dict()
        # override the default output from pydantic by calling `to_dict()` of serving_configuration
        if self.serving_configuration:
            _dict["servingConfiguration"] = self.serving_configuration.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in annotations (list)
        _items = []
        if self.annotations:
            for _item_annotations in self.annotations:
                if _item_annotations:
                    _items.append(_item_annotations.to_dict())
            _dict["annotations"] = _items
        # override the default output from pydantic by calling `to_dict()` of compute
        if self.compute:
            _dict["compute"] = self.compute.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in environment_variables (list)
        _items = []
        if self.environment_variables:
            for _item_environment_variables in self.environment_variables:
                if _item_environment_variables:
                    _items.append(_item_environment_variables.to_dict())
            _dict["environmentVariables"] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in exposed_urls (list)
        _items = []
        if self.exposed_urls:
            for _item_exposed_urls in self.exposed_urls:
                if _item_exposed_urls:
                    _items.append(_item_exposed_urls.to_dict())
            _dict["exposedUrls"] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in image_pull_secrets (list)
        _items = []
        if self.image_pull_secrets:
            for _item_image_pull_secrets in self.image_pull_secrets:
                if _item_image_pull_secrets:
                    _items.append(_item_image_pull_secrets.to_dict())
            _dict["imagePullSecrets"] = _items
        # override the default output from pydantic by calling `to_dict()` of each item in labels (list)
        _items = []
        if self.labels:
            for _item_labels in self.labels:
                if _item_labels:
                    _items.append(_item_labels.to_dict())
            _dict["labels"] = _items
        # override the default output from pydantic by calling `to_dict()` of node_affinity_required
        if self.node_affinity_required:
            _dict["nodeAffinityRequired"] = self.node_affinity_required.to_dict()
        # override the default output from pydantic by calling `to_dict()` of pod_affinity
        if self.pod_affinity:
            _dict["podAffinity"] = self.pod_affinity.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in ports (list)
        _items = []
        if self.ports:
            for _item_ports in self.ports:
                if _item_ports:
                    _items.append(_item_ports.to_dict())
            _dict["ports"] = _items
        # override the default output from pydantic by calling `to_dict()` of probes
        if self.probes:
            _dict["probes"] = self.probes.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in related_urls (list)
        _items = []
        if self.related_urls:
            for _item_related_urls in self.related_urls:
                if _item_related_urls:
                    _items.append(_item_related_urls.to_dict())
            _dict["relatedUrls"] = _items
        # override the default output from pydantic by calling `to_dict()` of security
        if self.security:
            _dict["security"] = self.security.to_dict()
        # override the default output from pydantic by calling `to_dict()` of storage
        if self.storage:
            _dict["storage"] = self.storage.to_dict()
        # override the default output from pydantic by calling `to_dict()` of each item in tolerations (list)
        _items = []
        if self.tolerations:
            for _item_tolerations in self.tolerations:
                if _item_tolerations:
                    _items.append(_item_tolerations.to_dict())
            _dict["tolerations"] = _items
        # set to None if leader (nullable) is None
        # and model_fields_set contains the field
        if self.leader is None and "leader" in self.model_fields_set:
            _dict["leader"] = None

        # set to None if worker (nullable) is None
        # and model_fields_set contains the field
        if self.worker is None and "worker" in self.model_fields_set:
            _dict["worker"] = None

        # set to None if category (nullable) is None
        # and model_fields_set contains the field
        if self.category is None and "category" in self.model_fields_set:
            _dict["category"] = None

        # set to None if node_pools (nullable) is None
        # and model_fields_set contains the field
        if self.node_pools is None and "node_pools" in self.model_fields_set:
            _dict["nodePools"] = None

        # set to None if priority_class (nullable) is None
        # and model_fields_set contains the field
        if self.priority_class is None and "priority_class" in self.model_fields_set:
            _dict["priorityClass"] = None

        # set to None if restart_policy (nullable) is None
        # and model_fields_set contains the field
        if self.restart_policy is None and "restart_policy" in self.model_fields_set:
            _dict["restartPolicy"] = None

        # set to None if serving_port (nullable) is None
        # and model_fields_set contains the field
        if self.serving_port is None and "serving_port" in self.model_fields_set:
            _dict["servingPort"] = None

        # set to None if startup_policy (nullable) is None
        # and model_fields_set contains the field
        if self.startup_policy is None and "startup_policy" in self.model_fields_set:
            _dict["startupPolicy"] = None

        # set to None if workers (nullable) is None
        # and model_fields_set contains the field
        if self.workers is None and "workers" in self.model_fields_set:
            _dict["workers"] = None

        # set to None if replicas (nullable) is None
        # and model_fields_set contains the field
        if self.replicas is None and "replicas" in self.model_fields_set:
            _dict["replicas"] = None

        # set to None if autoscaling (nullable) is None
        # and model_fields_set contains the field
        if self.autoscaling is None and "autoscaling" in self.model_fields_set:
            _dict["autoscaling"] = None

        # set to None if serving_configuration (nullable) is None
        # and model_fields_set contains the field
        if (
            self.serving_configuration is None
            and "serving_configuration" in self.model_fields_set
        ):
            _dict["servingConfiguration"] = None

        # set to None if annotations (nullable) is None
        # and model_fields_set contains the field
        if self.annotations is None and "annotations" in self.model_fields_set:
            _dict["annotations"] = None

        # set to None if args (nullable) is None
        # and model_fields_set contains the field
        if self.args is None and "args" in self.model_fields_set:
            _dict["args"] = None

        # set to None if auto_deletion_time_after_completion_seconds (nullable) is None
        # and model_fields_set contains the field
        if (
            self.auto_deletion_time_after_completion_seconds is None
            and "auto_deletion_time_after_completion_seconds" in self.model_fields_set
        ):
            _dict["autoDeletionTimeAfterCompletionSeconds"] = None

        # set to None if backoff_limit (nullable) is None
        # and model_fields_set contains the field
        if self.backoff_limit is None and "backoff_limit" in self.model_fields_set:
            _dict["backoffLimit"] = None

        # set to None if clean_pod_policy (nullable) is None
        # and model_fields_set contains the field
        if (
            self.clean_pod_policy is None
            and "clean_pod_policy" in self.model_fields_set
        ):
            _dict["cleanPodPolicy"] = None

        # set to None if command (nullable) is None
        # and model_fields_set contains the field
        if self.command is None and "command" in self.model_fields_set:
            _dict["command"] = None

        # set to None if completions (nullable) is None
        # and model_fields_set contains the field
        if self.completions is None and "completions" in self.model_fields_set:
            _dict["completions"] = None

        # set to None if compute (nullable) is None
        # and model_fields_set contains the field
        if self.compute is None and "compute" in self.model_fields_set:
            _dict["compute"] = None

        # set to None if create_home_dir (nullable) is None
        # and model_fields_set contains the field
        if self.create_home_dir is None and "create_home_dir" in self.model_fields_set:
            _dict["createHomeDir"] = None

        # set to None if distributed_framework (nullable) is None
        # and model_fields_set contains the field
        if (
            self.distributed_framework is None
            and "distributed_framework" in self.model_fields_set
        ):
            _dict["distributedFramework"] = None

        # set to None if environment_variables (nullable) is None
        # and model_fields_set contains the field
        if (
            self.environment_variables is None
            and "environment_variables" in self.model_fields_set
        ):
            _dict["environmentVariables"] = None

        # set to None if exposed_urls (nullable) is None
        # and model_fields_set contains the field
        if self.exposed_urls is None and "exposed_urls" in self.model_fields_set:
            _dict["exposedUrls"] = None

        # set to None if image (nullable) is None
        # and model_fields_set contains the field
        if self.image is None and "image" in self.model_fields_set:
            _dict["image"] = None

        # set to None if image_pull_policy (nullable) is None
        # and model_fields_set contains the field
        if (
            self.image_pull_policy is None
            and "image_pull_policy" in self.model_fields_set
        ):
            _dict["imagePullPolicy"] = None

        # set to None if image_pull_secrets (nullable) is None
        # and model_fields_set contains the field
        if (
            self.image_pull_secrets is None
            and "image_pull_secrets" in self.model_fields_set
        ):
            _dict["imagePullSecrets"] = None

        # set to None if labels (nullable) is None
        # and model_fields_set contains the field
        if self.labels is None and "labels" in self.model_fields_set:
            _dict["labels"] = None

        # set to None if max_replicas (nullable) is None
        # and model_fields_set contains the field
        if self.max_replicas is None and "max_replicas" in self.model_fields_set:
            _dict["maxReplicas"] = None

        # set to None if min_replicas (nullable) is None
        # and model_fields_set contains the field
        if self.min_replicas is None and "min_replicas" in self.model_fields_set:
            _dict["minReplicas"] = None

        # set to None if mpi_launcher_creation_policy (nullable) is None
        # and model_fields_set contains the field
        if (
            self.mpi_launcher_creation_policy is None
            and "mpi_launcher_creation_policy" in self.model_fields_set
        ):
            _dict["mpiLauncherCreationPolicy"] = None

        # set to None if node_affinity_required (nullable) is None
        # and model_fields_set contains the field
        if (
            self.node_affinity_required is None
            and "node_affinity_required" in self.model_fields_set
        ):
            _dict["nodeAffinityRequired"] = None

        # set to None if node_type (nullable) is None
        # and model_fields_set contains the field
        if self.node_type is None and "node_type" in self.model_fields_set:
            _dict["nodeType"] = None

        # set to None if num_workers (nullable) is None
        # and model_fields_set contains the field
        if self.num_workers is None and "num_workers" in self.model_fields_set:
            _dict["numWorkers"] = None

        # set to None if parallelism (nullable) is None
        # and model_fields_set contains the field
        if self.parallelism is None and "parallelism" in self.model_fields_set:
            _dict["parallelism"] = None

        # set to None if pod_affinity (nullable) is None
        # and model_fields_set contains the field
        if self.pod_affinity is None and "pod_affinity" in self.model_fields_set:
            _dict["podAffinity"] = None

        # set to None if ports (nullable) is None
        # and model_fields_set contains the field
        if self.ports is None and "ports" in self.model_fields_set:
            _dict["ports"] = None

        # set to None if probes (nullable) is None
        # and model_fields_set contains the field
        if self.probes is None and "probes" in self.model_fields_set:
            _dict["probes"] = None

        # set to None if related_urls (nullable) is None
        # and model_fields_set contains the field
        if self.related_urls is None and "related_urls" in self.model_fields_set:
            _dict["relatedUrls"] = None

        # set to None if security (nullable) is None
        # and model_fields_set contains the field
        if self.security is None and "security" in self.model_fields_set:
            _dict["security"] = None

        # set to None if slots_per_worker (nullable) is None
        # and model_fields_set contains the field
        if (
            self.slots_per_worker is None
            and "slots_per_worker" in self.model_fields_set
        ):
            _dict["slotsPerWorker"] = None

        # set to None if ssh_auth_mount_path (nullable) is None
        # and model_fields_set contains the field
        if (
            self.ssh_auth_mount_path is None
            and "ssh_auth_mount_path" in self.model_fields_set
        ):
            _dict["sshAuthMountPath"] = None

        # set to None if stdin (nullable) is None
        # and model_fields_set contains the field
        if self.stdin is None and "stdin" in self.model_fields_set:
            _dict["stdin"] = None

        # set to None if storage (nullable) is None
        # and model_fields_set contains the field
        if self.storage is None and "storage" in self.model_fields_set:
            _dict["storage"] = None

        # set to None if terminate_after_preemption (nullable) is None
        # and model_fields_set contains the field
        if (
            self.terminate_after_preemption is None
            and "terminate_after_preemption" in self.model_fields_set
        ):
            _dict["terminateAfterPreemption"] = None

        # set to None if termination_grace_period_seconds (nullable) is None
        # and model_fields_set contains the field
        if (
            self.termination_grace_period_seconds is None
            and "termination_grace_period_seconds" in self.model_fields_set
        ):
            _dict["terminationGracePeriodSeconds"] = None

        # set to None if tolerations (nullable) is None
        # and model_fields_set contains the field
        if self.tolerations is None and "tolerations" in self.model_fields_set:
            _dict["tolerations"] = None

        # set to None if tty (nullable) is None
        # and model_fields_set contains the field
        if self.tty is None and "tty" in self.model_fields_set:
            _dict["tty"] = None

        # set to None if working_dir (nullable) is None
        # and model_fields_set contains the field
        if self.working_dir is None and "working_dir" in self.model_fields_set:
            _dict["workingDir"] = None

        return _dict

    @classmethod
    def from_dict(cls, obj: Optional[Dict[str, Any]]) -> Optional[Self]:
        """Create an instance of SupersetSpec from a dict"""
        if obj is None:
            return None

        if not isinstance(obj, dict):
            return cls.model_validate(obj)

        _obj = cls.model_validate(
            {
                "leader": (
                    DistributedInferenceLeaderWorkerSpec1.from_dict(obj["leader"])
                    if obj.get("leader") is not None
                    else None
                ),
                "worker": (
                    DistributedInferenceLeaderWorkerSpec1.from_dict(obj["worker"])
                    if obj.get("worker") is not None
                    else None
                ),
                "category": obj.get("category"),
                "nodePools": obj.get("nodePools"),
                "priorityClass": obj.get("priorityClass"),
                "restartPolicy": obj.get("restartPolicy"),
                "servingPort": (
                    SupersetServingPort.from_dict(obj["servingPort"])
                    if obj.get("servingPort") is not None
                    else None
                ),
                "startupPolicy": (
                    obj.get("startupPolicy")
                    if obj.get("startupPolicy") is not None
                    else DistributedInferenceStartupPolicy.LEADERCREATED
                ),
                "workers": obj.get("workers") if obj.get("workers") is not None else 0,
                "replicas": (
                    obj.get("replicas") if obj.get("replicas") is not None else 1
                ),
                "autoscaling": (
                    AutoScaling.from_dict(obj["autoscaling"])
                    if obj.get("autoscaling") is not None
                    else None
                ),
                "servingConfiguration": (
                    ServingConfiguration.from_dict(obj["servingConfiguration"])
                    if obj.get("servingConfiguration") is not None
                    else None
                ),
                "annotations": (
                    [Annotation.from_dict(_item) for _item in obj["annotations"]]
                    if obj.get("annotations") is not None
                    else None
                ),
                "args": obj.get("args"),
                "autoDeletionTimeAfterCompletionSeconds": obj.get(
                    "autoDeletionTimeAfterCompletionSeconds"
                ),
                "backoffLimit": obj.get("backoffLimit"),
                "cleanPodPolicy": obj.get("cleanPodPolicy"),
                "command": obj.get("command"),
                "completions": obj.get("completions"),
                "compute": (
                    SupersetSpecAllOfCompute.from_dict(obj["compute"])
                    if obj.get("compute") is not None
                    else None
                ),
                "createHomeDir": obj.get("createHomeDir"),
                "distributedFramework": obj.get("distributedFramework"),
                "environmentVariables": (
                    [
                        EnvironmentVariable.from_dict(_item)
                        for _item in obj["environmentVariables"]
                    ]
                    if obj.get("environmentVariables") is not None
                    else None
                ),
                "exposedUrls": (
                    [ExposedUrl.from_dict(_item) for _item in obj["exposedUrls"]]
                    if obj.get("exposedUrls") is not None
                    else None
                ),
                "image": obj.get("image"),
                "imagePullPolicy": obj.get("imagePullPolicy"),
                "imagePullSecrets": (
                    [
                        ImagePullSecret.from_dict(_item)
                        for _item in obj["imagePullSecrets"]
                    ]
                    if obj.get("imagePullSecrets") is not None
                    else None
                ),
                "labels": (
                    [Label.from_dict(_item) for _item in obj["labels"]]
                    if obj.get("labels") is not None
                    else None
                ),
                "maxReplicas": obj.get("maxReplicas"),
                "minReplicas": obj.get("minReplicas"),
                "mpiLauncherCreationPolicy": obj.get("mpiLauncherCreationPolicy"),
                "nodeAffinityRequired": (
                    NodeAffinityRequired.from_dict(obj["nodeAffinityRequired"])
                    if obj.get("nodeAffinityRequired") is not None
                    else None
                ),
                "nodeType": obj.get("nodeType"),
                "numWorkers": obj.get("numWorkers"),
                "parallelism": obj.get("parallelism"),
                "podAffinity": (
                    PodAffinity.from_dict(obj["podAffinity"])
                    if obj.get("podAffinity") is not None
                    else None
                ),
                "ports": (
                    [Port.from_dict(_item) for _item in obj["ports"]]
                    if obj.get("ports") is not None
                    else None
                ),
                "probes": (
                    Probes.from_dict(obj["probes"])
                    if obj.get("probes") is not None
                    else None
                ),
                "relatedUrls": (
                    [RelatedUrl.from_dict(_item) for _item in obj["relatedUrls"]]
                    if obj.get("relatedUrls") is not None
                    else None
                ),
                "security": (
                    SupersetSpecAllOfSecurity.from_dict(obj["security"])
                    if obj.get("security") is not None
                    else None
                ),
                "slotsPerWorker": (
                    obj.get("slotsPerWorker")
                    if obj.get("slotsPerWorker") is not None
                    else 1
                ),
                "sshAuthMountPath": (
                    obj.get("sshAuthMountPath")
                    if obj.get("sshAuthMountPath") is not None
                    else "/root/.ssh"
                ),
                "stdin": obj.get("stdin"),
                "storage": (
                    SupersetSpecAllOfStorage.from_dict(obj["storage"])
                    if obj.get("storage") is not None
                    else None
                ),
                "terminateAfterPreemption": obj.get("terminateAfterPreemption"),
                "terminationGracePeriodSeconds": obj.get(
                    "terminationGracePeriodSeconds"
                ),
                "tolerations": (
                    [Toleration.from_dict(_item) for _item in obj["tolerations"]]
                    if obj.get("tolerations") is not None
                    else None
                ),
                "tty": obj.get("tty"),
                "workingDir": obj.get("workingDir"),
            }
        )
        return _obj
