import os
import uuid
import glob
from typing import List
from datetime import datetime, timezone

from supertable.config.defaults import logger
from supertable.utils.helper import generate_hash_uid
from supertable.utils.sql_parser import SQLParser


def _now_ms() -> int:
    """UTC timestamp in milliseconds."""
    return int(datetime.now(timezone.utc).timestamp() * 1000)


class QueryPlanManager:
    """
    Small helper that:
      - Generates a stable, short hash per query + meta pointer.
      - Creates a per-table temp directory for DuckDB profile output.
      - Exposes the full path where DuckDB should write its JSON profile.
      - Optionally cleans up old plan files to keep temp tidy.
    """

    def __init__(self, super_name: str, organization: str, current_meta_path: str, parser: SQLParser):
        self.identity = "queries"
        self.organization = organization
        self.original_table = parser.original_table

        # Temp directory under the super/table scope -> avoids collisions across supers
        self.temp_dir = os.path.join(self.organization, super_name, "tmp")
        # Create once; safe for concurrent processes
        os.makedirs(self.temp_dir, exist_ok=True)
        logger.debug(f"Using temp dir {self.temp_dir}")

        # Compose a deterministic (short) hash from meta path + parsed SQL
        parsed_sql = getattr(parser, "parsed_query", None) or getattr(parser, "original_query", "") or ""
        meta_id = current_meta_path or ""
        # Truncate hash to 16 chars to keep filenames short/cross-platform friendly
        self.query_hash = generate_hash_uid("|".join([meta_id, parsed_sql]))[:16]

        self.query_id = str(uuid.uuid4())
        self.query_plan_id = self.generate_query_plan_filename(alias="plan", extension="json")
        self.query_plan_path = os.path.join(self.temp_dir, self.query_plan_id)

        # Best-effort cleanup of very old plan files (non-fatal)
        try:
            self._cleanup_old_plans(max_keep=200)
        except Exception as e:
            logger.debug(f"Plan cleanup skipped: {e}")

    def generate_query_plan_filename(self, alias: str, extension: str = "json") -> str:
        """
        Filename pattern: <utc_ms>_<short_hash>_<alias>.<ext>
        Example: 1726166400000_1a2b3c4d5e6f7a8b_plan.json
        """
        utc_ms = _now_ms()
        return f"{utc_ms}_{self.query_hash}_{alias}.{extension}"

    # ---- Convenience for callers (optional) ----
    def profile_pragmas(self) -> List[str]:
        """
        PRAGMAs the caller (e.g., DataReader) can apply verbatim:
            con.execute("PRAGMA enable_profiling='json';")
            con.execute(f"PRAGMA profile_output='{path}';")
        """
        return [
            "PRAGMA enable_profiling='json';",
            f"PRAGMA profile_output='{self.query_plan_path}';",
        ]

    # ---- Housekeeping -------------------------------------------------------
    def _cleanup_old_plans(self, max_keep: int = 200) -> None:
        """
        Keep temp directory tidy:
          - Only looks at '*_<hash>_plan.json' files generated by this manager
          - Sorts by timestamp (prefix) descending and keeps 'max_keep' newest
          - Deletes the rest (best-effort)
        """
        pattern = os.path.join(self.temp_dir, f"*_{self.query_hash}_plan.json")
        files = sorted(glob.glob(pattern), key=_extract_ts_from_filename, reverse=True)

        if len(files) <= max_keep:
            return

        for old_file in files[max_keep:]:
            try:
                os.remove(old_file)
            except FileNotFoundError:
                continue
            except Exception as e:
                logger.debug(f"Could not delete old plan file {old_file}: {e}")


def _extract_ts_from_filename(path: str) -> int:
    """
    Extract the leading millisecond timestamp from '<ts>_<hash>_plan.json'.
    Returns 0 on failure (so bad names sort to the end).
    """
    try:
        base = os.path.basename(path)
        return int(base.split("_", 1)[0])
    except Exception:
        return 0
