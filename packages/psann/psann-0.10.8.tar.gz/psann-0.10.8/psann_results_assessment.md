**Reviewed Materials**
- `plan.txt:1` — hypotheses H1–H5 and compute-parity constraints
- `README.md:1` — PSANN overview, estimator interface, and backbones
- `TECHNICAL_DETAILS.md:1` — activation math, residual wrappers, HISSO, WaveResNet
- `PSANN_Parity_and_Probes_WaveResNet.ipynb:1` — experiment orchestrator and logs
- `tmp_outputs/colab_results (1)/experiment_metrics.csv:1` — real-data metrics (Beijing, Jena, HAR)
- `tmp_outputs/psann_synth_results (1)/synthetic_experiment_metrics.csv:1` — synthetic parity/missingness metrics
- `tmp_outputs/psann_synth_results (1)/synthetic_ablation_results.csv:1` — feature-group ablations
- `tmp_outputs/psann_synth_results (1)/synthetic_spectral_results.json:1` — Jacobian spectra/PR probe

**Hypothesis Support & Insights**
- H1 Generalization (nonstationary/high‑dimensional):
  - Jena 36h forecast: `ResPSANN_conv_spine` is best by RMSE (2.25) vs LSTM (2.34), WaveResNet (2.44), TCN (3.04) — `experiment_metrics.csv:19,25,31,28`.
  - Beijing PM2.5 (24h context, 6h horizon): TCN leads (RMSE 42.63), PSANN‑conv close (45.32), LSTM (46.85), WaveResNet (50.37) — `experiment_metrics.csv:13,4,10,16`.
  - HAR raw sequences (subject‑held‑out): TCN leads (acc 0.945), WaveResNet (0.931), PSANN‑conv (0.908), LSTM (0.894) — `experiment_metrics.csv:43,46,34,40`.
  - Takeaway: Under compute‑parity time budgets, PSANN with a tiny conv spine is competitive and can be state‑of‑the‑class on smooth seasonal forecasting (Jena), but does not universally dominate TCNs on longer‑memory or event‑like signals (HAR, Beijing).
- H2 Information usage: On the Beijing proxy, PSANN relies heavily on pollutant history; ablation of history collapses performance (ΔR² ≈ −1.82), meteorology removal is modest (ΔR² ≈ −0.066), calendar negligible (Δ=0) — `synthetic_ablation_results.csv:2`, `:3`, `:4`. Matches domain intuition and supports the plan’s PSD/SHAP aims.
- H3 Spectral/geometry diagnostics: Jacobian participation ratio on the seasonal proxy is lower for PSANN‑conv (PR≈1.60) than MLP (PR≈2.92), with similar top singular magnitudes — `synthetic_spectral_results.json:1`. This suggests PSANN compresses variation into fewer active modes, potentially aiding generalization on quasi‑periodic structure; more reps needed before asserting causality.
- H4 Robustness to shift/missingness: On Beijing proxy, PSANN‑conv remains close to TCN across missingness levels; e.g., MISS_0 R² 0.969 vs 0.966 (seed 7) — `synthetic_experiment_metrics.csv:14,15`; MISS_10 R² 0.963 vs 0.970 — `:26,27`; MISS_30 R² 0.967 vs 0.971 — `:38,39`. WaveResNet degrades sharply at higher missingness (e.g., MISS_10 R²≈0.917; MISS_30 R²≈0.874) — `:29,41`. Supports the hypothesis that PSANN with small temporal bias is robust to common shift/missingness patterns.
- H5 Limits & tiny temporal inductive bias: Attention‑only spines underperform markedly vs small conv spines (e.g., SPINE_FAIRNESS R² 0.672 attention vs 0.970 conv) — `synthetic_experiment_metrics.csv:51,50`. On HAR, the attention‑spine variant shows strong overfit (val≈0.972, test≈0.788), while conv‑spine generalizes better (test≈0.908) — `experiment_metrics.csv:36,37,34`. This supports the plan’s claim that a minimal temporal inductive bias (strided/dilated conv) is necessary for good PSANN performance on longer‑memory tasks.

**Limitations & Gaps**
- Compute parity is approximate: parameter counts vary (e.g., LSTM far fewer params than PSANN/TCN), and wall‑clock times differ (e.g., WaveResNet trains faster on some tasks) — `experiment_metrics.csv:4,10,13,16`. FLOP‑level or equal‑steps calibration is not reported, so fairness is not provably tight.
- Single‑run real‑data results: no confidence intervals or paired tests for Beijing/Jena/HAR; synthetic parity includes a few seeds but real‑data comparisons may still have variance.
- EAF coverage is proxy‑only so far: the delta‑TEMP proxy yields near‑zero/negative R² for all tabular models (limited diagnostic value) — `synthetic_experiment_metrics.csv:52–54` (see lines `:52`, `:53`, `:54`). Real EAF joins/aggregation (per `plan.txt:1`) have not been executed in these outputs.
- Cross‑station Beijing generalization not explicitly exercised in the real run; current Beijing results look station‑specific windowing with missingness, not held‑out stations.
- Attention spine underperformance on HAR suggests design/regularization sensitivity; without ablations on pooling/aggregation and subject‑aware augmentation, causality is still tentative.
- Spectral probe sample is small (2 models on one proxy); conclusions about geometry are preliminary without repeats and time‑evolution tracking.

**Strengths & What’s Confident**
- PSANN with a tiny conv spine is strong on smooth seasonal forecasting and competitive on noisy multivariate signals under matched time budgets (Jena best; Beijing close second) — `experiment_metrics.csv:19,13,4`.
- Robustness to missingness/shift is solid in proxies: PSANN tracks TCN and clearly exceeds WaveResNet under heavy masks — `synthetic_experiment_metrics.csv:26–41` (e.g., `:26,29,38,41`).
- Information‑usage ablations align with domain knowledge (history≫meteorology≫calendar) — `synthetic_ablation_results.csv:2–4`.
- The experiments demonstrate practical compute‑parity discipline (similar training durations/param scales) and cover both sequence and tabular regimes, increasing external validity.

**Targeted Next Probes (Minimal Additions)**
- Add 3–5 seeds per real‑data config (Beijing/Jena/HAR) and report mean±CI with paired tests; keep time budgets identical and stop early on plateaus.
- Beijing cross‑station: 2–3 folds with one held‑out station each under the same parity budget; report per‑station spread.
- HAR: replace attention spine with a 1–2 block dilated Conv1d (TCN‑lite) and compare to current conv spine; add light dropout/augmentations; keep params/time matched.
- EAF: run a small real‑data slice with per‑heat aggregation and top features (TEMP/O₂), respecting locale normalization; cap to a 30–60 min budget and one seed for a directional check.
- Geometry: compute Jacobian PR over training epochs (start/mid/end) for PSANN vs MLP on Jena to test whether lower PR correlates with generalization gains.

**Bottom Line**
- The collected results partially support the plan’s hypotheses: PSANNs with minimal temporal inductive bias generalize well and are robust to missingness, often competitive under compute parity. They are not universally superior to TCNs on tasks requiring longer memory, and attention‑only spines are risky. The ablations and spectral probe provide mechanistic hints (history reliance, lower participation ratio), but a few targeted replications and a small EAF real run will meaningfully tighten confidence without expanding scope.

