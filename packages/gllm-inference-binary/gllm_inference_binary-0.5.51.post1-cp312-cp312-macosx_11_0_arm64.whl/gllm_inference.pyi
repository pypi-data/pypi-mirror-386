# This file was generated by Nuitka

# Stubs included by default


__name__ = ...



# Modules used internally, to allow implicit dependencies to be seen:
import os
import typing
import gllm_core
import gllm_core.utils
import gllm_inference.em_invoker.AzureOpenAIEMInvoker
import gllm_inference.em_invoker.BedrockEMInvoker
import gllm_inference.em_invoker.CohereEMInvoker
import gllm_inference.em_invoker.GoogleEMInvoker
import gllm_inference.em_invoker.JinaEMInvoker
import gllm_inference.em_invoker.LangChainEMInvoker
import gllm_inference.em_invoker.OpenAICompatibleEMInvoker
import gllm_inference.em_invoker.OpenAIEMInvoker
import gllm_inference.em_invoker.TwelveLabsEMInvoker
import gllm_inference.em_invoker.VoyageEMInvoker
import gllm_inference.lm_invoker.AnthropicLMInvoker
import gllm_inference.lm_invoker.AzureOpenAILMInvoker
import gllm_inference.lm_invoker.BedrockLMInvoker
import gllm_inference.lm_invoker.DatasaurLMInvoker
import gllm_inference.lm_invoker.GoogleLMInvoker
import gllm_inference.lm_invoker.LangChainLMInvoker
import gllm_inference.lm_invoker.LiteLLMLMInvoker
import gllm_inference.lm_invoker.OpenAIChatCompletionsLMInvoker
import gllm_inference.lm_invoker.OpenAICompatibleLMInvoker
import gllm_inference.lm_invoker.OpenAILMInvoker
import gllm_inference.lm_invoker.PortkeyLMInvoker
import gllm_inference.lm_invoker.XAILMInvoker
import gllm_inference.prompt_builder.PromptBuilder
import gllm_inference.output_parser.JSONOutputParser
import json
import abc
import pandas
import pydantic
import re
import gllm_core.utils.retry
import gllm_inference.request_processor.LMRequestProcessor
import gllm_core.utils.imports
import gllm_inference.schema.ModelId
import gllm_inference.schema.ModelProvider
import gllm_inference.schema.TruncationConfig
import asyncio
import base64
import enum
import gllm_inference.exceptions.BaseInvokerError
import gllm_inference.exceptions.convert_http_status_to_base_invoker_error
import gllm_inference.schema.Attachment
import gllm_inference.schema.AttachmentType
import gllm_inference.schema.EMContent
import gllm_inference.schema.Vector
import aioboto3
import gllm_inference.utils.validate_string_enum
import cohere
import asyncio.CancelledError
import gllm_inference.exceptions.convert_to_base_invoker_error
import gllm_inference.schema.TruncateSide
import google
import google.auth
import google.genai
import google.genai.types
import httpx
import gllm_inference.exceptions.ProviderInternalError
import concurrent
import concurrent.futures
import concurrent.futures.ThreadPoolExecutor
import langchain_core
import langchain_core.embeddings
import gllm_inference.exceptions.InvokerRuntimeError
import gllm_inference.exceptions.build_debug_info
import gllm_inference.utils.load_langchain_model
import gllm_inference.utils.parse_model_data
import openai
import io
import twelvelabs
import sys
import voyageai
import voyageai.client_async
import http
import http.HTTPStatus
import uuid
import gllm_core.constants
import gllm_core.event
import gllm_core.schema
import gllm_core.schema.tool
import langchain_core.tools
import gllm_inference.schema.BatchStatus
import gllm_inference.schema.LMInput
import gllm_inference.schema.LMOutput
import gllm_inference.schema.Message
import gllm_inference.schema.Reasoning
import gllm_inference.schema.ResponseSchema
import gllm_inference.schema.ThinkingEvent
import gllm_inference.schema.TokenUsage
import gllm_inference.schema.ToolCall
import gllm_inference.schema.ToolResult
import anthropic
import anthropic.types
import anthropic.types.message_create_params
import anthropic.types.messages
import anthropic.types.messages.batch_create_params
import gllm_inference.schema.MessageRole
import langchain_core.language_models
import langchain_core.messages
import gllm_inference.exceptions._get_exception_key
import litellm
import inspect
import time
import jsonschema
import gllm_inference.lm_invoker.batch.BatchOperations
import gllm_inference.schema.LMEventType
import gllm_inference.schema.MessageContent
import __future__
import gllm_inference.schema.ActivityEvent
import gllm_inference.schema.CodeEvent
import gllm_inference.schema.CodeExecResult
import gllm_inference.schema.MCPCall
import gllm_inference.schema.MCPCallActivity
import gllm_inference.schema.MCPListToolsActivity
import gllm_inference.schema.MCPServer
import gllm_inference.schema.WebSearchActivity
import logging
import portkey_ai
import xai_sdk
import xai_sdk.chat
import xai_sdk.search
import xai_sdk.proto
import xai_sdk.proto.v5
import xai_sdk.proto.v5.chat_pb2
import jinja2
import jinja2.sandbox
import gllm_inference.schema.JinjaEnvType
import gllm_inference.prompt_builder.format_strategy.JinjaFormatStrategy
import gllm_inference.prompt_builder.format_strategy.StringFormatStrategy
import transformers
import gllm_inference.prompt_formatter.HuggingFacePromptFormatter
import traceback
import gllm_inference.realtime_chat.input_streamer.KeyboardInputStreamer
import gllm_inference.realtime_chat.output_streamer.ConsoleOutputStreamer
import google.genai.live
import gllm_core.utils.logger_manager
import mimetypes
import pathlib
import filetype
import magic
import requests
import binascii
import fnmatch
import importlib