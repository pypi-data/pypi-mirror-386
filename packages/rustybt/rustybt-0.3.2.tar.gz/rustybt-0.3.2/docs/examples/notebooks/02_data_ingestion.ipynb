{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion with RustyBT\n",
    "\n",
    "This notebook demonstrates how to fetch and prepare data from multiple sources for backtesting.\n",
    "\n",
    "**Data Sources Covered:**\n",
    "- yfinance (Yahoo Finance) - Free stocks, ETFs, forex\n",
    "- CCXT - Cryptocurrency exchanges (100+ exchanges)\n",
    "- CSV files - Custom data\n",
    "- Alpaca - Real-time and historical market data\n",
    "\n",
    "**What you'll learn:**\n",
    "- Fetching data from different providers\n",
    "- Data validation and quality checks\n",
    "- Creating custom data bundles\n",
    "- Caching for performance\n",
    "\n",
    "**Estimated runtime:** 5-10 minutes (depending on data downloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "from rustybt.analytics import create_progress_iterator, setup_notebook\n",
    "\n",
    "setup_notebook()\n",
    "\n",
    "import contextlib\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "\n",
    "from rustybt.data.adapters import CCXTAdapter, CSVAdapter, YFinanceAdapter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Yahoo Finance Data (Stocks & ETFs)\n",
    "\n",
    "Yahoo Finance provides free historical data for stocks, ETFs, indices, and forex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize yfinance adapter\n",
    "yf_adapter = YFinanceAdapter()\n",
    "\n",
    "# Fetch data for multiple stocks\n",
    "symbols = [\"AAPL\", \"GOOGL\", \"MSFT\", \"TSLA\"]\n",
    "start_date = pd.Timestamp(\"2023-01-01\")\n",
    "end_date = pd.Timestamp(\"2023-12-31\")\n",
    "\n",
    "\n",
    "# Fetch with progress bar\n",
    "all_data = []\n",
    "for symbol in create_progress_iterator(symbols, desc=\"Downloading\"):\n",
    "    data = await yf_adapter.fetch(\n",
    "        symbols=[symbol], start_date=start_date, end_date=end_date, resolution=\"1d\"\n",
    "    )\n",
    "    all_data.append(data)\n",
    "\n",
    "# Combine all data\n",
    "combined = pl.concat(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate data quality\n",
    "with contextlib.suppress(Exception):\n",
    "    yf_adapter.validate(combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cryptocurrency Data (CCXT)\n",
    "\n",
    "CCXT provides unified access to 100+ cryptocurrency exchanges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CCXT adapter for Binance\n",
    "binance = CCXTAdapter(exchange_id=\"binance\")\n",
    "\n",
    "# Fetch BTC and ETH data\n",
    "crypto_symbols = [\"BTC/USDT\", \"ETH/USDT\"]\n",
    "\n",
    "\n",
    "crypto_data = []\n",
    "for symbol in crypto_symbols:\n",
    "    data = await binance.fetch(\n",
    "        symbols=[symbol],\n",
    "        start_date=pd.Timestamp(\"2024-01-01\"),\n",
    "        end_date=pd.Timestamp(\"2024-01-31\"),\n",
    "        resolution=\"1h\",  # Hourly data\n",
    "    )\n",
    "    crypto_data.append(data)\n",
    "\n",
    "crypto_combined = pl.concat(crypto_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. CSV Data Import\n",
    "\n",
    "Import custom data from CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example CSV structure:\n",
    "csv_example = pd.DataFrame(\n",
    "    {\n",
    "        \"timestamp\": pd.date_range(\"2024-01-01\", periods=100, freq=\"D\"),\n",
    "        \"symbol\": \"CUSTOM\",\n",
    "        \"open\": 100 + np.random.randn(100).cumsum(),\n",
    "        \"high\": 105 + np.random.randn(100).cumsum(),\n",
    "        \"low\": 95 + np.random.randn(100).cumsum(),\n",
    "        \"close\": 100 + np.random.randn(100).cumsum(),\n",
    "        \"volume\": np.random.randint(1000000, 10000000, 100),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save example CSV\n",
    "csv_example.to_csv(\"example_data.csv\", index=False)\n",
    "\n",
    "# Load using CSV adapter\n",
    "csv_adapter = CSVAdapter()\n",
    "csv_data = csv_adapter.load(\"example_data.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Quality Checks\n",
    "\n",
    "Always validate data before using in backtests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_data_quality(df: pl.DataFrame, name: str = \"Data\") -> None:\n",
    "    \"\"\"Comprehensive data quality check.\"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Data Quality Check: {name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Check for nulls\n",
    "    null_counts = df.null_count()\n",
    "    total_nulls = sum(null_counts.to_dicts()[0].values())\n",
    "    if total_nulls > 0:\n",
    "        print(f\"‚ö†Ô∏è  Found {total_nulls} null values\")\n",
    "        print(null_counts)\n",
    "    else:\n",
    "        print(\"‚úÖ No null values found\")\n",
    "\n",
    "    # Check OHLC relationships\n",
    "    invalid = df.filter(\n",
    "        (pl.col(\"high\") < pl.col(\"low\"))\n",
    "        | (pl.col(\"high\") < pl.col(\"open\"))\n",
    "        | (pl.col(\"high\") < pl.col(\"close\"))\n",
    "        | (pl.col(\"low\") > pl.col(\"open\"))\n",
    "        | (pl.col(\"low\") > pl.col(\"close\"))\n",
    "    )\n",
    "\n",
    "    if len(invalid) > 0:\n",
    "        print(f\"‚ùå Found {len(invalid)} rows with invalid OHLC relationships\")\n",
    "        print(invalid.head())\n",
    "    else:\n",
    "        print(\"‚úÖ OHLC relationships are valid\")\n",
    "\n",
    "    # Check for duplicates\n",
    "    duplicates = df.filter(pl.col(\"timestamp\").is_duplicated())\n",
    "    if len(duplicates) > 0:\n",
    "        print(f\"‚ö†Ô∏è  Found {len(duplicates)} duplicate timestamps\")\n",
    "    else:\n",
    "        print(\"‚úÖ No duplicate timestamps\")\n",
    "\n",
    "    # Date range\n",
    "    min_date = df.select(pl.col(\"timestamp\").min()).item()\n",
    "    max_date = df.select(pl.col(\"timestamp\").max()).item()\n",
    "    row_count = len(df)\n",
    "    print(f\"\\nüìä Data Summary:\")\n",
    "    print(f\"   Rows: {row_count:,}\")\n",
    "    print(f\"   Date Range: {min_date} to {max_date}\")\n",
    "    print(f\"   Symbols: {df.select(pl.col('symbol').n_unique()).item()}\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "\n",
    "# Check quality\n",
    "check_data_quality(combined, \"Stock Data\")\n",
    "check_data_quality(crypto_combined, \"Crypto Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Save Data for Backtesting\n",
    "\n",
    "Save data in efficient formats for fast backtesting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to Parquet (recommended - fast and efficient)\n",
    "combined.write_parquet(\"stocks_2023.parquet\")\n",
    "crypto_combined.write_parquet(\"crypto_2024_01.parquet\")\n",
    "\n",
    "\n",
    "# Can also save to CSV for compatibility\n",
    "# combined.write_csv('stocks_2023.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Caching\n",
    "\n",
    "RustyBT supports caching to avoid re-downloading data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rustybt.data.catalog import DataCatalog\n",
    "\n",
    "# Initialize catalog with caching\n",
    "catalog = DataCatalog(cache_dir=\"./data_cache\")\n",
    "\n",
    "# Register data source\n",
    "catalog.register(\n",
    "    name=\"stocks_2023\",\n",
    "    adapter=yf_adapter,\n",
    "    symbols=[\"AAPL\", \"GOOGL\", \"MSFT\"],\n",
    "    start_date=pd.Timestamp(\"2023-01-01\"),\n",
    "    end_date=pd.Timestamp(\"2023-12-31\"),\n",
    ")\n",
    "\n",
    "# First call downloads data\n",
    "data1 = catalog.load(\"stocks_2023\")\n",
    "\n",
    "# Second call uses cache\n",
    "data2 = catalog.load(\"stocks_2023\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that you have data:\n",
    "\n",
    "1. **03_strategy_development.ipynb** - Build trading strategies with this data\n",
    "2. **10_full_workflow.ipynb** - See complete workflow from data to results\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "- ‚úÖ Multiple data sources supported (stocks, crypto, custom)\n",
    "- ‚úÖ Built-in data validation catches errors early\n",
    "- ‚úÖ Efficient Parquet storage for fast backtests\n",
    "- ‚úÖ Caching prevents redundant downloads\n",
    "- ‚úÖ Progress bars for long downloads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rustybt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
