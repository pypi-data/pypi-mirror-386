{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Genetic Algorithm Optimization on Non-Smooth Functions\n",
    "\n",
    "This notebook demonstrates genetic algorithm optimization on non-smooth, multimodal objective functions where GA excels compared to Bayesian optimization.\n",
    "\n",
    "## Contents\n",
    "1. Setup and imports\n",
    "2. Define Rastrigin function (non-smooth, multimodal)\n",
    "3. Run Genetic Algorithm optimization\n",
    "4. Compare with Bayesian optimization\n",
    "5. Visualize population evolution\n",
    "6. Diversity tracking analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decimal import Decimal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from rustybt.optimization.parameter_space import ContinuousParameter, ParameterSpace\n",
    "from rustybt.optimization.search.bayesian_search import BayesianOptimizer\n",
    "from rustybt.optimization.search.genetic_algorithm import GeneticAlgorithm\n",
    "\n",
    "# Set plot style\n",
    "plt.style.use(\"seaborn-v0_8-darkgrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define Rastrigin Function\n",
    "\n",
    "The Rastrigin function is a non-smooth, highly multimodal function with many local optima. It's a classic benchmark for testing optimization algorithms on difficult landscapes.\n",
    "\n",
    "$$f(x, y) = 20 + x^2 + y^2 - 10(\\cos(2\\pi x) + \\cos(2\\pi y))$$\n",
    "\n",
    "- **Global optimum**: $(0, 0)$ with $f(0, 0) = 0$\n",
    "- **Many local optima**: Due to cosine terms\n",
    "- **Non-smooth**: Discontinuities make gradient-based methods struggle\n",
    "\n",
    "**Why GA excels here:**\n",
    "- Population-based search explores multiple regions\n",
    "- Doesn't rely on smoothness assumptions\n",
    "- Crossover helps escape local optima"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rastrigin(params: dict[str, Decimal]) -> Decimal:\n",
    "    \"\"\"Rastrigin function (to maximize, so negate).\n",
    "\n",
    "    Args:\n",
    "        params: Dictionary with 'x' and 'y' parameters\n",
    "\n",
    "    Returns:\n",
    "        Negative Rastrigin value (for maximization)\n",
    "    \"\"\"\n",
    "    x = float(params[\"x\"])\n",
    "    y = float(params[\"y\"])\n",
    "    A = 10\n",
    "    result = 2 * A + (x**2 - A * np.cos(2 * np.pi * x)) + (y**2 - A * np.cos(2 * np.pi * y))\n",
    "    return Decimal(str(-result))  # Negate for maximization\n",
    "\n",
    "\n",
    "# Visualize the Rastrigin function\n",
    "x = np.linspace(-5.12, 5.12, 200)\n",
    "y = np.linspace(-5.12, 5.12, 200)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "\n",
    "A = 10\n",
    "Z = 2 * A + (X**2 - A * np.cos(2 * np.pi * X)) + (Y**2 - A * np.cos(2 * np.pi * Y))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "contour = ax.contour(X, Y, Z, levels=30, cmap=\"viridis\")\n",
    "ax.clabel(contour, inline=True, fontsize=8)\n",
    "ax.plot(0, 0, \"r*\", markersize=20, label=\"Global optimum (0, 0)\")\n",
    "ax.set_xlabel(\"x\")\n",
    "ax.set_ylabel(\"y\")\n",
    "ax.set_title(\"Rastrigin Function: Non-smooth with Many Local Optima\")\n",
    "ax.legend()\n",
    "plt.colorbar(contour, ax=ax, label=\"f(x, y)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Run Genetic Algorithm Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter space\n",
    "param_space = ParameterSpace(\n",
    "    parameters=[\n",
    "        ContinuousParameter(name=\"x\", min_value=Decimal(\"-5.12\"), max_value=Decimal(\"5.12\")),\n",
    "        ContinuousParameter(name=\"y\", min_value=Decimal(\"-5.12\"), max_value=Decimal(\"5.12\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Initialize Genetic Algorithm\n",
    "ga = GeneticAlgorithm(\n",
    "    parameter_space=param_space,\n",
    "    population_size=50,\n",
    "    max_generations=50,\n",
    "    selection=\"tournament\",\n",
    "    tournament_size=3,\n",
    "    crossover_prob=0.8,\n",
    "    mutation_prob=0.2,\n",
    "    elite_size=5,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "\n",
    "# Track all evaluated points for visualization\n",
    "ga_evaluations = []\n",
    "\n",
    "# Run optimization\n",
    "iteration = 0\n",
    "while not ga.is_complete():\n",
    "    params = ga.suggest()\n",
    "    score = rastrigin(params)\n",
    "    ga.update(params, score)\n",
    "\n",
    "    ga_evaluations.append((float(params[\"x\"]), float(params[\"y\"]), float(score)))\n",
    "\n",
    "    iteration += 1\n",
    "    if iteration % 100 == 0:\n",
    "        best_params, best_score = ga.get_best_result()\n",
    "\n",
    "# Get final results\n",
    "best_params, best_score = ga.get_best_result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Compare with Bayesian Optimization\n",
    "\n",
    "Let's compare GA with Bayesian optimization on the same problem to see how they differ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bayesian Optimizer\n",
    "bo = BayesianOptimizer(\n",
    "    parameter_space=param_space,\n",
    "    n_calls=2500,  # Match GA evaluation budget\n",
    "    n_initial_points=50,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "\n",
    "# Track all evaluated points\n",
    "bo_evaluations = []\n",
    "\n",
    "# Run optimization\n",
    "iteration = 0\n",
    "while not bo.is_complete():\n",
    "    params = bo.suggest()\n",
    "    score = rastrigin(params)\n",
    "    bo.update(params, score)\n",
    "\n",
    "    bo_evaluations.append((float(params[\"x\"]), float(params[\"y\"]), float(score)))\n",
    "\n",
    "    iteration += 1\n",
    "    if iteration % 100 == 0:\n",
    "        best_params_bo, best_score_bo = bo.get_best_result()\n",
    "\n",
    "# Get final results\n",
    "best_params_bo, best_score_bo = bo.get_best_result()\n",
    "\n",
    "# Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Visualize Population Evolution\n",
    "\n",
    "Visualize how the GA population explores the search space over generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot search trajectories\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# GA trajectory\n",
    "ga_x = [e[0] for e in ga_evaluations]\n",
    "ga_y = [e[1] for e in ga_evaluations]\n",
    "ga_scores = [e[2] for e in ga_evaluations]\n",
    "\n",
    "contour1 = ax1.contour(X, Y, Z, levels=20, cmap=\"gray\", alpha=0.3)\n",
    "scatter1 = ax1.scatter(ga_x, ga_y, c=ga_scores, s=20, cmap=\"viridis\", alpha=0.6)\n",
    "ax1.plot(0, 0, \"r*\", markersize=20, label=\"Global optimum\")\n",
    "ax1.plot(float(best_params[\"x\"]), float(best_params[\"y\"]), \"g*\", markersize=15, label=\"GA best\")\n",
    "ax1.set_xlabel(\"x\")\n",
    "ax1.set_ylabel(\"y\")\n",
    "ax1.set_title(\"Genetic Algorithm: Exploration Pattern\")\n",
    "ax1.legend()\n",
    "plt.colorbar(scatter1, ax=ax1, label=\"Score (negated Rastrigin)\")\n",
    "\n",
    "# Bayesian trajectory\n",
    "bo_x = [e[0] for e in bo_evaluations]\n",
    "bo_y = [e[1] for e in bo_evaluations]\n",
    "bo_scores = [e[2] for e in bo_evaluations]\n",
    "\n",
    "contour2 = ax2.contour(X, Y, Z, levels=20, cmap=\"gray\", alpha=0.3)\n",
    "scatter2 = ax2.scatter(bo_x, bo_y, c=bo_scores, s=20, cmap=\"viridis\", alpha=0.6)\n",
    "ax2.plot(0, 0, \"r*\", markersize=20, label=\"Global optimum\")\n",
    "ax2.plot(\n",
    "    float(best_params_bo[\"x\"]),\n",
    "    float(best_params_bo[\"y\"]),\n",
    "    \"g*\",\n",
    "    markersize=15,\n",
    "    label=\"Bayesian best\",\n",
    ")\n",
    "ax2.set_xlabel(\"x\")\n",
    "ax2.set_ylabel(\"y\")\n",
    "ax2.set_title(\"Bayesian Optimization: Exploration Pattern\")\n",
    "ax2.legend()\n",
    "plt.colorbar(scatter2, ax=ax2, label=\"Score (negated Rastrigin)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyze Generation Statistics\n",
    "\n",
    "Examine how fitness and diversity evolve over generations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get generation history\n",
    "history = ga.get_generation_history()\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Best fitness over generations\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(\n",
    "    history[\"generation\"],\n",
    "    [float(f) for f in history[\"best_fitness\"]],\n",
    "    \"b-\",\n",
    "    linewidth=2,\n",
    "    label=\"Best fitness\",\n",
    ")\n",
    "ax1.plot(\n",
    "    history[\"generation\"],\n",
    "    [float(f) for f in history[\"avg_fitness\"]],\n",
    "    \"r--\",\n",
    "    linewidth=2,\n",
    "    label=\"Avg fitness\",\n",
    ")\n",
    "ax1.set_xlabel(\"Generation\")\n",
    "ax1.set_ylabel(\"Fitness (negated Rastrigin)\")\n",
    "ax1.set_title(\"Fitness Evolution\")\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Plot 2: Diversity over generations\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(history[\"generation\"], history[\"diversity\"], \"g-\", linewidth=2)\n",
    "ax2.axhline(\n",
    "    y=ga.diversity_threshold,\n",
    "    color=\"r\",\n",
    "    linestyle=\"--\",\n",
    "    label=f\"Threshold ({ga.diversity_threshold})\",\n",
    ")\n",
    "ax2.set_xlabel(\"Generation\")\n",
    "ax2.set_ylabel(\"Population Diversity\")\n",
    "ax2.set_title(\"Diversity Tracking\")\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "# Plot 3: Fitness improvement rate\n",
    "ax3 = axes[1, 0]\n",
    "best_fitness = [float(f) for f in history[\"best_fitness\"]]\n",
    "improvement = [0] + [best_fitness[i] - best_fitness[i - 1] for i in range(1, len(best_fitness))]\n",
    "ax3.bar(history[\"generation\"], improvement, color=\"blue\", alpha=0.7)\n",
    "ax3.set_xlabel(\"Generation\")\n",
    "ax3.set_ylabel(\"Fitness Improvement\")\n",
    "ax3.set_title(\"Generation-to-Generation Improvement\")\n",
    "ax3.grid(True)\n",
    "\n",
    "# Plot 4: Convergence progress\n",
    "ax4 = axes[1, 1]\n",
    "# Distance from optimum (0, 0) for best individual per generation\n",
    "# For visualization, we'll show cumulative evaluations\n",
    "cumulative_evals = [(i + 1) * ga.population_size for i in history[\"generation\"]]\n",
    "ax4.plot(cumulative_evals, [float(f) for f in history[\"best_fitness\"]], \"b-\", linewidth=2)\n",
    "ax4.set_xlabel(\"Total Evaluations\")\n",
    "ax4.set_ylabel(\"Best Fitness\")\n",
    "ax4.set_title(\"Convergence Curve\")\n",
    "ax4.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### When to Use Genetic Algorithms\n",
    "\n",
    "**Use GA when:**\n",
    "1. **Non-smooth objectives**: Discontinuities, noise, or lack of gradients\n",
    "2. **Multimodal landscapes**: Many local optima that trap gradient-based methods\n",
    "3. **Mixed parameter types**: Continuous + discrete + categorical parameters\n",
    "4. **Cheap evaluations**: GA needs 100s-1000s of evaluations\n",
    "5. **Exploration important**: Want diverse solutions, not just single optimum\n",
    "\n",
    "**Don't use GA when:**\n",
    "1. **Smooth, unimodal**: Bayesian optimization is more sample-efficient\n",
    "2. **Expensive evaluations**: Each backtest takes minutes/hours\n",
    "3. **Very high dimensions**: >50 parameters (curse of dimensionality)\n",
    "\n",
    "### GA Configuration Tips\n",
    "\n",
    "1. **Population size**: 20-100 (larger for harder problems)\n",
    "2. **Selection**: Tournament (default) is robust; roulette for fitness-proportional\n",
    "3. **Crossover prob**: 0.7-0.9 (higher = more exploitation)\n",
    "4. **Mutation prob**: 0.1-0.3 (higher = more exploration)\n",
    "5. **Elite size**: 10-20% of population (prevents losing best solutions)\n",
    "6. **Diversity tracking**: Monitor to detect premature convergence\n",
    "\n",
    "### Comparison with Bayesian Optimization\n",
    "\n",
    "On the Rastrigin function (non-smooth, multimodal):\n",
    "- **GA**: Better at escaping local optima, explores broadly\n",
    "- **Bayesian**: Can get trapped in local optima, focuses too narrowly\n",
    "\n",
    "On smooth functions (e.g., sphere, Rosenbrock):\n",
    "- **GA**: Works but needs many evaluations\n",
    "- **Bayesian**: More sample-efficient, converges faster"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
