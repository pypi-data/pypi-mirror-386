{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "f391fbd1",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2023-10-23T22:57:59.269234Z",
                    "start_time": "2023-10-23T22:57:55.574937Z"
                }
            },
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "pd 2.1.1\n",
                        "dask 2023.6.0\n",
                        "pa 12.0.1\n",
                        "np 1.24.3\n",
                        "darshan 3.4.4.0\n"
                    ]
                }
            ],
            "source": [
                "from glob import glob\n",
                "import pandas as pd\n",
                "print(f\"pd {pd.__version__}\")\n",
                "import dask\n",
                "import dask.dataframe as dd\n",
                "print(f\"dask {dask.__version__}\")\n",
                "import pyarrow as pa\n",
                "print(f\"pa {pa.__version__}\")\n",
                "import numpy as np\n",
                "print(f\"np {np.__version__}\")\n",
                "from itertools import chain\n",
                "\n",
                "from dask.distributed import Client, LocalCluster, progress, wait\n",
                "from dask.distributed import Future, get_client\n",
                "from typing import Tuple, Union\n",
                "import os\n",
                "import intervals as I\n",
                "import math\n",
                "\n",
                "import re\n",
                "from rich.console import Console\n",
                "from rich.panel import Panel\n",
                "from rich.table import Table\n",
                "from rich.tree import Tree\n",
                "import subprocess\n",
                "import json\n",
                "import logging\n",
                "import darshan\n",
                "print(f\"darshan {darshan.__version__}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "b1feb065",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2023-10-23T20:22:22.975802Z",
                    "start_time": "2023-10-23T20:22:22.971468Z"
                }
            },
            "outputs": [],
            "source": [
                "logging.basicConfig(filename='darshan_main.log', encoding='utf-8', level=logging.DEBUG)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "cca20624",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2023-10-23T20:22:22.985653Z",
                    "start_time": "2023-10-23T20:22:22.978215Z"
                }
            },
            "outputs": [],
            "source": [
                "initialized = False"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "bd4d2a3d",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2023-10-23T20:22:23.014356Z",
                    "start_time": "2023-10-23T20:22:22.989752Z"
                }
            },
            "outputs": [],
            "source": [
                "\n",
                "def execute_script(cmd):\n",
                "    proc = subprocess.Popen(cmd, stdout=subprocess.PIPE, shell=True)\n",
                "    (out, err) = proc.communicate()\n",
                "    output = out.decode('utf-8')\n",
                "    logging.debug(f\"Executing cmd {cmd} and returned {output}\")\n",
                "    return output\n",
                "def create_index(filename):\n",
                "    directory = os.path.dirname(filename)\n",
                "    index_file = os.path.join(directory, \"index\", f\"{filename}.zindex\")\n",
                "    if not os.path.exists(index_file):\n",
                "        result = execute_script([f\"{ZINDEX_BIN}/zindex {filename} --regex 'id:\\b([0-9]+)' --numeric --unique --index-file file:{index_file}\"])\n",
                "        logging.debug(f\"Creating Index for {filename} returned {result}\")\n",
                "    return filename\n",
                "\n",
                "def get_linenumber(filename):\n",
                "    directory = os.path.dirname(filename)\n",
                "    index_file = os.path.join(directory, \"index\", f\"{filename}.zindex\")\n",
                "    line_number = execute_script([f\"/usr/bin/sqlite3 {index_file} 'select MAX(a. line) from LineOffsets a;'\"])\n",
                "    if line_number == \"\":\n",
                "        line_number = 1\n",
                "    else:\n",
                "        line_number = int(line_number.split(\"\\n\")[0])\n",
                "    logging.debug(f\" The {filename} has {line_number} lines\")\n",
                "    return DELIMITER.join([filename, str(line_number)])\n",
                "\n",
                "def get_size(filename):\n",
                "    directory = os.path.dirname(filename)\n",
                "    index_file = os.path.join(directory, \"index\", f\"{filename}.zindex\")\n",
                "    size = execute_script([f\"/usr/bin/sqlite3 {index_file} 'SELECT SUM(length)  FROM LineOffsets;'\"])\n",
                "    #print(size)\n",
                "    if size == \"\":\n",
                "        size = 1\n",
                "    else:\n",
                "        size = int(size.split(\"\\n\")[0])\n",
                "    logging.debug(f\" The {filename} has {size/1024**3} GB size\")\n",
                "    return int(size)\n",
                "\n",
                "\n",
                "def generate_line_batches(args):\n",
                "    filename, max_line = args.split(DELIMITER)\n",
                "    max_line = int(max_line)\n",
                "    #print(args)\n",
                "    for start in range(0, max_line, BATCH):\n",
                "        logging.debug(f\"Created a batch for {filename} from [{start}, {start + BATCH}] lines\")\n",
                "        yield DELIMITER.join([filename, str(start)]) \n",
                "\n",
                "def load_indexed_gzip_files(args):\n",
                "    for arg in args:\n",
                "        filename, start = arg.split(DELIMITER)\n",
                "        directory = os.path.dirname(filename)\n",
                "        index_file = os.path.join(directory, \"index\", f\"{filename}.zindex\")\n",
                "        start = int(start)\n",
                "        result = execute_script(f\"{ZINDEX_BIN}/zq --index-file {index_file} {filename} --raw 'select a.line from LineOffsets a where a.line >= {start} AND a.line < {start+BATCH};'\")\n",
                "        #print(value)\n",
                "        if result == \"\":\n",
                "            logging.debug(f\"Found an empty line for {filename} range [{start}, {start + BATCH}] lines\")\n",
                "            return None\n",
                "        else:\n",
                "            json_lines = result.split(\"\\n\")\n",
                "            for json_line in json_lines:\n",
                "                logging.debug(f\"Found an {json_line} line for {filename} range [{start}, {start + BATCH}] lines\")\n",
                "                yield json_line\n",
                "\n",
                "def load_objects(line, fn, time_granularity):\n",
                "    d = {} \n",
                "    if line is not None and line !=\"\" and \"[\" != line[0] and line != \"\\n\" :\n",
                "        val = {}\n",
                "        try:\n",
                "            val = json.loads(line)\n",
                "            if \"name\" in val:\n",
                "                d[\"index\"] = val[\"id\"]\n",
                "                d[\"name\"] = val[\"name\"]\n",
                "                d[\"cat\"] = val[\"cat\"]\n",
                "                d[\"pid\"] = val[\"pid\"]\n",
                "                d[\"tid\"] = val[\"tid\"]\n",
                "                d[\"dur\"] = val[\"dur\"]\n",
                "                d[\"tinterval\"] = I.to_string(I.closed(val[\"ts\"] , val[\"ts\"] + val[\"dur\"]))\n",
                "                d[\"trange\"] = int(((val[\"ts\"] + val[\"dur\"])/2.0) / time_granularity)\n",
                "                d.update(io_function(val, d))\n",
                "                if fn:\n",
                "                    d.update(fn(val, d))\n",
                "                logging.debug(f\"built an dictionary for line {d}\")\n",
                "        except ValueError as error:\n",
                "            logging.error(f\"Processing {line} failed with {error}\")\n",
                "    return d"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "6a0ad661",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2023-10-23T20:22:23.027832Z",
                    "start_time": "2023-10-23T20:22:23.017041Z"
                }
            },
            "outputs": [],
            "source": [
                "def io_function(json_object, current_dict):\n",
                "    d = {}\n",
                "    d[\"phase\"] = 0\n",
                "    if \"compute\" in json_object[\"name\"]:\n",
                "        d[\"compute_time\"] = current_dict[\"tinterval\"]\n",
                "        d[\"phase\"] = 1\n",
                "    else:\n",
                "        d[\"compute_time\"] = I.to_string(I.empty())\n",
                "    if \"POSIX\" in json_object[\"cat\"]:\n",
                "        d[\"io_time\"] = current_dict[\"tinterval\"]\n",
                "        d[\"phase\"] = 2\n",
                "    else:\n",
                "        d[\"io_time\"] = I.to_string(I.empty())\n",
                "    if \"args\" in json_object:\n",
                "        if \"filename\" in json_object[\"args\"]:\n",
                "            d[\"filename\"] = json_object[\"args\"][\"filename\"]        \n",
                "        if \"hostname\" in json_object[\"args\"]:\n",
                "            d[\"hostname\"] = json_object[\"args\"][\"hostname\"]\n",
                "            \n",
                "        if \"POSIX\" == json_object[\"cat\"]:\n",
                "            if \"write\" in json_object[\"name\"]:\n",
                "                d[\"size\"] = int(json_object[\"args\"][\"ret\"])\n",
                "            elif \"read\" in json_object[\"name\"] and \"readdir\" not in json_object[\"name\"]:\n",
                "                d[\"size\"] = int(json_object[\"args\"][\"ret\"])\n",
                "        else:\n",
                "            if \"image_size\" in json_object[\"args\"]:\n",
                "                d[\"size\"] = json_object[\"args\"][\"image_size\"]\n",
                "    return d\n",
                "\n",
                "def io_columns():\n",
                "    return {\n",
                "        'hostname': \"string[pyarrow]\",\n",
                "        'compute_time': \"string[pyarrow]\",\n",
                "        'io_time': \"string[pyarrow]\",\n",
                "        'filename': \"string[pyarrow]\",\n",
                "        'phase': \"uint16[pyarrow]\",\n",
                "        'size': \"uint64[pyarrow]\"\n",
                "    }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "f3036af9",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2023-10-23T20:22:23.043777Z",
                    "start_time": "2023-10-23T20:22:23.030719Z"
                }
            },
            "outputs": [],
            "source": [
                "def group_func(df):\n",
                "    val = I.empty()\n",
                "    for index, value in df.items():\n",
                "        val = val.union(I.from_string(str(value), int))\n",
                "    logging.debug(f\"Grouped Range into {val}\")\n",
                "    return I.to_string(val)\n",
                "\n",
                "def union_portions():        \n",
                "    return dd.Aggregation(\n",
                "        'union_portions',\n",
                "        chunk=lambda s: s.apply(group_func),     \n",
                "        agg=lambda s: s.apply(group_func)\n",
                "    )\n",
                "def difference_portion(df, a, b):\n",
                "    return I.from_string(str(df[a]), int) - I.from_string(str(df[b]), int)\n",
                "def size_portion(df, col):\n",
                "    val = 0.0\n",
                "    ia = I.from_string(str(df[col]), int)\n",
                "    for i in list(ia):\n",
                "        if not i.is_empty():\n",
                "            val += i.upper - i.lower\n",
                "    logging.debug(f\"Calculating size of Interval {val}\")\n",
                "    return val\n",
                "def percentile(n):\n",
                "    return dd.Aggregation(\n",
                "    name='percentile_{:02.0f}'.format(n*100),\n",
                "    # this computes the median on each partition\n",
                "    chunk=lambda s: s.quantile(n),\n",
                "    # this combines results across partitions; the input should just be a list of length 1\n",
                "    agg=lambda s0: s0.quantile(n),\n",
                "    )\n",
                "median_fun = dd.Aggregation(\n",
                "    name=\"median\",\n",
                "    # this computes the median on each partition\n",
                "    chunk=lambda s: s.median(),\n",
                "    # this combines results across partitions; the input should just be a list of length 1\n",
                "    agg=lambda s0: s0.median(),\n",
                ")\n",
                "def human_format(num):\n",
                "    if num:\n",
                "        num = float('{:.3g}'.format(num))\n",
                "        magnitude = 0\n",
                "        while abs(num) >= 1024:\n",
                "            magnitude += 1\n",
                "            num /= 1024.0\n",
                "        return '{}{}'.format('{:.0f}'.format(num).rstrip('.'), ['', 'KB', 'MB', 'GB', 'TB'][magnitude])\n",
                "    else:\n",
                "        return \"NA\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "5cba4b0d",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2023-10-23T22:57:45.082451Z",
                    "start_time": "2023-10-23T22:57:45.071482Z"
                }
            },
            "outputs": [],
            "source": [
                "def generate_darshan_records(log_file):\n",
                "    def get_dict(row):\n",
                "        d = {}\n",
                "        d[\"size\"] = row[\"length\"]\n",
                "        d[\"ts\"] = int(row[\"start_time\"] * 10e6)\n",
                "        d[\"dur\"] = int(row[\"end_time\"] * 10e6) -  d[\"ts\"]\n",
                "        d[\"tinterval\"] = I.to_string(I.closed(d[\"ts\"] , d[\"ts\"] + d[\"dur\"]))\n",
                "        d[\"trange\"] = int(((d[\"ts\"] + d[\"dur\"])/2.0) / time_granularity)\n",
                "        d[\"phase\"] = 2\n",
                "        d[\"compute_time\"] = I.to_string(I.empty())\n",
                "        d[\"io_time\"] = d[\"tinterval\"]\n",
                "        return d\n",
                "    report = darshan.DarshanReport(log_file, read_all=True)\n",
                "    if \"DXT_POSIX\" in report.modules:\n",
                "        time_granularity=10e3\n",
                "        for val in report.records['DXT_POSIX'].to_df():\n",
                "            d = {}\n",
                "            fileid = val[\"id\"]\n",
                "            write_df = val[\"write_segments\"]\n",
                "            read_df = val[\"read_segments\"]\n",
                "            d[\"hostname\"] = val[\"hostname\"]\n",
                "            d[\"pid\"] = val[\"rank\"]\n",
                "            d[\"tid\"] = 0\n",
                "            d[\"cat\"] = \"POSIX\"\n",
                "            d[\"filename\"] = report.data['name_records'][fileid]\n",
                "            for index, row in write_df.iterrows():\n",
                "                d[\"name\"] = \"write\"\n",
                "                d.update(get_dict(row))\n",
                "                yield d\n",
                "            for index, row in read_df.iterrows():\n",
                "                d[\"name\"] = \"read\"\n",
                "                d.update(get_dict(row))\n",
                "                yield d   "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "1a04b853",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2023-10-23T20:22:23.064431Z",
                    "start_time": "2023-10-23T20:22:23.060313Z"
                }
            },
            "outputs": [],
            "source": [
                "HOST_PATTERN=r'corona(\\d+)'\n",
                "ZINDEX_BIN=\"/usr/WS2/iopp/software/zindex/build/Release\"\n",
                "REBUILD_INDEX=False\n",
                "BATCH=1024**2\n",
                "DELIMITER=\";\""
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "04f0a14d",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2023-10-23T20:22:23.105349Z",
                    "start_time": "2023-10-23T20:22:23.067205Z"
                }
            },
            "outputs": [],
            "source": [
                "class DarshanAnalyzer:\n",
                "    \n",
                "    def __init__(self, file_pattern, time_granularity=10e3):\n",
                "        file_pattern = glob(file_pattern)\n",
                "        for file in file_pattern:\n",
                "            if not file.endswith('.darshan'):\n",
                "                raise Exception(f\"Only darshan files supported: {file}.\")\n",
                "        logging.debug(f\"Processing files {file_pattern}\")\n",
                "        create_bag = dask.bag.from_delayed([dask.delayed(generate_darshan_records)(file) \n",
                "                                                for file in file_pattern])\n",
                "        columns = {'name':\"string[pyarrow]\", 'cat': \"string[pyarrow]\",\n",
                "                   'pid': \"uint64[pyarrow]\",'tid': \"uint64[pyarrow]\",\n",
                "                   'dur': \"uint64[pyarrow]\", 'tinterval': \"string[pyarrow]\",\n",
                "                   'trange': \"uint64[pyarrow]\", 'hostname': \"string[pyarrow]\",\n",
                "                    'compute_time': \"string[pyarrow]\", 'io_time': \"string[pyarrow]\",\n",
                "                    'filename': \"string[pyarrow]\", 'phase': \"uint16[pyarrow]\",\n",
                "                    'size': \"uint64[pyarrow]\"}\n",
                "        events = create_bag.to_dataframe(meta=columns)\n",
                "        n_partition = 1\n",
                "        logging.debug(f\"Number of partitions used are {n_partition}\")\n",
                "        self.events = events.repartition(npartitions=n_partition).compute()\n",
                "        logging.info(f\"Loaded events\")\n",
                "        \n",
                "    \n",
                "    def _calculate_time(self):\n",
                "        grouped_df = self.events.groupby(\"trange\").agg({\"io_time\":union_portions(),\"compute_time\":union_portions()})\n",
                "        grouped_df[\"only_io\"] = grouped_df[[\"io_time\", \"compute_time\"]].apply(difference_portion, a=\"io_time\", b=\"compute_time\", axis=1, meta=(\"string[pyarrow]\"))\n",
                "        grouped_df[\"only_compute\"] = grouped_df[[\"io_time\", \"compute_time\"]].apply(difference_portion, a=\"compute_time\", b=\"io_time\", axis=1, meta=(\"string[pyarrow]\"))\n",
                "        total_io_time, total_compute_time, only_io, only_compute = dask.compute(\n",
                "            grouped_df[[\"io_time\"]].apply(size_portion, col=\"io_time\", axis=1).sum(),\n",
                "            grouped_df[[\"compute_time\"]].apply(size_portion, col=\"compute_time\", axis=1).sum(),\n",
                "            grouped_df[[\"only_io\"]].apply(size_portion, col=\"only_io\", axis=1).sum(),\n",
                "            grouped_df[[\"only_compute\"]].apply(size_portion, col=\"only_compute\", axis=1).sum(),\n",
                "        )\n",
                "        logging.info(f\"Calculated times are total_io_time {total_io_time}us, total_compute_time {total_compute_time}us, only_io {only_io}us, only_compute {only_compute}us\")\n",
                "        return total_io_time, total_compute_time, only_io, only_compute\n",
                "    \n",
                "    def _create_interval(self, list_items):\n",
                "        logging.debug(f\"Creating interval from {list_items}\")\n",
                "        prev = list_items[0]\n",
                "        val = I.closed(prev,prev)\n",
                "        for proc in list_items[1:]:\n",
                "            val = val | I.closed(prev,proc)\n",
                "            prev = proc\n",
                "        logging.info(f\"Created an interval of {val}\")\n",
                "        return val\n",
                "    \n",
                "    def _create_host_intervals(self, hosts_list):\n",
                "        logging.debug(f\"Creating regex for {hosts_list}\")\n",
                "        is_first = True\n",
                "        value = I.empty()\n",
                "        for host in hosts_list:    \n",
                "            val = int(re.findall(HOST_PATTERN, host)[0])\n",
                "            if is_first:\n",
                "                prev = val\n",
                "                is_first = False\n",
                "                value = I.closed(prev,prev)\n",
                "            else:\n",
                "                value = value | I.closed(prev,val)\n",
                "        val = re.findall(HOST_PATTERN, hosts_list[0])[0]\n",
                "        regex = hosts_list[0].replace(val,str(value))\n",
                "        logging.info(f\"Created regex value {val}\")\n",
                "        return regex\n",
                "    \n",
                "    def _remove_numbers(self, string_items):\n",
                "        logging.debug(f\"Removing numbers from {string_items}\")\n",
                "        item_sets = set()\n",
                "        for file in string_items:\n",
                "            item_sets.add(re.sub(r'\\d', 'X', str(file)))\n",
                "        logging.info(f\"List after removing numbers {list(item_sets)}\")\n",
                "        return list(item_sets)\n",
                "    \n",
                "    def summary(self):\n",
                "        total_io_time, total_compute_time, only_io, only_compute = self._calculate_time()\n",
                "        hosts_used, filenames_accessed, num_procs, compute_tid, posix_tid, io_by_operations = dask.compute(\n",
                "            self.events[\"hostname\"].unique(),\n",
                "            self.events[\"filename\"].unique(),\n",
                "            self.events[\"pid\"].unique(),\n",
                "            self.events.query(\"phase == 1\")[\"tid\"].unique(),\n",
                "            self.events.query(\"phase == 2\")[\"tid\"].unique(),\n",
                "            self.events.query(\"phase == 2\").groupby([\"name\"]).agg({\"dur\":[sum,\"count\"], \"size\":[sum,\"mean\",median_fun,min,max,percentile(.25),percentile(.75)]})\n",
                "        )\n",
                "        num_events = len(self.events)\n",
                "        \n",
                "        hosts_used = hosts_used.to_list()\n",
                "        hosts_used_regex_str = self._create_host_intervals(hosts_used)\n",
                "        \n",
                "        filenames_accessed = filenames_accessed.to_list()\n",
                "        filename_basename_regex_str = self._remove_numbers(filenames_accessed)\n",
                "        \n",
                "        num_procs = num_procs.to_list()\n",
                "        proc_name_regex = self._create_interval(num_procs)\n",
                "        \n",
                "        io_by_ops_dict = io_by_operations.T.to_dict()\n",
                "\n",
                "        # Create a new Table object from Rich library\n",
                "        table = Table(box=None, show_header=False)\n",
                "\n",
                "        # Add columns to the table for the key and value\n",
                "        table.add_column(style=\"cyan\")\n",
                "        table.add_column()\n",
                "        app_tree = Tree(\"Scheduler Allocation Details\")\n",
                "        app_tree.add(f\"Nodes: {str(len(hosts_used))} {hosts_used_regex_str}\")\n",
                "        app_tree.add(f\"Processes: {str(len(num_procs))} {str(proc_name_regex)}\")\n",
                "        thread_tree = Tree(\"Thread allocations across nodes (includes dynamically created threads)\")\n",
                "        thread_tree.add(f\"Compute: {str(len(compute_tid))}\")\n",
                "        thread_tree.add(f\"I/O: {str(len(posix_tid))}\")\n",
                "        app_tree.add(thread_tree)\n",
                "        app_tree.add(f\"Events Recorded: {str(num_events)}\")\n",
                "        table.add_row(\"Allocation\",app_tree)\n",
                "\n",
                "        data_tree = Tree(\"Description of Dataset Used\")\n",
                "        data_tree.add(f\"Files: {str(len(filenames_accessed))} {filename_basename_regex_str}\")\n",
                "        table.add_row(\"Dataset\",data_tree)\n",
                "\n",
                "        io_tree = Tree(\"Behavior of Application\")\n",
                "        io_time = Tree(\"Split of Time in application\")\n",
                "        io_time.add(f\"Compute: {total_compute_time/1e6:.3f} sec\")\n",
                "        io_time.add(f\"Overall I/O: {total_io_time/1e6:.3f} sec\")\n",
                "        io_time.add(f\"Unoverlapped I/O: {only_io/1e6:.3f} sec\")\n",
                "        io_time.add(f\"Unoverlapped Compute: {only_compute/1e6:.3f} sec\")\n",
                "        io_tree.add(io_time)\n",
                "        padding_size = 6\n",
                "        key_padding_size = 15\n",
                "        io_ts = Tree(\"Transfer size distribution by function\")\n",
                "        io_ts.add(f\"{'Function':<{key_padding_size}}|{'min':<{padding_size}}|{'25':<{padding_size}}|{'mean':<{padding_size}}|{'median':<{padding_size}}|{'75':<{padding_size}}|{'max':<{padding_size}}|\")\n",
                "        for key, value in io_by_ops_dict.items():\n",
                "            if \"close\" not in key or \"open\" not in key:\n",
                "                io_ts.add(f\"{key.split('.')[-1]:<{key_padding_size}}|{human_format(value[('size', 'min')]):<{padding_size}}|{human_format(value[('size', 'percentile_25')]):<{padding_size}}|{human_format(value[('size', 'mean')]):<{padding_size}}|{human_format(value[('size', 'median')]):<{padding_size}}|{human_format(value[('size', 'percentile_75')]):<{padding_size}}|{human_format(value[('size', 'max')]):<{padding_size}}|\")\n",
                "        io_tree.add(io_ts)\n",
                "        io_ops = Tree(\"Event count by function\")\n",
                "        for key, value in io_by_ops_dict.items():\n",
                "            io_ops.add(f\"{key.split('.')[-1]} : {value[('dur', 'count')]}\")\n",
                "        io_tree.add(io_ops)               \n",
                "        table.add_row(\"I/O Behavior\",io_tree)\n",
                "        console = Console()\n",
                "\n",
                "        # Print the table with Rich formatting\n",
                "        console.print(Panel(table, title='Summary'))\n",
                "        "
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "42045918",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2023-10-23T20:22:25.550906Z",
                    "start_time": "2023-10-23T20:22:23.107915Z"
                }
            },
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/usr/WS2/haridev/venvs/dftracer_venv/lib/python3.9/site-packages/distributed/node.py:182: UserWarning: Port 8787 is already in use.\n",
                        "Perhaps you already have a cluster running?\n",
                        "Hosting the HTTP server on port 34401 instead\n",
                        "  warnings.warn(\n"
                    ]
                }
            ],
            "source": [
                "if not initialized:\n",
                "    workers = 16\n",
                "    cluster = LocalCluster(n_workers=workers)  # Launches a scheduler and workers locally\n",
                "    client = Client(cluster)  # Connect to distributed cluster and override default\n",
                "    initialized = True\n",
                "    logging.info(f\"Initialized Client with {workers} workers\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "49b47855",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2023-10-23T22:57:35.943604Z",
                    "start_time": "2023-10-23T22:57:35.923484Z"
                }
            },
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "84"
                        ]
                    },
                    "execution_count": 3,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "filename = \"/usr/workspace/haridev/dlio_paper_results/darshan_overhead_corona_1_40/*.darshan\"\n",
                "from glob import glob\n",
                "file_pattern = glob(filename)\n",
                "len(file_pattern)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0bac570c",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "6108176f",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2023-10-23T22:58:11.256383Z",
                    "start_time": "2023-10-23T22:58:05.801113Z"
                }
            },
            "outputs": [],
            "source": [
                "file_pattern = glob(filename)\n",
                "all_records = []\n",
                "for file in file_pattern:\n",
                "    for record in generate_darshan_records(file):\n",
                "        all_records.append(all_records)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "1bb56a8b",
            "metadata": {
                "ExecuteTime": {
                    "start_time": "2023-10-23T22:48:56.860Z"
                }
            },
            "outputs": [],
            "source": [
                "pd.DataFrame.from_dict(all_records, orient='columns')"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "a3adaa4a",
            "metadata": {},
            "outputs": [],
            "source": []
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "5f0e68ec",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2023-10-23T22:59:28.919138Z",
                    "start_time": "2023-10-23T22:58:13.996836Z"
                }
            },
            "outputs": [],
            "source": [
                "df = pd.DataFrame(all_records)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "71b652e8",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2023-10-23T19:58:55.180711Z",
                    "start_time": "2023-10-23T19:58:47.059884Z"
                }
            },
            "outputs": [],
            "source": [
                "analyzer = DarshanAnalyzer(filename)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 25,
            "id": "4b439617",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2023-10-23T19:22:05.960335Z",
                    "start_time": "2023-10-23T19:22:04.800667Z"
                }
            },
            "outputs": [
                {
                    "ename": "TypeError",
                    "evalue": "'Aggregation' object is not callable",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43manalyzer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_calculate_time\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
                        "Cell \u001b[0;32mIn[21], line 26\u001b[0m, in \u001b[0;36mDarshanAnalyzer._calculate_time\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_calculate_time\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m---> 26\u001b[0m     grouped_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevents\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrange\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mio_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43munion_portions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompute_time\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43munion_portions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     grouped_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_io\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m grouped_df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompute_time\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mapply(difference_portion, a\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, b\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompute_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, meta\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring[pyarrow]\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     28\u001b[0m     grouped_df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124monly_compute\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m grouped_df[[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompute_time\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\u001b[38;5;241m.\u001b[39mapply(difference_portion, a\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompute_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, b\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mio_time\u001b[39m\u001b[38;5;124m\"\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, meta\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstring[pyarrow]\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
                        "File \u001b[0;32m/usr/WS2/haridev/venvs/dftracer_venv/lib/python3.9/site-packages/pandas/core/groupby/generic.py:1442\u001b[0m, in \u001b[0;36mDataFrameGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1439\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m engine_kwargs\n\u001b[1;32m   1441\u001b[0m op \u001b[38;5;241m=\u001b[39m GroupByApply(\u001b[38;5;28mself\u001b[39m, func, args\u001b[38;5;241m=\u001b[39margs, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[0;32m-> 1442\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1443\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_dict_like(func) \u001b[38;5;129;01mand\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1444\u001b[0m     \u001b[38;5;66;03m# GH #52849\u001b[39;00m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mas_index \u001b[38;5;129;01mand\u001b[39;00m is_list_like(func):\n",
                        "File \u001b[0;32m/usr/WS2/haridev/venvs/dftracer_venv/lib/python3.9/site-packages/pandas/core/apply.py:175\u001b[0m, in \u001b[0;36mApply.agg\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dict_like(func):\n\u001b[0;32m--> 175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    176\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(func):\n\u001b[1;32m    177\u001b[0m     \u001b[38;5;66;03m# we require a list, but not a 'str'\u001b[39;00m\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magg_list_like()\n",
                        "File \u001b[0;32m/usr/WS2/haridev/venvs/dftracer_venv/lib/python3.9/site-packages/pandas/core/apply.py:406\u001b[0m, in \u001b[0;36mApply.agg_dict_like\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21magg_dict_like\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m    399\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m    Compute aggregation in the case of a dict-like argument.\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[38;5;124;03m    Result of aggregation.\u001b[39;00m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 406\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_or_apply_dict_like\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43magg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
                        "File \u001b[0;32m/usr/WS2/haridev/venvs/dftracer_venv/lib/python3.9/site-packages/pandas/core/apply.py:1388\u001b[0m, in \u001b[0;36mGroupByApply.agg_or_apply_dict_like\u001b[0;34m(self, op_name)\u001b[0m\n\u001b[1;32m   1383\u001b[0m     kwargs\u001b[38;5;241m.\u001b[39mupdate({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mengine_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: engine_kwargs})\n\u001b[1;32m   1385\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m com\u001b[38;5;241m.\u001b[39mtemp_setattr(\n\u001b[1;32m   1386\u001b[0m     obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m, condition\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mhasattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1387\u001b[0m ):\n\u001b[0;32m-> 1388\u001b[0m     result_index, result_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_dict_like\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1389\u001b[0m \u001b[43m        \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_obj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselection\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m   1390\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1391\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrap_results_dict_like(selected_obj, result_index, result_data)\n\u001b[1;32m   1392\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
                        "File \u001b[0;32m/usr/WS2/haridev/venvs/dftracer_venv/lib/python3.9/site-packages/pandas/core/apply.py:479\u001b[0m, in \u001b[0;36mApply.compute_dict_like\u001b[0;34m(self, op_name, selected_obj, selection, kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m         results \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m key_data\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;66;03m# key used for column selection and output\u001b[39;00m\n\u001b[0;32m--> 479\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    480\u001b[0m         \u001b[38;5;28mgetattr\u001b[39m(obj\u001b[38;5;241m.\u001b[39m_gotitem(key, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m), op_name)(how, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, how \u001b[38;5;129;01min\u001b[39;00m func\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    482\u001b[0m     ]\n\u001b[1;32m    483\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(func\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keys, results\n",
                        "File \u001b[0;32m/usr/WS2/haridev/venvs/dftracer_venv/lib/python3.9/site-packages/pandas/core/apply.py:480\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    476\u001b[0m         results \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m key_data\n\u001b[1;32m    477\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    478\u001b[0m     \u001b[38;5;66;03m# key used for column selection and output\u001b[39;00m\n\u001b[1;32m    479\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 480\u001b[0m         \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gotitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m key, how \u001b[38;5;129;01min\u001b[39;00m func\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    482\u001b[0m     ]\n\u001b[1;32m    483\u001b[0m     keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(func\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m    485\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keys, results\n",
                        "File \u001b[0;32m/usr/WS2/haridev/venvs/dftracer_venv/lib/python3.9/site-packages/pandas/core/groupby/generic.py:289\u001b[0m, in \u001b[0;36mSeriesGroupBy.aggregate\u001b[0;34m(self, func, engine, engine_kwargs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    286\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_python_agg_general(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    288\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_python_agg_general\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    291\u001b[0m     \u001b[38;5;66;03m# KeyError raised in test_groupby.test_basic is bc the func does\u001b[39;00m\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;66;03m#  a dictionary lookup on group.name, but group name is not\u001b[39;00m\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;66;03m#  pinned in _python_agg_general, only in _aggregate_named\u001b[39;00m\n\u001b[1;32m    294\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_aggregate_named(func, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
                        "File \u001b[0;32m/usr/WS2/haridev/venvs/dftracer_venv/lib/python3.9/site-packages/pandas/core/groupby/generic.py:322\u001b[0m, in \u001b[0;36mSeriesGroupBy._python_agg_general\u001b[0;34m(self, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    319\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: func(x, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    321\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\n\u001b[0;32m--> 322\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    323\u001b[0m res \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39m_constructor(result, name\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_aggregated_output(res)\n",
                        "File \u001b[0;32m/usr/WS2/haridev/venvs/dftracer_venv/lib/python3.9/site-packages/pandas/core/groupby/ops.py:850\u001b[0m, in \u001b[0;36mBaseGrouper.agg_series\u001b[0;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[1;32m    843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(obj) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39m_values, np\u001b[38;5;241m.\u001b[39mndarray):\n\u001b[1;32m    844\u001b[0m     \u001b[38;5;66;03m# we can preserve a little bit more aggressively with EA dtype\u001b[39;00m\n\u001b[1;32m    845\u001b[0m     \u001b[38;5;66;03m#  because maybe_cast_pointwise_result will do a try/except\u001b[39;00m\n\u001b[1;32m    846\u001b[0m     \u001b[38;5;66;03m#  with _from_sequence.  NB we are assuming here that _from_sequence\u001b[39;00m\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;66;03m#  is sufficiently strict that it casts appropriately.\u001b[39;00m\n\u001b[1;32m    848\u001b[0m     preserve_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 850\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aggregate_series_pure_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    852\u001b[0m npvalues \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(result, try_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    853\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m preserve_dtype:\n",
                        "File \u001b[0;32m/usr/WS2/haridev/venvs/dftracer_venv/lib/python3.9/site-packages/pandas/core/groupby/ops.py:871\u001b[0m, in \u001b[0;36mBaseGrouper._aggregate_series_pure_python\u001b[0;34m(self, obj, func)\u001b[0m\n\u001b[1;32m    868\u001b[0m splitter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_splitter(obj, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    870\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(splitter):\n\u001b[0;32m--> 871\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    872\u001b[0m     res \u001b[38;5;241m=\u001b[39m extract_result(res)\n\u001b[1;32m    874\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m initialized:\n\u001b[1;32m    875\u001b[0m         \u001b[38;5;66;03m# We only do this validation on the first iteration\u001b[39;00m\n",
                        "File \u001b[0;32m/usr/WS2/haridev/venvs/dftracer_venv/lib/python3.9/site-packages/pandas/core/groupby/generic.py:319\u001b[0m, in \u001b[0;36mSeriesGroupBy._python_agg_general.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    317\u001b[0m     alias \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39m_builtin_table_alias[func]\n\u001b[1;32m    318\u001b[0m     warn_alias_replacement(\u001b[38;5;28mself\u001b[39m, orig_func, alias)\n\u001b[0;32m--> 319\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obj_with_exclusions\n\u001b[1;32m    322\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgrouper\u001b[38;5;241m.\u001b[39magg_series(obj, f)\n",
                        "\u001b[0;31mTypeError\u001b[0m: 'Aggregation' object is not callable"
                    ]
                }
            ],
            "source": [
                "analyzer._calculate_time()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "7699fee2",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2023-10-23T19:22:06.033051Z",
                    "start_time": "2023-10-23T19:22:06.033035Z"
                }
            },
            "outputs": [],
            "source": [
                "analyzer.events[\"filename\"].unique().compute()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "0fa40f6f",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2023-10-23T19:22:06.034567Z",
                    "start_time": "2023-10-23T19:22:06.034552Z"
                }
            },
            "outputs": [],
            "source": [
                "analyzer.events.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "5cfc44e6",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2023-10-23T19:22:06.036289Z",
                    "start_time": "2023-10-23T19:22:06.036275Z"
                }
            },
            "outputs": [],
            "source": [
                "%%timeit -n 1 -r 10"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "db5d6020",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2023-10-23T19:22:06.038724Z",
                    "start_time": "2023-10-23T19:22:06.038706Z"
                },
                "scrolled": true
            },
            "outputs": [],
            "source": [
                "\n",
                "analyzer.summary()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "10f86abc",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2023-10-23T19:22:06.040174Z",
                    "start_time": "2023-10-23T19:22:06.040159Z"
                }
            },
            "outputs": [],
            "source": [
                "cat df_analyzer_main.log"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "00ccbefc",
            "metadata": {
                "ExecuteTime": {
                    "end_time": "2023-10-23T19:22:06.042327Z",
                    "start_time": "2023-10-23T19:22:06.042311Z"
                }
            },
            "outputs": [],
            "source": [
                "cat /usr/workspace/haridev/dftracer-results/cosmoflow-profile-compress-meta-scale-new-16/dlio.log | grep Ending"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "56b517d3",
            "metadata": {},
            "outputs": [],
            "source": []
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "DFT corona",
            "language": "python",
            "name": "df_corona"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9.12"
        },
        "varInspector": {
            "cols": {
                "lenName": 16,
                "lenType": 16,
                "lenVar": 40
            },
            "kernels_config": {
                "python": {
                    "delete_cmd_postfix": "",
                    "delete_cmd_prefix": "del ",
                    "library": "var_list.py",
                    "varRefreshCmd": "print(var_dic_list())"
                },
                "r": {
                    "delete_cmd_postfix": ") ",
                    "delete_cmd_prefix": "rm(",
                    "library": "var_list.r",
                    "varRefreshCmd": "cat(var_dic_list()) "
                }
            },
            "types_to_exclude": [
                "module",
                "function",
                "builtin_function_or_method",
                "instance",
                "_Feature"
            ],
            "window_display": false
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}