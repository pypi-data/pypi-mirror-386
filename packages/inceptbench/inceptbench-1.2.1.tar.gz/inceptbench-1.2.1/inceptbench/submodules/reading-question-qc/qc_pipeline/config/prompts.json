{
  "question_qc": {
    "distractor_checks": {
      "grammatical_parallel": {
        "prompt": "You are evaluating whether answer choices have consistent grammatical structure.\nQuestion: {question}\nAnswer Choices:\nA) {choice_A}\nB) {choice_B}\nC) {choice_C}\nD) {choice_D}\nTask: Determine if all choices follow the same grammatical pattern.\nExamples of GOOD parallelism:\nAll infinitive phrases: \"to make\", \"to carry\", \"to take\", \"to hold\"\nAll single words: \"happy\", \"sad\", \"angry\", \"excited\"\nAll noun phrases: \"the main character\", \"the setting\", \"the conflict\", \"the theme\"\nAll complete sentences: \"He was tired.\", \"He was hungry.\", \"He was lost.\", \"He was scared.\"\nExamples of BAD parallelism:\nMixed structures: \"to make\", \"carrying\", \"he takes\", \"holds it\"\nMixed lengths: \"run\", \"walking quickly\", \"to jump over the fence\", \"swimming\"\nIMPORTANT: Your goal is to fail questions that are unfair due to the grammatical structure of their options. Minor variations are to be accepted as long as they do not provide an unfair advantage or disadvantage.\n\nInstructions:\nEvaluate ONLY grammatical consistency. Return exactly one number:\n1 if all choices follow the same grammatical pattern\n0 if choices have inconsistent grammatical structures\n<quality_check>\n<score>1</score>\n<reasoning>Brief explanation of why all choices do or do not meet the criteria</reasoning>\n</quality_check>\n\nScore: 1 if criteria are met, 0 if criteria are not met",
        "response_format": "xml"
      },
      "plausibility": {
        "prompt": "You are evaluating whether incorrect answer choices are plausible distractors.\nPassage: {passage}\nQuestion: {question}\nAnswer Choices:\nA) {choice_A}\nB) {choice_B}\nC) {choice_C}\nD) {choice_D}\nCorrect Answer: {correct_answer}\nTask: Evaluate if each INCORRECT choice is a believable wrong answer.\nGOOD distractors:\nRepresent common misconceptions\nAre logically related to the question\nCould reasonably fool a student who partially understands\nBAD distractors:\nAre obviously wrong to any reasonable student\nAre completely unrelated to the question topic\nAre \"throwaway\" options with no educational value\nInstructions:\nConsider only the incorrect choices. Return exactly one number:\n1 if all incorrect choices are plausible distractors\n0 if any incorrect choice is obviously wrong or unrelated\n<quality_check>\n<score>1</score>\n<reasoning>Brief explanation of why all choices do or do not meet the criteria</reasoning>\n</quality_check>\n\nScore: 1 if criteria are met, 0 if criteria are not met",
        "response_format": "xml"
      },
      "homogeneity": {
        "prompt": "You are evaluating whether all answer choices belong to the same conceptual category.\nQuestion: {question}\nAnswer Choices:\nA) {choice_A}\nB) {choice_B}\nC) {choice_C}\nD) {choice_D}\nTask: Determine if all choices address the same type of concept.\nGOOD homogeneity examples:\nWord meaning question → All choices are possible definitions\nCharacter motivation question → All choices are possible motivations\nText structure question → All choices are structural elements\nMain idea question → All choices are potential main ideas\nBAD homogeneity examples:\nWord meaning question → Mix of definitions and plot events\nCharacter question → Mix of character traits and setting details\nMain idea question → Mix of themes and specific facts\nInstructions:\nReturn exactly one number:\n1 if all choices belong to the same conceptual category\n0 if choices span different conceptual categories\n<quality_check>\n<score>1</score>\n<reasoning>Brief explanation of why all choices do or do not meet the criteria</reasoning>\n</quality_check>\n\nScore: 1 if criteria are met, 0 if criteria are not met",
        "response_format": "xml"
      },
      "specificity_balance": {
        "prompt": "You are evaluating whether answer choices have similar levels of detail and specificity.\nQuestion: {question}\nAnswer Choices:\nA) {choice_A}\nB) {choice_B}\nC) {choice_C}\nD) {choice_D}\nTask: Determine if choices are at similar specificity levels.\nGOOD specificity balance:\nAll general: \"happy\", \"sad\", \"angry\", \"worried\"\nAll specific: \"photosynthesis\", \"respiration\", \"transpiration\", \"germination\"\nAll at same detail level: \"ran quickly\", \"walked slowly\", \"jumped high\", \"crawled carefully\"\nBAD specificity balance:\nMixed levels: \"sad\" vs \"experiencing deep melancholy\" vs \"blue\"\nOne outlier: Three simple words + one complex phrase\nTechnical vs casual: \"H2O\" vs \"water\" vs \"liquid\" vs \"beverage\"\nInstructions:\nReturn exactly one number:\n1 if all choices are at similar levels of specificity/detail\n0 if there are significant differences in specificity levels\n<quality_check>\n<score>1</score>\n<reasoning>Brief explanation of why all choices do or do not meet the criteria</reasoning>\n</quality_check>\n\nScore: 1 if criteria are met, 0 if criteria are not met",
        "response_format": "xml"
      },
      "too_close": {
        "prompt": "You are an expert in semantic analysis of educational questions. Your task is to identify questions where one or more distractors (incorrect options) are **too close** to the correct answer, making the question unfair or ambiguous.\n\nA distractor is **TOO CLOSE** if:\n1. **Synonymous or Near-Synonymous**: Uses different wording but conveys essentially the same meaning\n2. **Degree-Only Difference**: Differs only in degree/intensity when the passage doesn't clearly establish the distinction\n3. **Equally Text-Supported**: Both correct answer and distractor are equally supported by passage evidence\n4. **Double-Key Risk**: Two options could both be considered correct based on reasonable interpretation\n5. **Grade-Inappropriate Distinction**: The distinction requires knowledge beyond the target grade level\n\nAnalyze the question carefully and output **ONLY** valid JSON following this schema:\n{\n  \"too_close\": true,\n  \"problematic_options\": [\"A\", \"C\"],\n  \"explanation\": \"Options A and C are both synonymous and equally supported by lines 4-6.\",\n  \"notes\": \"Grade 4 students cannot distinguish between 'elated' and 'thrilled'.\"\n}\n\nTarget Grade: {grade}\n\nPassage:\n{passage}\n\nQuestion:\n{question}\n\nOptions:\nA) {option_a}\nB) {option_b}\nC) {option_c}\nD) {option_d}\n\nCorrect Answer (letter): {correct_letter}\n\nTask:\nIdentify whether ANY distractor(s) are TOO-CLOSE by the rubric. Return ONLY the JSON per schema.",
        "response_format": "json"
      }
    },
    "question_checks": {
      "standard_alignment": {
        "prompt": "You are evaluating whether this question properly assesses the assigned learning standard.\nPassage: {passage}\nQuestion: {question}\nStandard: {standard_code} - {standard_description}\nDOK Level: {dok}\nAnswer Choices:\nA) {choice_A}\nB) {choice_B}\nC) {choice_C}\nD) {choice_D}\nCommon Standards (Examples):\nRI.3.1: Ask and answer questions to demonstrate understanding, referring explicitly to text\nRI.3.2: Determine the main idea; recount key details\nRI.3.4: Determine meaning of words/phrases in text\nRL.3.1: Ask and answer questions about key details\nRL.3.3: Describe characters and explain how their actions contribute to story events\nTask: Determine if this question directly assesses the assigned standard.\nInstructions:\nCompare what the question is testing against what the standard requires. The question must directly assess the standard's specific skill, not a tangentially related skill.\nReturn exactly one number:\n1 if the question directly and appropriately assesses the assigned standard\n0 if the question assesses a different skill or misaligns with the standard\n<quality_check>\n<score>1</score>\n<reasoning>Brief explanation of why all choices do or do not meet the criteria</reasoning>\n</quality_check>\n\nScore: 1 if criteria are met, 0 if criteria are not met",
        "response_format": "xml"
      },
      "clarity_precision": {
        "prompt": "You are evaluating whether this question is clearly written and unambiguous.\nPassage: {passage}\nQuestion: {question}\nAnswer Choices:\nA) {choice_A}\nB) {choice_B}\nC) {choice_C}\nD) {choice_D}\nClarity Issues to Check:\nAmbiguous pronouns (unclear \"it\", \"this\", \"that\", \"they\")\nDouble negatives\nOverly complex sentence structure\nVague or imprecise language\nMultiple possible interpretations\nUnnecessary wordiness\nTask: Determine if the question is clear and precise.\nInstructions:\nEvaluate whether a student would understand exactly what is being asked. The question should have one clear interpretation and be written at appropriate complexity for the grade level.\nReturn exactly one number:\n1 if the question is clear, precise, and unambiguous\n0 if the question has clarity issues that could confuse students\n<quality_check>\n<score>1</score>\n<reasoning>Brief explanation of why all choices do or do not meet the criteria</reasoning>\n</quality_check>\n\nScore: 1 if criteria are met, 0 if criteria are not met",
        "response_format": "xml"
      },
      "single_correct_answer": {
        "prompt": "You are validating that this multiple choice question has exactly one defensibly correct answer.\nQuestion: {question}\nPassage: {passage}\nAnswer Choices:\nA) {choice_A}\nB) {choice_B}\nC) {choice_C}\nD) {choice_D}\nIndicated Correct Answer: {correct_answer}\nTask: Verify there is exactly one correct answer that can be defended based on the passage.\nRed Flags:\nMultiple answers could be argued as correct\nCorrect answer requires unsupported inferences\nQuestion asks for opinions as if they were facts\nPassage contradicts itself on the topic\nInstructions:\nBased on the passage, determine if exactly one answer is clearly correct and the others are clearly incorrect.\nReturn exactly one number:\n1 if there is exactly one defensibly correct answer\n0 if multiple answers could be correct or no answer is clearly correct\n<quality_check>\n<score>1</score>\n<reasoning>Brief explanation of why all choices do or do not meet the criteria</reasoning>\n</quality_check>\n\nScore: 1 if criteria are met, 0 if criteria are not met",
        "response_format": "xml"
      },
      "passage_reference": {
        "prompt": "You are verifying that specific passage references in the question are accurate and exist.\nQuestion: {question}\nPassage: {passage}\nTask: Verify that any specific references to passage elements are accurate.\nTypes of References to Check:\nParagraph numbers (e.g., \"In paragraph 3...\")\nLine numbers (e.g., \"Line 15 shows...\")\nSection titles (e.g., \"The section titled 'Getting Started'...\")\nSpecific quotes or phrases referenced\nPage numbers or other structural elements\nVerification Process:\n1. Identify any specific structural references in the question\n2. Check if those elements actually exist in the passage\n3. Confirm the references are accurate and accessible\nExamples of GOOD references:\n\"In paragraph 2...\" when passage has 4 paragraphs\n\"The section titled 'Materials Needed'...\" when that section exists\n\"According to the last paragraph...\" when referring to final paragraph\nExamples of BAD references:\n\"In paragraph 5...\" when passage only has 3 paragraphs\n\"The section titled 'Conclusion'...\" when no such section exists\n\"Line 20...\" when passage only has 15 lines\nInstructions:\nCheck if all specific passage references in the question are accurate and correspond to actual elements in the passage.\nReturn exactly one number:\n1 if all passage references are accurate and exist; OR if the question does not contain passage references\n0 if any passage reference is inaccurate or doesn't exist\n<quality_check>\n<score>1</score>\n<reasoning>Brief explanation of why all choices do or do not meet the criteria</reasoning>\n</quality_check>\n\nScore: 1 if criteria are met, 0 if criteria are not met",
        "response_format": "xml"
      },
      "difficulty_assessment": {
        "prompt": "You are an expert in educational assessment and psychometrics, with a specialization in analyzing the cognitive complexity and difficulty of test questions for K-12 education. Your task is to determine if a candidate test question is appropriate for its intended grade level.\n\nYou will be given the following information:\n1.  **Candidate Question:** The question to be evaluated.\n2.  **Intended Grade Level:** The grade for which the question is intended.\n3.  **Example Questions:** A set of the 5 most semantically similar validated test questions that are considered benchmarks for the specified grade level.\n\nYour analysis must be based on two primary dimensions:\n1.  **Level of Inference Required:** The cognitive steps a student must take to arrive at the correct answer (Low, Moderate, High).\n2.  **Distractor Difficulty:** The quality and plausibility of the incorrect answer choices (Weak, Plausible, Strong).\n\n**Process:**\n1.  **Analyze the Candidate Question:** Assess its inference level and distractor difficulty.\n2.  **Analyze the Example Questions:** Establish a baseline for the typical cognitive demands for this grade level based on these highly similar examples.\n3.  **Compare and Conclude:** Compare the candidate to the baseline and make a final judgment.\n4.  **Provide a Rationale:** Justify your reasoning clearly.\n\n**Output Format:**\n**Judgment:** [Appropriate / Too Hard / Too Easy]\n**Rationale:**\n* **Candidate Question Analysis:**\n    * **Inference Level:** [Your assessment and brief explanation]\n    * **Distractor Difficulty:** [Your assessment and brief explanation]\n* **Comparison to Grade Level Examples:**\n    * [Your comparative analysis, explaining how the candidate question's demands align with or deviate from the benchmark examples.]",
        "response_format": "text"
      }
    }
  },
  "explanation_qc": {
    "correct": {
      "01_correctness_explanation": {
        "prompt": "You are evaluating feedback for a correct answer choice in a reading comprehension question.\nTASK: Determine if the feedback explicitly states WHY the answer is correct.\nEVALUATION CRITERIA:\nThe feedback must clearly articulate the core logic behind why this specific answer choice is correct\nIt should explain the reasoning process, not just confirm correctness\nGeneric statements like \"this is right\" or \"good job\" do NOT count\nThe explanation should be specific to this particular question and answer\nINPUT:\nQuestion: [QUESTION]\nCorrect Answer: [CORRECT_ANSWER]\nPassage: [PASSAGE]\nFeedback: [FEEDBACK_TO_EVALUATE]\nOUTPUT FORMAT:\nScore: [0 or 1]\nReasoning: [One sentence explaining why you gave this score]",
        "response_format": "json"
      },
      "02_textual_evidence": {
        "prompt": "You are evaluating feedback for a correct answer choice in a reading comprehension question.\nTASK: Determine if the feedback quotes or references specific supporting evidence from the passage.\nEVALUATION CRITERIA:\nThe feedback must include direct quotes, paraphrases, or specific references to passage content\nVague references like \"the text shows\" without specifics do NOT count\nThe evidence cited must actually support the correct answer\nReferences should be accurate to the actual passage content\nINPUT:\nQuestion: [QUESTION]\nCorrect Answer: [CORRECT_ANSWER]\nPassage: [PASSAGE]\nFeedback: [FEEDBACK_TO_EVALUATE]\nOUTPUT FORMAT:\nScore: [0 or 1]\nReasoning: [One sentence explaining why you gave this score]",
        "response_format": "json"
      },
      "03_skill_reinforcement": {
        "prompt": "You are evaluating feedback for a correct answer choice in a reading comprehension question.\nTASK: Determine if the feedback names the specific reading skill or strategy the student successfully used.\nEVALUATION CRITERIA:\nThe feedback must explicitly identify the reading comprehension skill (e.g., \"main idea,\" \"inference,\" \"supporting details,\" \"author's purpose\")\nGeneric praise like \"good reading\" or \"nice work\" does NOT count\nThe skill named should be accurate for the question type\nThe skill should be named clearly, not just implied\nINPUT:\nQuestion: [QUESTION]\nCorrect Answer: [CORRECT_ANSWER]\nFeedback: [FEEDBACK_TO_EVALUATE]\nOUTPUT FORMAT:\nScore: [0 or 1]\nReasoning: [One sentence explaining why you gave this score]",
        "response_format": "json"
      }
    },
    "distractor": {
      "04_specific_error": {
        "prompt": "You are evaluating feedback for an incorrect answer choice in a reading comprehension question.\nTASK: Determine if the feedback explains WHY this specific choice is incorrect.\nEVALUATION CRITERIA:\nThe feedback must address the particular distractor chosen, not give generic \"wrong answer\" responses\nIt should explain what makes this specific option incorrect\nThe explanation should be tailored to this distractor's unique error\nGeneric statements like \"this is wrong\" or \"try again\" do NOT count\nINPUT:\nQuestion: [QUESTION]\nIncorrect Answer: [DISTRACTOR_CHOICE]\nCorrect Answer: [CORRECT_ANSWER]\nPassage: [PASSAGE]\nFeedback: [FEEDBACK_TO_EVALUATE]\nOUTPUT FORMAT:\nScore: [0 or 1]\nReasoning: [One sentence explaining why you gave this score]",
        "response_format": "json"
      },
      "05_misconception_diagnosis": {
        "prompt": "You are evaluating feedback for an incorrect answer choice in a reading comprehension question.\nTASK: Determine if the feedback identifies the likely misconception or error type behind the student's choice.\nEVALUATION CRITERIA:\nThe feedback must categorize or name the type of thinking error (e.g., \"literal interpretation,\" \"overgeneralization,\" \"detail confusion\")\nIt should diagnose WHY a student might logically choose this wrong answer\nThe misconception should be accurately identified for this error type\nSimply saying \"mistake\" or \"error\" without diagnosis does NOT count\nINPUT:\nQuestion: [QUESTION]\nIncorrect Answer: [DISTRACTOR_CHOICE]\nCorrect Answer: [CORRECT_ANSWER]\nPassage: [PASSAGE]\nFeedback: [FEEDBACK_TO_EVALUATE]\nOUTPUT FORMAT:\nScore: [0 or 1]\nReasoning: [One sentence explaining why you gave this score]",
        "response_format": "json"
      },
      "06_textual_refutation": {
        "prompt": "You are evaluating feedback for an incorrect answer choice in a reading comprehension question.\nTASK: Determine if the feedback uses textual evidence to refute or contradict the incorrect choice.\nEVALUATION CRITERIA:\nThe feedback must cite specific passage content that contradicts the wrong answer\nIt should include direct quotes, paraphrases, or specific references that disprove the distractor\nThe textual evidence must actually contradict the incorrect choice\nVague references like \"the text disagrees\" without specifics do NOT count\nINPUT:\nQuestion: [QUESTION]\nIncorrect Answer: [DISTRACTOR_CHOICE]\nCorrect Answer: [CORRECT_ANSWER]\nPassage: [PASSAGE]\nFeedback: [FEEDBACK_TO_EVALUATE]\nOUTPUT FORMAT:\nScore: [0 or 1]\nReasoning: [One sentence explaining why you gave this score]",
        "response_format": "json"
      },
      "07_correct_guidance": {
        "prompt": "You are evaluating feedback for an incorrect answer choice in a reading comprehension question.\nTASK: Determine if the feedback states or guides the student toward the correct answer.\nEVALUATION CRITERIA:\nThe feedback must either explicitly state the correct answer or clearly guide the student to it\nThe student should leave knowing what the right answer is and why\nGuidance can be direct (\"The answer is [correct option text]\") or indirect but clear direction\nThe correct information must be accurate\nINPUT:\nQuestion: [QUESTION]\nIncorrect Answer: [DISTRACTOR_CHOICE]\nCorrect Answer: [CORRECT_ANSWER]\nAll Answer Choices: [ALL_OPTIONS]\nFeedback: [FEEDBACK_TO_EVALUATE]\nOUTPUT FORMAT:\nScore: [0 or 1]\nReasoning: [One sentence explaining why you gave this score]",
        "response_format": "json"
      },
      "08_actionable_strategy": {
        "prompt": "You are evaluating feedback for an incorrect answer choice in a reading comprehension question.\nTASK: Determine if the feedback provides a constructive tip that the student can apply to future questions.\nEVALUATION CRITERIA:\nThe feedback must include forward-looking advice or strategy for similar questions\nThe tip should be specific and actionable, not vague encouragement\nIt should help the student avoid this type of error in the future\nGeneric advice like \"read more carefully\" does NOT count as actionable\nINPUT:\nQuestion: [QUESTION]\nIncorrect Answer: [DISTRACTOR_CHOICE]\nFeedback: [FEEDBACK_TO_EVALUATE]\nOUTPUT FORMAT:\nScore: [0 or 1]\nReasoning: [One sentence explaining why you gave this score]",
        "response_format": "json"
      },
      "09_reasoning_model": {
        "prompt": "You are evaluating feedback for an incorrect answer choice in a reading comprehension question.\nTASK: Determine if the feedback models or demonstrates the correct thought process for arriving at the right answer.\nEVALUATION CRITERIA:\nThe feedback must walk through the expert reasoning process step-by-step\nIt should show HOW to think about this type of question correctly\nThe thinking process should be explicit and followable\nSimply stating the correct answer without showing the reasoning does NOT count\nINPUT:\nQuestion: [QUESTION]\nIncorrect Answer: [DISTRACTOR_CHOICE]\nCorrect Answer: [CORRECT_ANSWER]\nPassage: [PASSAGE]\nFeedback: [FEEDBACK_TO_EVALUATE]\nOUTPUT FORMAT:\nScore: [0 or 1]\nReasoning: [One sentence explaining why you gave this score]",
        "response_format": "json"
      }
    },
    "all": {
      "10_tone": {
        "prompt": "You are evaluating feedback for a reading comprehension question answer choice.\nTASK: Determine if the feedback uses encouraging, supportive language that builds confidence.\nEVALUATION CRITERIA:\nThe tone should be positive and motivating, even when correcting errors\nLanguage should be supportive and focused on learning, not criticism\nFor correct answers: celebratory and affirming\nFor incorrect answers: neutral but still encouraging about the learning process\nAvoid harsh, dismissive, or discouraging language\nINPUT:\nQuestion: [QUESTION]\nAnswer Choice: [ANSWER_CHOICE]\nIs Correct: [YES/NO]\nFeedback: [FEEDBACK_TO_EVALUATE]\nOUTPUT FORMAT:\nScore: [0 or 1]\nReasoning: [One sentence explaining why you gave this score]",
        "response_format": "json"
      },
      "11_conciseness": {
        "prompt": "You are evaluating feedback for a reading comprehension question answer choice.\nTASK: Determine if the feedback is concise (1-4 sentences) and avoids cognitive overload.\nEVALUATION CRITERIA:\nThe feedback should be between 1-4 complete sentences\nInformation should be focused and digestible\nAvoid unnecessary repetition or overly complex explanations\nEach sentence should serve a clear purpose\nThe feedback should cover essential points without being overwhelming\nINPUT:\nQuestion: [QUESTION]\nAnswer Choice: [ANSWER_CHOICE]\nFeedback: [FEEDBACK_TO_EVALUATE]\nOUTPUT FORMAT:\nScore: [0 or 1]\nReasoning: [One sentence explaining why you gave this score]",
        "response_format": "json"
      },
      "12_grade_appropriateness": {
        "prompt": "You are evaluating feedback for a reading comprehension question answer choice.\nTASK: Determine if the feedback language is suitable for the specified target grade level.\nEVALUATION CRITERIA FOR GRADE LEVELS:\nElementary (3-5): Simple sentence structures, basic vocabulary, concrete language, familiar concepts\nMiddle (6-8): More complex sentences, academic vocabulary with context, some abstract concepts\nHigh (9-12): Sophisticated analysis terms, complex sentence structures, advanced academic vocabulary\nThe feedback vocabulary, sentence complexity, and concepts should match what students at this grade level can understand.\nINPUT:\nTarget Grade Level: [GRADE_LEVEL]\nQuestion: [QUESTION]\nAnswer Choice: [ANSWER_CHOICE]\nFeedback: [FEEDBACK_TO_EVALUATE]\nOUTPUT FORMAT:\nScore: [0 or 1]\nReasoning: [One sentence explaining why you gave this score]",
        "response_format": "json"
      }
    }
  }
}