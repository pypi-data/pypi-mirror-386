# Dataset Toolkit

ä¸€ä¸ªåŠŸèƒ½å¼ºå¤§ã€æ˜“äºä½¿ç”¨çš„ Python å·¥å…·åŒ…ï¼Œç”¨äºå¤„ç†è®¡ç®—æœºè§†è§‰æ•°æ®é›†ã€‚æ”¯æŒå¤šç§æ•°æ®æ ¼å¼çš„åŠ è½½ã€åˆå¹¶ã€è½¬æ¢å’Œå¯¼å‡ºã€‚

## âœ¨ ç‰¹æ€§

- ğŸ”„ **å¤šæ ¼å¼æ”¯æŒ**ï¼šæ”¯æŒ YOLOã€COCO ç­‰å¸¸è§æ ¼å¼
- ğŸ”— **æ•°æ®é›†åˆå¹¶**ï¼šè½»æ¾åˆå¹¶å¤šä¸ªæ•°æ®é›†ï¼Œæ”¯æŒç±»åˆ«é‡æ˜ å°„
- ğŸ“¤ **çµæ´»å¯¼å‡º**ï¼šå¯¼å‡ºä¸º COCO JSONã€TXT ç­‰å¤šç§æ ¼å¼
- ğŸ› ï¸ **å·¥å…·å‡½æ•°**ï¼šæä¾›åæ ‡è½¬æ¢ç­‰å®ç”¨å·¥å…·
- ğŸ“¦ **æ ‡å‡†åŒ–æ•°æ®æ¨¡å‹**ï¼šç»Ÿä¸€çš„å†…éƒ¨æ•°æ®è¡¨ç¤ºï¼Œæ–¹ä¾¿æ‰©å±•
- ğŸ“Š **æ¨¡å‹è¯„ä¼°**ï¼šå®Œæ•´çš„ç›®æ ‡æ£€æµ‹æ¨¡å‹è¯„ä¼°ç³»ç»Ÿï¼ˆv0.2.0+ï¼‰

## ğŸ“¦ å®‰è£…

### ä» PyPI å®‰è£…ï¼ˆæ¨èï¼‰

```bash
pip install dataset-toolkit
```

### ä»æºç å®‰è£…

```bash
git clone https://github.com/yourusername/dataset-toolkit.git
cd dataset-toolkit
pip install -e .
```

### å¼€å‘æ¨¡å¼å®‰è£…

```bash
pip install -e ".[dev]"
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åŸºæœ¬ç”¨æ³•

```python
from dataset_toolkit import load_yolo_from_local, merge_datasets, export_to_coco

# 1. åŠ è½½ YOLO æ ¼å¼æ•°æ®é›†
dataset1 = load_yolo_from_local(
    "/path/to/dataset1",
    categories={0: 'cat', 1: 'dog'}
)

dataset2 = load_yolo_from_local(
    "/path/to/dataset2",
    categories={0: 'car', 1: 'bicycle'}
)

# 2. åˆå¹¶æ•°æ®é›†ï¼ˆå¸¦ç±»åˆ«é‡æ˜ å°„ï¼‰
final_categories = {0: 'animal', 1: 'vehicle'}
category_mapping = {
    'cat': 'animal',
    'dog': 'animal',
    'car': 'vehicle',
    'bicycle': 'vehicle'
}

merged = merge_datasets(
    datasets=[dataset1, dataset2],
    category_mapping=category_mapping,
    final_categories=final_categories,
    new_dataset_name="merged_dataset"
)

# 3. å¯¼å‡ºä¸º COCO æ ¼å¼
export_to_coco(merged, "output/merged.json")
```

### é“¾å¼ API ç”¨æ³•

```python
from dataset_toolkit import DatasetPipeline

# ä½¿ç”¨ç®¡é“æ¨¡å¼å¤„ç†æ•°æ®é›†
pipeline = DatasetPipeline()
result = (pipeline
    .load_yolo("/path/to/dataset1", {0: 'cat', 1: 'dog'})
    .load_yolo("/path/to/dataset2", {0: 'car'})
    .merge(
        category_mapping={'cat': 'animal', 'dog': 'animal', 'car': 'vehicle'},
        final_categories={0: 'animal', 1: 'vehicle'}
    )
    .export_coco("output/merged.json")
    .execute())
```

### æ¨¡å‹è¯„ä¼°ï¼ˆv0.2.0+ï¼‰

```python
from dataset_toolkit import (
    load_yolo_from_local,
    load_predictions_from_streamlined,
    Evaluator
)

# 1. åŠ è½½GTå’Œé¢„æµ‹ç»“æœ
gt_dataset = load_yolo_from_local("/data/test/labels", {0: 'parcel'})
pred_dataset = load_predictions_from_streamlined(
    "/results/predictions",
    categories={0: 'parcel'},
    image_dir="/data/test/images"
)

# 2. åˆ›å»ºè¯„ä¼°å™¨
evaluator = Evaluator(
    positive_gt=gt_dataset,
    positive_pred=pred_dataset,
    iou_threshold=0.5
)

# 3. è®¡ç®—æŒ‡æ ‡
metrics = evaluator.calculate_metrics(confidence_threshold=0.5)
print(f"Precision: {metrics['precision']:.4f}")
print(f"Recall: {metrics['recall']:.4f}")
print(f"F1-Score: {metrics['f1']:.4f}")

# 4. å¯»æ‰¾æœ€ä¼˜é˜ˆå€¼
optimal = evaluator.find_optimal_threshold(metric='f1')
print(f"æœ€ä¼˜é˜ˆå€¼: {optimal['optimal_threshold']}")
```

è¯¦ç»†æ–‡æ¡£è¯·å‚è€ƒ [EVALUATION_GUIDE.md](EVALUATION_GUIDE.md)

## ğŸ“š API æ–‡æ¡£

### æ•°æ®åŠ è½½å™¨

#### `load_yolo_from_local(dataset_path, categories)`

ä»æœ¬åœ°æ–‡ä»¶ç³»ç»ŸåŠ è½½ YOLO æ ¼å¼çš„æ•°æ®é›†ã€‚

**å‚æ•°ï¼š**
- `dataset_path` (str): æ•°æ®é›†æ ¹ç›®å½•è·¯å¾„ï¼Œåº”åŒ…å« `images/` å’Œ `labels/` å­ç›®å½•
- `categories` (Dict[int, str]): ç±»åˆ«IDåˆ°ç±»åˆ«åçš„æ˜ å°„

**è¿”å›ï¼š**
- `Dataset`: æ ‡å‡†åŒ–çš„æ•°æ®é›†å¯¹è±¡

**ç¤ºä¾‹ï¼š**
```python
dataset = load_yolo_from_local(
    "/data/my_dataset",
    categories={0: 'person', 1: 'car'}
)
```

### æ•°æ®å¤„ç†å™¨

#### `merge_datasets(datasets, category_mapping, final_categories, new_dataset_name)`

åˆå¹¶å¤šä¸ªæ•°æ®é›†ï¼Œæ”¯æŒç±»åˆ«é‡æ˜ å°„ã€‚

**å‚æ•°ï¼š**
- `datasets` (List[Dataset]): è¦åˆå¹¶çš„æ•°æ®é›†åˆ—è¡¨
- `category_mapping` (Dict[str, str]): æ—§ç±»åˆ«ååˆ°æ–°ç±»åˆ«åçš„æ˜ å°„
- `final_categories` (Dict[int, str]): æœ€ç»ˆçš„ç±»åˆ«ä½“ç³»
- `new_dataset_name` (str, optional): åˆå¹¶åæ•°æ®é›†çš„åç§°

**è¿”å›ï¼š**
- `Dataset`: åˆå¹¶åçš„æ•°æ®é›†å¯¹è±¡

### æ•°æ®å¯¼å‡ºå™¨

#### `export_to_coco(dataset, output_path)`

å¯¼å‡ºä¸º COCO JSON æ ¼å¼ã€‚

**å‚æ•°ï¼š**
- `dataset` (Dataset): è¦å¯¼å‡ºçš„æ•°æ®é›†
- `output_path` (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„

#### `export_to_txt(dataset, output_path, use_relative_paths, base_path)`

å¯¼å‡ºä¸º TXT æ ¼å¼ã€‚

**å‚æ•°ï¼š**
- `dataset` (Dataset): è¦å¯¼å‡ºçš„æ•°æ®é›†
- `output_path` (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„
- `use_relative_paths` (bool, optional): æ˜¯å¦ä½¿ç”¨ç›¸å¯¹è·¯å¾„
- `base_path` (str, optional): ç›¸å¯¹è·¯å¾„çš„åŸºå‡†ç›®å½•

## ğŸ—ï¸ æ¶æ„è®¾è®¡

```
dataset_toolkit/
â”œâ”€â”€ models.py              # æ•°æ®æ¨¡å‹å®šä¹‰
â”œâ”€â”€ loaders/              # æ•°æ®åŠ è½½å™¨
â”‚   â”œâ”€â”€ local_loader.py   # æœ¬åœ°æ–‡ä»¶ç³»ç»ŸåŠ è½½å™¨
â”‚   â””â”€â”€ remote_loader.py  # è¿œç¨‹æ•°æ®æºåŠ è½½å™¨ï¼ˆå¾…å¼€å‘ï¼‰
â”œâ”€â”€ processors/           # æ•°æ®å¤„ç†å™¨
â”‚   â”œâ”€â”€ merger.py         # æ•°æ®é›†åˆå¹¶
â”‚   â””â”€â”€ filter.py         # æ•°æ®è¿‡æ»¤ï¼ˆå¾…å¼€å‘ï¼‰
â”œâ”€â”€ exporters/            # æ•°æ®å¯¼å‡ºå™¨
â”‚   â”œâ”€â”€ coco_exporter.py  # COCOæ ¼å¼å¯¼å‡º
â”‚   â””â”€â”€ txt_exporter.py   # TXTæ ¼å¼å¯¼å‡º
â””â”€â”€ utils/                # å·¥å…·å‡½æ•°
    â””â”€â”€ coords.py         # åæ ‡è½¬æ¢
```

## ğŸ”§ é«˜çº§ç”¨æ³•

### è‡ªå®šä¹‰æ•°æ®åŠ è½½å™¨

```python
from dataset_toolkit.models import Dataset, ImageAnnotation
from dataset_toolkit.loaders import BaseLoader

class CustomLoader(BaseLoader):
    def load(self, path, **kwargs):
        # å®ç°ä½ çš„è‡ªå®šä¹‰åŠ è½½é€»è¾‘
        dataset = Dataset(name="custom")
        # ... åŠ è½½æ•°æ® ...
        return dataset
```

### æ‰¹é‡å¤„ç†

```python
from pathlib import Path
from dataset_toolkit import load_yolo_from_local, export_to_coco

# æ‰¹é‡å¤„ç†å¤šä¸ªæ•°æ®é›†
dataset_dirs = [
    "/data/dataset1",
    "/data/dataset2",
    "/data/dataset3"
]

categories = {0: 'object'}

for dataset_dir in dataset_dirs:
    ds = load_yolo_from_local(dataset_dir, categories)
    output_name = Path(dataset_dir).name + ".json"
    export_to_coco(ds, f"output/{output_name}")
```

## ğŸ§ª æµ‹è¯•

è¿è¡Œæµ‹è¯•ï¼š

```bash
pytest
```

ç”Ÿæˆæµ‹è¯•è¦†ç›–ç‡æŠ¥å‘Šï¼š

```bash
pytest --cov=dataset_toolkit --cov-report=html
```

## ğŸ“ å¼€å‘è®¡åˆ’

- [ ] æ”¯æŒæ›´å¤šæ•°æ®æ ¼å¼ï¼ˆPascal VOCã€YOLO v8ç­‰ï¼‰
- [ ] æ·»åŠ æ•°æ®å¢å¼ºåŠŸèƒ½
- [ ] æ”¯æŒè¿œç¨‹æ•°æ®æºï¼ˆS3ã€HTTPç­‰ï¼‰
- [ ] æ·»åŠ æ•°æ®ç»Ÿè®¡å’Œå¯è§†åŒ–åŠŸèƒ½
- [ ] æä¾›å‘½ä»¤è¡Œå·¥å…·
- [ ] æ”¯æŒè§†é¢‘æ•°æ®é›†

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤ Issue å’Œ Pull Requestï¼

## ğŸ“„ è®¸å¯è¯

MIT License

## ğŸ“§ è”ç³»æ–¹å¼

å¦‚æœ‰é—®é¢˜ï¼Œè¯·é€šè¿‡ä»¥ä¸‹æ–¹å¼è”ç³»ï¼š
- Email: your.email@example.com
- GitHub Issues: https://github.com/yourusername/dataset-toolkit/issues

