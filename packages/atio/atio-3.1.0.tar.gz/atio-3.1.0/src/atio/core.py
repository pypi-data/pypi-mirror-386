# Copyright 2025 Atio Developers
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import os
import tempfile
import threading
import time
import numpy as np
from queue import Queue
from .plugins import get_writer
from .utils import setup_logger, ProgressBar, FileLock

def write(obj, target_path=None, format=None, show_progress=False, verbose=False, **kwargs):
    """
    ë°ì´í„° ê°ì²´(obj)ë¥¼ ì•ˆì „í•˜ê²Œ target_path ë˜ëŠ” ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥í•©ë‹ˆë‹¤.

    - íŒŒì¼ ê¸°ë°˜ ì“°ê¸° (format: 'csv', 'parquet', 'excel' ë“±):
      - target_path (str): í•„ìˆ˜. ë°ì´í„°ê°€ ì €ì¥ë  íŒŒì¼ ê²½ë¡œì…ë‹ˆë‹¤.
      - ë¡¤ë°± ê¸°ëŠ¥ì´ ìˆëŠ” ì›ìì  ì“°ê¸°ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.

    - ë°ì´í„°ë² ì´ìŠ¤ ê¸°ë°˜ ì“°ê¸° (format: 'sql', 'database'):
      - target_path: ì‚¬ìš©ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
      - kwargs (dict): ë°ì´í„°ë² ì´ìŠ¤ ì“°ê¸°ì— í•„ìš”í•œ ì¶”ê°€ ì¸ìë“¤ì…ë‹ˆë‹¤.
        - pandas.to_sql: 'name'(í…Œì´ë¸”ëª…), 'con'(ì»¤ë„¥ì…˜ ê°ì²´)ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.
        - polars.write_database: 'table_name', 'connection_uri'ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.
    
    Args:
        obj: ì €ì¥í•  ë°ì´í„° ê°ì²´ (e.g., pandas.DataFrame, polars.DataFrame, np.ndarray).
        target_path (str, optional): íŒŒì¼ ì €ì¥ ê²½ë¡œ. íŒŒì¼ ê¸°ë°˜ ì“°ê¸° ì‹œ í•„ìˆ˜. Defaults to None.
        format (str, optional): ì €ì¥í•  í¬ë§·. Defaults to None.
        show_progress (bool): ì§„í–‰ë„ í‘œì‹œ ì—¬ë¶€. Defaults to False.
        verbose (bool): ìƒì„¸í•œ ì„±ëŠ¥ ì§„ë‹¨ ì •ë³´ ì¶œë ¥ ì—¬ë¶€. Defaults to False.
        **kwargs: ê° ì“°ê¸° í•¨ìˆ˜ì— ì „ë‹¬ë  ì¶”ê°€ í‚¤ì›Œë“œ ì¸ì.
    """
    logger = setup_logger(debug_level=verbose)
    t0 = time.perf_counter()

    # --- 1. ë°ì´í„°ë² ì´ìŠ¤ ì“°ê¸° íŠ¹ë³„ ì²˜ë¦¬ ---
    # ë°ì´í„°ë² ì´ìŠ¤ ì“°ê¸°ëŠ” íŒŒì¼ ê²½ë¡œ ê¸°ë°˜ì˜ ì›ìì  ì“°ê¸° ë¡œì§ì„ ë”°ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤.
    if format in ('sql', 'database'):
        logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì“°ê¸° ëª¨ë“œ ì‹œì‘ (format: {format})")
        writer_method_name = get_writer(obj, format)
        
        if writer_method_name is None:
            err_msg = f"ê°ì²´ íƒ€ì… {type(obj).__name__}ì— ëŒ€í•´ ì§€ì›í•˜ì§€ ì•ŠëŠ” format: {format}"
            logger.error(err_msg)
            raise ValueError(err_msg)

        try:
            writer_func = getattr(obj, writer_method_name)
            
            # ê° ë°ì´í„°ë² ì´ìŠ¤ ì“°ê¸° í•¨ìˆ˜ì— í•„ìš”í•œ í•„ìˆ˜ ì¸ì í™•ì¸
            if format == 'sql': # Pandas
                if 'name' not in kwargs or 'con' not in kwargs:
                    raise ValueError("'name'(í…Œì´ë¸”ëª…)ê³¼ 'con'(DB ì»¤ë„¥ì…˜) ì¸ìëŠ” 'sql' í¬ë§·ì— í•„ìˆ˜ì…ë‹ˆë‹¤.")
            elif format == 'database': # Polars
                if 'table_name' not in kwargs or 'connection_uri' not in kwargs:
                    raise ValueError("'table_name'ê³¼ 'connection_uri' ì¸ìëŠ” 'database' í¬ë§·ì— í•„ìˆ˜ì…ë‹ˆë‹¤.")
                
                # Polars write_databaseëŠ” 'connection' íŒŒë¼ë¯¸í„°ë¥¼ ì‚¬ìš©í•˜ë¯€ë¡œ ë³€í™˜
                kwargs['connection'] = kwargs.pop('connection_uri')
            
            # target_pathëŠ” ë¬´ì‹œí•˜ê³  **kwargsë¡œ ë°›ì€ ì¸ìë“¤ì„ ì‚¬ìš©í•˜ì—¬ DBì— ì§ì ‘ ì”ë‹ˆë‹¤.
            writer_func(**kwargs)
            
            t_end = time.perf_counter()
            logger.info(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì“°ê¸° ì™„ë£Œ (ì´ ì†Œìš” ì‹œê°„: {t_end - t0:.4f}s)")
            return # DB ì“°ê¸° ì™„ë£Œ í›„ í•¨ìˆ˜ ì¢…ë£Œ

        except Exception as e:
            t_err = time.perf_counter()
            logger.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì“°ê¸° ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            logger.info(f"ë°ì´í„°ë² ì´ìŠ¤ ì“°ê¸° ì‹¤íŒ¨ (ì†Œìš” ì‹œê°„: {t_err - t0:.4f}s, ì—ëŸ¬: {type(e).__name__})")
            raise e

    # --- 2. íŒŒì¼ ê¸°ë°˜ ì›ìì  ì“°ê¸° ---
    if target_path is None:
        raise ValueError("íŒŒì¼ ê¸°ë°˜ ì“°ê¸°(ì˜ˆ: 'csv', 'parquet')ì—ëŠ” 'target_path' ì¸ìê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.")

    dir_name = os.path.dirname(os.path.abspath(target_path))
    base_name = os.path.basename(target_path)
    os.makedirs(dir_name, exist_ok=True)

    # ë¡¤ë°±ì„ ìœ„í•œ ë°±ì—… ê²½ë¡œ ì„¤ì •
    backup_path = target_path + "._backup"
    original_exists = os.path.exists(target_path)
    
    with tempfile.TemporaryDirectory(dir=dir_name) as tmpdir:
        tmp_path = os.path.join(tmpdir, base_name)
        logger.info(f"ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±: {tmpdir}")
        logger.info(f"ì„ì‹œ íŒŒì¼ ê²½ë¡œ: {tmp_path}")
        
        t1 = time.perf_counter()
        
        writer = get_writer(obj, format)
        if writer is None:
            logger.error(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” format: {format}")
            if verbose:
                logger.debug(f"Atomic write step timings (FAILED at setup): "
                            f"setup={t1-t0:.4f}s, total={time.perf_counter()-t0:.4f}s")
            logger.info(f"Atomic write failed at setup stage (took {time.perf_counter()-t0:.4f}s)")
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” format: {format}")
        logger.info(f"ì‚¬ìš©í•  writer: {writer} (format: {format})")

        try:
            if not show_progress:
                _execute_write(writer, obj, tmp_path, **kwargs)
            else:
                _execute_write_with_progress(writer, obj, tmp_path, **kwargs)
            
            t2 = time.perf_counter()
            logger.info(f"ë°ì´í„° ì„ì‹œ íŒŒì¼ì— ì €ì¥ ì™„ë£Œ: {tmp_path}")

        except Exception as e:
            # ì“°ê¸° ì‹¤íŒ¨ ì‹œì—ëŠ” ë¡¤ë°±í•  í•„ìš”ê°€ ì—†ìŒ (ì›ë³¸ íŒŒì¼ì€ ê·¸ëŒ€ë¡œ ìˆìŒ)
            t_error = time.perf_counter()
            logger.error(f"ì„ì‹œ íŒŒì¼ ì €ì¥ ì¤‘ ì˜ˆì™¸ ë°œìƒ: {e}")
            if verbose:
                logger.debug(f"Atomic write step timings (ERROR during write): "
                            f"setup={t1-t0:.4f}s, write_call={t_error-t1:.4f}s (ì‹¤íŒ¨), "
                            f"total={t_error-t0:.4f}s, error_type={type(e).__name__}")
            logger.info(f"Atomic write failed during write stage (took {t_error-t0:.4f}s, error: {type(e).__name__})")
            raise e

        # [ë¡¤ë°± STEP 1] ê¸°ì¡´ íŒŒì¼ ë°±ì—…
        if original_exists:
            logger.info(f"ê¸°ì¡´ íŒŒì¼ ë°±ì—…: {target_path} -> {backup_path}")
            try:
                # renameì€ atomic ì—°ì‚°ì´ë¯€ë¡œ ë°±ì—… ê³¼ì •ë„ ì•ˆì „í•©ë‹ˆë‹¤.
                os.rename(target_path, backup_path)
            except Exception as e:
                logger.error(f"ë°±ì—… ìƒì„± ì‹¤íŒ¨. ì‘ì—…ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤: {e}")
                # ë°±ì—… ì‹¤íŒ¨ ì‹œ ë” ì´ìƒ ì§„í–‰í•˜ë©´ ì•ˆ ë˜ë¯€ë¡œ ì˜ˆì™¸ë¥¼ ë°œìƒì‹œí‚µë‹ˆë‹¤.
                raise IOError(f"Failed to create backup for {target_path}") from e

        try:
            # [ë¡¤ë°± STEP 2] ì›ìì  êµì²´
            os.replace(tmp_path, target_path)
            t3 = time.perf_counter()
            logger.info(f"ì›ìì  êµì²´ ì™„ë£Œ: {tmp_path} -> {target_path}")
            
            # [ë¡¤ë°± STEP 3] _SUCCESS í”Œë˜ê·¸ ìƒì„±
            success_path = os.path.join(os.path.dirname(target_path), f".{os.path.basename(target_path)}._SUCCESS")
            with open(success_path, "w") as f:
                f.write("OK\n")
            t4 = time.perf_counter()
            logger.info(f"_SUCCESS í”Œë˜ê·¸ íŒŒì¼ ìƒì„±: {success_path}")
            
            # [ë¡¤ë°± STEP 4] ì„±ê³µ ì‹œ ë°±ì—… íŒŒì¼ ì‚­ì œ
            if original_exists:
                os.remove(backup_path)
                logger.info(f"ì‘ì—… ì„±ê³µ, ë°±ì—… íŒŒì¼ ì‚­ì œ ì™„ë£Œ: {backup_path}")

            if verbose:
                logger.debug(f"Atomic write step timings (SUCCESS): "
                             f"setup={t1-t0:.4f}s, write_call={t2-t1:.4f}s, "
                             f"replace={t3-t2:.4f}s, success_flag={t4-t3:.4f}s, "
                             f"total={t4-t0:.4f}s")
            logger.info(f"âœ… Atomic write completed successfully (took {t4-t0:.4f}s)")

        except Exception as e:
            # [ë¡¤ë°± STEP 5] êµì²´ ë˜ëŠ” í”Œë˜ê·¸ ìƒì„± ì‹¤íŒ¨ ì‹œ ë¡¤ë°± ì‹¤í–‰
            t_final_error = time.perf_counter()
            logger.error(f"ìµœì¢… ì €ì¥ ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜ ë°œìƒ. ë¡¤ë°±ì„ ì‹œì‘í•©ë‹ˆë‹¤. ì›ì¸: {e}")

            if original_exists:
                try:
                    # ìƒˆë¡œ ì“´ ë¶ˆì™„ì „í•œ íŒŒì¼ì´ ìˆë‹¤ë©´ ì‚­ì œ
                    if os.path.exists(target_path):
                        os.remove(target_path)
                    
                    # ë°±ì—…í•´ë‘” ì›ë³¸ íŒŒì¼ì„ ë‹¤ì‹œ ë³µêµ¬
                    os.rename(backup_path, target_path)
                    logger.info(f"ë¡¤ë°± ì„±ê³µ: ì›ë³¸ íŒŒì¼ ë³µêµ¬ ì™„ë£Œ ({backup_path} -> {target_path})")
                
                except Exception as rollback_e:
                    logger.critical(f"ì¹˜ëª…ì  ì˜¤ë¥˜: ë¡¤ë°± ì‹¤íŒ¨! {rollback_e}")
                    logger.critical(f"ì‹œìŠ¤í…œì´ ë¶ˆì•ˆì •í•œ ìƒíƒœì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìˆ˜ë™ í™•ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                    logger.critical(f"ë‚¨ì•„ìˆëŠ” íŒŒì¼: (ìƒˆ ë°ì´í„°) {target_path}, (ì›ë³¸ ë°±ì—…) {backup_path}")

            if verbose:
                 logger.debug(f"Atomic write step timings (FAILED AND ROLLED BACK): "
                             f"setup={t1-t0:.4f}s, write_call={t2-t1:.4f}s, "
                             f"final_stage_fail_time={t_final_error-t2:.4f}s, "
                             f"total={t_final_error-t0:.4f}s, error_type={type(e).__name__}")
            logger.info(f"Atomic write failed and rolled back (took {t_final_error-t0:.4f}s, error: {type(e).__name__})")
            
            # ì›ë³¸ ì˜ˆì™¸ë¥¼ ë‹¤ì‹œ ë°œìƒì‹œì¼œ ì‚¬ìš©ìì—ê²Œ ì•Œë¦½ë‹ˆë‹¤.
            raise e

def _execute_write(writer, obj, path, **kwargs):
    """
    ë‚´ë¶€ ì“°ê¸° ì‹¤í–‰ í•¨ìˆ˜. í•¸ë“¤ëŸ¬ íƒ€ì…ì— ë”°ë¼ ë¶„ê¸°í•˜ì—¬ ì‹¤ì œ ì“°ê¸° ì‘ì—…ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.
    - callable(writer): `np.save`ì™€ ê°™ì€ í•¨ìˆ˜ í•¸ë“¤ëŸ¬
    - str(writer): `to_csv`ì™€ ê°™ì€ ê°ì²´ì˜ ë©”ì†Œë“œ í•¸ë“¤ëŸ¬
    """
    # 1. writerê°€ í˜¸ì¶œ ê°€ëŠ¥í•œ 'í•¨ìˆ˜'ì¸ ê²½ìš° (e.g., np.save, np.savetxt)
    if callable(writer):
        # 1a. np.savez, np.savez_compressed íŠ¹ë³„ ì²˜ë¦¬: ì—¬ëŸ¬ ë°°ì—´ì„ dictë¡œ ë°›ì•„ ì €ì¥
        if writer in (np.savez, np.savez_compressed):
            if not isinstance(obj, dict):
                raise TypeError(
                    f"'{writer.__name__}'ë¡œ ì—¬ëŸ¬ ë°°ì—´ì„ ì €ì¥í•˜ë ¤ë©´, "
                    f"ë°ì´í„° ê°ì²´ëŠ” dict íƒ€ì…ì´ì–´ì•¼ í•©ë‹ˆë‹¤. (í˜„ì¬: {type(obj).__name__})"
                )
            writer(path, **obj)
        # 1b. ê·¸ ì™¸ ì¼ë°˜ì ì¸ í•¨ìˆ˜ í•¸ë“¤ëŸ¬ ì²˜ë¦¬
        else:
            writer(path, obj, **kwargs)

    # 2. writerê°€ 'ë©”ì†Œë“œ ì´ë¦„(ë¬¸ìì—´)'ì¸ ê²½ìš° (e.g., 'to_csv', 'to_excel')
    # ì´ ê²½ìš°, obj.to_csv(path, **kwargs) ì™€ ê°™ì´ í˜¸ì¶œë©ë‹ˆë‹¤.
    else:
        getattr(obj, writer)(path, **kwargs)

def _execute_write_with_progress(writer, obj, path, **kwargs):
    """ë©€í‹°ìŠ¤ë ˆë”©ìœ¼ë¡œ ì“°ê¸° ì‘ì—…ê³¼ ì§„í–‰ë„ í‘œì‹œë¥¼ í•¨ê»˜ ì‹¤í–‰í•˜ëŠ” ë‚´ë¶€ í•¨ìˆ˜"""
    stop_event = threading.Event()
    exception_queue = Queue()

    # ì‹¤ì œ ì“°ê¸° ì‘ì—…ì„ ìˆ˜í–‰í•  'ì‘ì—… ìŠ¤ë ˆë“œ'ì˜ ëª©í‘œ í•¨ìˆ˜
    def worker_task():
        try:
            _execute_write(writer, obj, path, **kwargs)
        except Exception as e:
            exception_queue.put(e)

    # ìŠ¤ë ˆë“œ ìƒì„±
    worker_thread = threading.Thread(target=worker_task)
    progress_bar = ProgressBar(filepath=path, stop_event=stop_event, description="Writing")
    monitor_thread = threading.Thread(target=progress_bar.run)

    # ìŠ¤ë ˆë“œ ì‹œì‘
    worker_thread.start()
    monitor_thread.start()

    # ì‘ì—… ìŠ¤ë ˆë“œê°€ ëë‚  ë•Œê¹Œì§€ ëŒ€ê¸°
    worker_thread.join()

    # ëª¨ë‹ˆí„° ìŠ¤ë ˆë“œì— ì¤‘ì§€ ì‹ í˜¸ ì „ì†¡ ë° ì¢…ë£Œ ëŒ€ê¸°
    stop_event.set()
    monitor_thread.join()

    # ì‘ì—… ìŠ¤ë ˆë“œì—ì„œ ì˜ˆì™¸ê°€ ë°œìƒí–ˆëŠ”ì§€ í™•ì¸í•˜ê³ , ìˆì—ˆë‹¤ë©´ ë‹¤ì‹œ ë°œìƒì‹œí‚´
    if not exception_queue.empty():
        raise exception_queue.get_nowait()

import uuid
import pyarrow as pa
from .utils import read_json, write_json
import io
import pyarrow.parquet as pq
from tqdm import tqdm

def write_snapshot(obj, table_path, mode='overwrite', message=None, show_progress=False, **kwargs):
    """
    ë°ì´í„° ê°ì²´ë¥¼ ì—´ ë‹¨ìœ„ ì²­í¬ë¡œ ë¶„í•´í•˜ì—¬ ë²„ì „ ê´€ë¦¬(ìŠ¤ëƒ…ìƒ·) ë°©ì‹ìœ¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.

    Args:
        obj: ì €ì¥í•  ë°ì´í„° ê°ì²´ (pandas, polars, numpy, pyarrow.Table).
        table_path (str): í…Œì´ë¸” ë°ì´í„°ê°€ ì €ì¥ë  ìµœìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ.
        mode (str): 'overwrite' (ê¸°ë³¸ê°’) ë˜ëŠ” 'append'.
                    - 'overwrite': í…Œì´ë¸”ì„ í˜„ì¬ ë°ì´í„°ë¡œ ì™„ì „íˆ ëŒ€ì²´í•©ë‹ˆë‹¤.
                    - 'append': ê¸°ì¡´ ë²„ì „ì˜ ë°ì´í„°ì— í˜„ì¬ ë°ì´í„°ë¥¼ ì¶”ê°€(ì—´ ê¸°ì¤€)í•©ë‹ˆë‹¤.
        show_progress (bool): ì§„í–‰ë¥  í‘œì‹œ ì—¬ë¶€. Defaults to False.
    """
    with FileLock(table_path):
        logger = setup_logger(debug_level=False)

        format='parquet'

        # 1. ê²½ë¡œ ì„¤ì • ë° í´ë” ìƒì„±
        data_dir = os.path.join(table_path, 'data')
        metadata_dir = os.path.join(table_path, 'metadata')
        os.makedirs(data_dir, exist_ok=True)
        os.makedirs(metadata_dir, exist_ok=True)
        
        # 2. í˜„ì¬ ë²„ì „ í™•ì¸
        pointer_path = os.path.join(table_path, '_current_version.json')
        current_version = 0
        if os.path.exists(pointer_path):
            current_version = read_json(pointer_path)['version_id']
        new_version = current_version + 1

        # 3. Arrow Tableë¡œ í‘œì¤€í™” (NumPy ì§€ì› ì¶”ê°€)
        if isinstance(obj, pa.Table):
            arrow_table = obj
        elif hasattr(obj, 'to_arrow'):  # Polars
            arrow_table = obj.to_arrow()
        elif hasattr(obj, '__arrow_array__') or hasattr(obj, '__dataframe__'): # Pandas
            arrow_table = pa.Table.from_pandas(obj)
        elif "numpy" in str(type(obj)): # NumPy ì²˜ë¦¬ ë¶€ë¶„
            # (í•µì‹¬ ìˆ˜ì •) ë°°ì—´ì˜ ì°¨ì›(ndim)ì— ë”°ë¼ ë‹¤ë¥´ê²Œ ì²˜ë¦¬
            if obj.ndim == 1:
                # 1ì°¨ì› ë°°ì—´ì€ ê¸°ì¡´ ë°©ì‹ ê·¸ëŒ€ë¡œ ì‚¬ìš©
                arrow_table = pa.Table.from_arrays([obj], names=['_col_0'])
            else:
                # 2ì°¨ì› ì´ìƒ ë°°ì—´ì€ "ë¦¬ìŠ¤íŠ¸ì˜ ë¦¬ìŠ¤íŠ¸"ë¡œ ë³€í™˜ í›„ Arrow Arrayë¡œ ë§Œë“¦
                arrow_table = pa.Table.from_arrays([pa.array(obj.tolist())], names=['_col_0'])
                
        else:
            raise TypeError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ë°ì´í„° íƒ€ì…: {type(obj)}")

        # 4. ì„ì‹œ ë””ë ‰í† ë¦¬ì—ì„œ ì—´ ë‹¨ìœ„ í•´ì‹œ ê³„ì‚° ë° ì¤‘ë³µ ì—†ëŠ” ì“°ê¸°
        with tempfile.TemporaryDirectory() as tmpdir:
            new_snapshot_columns = []
            temp_data_files_to_commit = {}  # {ì„ì‹œê²½ë¡œ: ìµœì¢…ê²½ë¡œ}

            iterable = tqdm(
                arrow_table.column_names,
                desc="Saving snapshot columns",
                disable=not show_progress
            )

            for col_name in iterable:
                i = arrow_table.schema.get_field_index(col_name)
                column_array = arrow_table.column(i)
                col_hash = _get_column_hash(column_array, col_name)
                chunk_filename = f"{col_hash}.{format}"
                final_data_path = os.path.join(data_dir, chunk_filename)
                
                if not os.path.exists(final_data_path):
                    tmp_data_path = os.path.join(tmpdir, chunk_filename)
                    
                    table_to_write = pa.Table.from_arrays([column_array], names=[col_name])

                    pq.write_table(table_to_write, tmp_data_path, compression='zstd')
                    # ë°”ê¹¥ìª½ with open() êµ¬ë¬¸ì´ ëë‚˜ë©´ì„œ íŒŒì¼ì´ í™•ì‹¤í•˜ê²Œ ë‹«í™ë‹ˆë‹¤.
                        
                    temp_data_files_to_commit[tmp_data_path] = final_data_path
                
                new_snapshot_columns.append({"name": col_name, "hash": col_hash, "format": format})

            # 5. Snapshot ìƒì„± (overwrite/append ëª¨ë“œ ë¶„ê¸° ì²˜ë¦¬)
            snapshot_id = int(time.time())
            snapshot_filename = f"snapshot-{snapshot_id}-{uuid.uuid4()}.json"
            
            final_columns_for_snapshot = new_snapshot_columns

            # append ëª¨ë“œì´ê³  ì´ì „ ë²„ì „ì´ ì¡´ì¬í•  ê²½ìš°, ì´ì „ ìŠ¤ëƒ…ìƒ·ì˜ ì»¬ëŸ¼ ëª©ë¡ì„ ê°€ì ¸ì™€ ë³‘í•©
            if mode.lower() == 'append' and current_version > 0:
                try:
                    prev_metadata_path = os.path.join(metadata_dir, f'v{current_version}.metadata.json')
                    prev_metadata = read_json(prev_metadata_path)
                    prev_snapshot_filename = prev_metadata['snapshot_filename']
                    prev_snapshot_path = os.path.join(table_path, prev_snapshot_filename)
                    prev_snapshot = read_json(prev_snapshot_path)
                    
                    previous_columns = prev_snapshot.get('columns', [])
                    final_columns_for_snapshot = previous_columns + new_snapshot_columns
                    logger.info(f"Append ëª¨ë“œ: v{current_version}ì˜ ì»¬ëŸ¼ {len(previous_columns)}ê°œì— {len(new_snapshot_columns)}ê°œë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.")

                except (FileNotFoundError, KeyError) as e:
                    logger.warning(f"Append ëª¨ë“œ ì‹¤í–‰ ì¤‘ ì´ì „ ë²„ì „ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì–´ Overwrite ëª¨ë“œë¡œ ë™ì‘í•©ë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
            
            new_snapshot = {
                'snapshot_id': snapshot_id,
                'timestamp': time.time(),
                'message': message or f"Version {new_version} created.",
                'columns': final_columns_for_snapshot
            }
            write_json(new_snapshot, os.path.join(tmpdir, snapshot_filename))
            
            # 6. Metadata ë° í¬ì¸í„° ìƒì„±
            new_metadata = {
                'version_id': new_version,
                'snapshot_id': snapshot_id,
                'snapshot_filename': os.path.join('metadata', snapshot_filename)
            }
            metadata_filename = f"v{new_version}.metadata.json"
            write_json(new_metadata, os.path.join(tmpdir, metadata_filename))

            new_pointer = {'version_id': new_version}
            tmp_pointer_path = os.path.join(tmpdir, '_current_version.json')
            write_json(new_pointer, tmp_pointer_path)

            # 7. ìµœì¢… ì»¤ë°‹ (ìƒˆë¡œ ì“°ì—¬ì§„ ë°ì´í„° íŒŒì¼ê³¼ ë©”íƒ€ë°ì´í„° íŒŒì¼ë“¤ì„ ìµœì¢… ìœ„ì¹˜ë¡œ ì´ë™)
            for tmp_path, final_path in temp_data_files_to_commit.items():
                os.rename(tmp_path, final_path)
            
            os.rename(os.path.join(tmpdir, snapshot_filename), os.path.join(metadata_dir, snapshot_filename))
            os.rename(os.path.join(tmpdir, metadata_filename), os.path.join(metadata_dir, metadata_filename))
            os.replace(tmp_pointer_path, pointer_path) # ì›ìì  ì—°ì‚°ìœ¼ë¡œ í¬ì¸í„° êµì²´
            
            logger.info(f"âœ… ìŠ¤ëƒ…ìƒ· ì €ì¥ ì™„ë£Œ! '{table_path}'ê°€ ë²„ì „ {new_version}ìœ¼ë¡œ ì—…ë°ì´íŠ¸ë˜ì—ˆìŠµë‹ˆë‹¤. (ëª¨ë“œ: {mode})")


import pyarrow as pa
import pandas as pd
import polars as pl

def read_table(table_path, version=None, output_as='pandas'):
    """
    ì§€ì •ëœ ë²„ì „ì˜ ìŠ¤ëƒ…ìƒ·ì„ ì½ì–´ ë°ì´í„° ê°ì²´ë¡œ ì¬êµ¬ì„±í•©ë‹ˆë‹¤.

    Args:
        table_path (str): í…Œì´ë¸” ë°ì´í„°ê°€ ì €ì¥ë  ìµœìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ.
        version (int or str, optional): 
            - int: ë¶ˆëŸ¬ì˜¬ ë²„ì „ ID (e.g., 5).
            - str: ë¶ˆëŸ¬ì˜¬ ë²„ì „ì˜ íƒœê·¸ ì´ë¦„ (e.g., 'best-model').
            - None: ìµœì‹  ë²„ì „ì„ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤. Defaults to None.
        output_as (str): ë°˜í™˜í•  ë°ì´í„° ê°ì²´ íƒ€ì… ('pandas', 'polars', 'arrow', 'numpy').
    
    Returns:
        ì§€ì •ëœ í¬ë§·ì˜ ë°ì´í„° ê°ì²´ (e.g., pandas.DataFrame).
    """
    logger = setup_logger()

    # --- 1. ì½ì„ ë²„ì „ì˜ ìŠ¤ëƒ…ìƒ· íŒŒì¼ ê²½ë¡œ ì°¾ê¸° ---
    try:
        version_id = None
        
        # ğŸ’¡ [í•µì‹¬ ë³€ê²½] version íŒŒë¼ë¯¸í„°ì˜ íƒ€ì…ì— ë”°ë¼ ë¶„ê¸° ì²˜ë¦¬
        if version is None:
            # Case 1: ìµœì‹  ë²„ì „ ì½ê¸° (ê¸°ì¡´ê³¼ ë™ì¼)
            pointer_path = os.path.join(table_path, '_current_version.json')
            version_id = read_json(pointer_path)['version_id']
            logger.info(f"ìµœì‹  ë²„ì „(v{version_id})ì„ ì½ìŠµë‹ˆë‹¤.")
        
        elif isinstance(version, int):
            # Case 2: ë²„ì „ ID(ìˆ«ì)ë¡œ ì½ê¸°
            version_id = version
            logger.info(f"ì§€ì •ëœ ë²„ì „ ID(v{version_id})ë¥¼ ì½ìŠµë‹ˆë‹¤.")

        elif isinstance(version, str):
            # Case 3: íƒœê·¸(ë¬¸ìì—´)ë¡œ ì½ê¸°
            tags_path = os.path.join(table_path, 'tags.json')
            if not os.path.exists(tags_path):
                raise FileNotFoundError("íƒœê·¸ íŒŒì¼(tags.json)ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            tags = read_json(tags_path)
            version_id = tags.get(version) # .get()ì„ ì‚¬ìš©í•´ íƒœê·¸ê°€ ì—†ì„ ê²½ìš° None ë°˜í™˜
            
            if version_id is None:
                raise KeyError(f"íƒœê·¸ '{version}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            logger.info(f"íƒœê·¸ '{version}'ì— í•´ë‹¹í•˜ëŠ” ë²„ì „(v{version_id})ì„ ì½ìŠµë‹ˆë‹¤.")
        
        if version_id is None:
            raise ValueError("ìœ íš¨í•œ ë²„ì „ì„ ì°¾ê±°ë‚˜ ì§€ì •í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
        
        metadata_path = os.path.join(table_path, 'metadata', f'v{version_id}.metadata.json')
        metadata = read_json(metadata_path)
        snapshot_filename = metadata['snapshot_filename']
        snapshot_path = os.path.join(table_path, snapshot_filename)
        snapshot = read_json(snapshot_path)

    except (ValueError) as e:
        logger.error(f"ì½ê¸° ì‹¤íŒ¨: {e}")
        raise e
    except FileNotFoundError as e:
        logger.error(f"ì½ê¸° ì‹¤íŒ¨: í•„ìš”í•œ ë©”íƒ€ë°ì´í„° ë˜ëŠ” ìŠ¤ëƒ…ìƒ· íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ê²½ë¡œ: {e.filename}")
        raise e
    except (KeyError, IndexError) as e:
        logger.error(f"ì½ê¸° ì‹¤íŒ¨: ë©”íƒ€ë°ì´í„° íŒŒì¼ì˜ í˜•ì‹ì´ ì˜ëª»ë˜ì—ˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {e}")
        raise e

    # --- 2. ìŠ¤ëƒ…ìƒ· ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ Arrow ì»¬ëŸ¼ë“¤ì„ ì½ì–´ì˜¤ê¸° ---
    columns_to_load = snapshot.get('columns', [])
    if not columns_to_load:
        logger.warning(f"ë²„ì „ {version_id}ì€ ë°ì´í„°ê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤. ë¹ˆ ê°ì²´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.")
        if output_as == 'pandas': return pd.DataFrame()
        if output_as == 'polars': return pl.DataFrame()
        if output_as == 'arrow': return pa.Table.from_pydict({})
        if output_as == 'numpy': return np.array([])
        return None

    arrow_arrays = []
    column_names = []
    data_dir = os.path.join(table_path, 'data')

    for col_info in columns_to_load:
        col_name = col_info['name']
        col_hash = col_info['hash']
        col_format = col_info.get('format', 'arrow') # í•˜ìœ„ í˜¸í™˜ì„±ì„ ìœ„í•´ format í•„ë“œ ì‚¬ìš©
        
        chunk_path = os.path.join(data_dir, f"{col_hash}.{col_format}")
        
        # Arrow IPC(Feather V2) í¬ë§·ìœ¼ë¡œ ì €ì¥ëœ ë‹¨ì¼ ì»¬ëŸ¼ íŒŒì¼ ì½ê¸°
        arrow_table_chunk = pq.read_table(chunk_path)
        # íŒŒì¼ì—ëŠ” ì»¬ëŸ¼ì´ í•˜ë‚˜ë§Œ ë“¤ì–´ìˆìœ¼ë¯€ë¡œ ë¡œì§ì€ ë™ì¼
        arrow_arrays.append(arrow_table_chunk.column(0))
        column_names.append(col_name)

    # --- 3. ì½ì–´ì˜¨ ì»¬ëŸ¼ë“¤ì„ í•˜ë‚˜ì˜ Arrow Tableë¡œ ì¡°í•© ---
    final_arrow_table = pa.Table.from_arrays(arrow_arrays, names=column_names)
    logger.info(f"ë°ì´í„° ë¡œë“œ ì™„ë£Œ. ì´ {final_arrow_table.num_rows}í–‰, {final_arrow_table.num_columns}ì—´.")

    # --- 4. ì‚¬ìš©ìê°€ ìš”ì²­í•œ í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ë°˜í™˜ ---
    if output_as.lower() == 'pandas':
        return final_arrow_table.to_pandas()
    elif output_as.lower() == 'polars':
        return pl.from_arrow(final_arrow_table)
    elif output_as.lower() == 'arrow':
        return final_arrow_table
    elif output_as.lower() == 'numpy':
        # (í•µì‹¬ ìˆ˜ì •) NumPy ë³€í™˜ ë¡œì§ ë³´ê°•
        if final_arrow_table.num_columns == 1:
            column = final_arrow_table.column(0)
            # ì»¬ëŸ¼ íƒ€ì…ì´ ë¦¬ìŠ¤íŠ¸ì¸ì§€(2D+ ë°°ì—´ì´ì—ˆëŠ”ì§€) í™•ì¸
            if pa.types.is_list(column.type):
                # ë¦¬ìŠ¤íŠ¸ ì»¬ëŸ¼ì´ë©´, to_pylist()ë¡œ íŒŒì´ì¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“  í›„ np.arrayë¡œ ì¬ì¡°ë¦½
                return np.array(column.to_pylist())
            else:
                # ë‹¨ìˆœ 1D ë°°ì—´ì´ì—ˆìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ ì‚¬ìš©
                return column.to_numpy()
        else:
            logger.warning("NumPy ì¶œë ¥ì€ ì»¬ëŸ¼ì´ í•˜ë‚˜ì¼ ë•Œë§Œ ì§€ì›ë©ë‹ˆë‹¤. Arrow í…Œì´ë¸”ì„ ë°˜í™˜í•©ë‹ˆë‹¤.")
            return final_arrow_table
    
    raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì¶œë ¥ í˜•ì‹ì…ë‹ˆë‹¤: {output_as}")


def tag_version(table_path: str, version_id: int, tag_name: str, logger=None):
    """
    íŠ¹ì • ë²„ì „ IDì— íƒœê·¸ë¥¼ ì§€ì •í•˜ê±°ë‚˜ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

    - 'tags.json' íŒŒì¼ì— íƒœê·¸ì™€ ë²„ì „ IDì˜ ë§¤í•‘ì„ ì €ì¥í•©ë‹ˆë‹¤.
    - íƒœê·¸ ì´ë¦„ì´ ì´ë¯¸ ì¡´ì¬í•˜ë©´ ê°€ë¦¬í‚¤ëŠ” ë²„ì „ IDë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.

    Args:
        table_path (str): í…Œì´ë¸” ë°ì´í„°ê°€ ì €ì¥ëœ ìµœìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ.
        version_id (int): íƒœê·¸ë¥¼ ë¶™ì¼ ë²„ì „ì˜ ID.
        tag_name (str): ì§€ì •í•  íƒœê·¸ ì´ë¦„ (e.g., "best-model", "archive-stable").
        logger: ë¡œê¹…ì„ ìœ„í•œ ë¡œê±° ê°ì²´.
    
    Returns:
        bool: ì„±ê³µ ì‹œ True, ì‹¤íŒ¨ ì‹œ Falseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    with FileLock(table_path):
        if logger is None:
            logger = setup_logger()

        # 1. íƒœê·¸ë¥¼ ë¶™ì´ë ¤ëŠ” ë²„ì „ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        metadata_path = os.path.join(table_path, 'metadata', f'v{version_id}.metadata.json')
        if not os.path.exists(metadata_path):
            logger.error(f"íƒœê·¸ ì§€ì • ì‹¤íŒ¨: ë²„ì „ {version_id}ì´(ê°€) ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return False

        # 2. ê¸°ì¡´ íƒœê·¸ íŒŒì¼ ì½ê¸° (ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±)
        tags_path = os.path.join(table_path, 'tags.json')
        tags = {}
        if os.path.exists(tags_path):
            try:
                tags = read_json(tags_path)
                if not isinstance(tags, dict):
                    logger.warning(f"'tags.json' íŒŒì¼ì´ ì†ìƒëœ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                    tags = {}
            except json.JSONDecodeError:
                logger.warning(f"'tags.json' íŒŒì¼ì„ ì½ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤.")
                tags = {}
        
        # 3. íƒœê·¸ ì •ë³´ ì—…ë°ì´íŠ¸
        old_version = tags.get(tag_name)
        if old_version == version_id:
            logger.info(f"íƒœê·¸ '{tag_name}'ì€(ëŠ”) ì´ë¯¸ ë²„ì „ {version_id}ì„(ë¥¼) ê°€ë¦¬í‚¤ê³  ìˆìŠµë‹ˆë‹¤. ë³€ê²½ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤.")
            return True
        
        tags[tag_name] = version_id

        # 4. ì—…ë°ì´íŠ¸ëœ íƒœê·¸ ì •ë³´ ì €ì¥
        try:
            # ì•ˆì „í•œ ì“°ê¸°ë¥¼ ìœ„í•´ ì„ì‹œ íŒŒì¼ ì‚¬ìš© í›„ ì›ìì  êµì²´
            tmp_tags_path = tags_path + f".{uuid.uuid4()}.tmp"
            write_json(tags, tmp_tags_path)
            os.replace(tmp_tags_path, tags_path)

            if old_version is not None:
                logger.info(f"âœ… íƒœê·¸ ì—…ë°ì´íŠ¸ ì„±ê³µ: '{tag_name}' -> v{version_id} (ì´ì „: v{old_version})")
            else:
                logger.info(f"âœ… íƒœê·¸ ìƒì„± ì„±ê³µ: '{tag_name}' -> v{version_id}")
            return True
        except Exception as e:
            logger.error(f"íƒœê·¸ íŒŒì¼('{tags_path}')ì„ ì“°ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            # ì„ì‹œ íŒŒì¼ì´ ë‚¨ì•„ìˆì„ ê²½ìš° ì •ë¦¬
            if os.path.exists(tmp_tags_path):
                os.remove(tmp_tags_path)
            return False

def list_snapshots(table_path: str, logger=None):
    """
    ì €ì¥ëœ ëª¨ë“  ìŠ¤ëƒ…ìƒ·ì˜ ë²„ì „ ì •ë³´ë¥¼ ì¡°íšŒí•˜ì—¬ ë¦¬ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.

    - ê° ë²„ì „ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì½ì–´ ID, ìƒì„± ì‹œê°„, ë©”ì‹œì§€ ë“±ì˜ ì •ë³´ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.
    - 'tags.json' íŒŒì¼ì„ ì½ì–´ ê° ë²„ì „ì— ì–´ë–¤ íƒœê·¸ê°€ ë¶™ì–´ìˆëŠ”ì§€ í‘œì‹œí•©ë‹ˆë‹¤.
    - ìµœì‹  ë²„ì „ì„ íŠ¹ë³„íˆ í‘œì‹œí•´ì¤ë‹ˆë‹¤.

    Args:
        table_path (str): ì¡°íšŒí•  í…Œì´ë¸”ì˜ ìµœìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ.
        logger: ë¡œê¹…ì„ ìœ„í•œ ë¡œê±° ê°ì²´.
        
    Returns:
        list[dict]: ê° ë²„ì „ì˜ ìƒì„¸ ì •ë³´ê°€ ë‹´ê¸´ ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸.
                     ë¦¬ìŠ¤íŠ¸ëŠ” ë²„ì „ ID ìˆœì„œë¡œ ì •ë ¬ë©ë‹ˆë‹¤.
                     ì •ë³´ê°€ ì—†ëŠ” ê²½ìš° ë¹ˆ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    if logger is None:
        logger = setup_logger()

    metadata_dir = os.path.join(table_path, 'metadata')
    if not os.path.isdir(metadata_dir):
        logger.warning(f"í…Œì´ë¸” ê²½ë¡œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: '{table_path}'")
        return []

    # 1. íƒœê·¸ ì •ë³´ ë¡œë“œ
    tags_path = os.path.join(table_path, 'tags.json')
    version_to_tags = {}
    if os.path.exists(tags_path):
        try:
            tags_data = read_json(tags_path)
            # {tag: version_id} -> {version_id: [tag1, tag2]} í˜•íƒœë¡œ ë³€í™˜
            for tag, version_id in tags_data.items():
                if version_id not in version_to_tags:
                    version_to_tags[version_id] = []
                version_to_tags[version_id].append(tag)
        except Exception:
            logger.warning("'tags.json' íŒŒì¼ì„ ì½ëŠ” ë° ì‹¤íŒ¨í•˜ì—¬ íƒœê·¸ ì •ë³´ë¥¼ ìƒëµí•©ë‹ˆë‹¤.")

    # 2. í˜„ì¬ ë²„ì „ ì •ë³´ ë¡œë“œ
    current_version_id = -1
    pointer_path = os.path.join(table_path, '_current_version.json')
    if os.path.exists(pointer_path):
        current_version_id = read_json(pointer_path).get('version_id', -1)

    # 3. ëª¨ë“  ë²„ì „ ë©”íƒ€ë°ì´í„° ìˆœíšŒ ë° ì •ë³´ ìˆ˜ì§‘
    snapshots_info = []
    for filename in os.listdir(metadata_dir):
        if not (filename.startswith('v') and filename.endswith('.metadata.json')):
            continue
        
        try:
            metadata = read_json(os.path.join(metadata_dir, filename))
            version_id = metadata['version_id']
            snapshot_path = os.path.join(table_path, metadata['snapshot_filename'])
            snapshot_data = read_json(snapshot_path)
            
            info = {
                "version_id": version_id,
                "is_latest": version_id == current_version_id,
                "tags": sorted(version_to_tags.get(version_id, [])),
                "message": snapshot_data.get('message', '')
            }
            snapshots_info.append(info)
        except (KeyError, FileNotFoundError, json.JSONDecodeError):
            logger.warning(f"ë©”íƒ€ë°ì´í„° íŒŒì¼ '{filename}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí•˜ì—¬ ê±´ë„ˆëœë‹ˆë‹¤.")
            continue
    
    # 4. ë²„ì „ ID ê¸°ì¤€ìœ¼ë¡œ ì˜¤ë¦„ì°¨ìˆœ ì •ë ¬í•˜ì—¬ ë°˜í™˜
    return sorted(snapshots_info, key=lambda x: x['version_id'])


import shutil
# (ë‹¤ë¥¸ import ë¬¸ë“¤ì€ ê·¸ëŒ€ë¡œ ìœ ì§€)

def delete_version(table_path, version_id, dry_run=False, logger=None):
    """
    íŠ¹ì • ë²„ì „ì„ ì‚­ì œí•˜ê³ , ë” ì´ìƒ ì°¸ì¡°ë˜ì§€ ì•ŠëŠ” ë°ì´í„° íŒŒì¼(ê°€ë¹„ì§€)ì„ ì •ë¦¬í•©ë‹ˆë‹¤.

    Args:
        table_path (str): í…Œì´ë¸” ë°ì´í„°ê°€ ì €ì¥ëœ ìµœìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ.
        version_id (int): ì‚­ì œí•  ë²„ì „ì˜ ID.
        dry_run (bool): Trueì´ë©´ ì‹¤ì œë¡œ ì‚­ì œí•˜ì§€ ì•Šê³  ëŒ€ìƒ ëª©ë¡ë§Œ ì¶œë ¥í•©ë‹ˆë‹¤.
    """
    with FileLock(table_path):
        if logger is None:
            logger = setup_logger()

        # --- 1ë‹¨ê³„: ë²„ì „ ë©”íƒ€ë°ì´í„° ì‚­ì œ ---
        logger.info(f"ë²„ì „ {version_id} ì‚­ì œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
        
        # ì•ˆì „ì¥ì¹˜: í˜„ì¬ í™œì„±í™”ëœ ìµœì‹  ë²„ì „ì€ ì‚­ì œí•  ìˆ˜ ì—†ë„ë¡ ë°©ì§€
        pointer_path = os.path.join(table_path, '_current_version.json')
        try:
            current_version = read_json(pointer_path)['version_id']
            if version_id == current_version:
                logger.error(f"ì‚­ì œ ì‹¤íŒ¨: í˜„ì¬ í™œì„±í™”ëœ ìµœì‹  ë²„ì „(v{version_id})ì€ ì‚­ì œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                logger.error("ë‹¤ë¥¸ ë²„ì „ìœ¼ë¡œ ë¡¤ë°±(rollback)í•œ í›„ ì‹œë„í•´ ì£¼ì„¸ìš”.")
                return False
        except FileNotFoundError:
            pass

        metadata_path = os.path.join(table_path, 'metadata', f'v{version_id}.metadata.json')
        if not os.path.exists(metadata_path):
            logger.warning(f"ì‚­ì œí•  ë²„ì „(v{version_id})ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return False
            
        try:
            # vX.metadata.json íŒŒì¼ë§Œ ë¨¼ì € ì‚­ì œ
            if not dry_run:
                os.remove(metadata_path)
            logger.info(f"ë²„ì „ {version_id}ì˜ ë©”íƒ€ë°ì´í„°ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì‚­ì œí–ˆìŠµë‹ˆë‹¤.")
        except OSError as e:
            logger.error(f"ë²„ì „ {version_id}ì˜ ë©”íƒ€ë°ì´í„° ì‚­ì œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False

        # --- 2ë‹¨ê³„: ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ (Vacuum) ì‹œì‘ ---
        logger.info("ê°€ë¹„ì§€ ì»¬ë ‰ì…˜ì„ ì‹œì‘í•©ë‹ˆë‹¤ (ì‚¬ìš©ë˜ì§€ ì•ŠëŠ” íŒŒì¼ ì •ë¦¬)...")
        
        metadata_dir = os.path.join(table_path, 'metadata')
        data_dir = os.path.join(table_path, 'data')

        # "ì‚´ì•„ìˆëŠ”" ëª¨ë“  ê°ì²´(ìŠ¤ëƒ…ìƒ·, ë°ì´í„° í•´ì‹œ)ì˜ ëª©ë¡ ë§Œë“¤ê¸°
        live_snapshot_files = set()
        live_data_hashes = set()

        for meta_filename in os.listdir(metadata_dir):
            if meta_filename.startswith('v') and meta_filename.endswith('.metadata.json'):
                try:
                    meta = read_json(os.path.join(metadata_dir, meta_filename))
                    snapshot_filename = os.path.basename(meta['snapshot_filename'])
                    live_snapshot_files.add(snapshot_filename)
                    
                    snapshot = read_json(os.path.join(table_path, meta['snapshot_filename']))
                    for col_info in snapshot.get('columns', []):
                        live_data_hashes.add(col_info['hash'])
                except (FileNotFoundError, KeyError):
                    continue

        # "ê³ ì•„" ê°ì²´(ì‚­ì œ ëŒ€ìƒ) ì‹ë³„
        files_to_delete = []
        if os.path.isdir(data_dir):
            for data_filename in os.listdir(data_dir):
                file_hash = os.path.splitext(data_filename)[0]
                if file_hash not in live_data_hashes:
                    files_to_delete.append(os.path.join(data_dir, data_filename))

        for snapshot_filename in os.listdir(metadata_dir):
            if snapshot_filename.startswith('snapshot-') and snapshot_filename not in live_snapshot_files:
                files_to_delete.append(os.path.join(metadata_dir, snapshot_filename))
        
        # ìµœì¢… ì‚­ì œ ì‹¤í–‰
        if not files_to_delete:
            logger.info("ì •ë¦¬í•  ì¶”ê°€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return True

        logger.info(f"ì´ {len(files_to_delete)}ê°œì˜ ì •ë¦¬ ëŒ€ìƒì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.")
        if dry_run:
            print("\n--- [Dry Run] ì•„ë˜ íŒŒì¼ë“¤ì´ ì‚­ì œë  ì˜ˆì •ì…ë‹ˆë‹¤ ---")
            for f in sorted(files_to_delete):
                print(f"  - {os.path.relpath(f, table_path)}")
        else:
            logger.info("ì‹¤ì œ íŒŒì¼ ì‚­ì œë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...")
            deleted_count = 0
            for f in files_to_delete:
                try:
                    os.remove(f)
                    deleted_count += 1
                except OSError as e:
                    logger.error(f"íŒŒì¼ ì‚­ì œ ì‹¤íŒ¨: {f}, ì˜¤ë¥˜: {e}")
            logger.info(f"âœ… ì´ {deleted_count}ê°œì˜ íŒŒì¼ ì‚­ì œ ì‘ì—…ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return True

import json

def rollback(table_path, version_id, logger=None):
    """
    í…Œì´ë¸”ì˜ í˜„ì¬ ë²„ì „ì„ ì§€ì •ëœ ë²„ì „ IDë¡œ ë¡¤ë°±í•©ë‹ˆë‹¤.

    Args:
        table_path (str): í…Œì´ë¸” ë°ì´í„°ê°€ ì €ì¥ëœ ìµœìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ.
        version_id (int): ë¡¤ë°±í•  ëª©í‘œ ë²„ì „ì˜ ID.

    Returns:
        bool: ì„±ê³µ ì‹œ True, ì‹¤íŒ¨ ì‹œ Falseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    with FileLock(table_path):
        if logger is None:
            logger = setup_logger()

        # 1. ë¡¤ë°±í•˜ë ¤ëŠ” ë²„ì „ì´ ì‹¤ì œë¡œ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
        metadata_path = os.path.join(table_path, 'metadata', f'v{version_id}.metadata.json')
        if not os.path.exists(metadata_path):
            logger.error(f"ë¡¤ë°± ì‹¤íŒ¨: ë²„ì „ {version_id}ì´(ê°€) ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return False

        # 2. _current_version.json í¬ì¸í„° íŒŒì¼ì˜ ë‚´ìš©ì„ ìˆ˜ì •
        pointer_path = os.path.join(table_path, '_current_version.json')
        try:
            with open(pointer_path, 'w', encoding='utf-8') as f:
                json.dump({'version_id': version_id}, f)
            logger.info(f"âœ… ë¡¤ë°± ì„±ê³µ! í˜„ì¬ ë²„ì „ì´ v{version_id}(ìœ¼)ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return True
        except OSError as e:
            logger.error(f"ë¡¤ë°± ì‹¤íŒ¨: í¬ì¸í„° íŒŒì¼ì„ ì“°ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {e}")
            return False
        
def revert(table_path: str, version_id_to_revert: int, message: str = None, logger=None):
    """
    íŠ¹ì • ê³¼ê±° ë²„ì „ì˜ ìƒíƒœë¥¼ ê°€ì ¸ì™€ ìƒˆë¡œìš´ ë²„ì „ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤. (git revertì™€ ìœ ì‚¬)
    
    ì´ ì‘ì—…ì€ ê¸°ë¡ì„ ì‚­ì œí•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ëŒ€ì‹ , ì§€ì •ëœ ë²„ì „ì˜ ìŠ¤ëƒ…ìƒ·ì„ ê·¸ëŒ€ë¡œ ë³µì‚¬í•˜ì—¬
    ìƒˆë¡œìš´ ë²„ì „ìœ¼ë¡œ ì»¤ë°‹í•©ë‹ˆë‹¤.

    Args:
        table_path (str): í…Œì´ë¸” ë°ì´í„°ê°€ ì €ì¥ëœ ìµœìƒìœ„ ë””ë ‰í† ë¦¬ ê²½ë¡œ.
        version_id_to_revert (int): ìƒíƒœë¥¼ ë˜ëŒë¦¬ê³  ì‹¶ì€ ëª©í‘œ ë²„ì „ ID.
        message (str, optional): ìƒˆ ë²„ì „ì— ê¸°ë¡ë  ì»¤ë°‹ ë©”ì‹œì§€.
        logger: ë¡œê¹…ì„ ìœ„í•œ ë¡œê±° ê°ì²´.

    Returns:
        bool: ì„±ê³µ ì‹œ True, ì‹¤íŒ¨ ì‹œ Falseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
    """
    with FileLock(table_path):
        if logger is None:
            logger = setup_logger()

        metadata_dir = os.path.join(table_path, 'metadata')
        
        # --- 1. ë˜ëŒë¦´ ë²„ì „ì˜ ìŠ¤ëƒ…ìƒ· ì •ë³´ ì½ê¸° ---
        revert_metadata_path = os.path.join(metadata_dir, f'v{version_id_to_revert}.metadata.json')
        if not os.path.exists(revert_metadata_path):
            logger.error(f"ë¦¬ë²„íŠ¸ ì‹¤íŒ¨: ë˜ëŒë¦´ ëŒ€ìƒ ë²„ì „(v{version_id_to_revert})ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            return False

        try:
            revert_metadata = read_json(revert_metadata_path)
            revert_snapshot_path = os.path.join(table_path, revert_metadata['snapshot_filename'])
            revert_snapshot_content = read_json(revert_snapshot_path)
            logger.info(f"v{version_id_to_revert}ì˜ ìŠ¤ëƒ…ìƒ· ì •ë³´ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì½ì—ˆìŠµë‹ˆë‹¤.")
        except (KeyError, FileNotFoundError, json.JSONDecodeError) as e:
            logger.error(f"ë¦¬ë²„íŠ¸ ì‹¤íŒ¨: v{version_id_to_revert}ì˜ ìŠ¤ëƒ…ìƒ· ë˜ëŠ” ë©”íƒ€ë°ì´í„°ë¥¼ ì½ëŠ” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False

        # --- 2. ìƒˆë¡œìš´ ë²„ì „ ë° ìŠ¤ëƒ…ìƒ· ìƒì„± ì¤€ë¹„ ---
        pointer_path = os.path.join(table_path, '_current_version.json')
        current_version = 0
        if os.path.exists(pointer_path):
            current_version = read_json(pointer_path).get('version_id', 0)
        
        new_version_id = current_version + 1
        new_snapshot_id = int(time.time())

        # ìƒˆ ìŠ¤ëƒ…ìƒ· íŒŒì¼ ì´ë¦„ ë° ê²½ë¡œ ì„¤ì •
        new_snapshot_filename = f"snapshot-{new_snapshot_id}-{uuid.uuid4()}.json"
        new_snapshot_relative_path = os.path.join('metadata', new_snapshot_filename)
        new_snapshot_absolute_path = os.path.join(metadata_dir, new_snapshot_filename)

        # ìƒˆ ìŠ¤ëƒ…ìƒ· ë‚´ìš© êµ¬ì„± (ê³¼ê±° ë²„ì „ì˜ ì»¬ëŸ¼ ì •ë³´ë¥¼ ê·¸ëŒ€ë¡œ ì‚¬ìš©)
        new_snapshot_content = {
            'snapshot_id': new_snapshot_id,
            'timestamp': time.time(),
            'message': message or f"Reverted to state of v{version_id_to_revert}",
            'columns': revert_snapshot_content.get('columns', []) # í•µì‹¬: ë°ì´í„° í¬ì¸í„°(í•´ì‹œ) ëª©ë¡ì„ ë³µì‚¬
        }

        # ìƒˆ ë©”íƒ€ë°ì´í„° ë‚´ìš© êµ¬ì„±
        new_metadata_content = {
            'version_id': new_version_id,
            'snapshot_id': new_snapshot_id,
            'snapshot_filename': new_snapshot_relative_path
        }
        new_metadata_absolute_path = os.path.join(metadata_dir, f"v{new_version_id}.metadata.json")

        # --- 3. ìƒˆë¡œìš´ íŒŒì¼ë“¤ ì €ì¥ ë° í¬ì¸í„° ì—…ë°ì´íŠ¸ (ì»¤ë°‹) ---
        try:
            # ìƒˆ ìŠ¤ëƒ…ìƒ·ê³¼ ë©”íƒ€ë°ì´í„° íŒŒì¼ ì“°ê¸°
            write_json(new_snapshot_content, new_snapshot_absolute_path)
            write_json(new_metadata_content, new_metadata_absolute_path)
            
            # í¬ì¸í„° íŒŒì¼ ì›ìì ìœ¼ë¡œ êµì²´
            new_pointer = {'version_id': new_version_id}
            tmp_pointer_path = pointer_path + f".{uuid.uuid4()}.tmp"
            write_json(new_pointer, tmp_pointer_path)
            os.replace(tmp_pointer_path, pointer_path)
            
            logger.info(f"âœ… ë¦¬ë²„íŠ¸ ì„±ê³µ: v{version_id_to_revert}ì˜ ìƒíƒœë¡œ ìƒˆë¡œìš´ ë²„ì „(v{new_version_id})ì„ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")
            return True
        except Exception as e:
            logger.error(f"ë¦¬ë²„íŠ¸ ì‹¤íŒ¨: ìµœì¢… ì»¤ë°‹ ë‹¨ê³„ì—ì„œ ì˜¤ë¥˜ ë°œìƒ: {e}")
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ìƒì„±ëœ íŒŒì¼ë“¤ ì •ë¦¬
            if os.path.exists(new_snapshot_absolute_path):
                os.remove(new_snapshot_absolute_path)
            if os.path.exists(new_metadata_absolute_path):
                os.remove(new_metadata_absolute_path)
            return False
    
def export_to_datalake(table_path, version, output_path, **kwargs):
    """ì§€ì •ëœ ë²„ì „ì˜ atio ìŠ¤ëƒ…ìƒ·ì„ ë‹¨ì¼ Parquet íŒŒì¼ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤."""

    # 1. atioì˜ read_tableì„ ì‚¬ìš©í•´ ì™„ì „í•œ í…Œì´ë¸”ì„ ë©”ëª¨ë¦¬ë¡œ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤.
    full_table = read_table(table_path, version=version, output_as='arrow')

    # 2. ì´ í…Œì´ë¸”ì„ 'í•˜ë‚˜ì˜' í‘œì¤€ Parquet íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
    import pyarrow.parquet as pq
    pq.write_table(full_table, output_path, **kwargs)

import fastcdc
from concurrent.futures import ProcessPoolExecutor, as_completed
from .utils import get_process_pool
import xxhash

def _process_chunk_from_file_task(args):
    """
    'ì‘ì—… ì§€ì‹œì„œ'ë¥¼ ë°›ì•„ íŒŒì¼ì„ ì§ì ‘ ì½ê³  ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    """
    file_path, offset, length, data_dir = args
    
    # íŒŒì¼ì„ ì§ì ‘ ì—´ê³ , í•´ë‹¹ ìœ„ì¹˜ë¡œ ì´ë™(seek)í•˜ì—¬ í•„ìš”í•œ ë§Œí¼ë§Œ ì½ìŒ
    with open(file_path, 'rb') as f:
        f.seek(offset)
        chunk_content = f.read(length)
    
    chunk_hash = xxhash.xxh64(chunk_content).hexdigest()
    
    chunk_save_path = os.path.join(data_dir, chunk_hash)
    if not os.path.exists(chunk_save_path):
        return (chunk_hash, chunk_content)
    
    return (chunk_hash, None)


def _get_column_hash(arrow_column: pa.Array, column_name: str) -> str:
    """Arrow ì»¬ëŸ¼(ChunkedArray)ì˜ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ sha256 í•´ì‹œë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤."""
    mock_sink = io.BytesIO()

    # (í•µì‹¬ ìˆ˜ì •) ChunkedArrayë¥¼ í•˜ë‚˜ì˜ Arrayë¡œ í•©ì¹©ë‹ˆë‹¤.
    if isinstance(arrow_column, pa.ChunkedArray):
        array_to_write = arrow_column.combine_chunks()
    else:
        array_to_write = arrow_column

    batch = pa.RecordBatch.from_arrays([array_to_write], names=[column_name])
    
    with pa.ipc.new_stream(mock_sink, batch.schema) as writer:
        writer.write_batch(batch)

    return xxhash.xxh64(mock_sink.getvalue()).hexdigest()


def write_model_snapshot(model_path: str, table_path: str, show_progress: bool = False):
    """
    - PyTorch ë˜ëŠ” TensorFlow ëª¨ë¸ì˜ ìŠ¤ëƒ…ìƒ·ì„ ì €ì¥í•©ë‹ˆë‹¤.
    - ìƒì‚°ì-ì†Œë¹„ì íŒ¨í„´ìœ¼ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ìµœì í™”í•©ë‹ˆë‹¤.
    - ë‹¤ì¤‘ íŒŒì¼ ëª¨ë¸(TensorFlow) ì²˜ë¦¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤.
    """
    with FileLock(table_path):
        logger = setup_logger()

        # --- 1. ê²½ë¡œ ì„¤ì • ë° ë²„ì „ ê´€ë¦¬ ---
        data_dir = os.path.join(table_path, 'data')
        metadata_dir = os.path.join(table_path, 'metadata')
        os.makedirs(data_dir, exist_ok=True)
        os.makedirs(metadata_dir, exist_ok=True)
        
        pointer_path = os.path.join(table_path, '_current_version.json')
        current_version = 0
        if os.path.exists(pointer_path):
            current_version = read_json(pointer_path)['version_id']
        new_version = current_version + 1
        
        logger.info(f"ëª¨ë¸ ìŠ¤ëƒ…ìƒ· v{new_version} ìƒì„±ì„ ì‹œì‘í•©ë‹ˆë‹¤...")

        # --- 2. ëª¨ë¸ íƒ€ì… ê°ì§€ ë° ì²˜ë¦¬í•  íŒŒì¼ ëª©ë¡ ìƒì„± ---
        is_pytorch = os.path.isfile(model_path) and model_path.endswith(('.pth', '.pt'))
        is_tensorflow = os.path.isdir(model_path) and os.path.exists(os.path.join(model_path, 'saved_model.pb'))

        files_to_process = []
        if is_pytorch:
            files_to_process.append(model_path)
            model_path_base = os.path.dirname(model_path)
        elif is_tensorflow:
            for root, _, files in os.walk(model_path):
                for filename in files:
                    files_to_process.append(os.path.join(root, filename))
            model_path_base = model_path
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ í˜•ì‹ì…ë‹ˆë‹¤: {model_path}")

        # --- 3. ìƒì‚°ì-ì†Œë¹„ì íŒ¨í„´ìœ¼ë¡œ ë³‘ë ¬ ì²˜ë¦¬ ---
        all_files_info = []
        new_chunks_to_write = {}
        executor = get_process_pool()

        # íŒŒì¼ ëª©ë¡ì„ ìˆœíšŒ (TensorFlowì˜ ê²½ìš° ì—¬ëŸ¬ íŒŒì¼, PyTorchëŠ” 1ê°œ)
        for file_path in tqdm(files_to_process, desc="Total Progress", disable=not show_progress or len(files_to_process) == 1):
            relative_path = os.path.relpath(file_path, model_path_base).replace('\\', '/')
            file_info = {"path": relative_path, "chunks": []}
        
        with open(file_path, 'rb') as f:
            # ìƒì‚°ì: fastcdcê°€ ì²­í¬ 'ì •ë³´'ë¥¼ í•˜ë‚˜ì”© ìƒì„±í•˜ëŠ” ì œë„ˆë ˆì´í„°
            cdc = fastcdc.fastcdc(f, avg_size=65536, fat=True)
            
            # [í•µì‹¬] ì²­í¬ ì •ë³´ë¥¼ ë§Œë“¤ë©´ì„œ 'ë™ì‹œì—' ì‘ì—…ì„ ì œì¶œí•˜ëŠ” future ì œë„ˆë ˆì´í„°ë¥¼ ìƒì„±
            def submit_tasks_generator():
                for chunk in cdc:
                    job_ticket = (file_path, chunk.offset, chunk.length, data_dir)
                    yield executor.submit(_process_chunk_from_file_task, job_ticket)

            # ì´ ì²­í¬ ìˆ˜ë¥¼ ì•Œ ìˆ˜ ì—†ìœ¼ë¯€ë¡œ íŒŒì¼ í¬ê¸°ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì§„í–‰ë¥  í‘œì‹œ
            file_size = os.path.getsize(file_path)
            progress_desc = os.path.basename(file_path)
            
            with tqdm(total=file_size, desc=f"Processing {progress_desc}", unit='B', unit_scale=True, disable=not show_progress, leave=False) as pbar:
                # as_completedëŠ” futureê°€ ì™„ë£Œë˜ëŠ” ëŒ€ë¡œ ê²°ê³¼ë¥¼ ë°˜í™˜
                for future in as_completed(submit_tasks_generator()):
                    chunk_hash, chunk_content = future.result()
                    file_info["chunks"].append(chunk_hash)
                    if chunk_content is not None:
                        new_chunks_to_write[chunk_hash] = chunk_content
                    
                    # ëŒ€ëµì ì¸ ì²­í¬ í¬ê¸°ë§Œí¼ ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                    pbar.update(65536)

        all_files_info.append(file_info)

        logger.info(f"ë°ì´í„° ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ")

        # --- 4. ìµœì¢… ì»¤ë°‹ ë° ë©”íƒ€ë°ì´í„° ìƒì„± ---
        for chunk_hash, chunk_content in tqdm(new_chunks_to_write.items(), desc="Committing new chunks", disable=not show_progress):
            with open(os.path.join(data_dir, chunk_hash), 'wb') as f:
                f.write(chunk_content)

        snapshot_id = int(time.time())
        snapshot_filename = f"snapshot-{snapshot_id}-{uuid.uuid4()}.json"
        
        new_snapshot = {'snapshot_id': snapshot_id, 'timestamp': time.time(), 'files': sorted(all_files_info, key=lambda x: x['path'])}
        write_json(new_snapshot, os.path.join(metadata_dir, snapshot_filename))
        
        new_metadata = {'version_id': new_version, 'snapshot_id': snapshot_id, 'snapshot_filename': os.path.join('metadata', snapshot_filename)}
        metadata_filename = f"v{new_version}.metadata.json"
        write_json(new_metadata, os.path.join(metadata_dir, metadata_filename))

        new_pointer = {'version_id': new_version}
        tmp_pointer_path = os.path.join(metadata_dir, f"_pointer_{uuid.uuid4()}.json")
        write_json(new_pointer, tmp_pointer_path)
        os.replace(tmp_pointer_path, pointer_path)

        end_time = time.perf_counter()
        logger.info(f"âœ… ëª¨ë¸ ìŠ¤ëƒ…ìƒ· v{new_version} ìƒì„±ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

from concurrent.futures import ThreadPoolExecutor, as_completed

try:
    import torch
    _PYTORCH_AVAILABLE = True
except ImportError:
    _PYTORCH_AVAILABLE = False

try:
    import tensorflow as tf
    _TENSORFLOW_AVAILABLE = True
except ImportError:
    _TENSORFLOW_AVAILABLE = False

def _read_chunk(chunk_path: str) -> bytes:
    """ë‹¨ì¼ ì²­í¬ íŒŒì¼ì„ ì½ì–´ ë‚´ìš©ì„ ë°˜í™˜í•˜ëŠ” ê°„ë‹¨í•œ í•¨ìˆ˜ (ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰ë  ì‘ì—…)"""
    with open(chunk_path, 'rb') as f:
        return f.read()

def _reassemble_from_chunks_threaded(table_path, snapshot, destination_path=None, max_workers=None, show_progress=False):
    """
    ìŠ¤ëƒ…ìƒ· ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì²­í¬ë“¤ì„ 'ThreadPoolExecutor'ë¥¼ ì‚¬ìš©í•´ ë³‘ë ¬ë¡œ ì¡°í•©í•˜ì—¬ ëª¨ë¸ì„ ë³µì›í•©ë‹ˆë‹¤.
    max_workers: ì‚¬ìš©í•  ìŠ¤ë ˆë“œì˜ ìµœëŒ€ ê°œìˆ˜ (Noneì´ë©´ ê¸°ë³¸ê°’ ì‚¬ìš©)
    """
    data_dir = os.path.join(table_path, 'data')
    files_info = snapshot.get('files', [])

    # ThreadPoolExecutorë¥¼ withë¬¸ê³¼ í•¨ê»˜ ì‚¬ìš©í•´ ì•ˆì „í•˜ê²Œ ìŠ¤ë ˆë“œ í’€ì„ ê´€ë¦¬í•©ë‹ˆë‹¤.
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # --- PyTorch ë‹¨ì¼ íŒŒì¼ ì²˜ë¦¬ ---
        if len(files_info) == 1 and not os.path.dirname(files_info[0]['path']):
            file_info = files_info[0]
            chunk_paths = [os.path.join(data_dir, h) for h in file_info['chunks']]
            
            chunk_iterator = executor.map(_read_chunk, chunk_paths)
            
            all_chunk_data_iterator = tqdm(
                chunk_iterator,
                total=len(chunk_paths),
                desc=f"Reassembling {os.path.basename(file_info['path'])}",
                disable=not show_progress,
                unit=' chunks'
            )

            # 1. ì¸ë©”ëª¨ë¦¬ ë³µì›
            if destination_path is None:
                in_memory_file = io.BytesIO(b"".join(all_chunk_data_iterator))
                in_memory_file.seek(0)
                return in_memory_file
            
            # 2. ë””ìŠ¤í¬ íŒŒì¼ë¡œ ë³µì›
            else:
                if os.path.isdir(destination_path):
                    output_path = os.path.join(destination_path, file_info['path'])
                else:
                    output_path = destination_path
                
                os.makedirs(os.path.dirname(output_path), exist_ok=True)

                with open(output_path, 'wb') as f_out:
                    for chunk_data in all_chunk_data_iterator:
                        f_out.write(chunk_data)
                return output_path
                
        # --- TensorFlow ë””ë ‰í† ë¦¬ êµ¬ì¡° ì²˜ë¦¬ ---
        else:
            if destination_path is None:
                destination_path = tempfile.mkdtemp()
            
            os.makedirs(destination_path, exist_ok=True)

            iterable = tqdm(files_info, desc="Reassembling TensorFlow model", disable=not show_progress, unit=" file")
            for file_info in iterable:
                dir_name = os.path.dirname(file_info['path'])
                if dir_name:
                    os.makedirs(os.path.join(destination_path, dir_name), exist_ok=True)
                
                chunk_paths = [os.path.join(data_dir, h) for h in file_info['chunks']]
                
                # ê° íŒŒì¼ì˜ ì²­í¬ë¥¼ ë³‘ë ¬ë¡œ ì½ê¸°
                all_chunk_data = executor.map(_read_chunk, chunk_paths)
                
                output_path = os.path.join(destination_path, file_info['path'])
                with open(output_path, 'wb') as f_out:
                    # íŒŒì¼ ì“°ê¸° ì‹œì—ëŠ” ë‚´ë¶€ tqdm ì—†ì´ ë°”ë¡œ ì¡°í•©
                    f_out.write(b"".join(all_chunk_data))
            
            return destination_path

def read_model_snapshot(table_path, version=None, mode='auto', destination_path=None, max_workers=None, show_progress=False):
    """
    ëª¨ë¸ ìŠ¤ëƒ…ìƒ·ì„ ì½ì–´ì˜µë‹ˆë‹¤. ë‚´ë¶€ì ìœ¼ë¡œ ë©€í‹°ìŠ¤ë ˆë”©ì„ ì‚¬ìš©í•˜ì—¬ I/Oë¥¼ ë³‘ë ¬ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    
    Args:
        max_workers (int, optional): ë³‘ë ¬ I/Oì— ì‚¬ìš©í•  ìŠ¤ë ˆë“œ ê°œìˆ˜. 
                                     Noneì´ë©´ íŒŒì´ì¬ ê¸°ë³¸ê°’(ë³´í†µ CPU ì½”ì–´ ìˆ˜ * 5)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
        show_progress (bool): ì§„í–‰ë¥  í‘œì‹œ ì—¬ë¶€. Defaults to False.
    """
    logger = setup_logger()

    # (ë©”íƒ€ë°ì´í„° ì½ëŠ” ë¶€ë¶„ì€ ê¸°ì¡´ê³¼ ë™ì¼)
    try:
        if version is None:
            pointer_path = os.path.join(table_path, '_current_version.json')
            version_id = read_json(pointer_path)['version_id']
        else:
            version_id = version
        
        metadata_path = os.path.join(table_path, 'metadata', f'v{version_id}.metadata.json')
        metadata = read_json(metadata_path)
        snapshot_path = os.path.join(table_path, metadata['snapshot_filename'])
        snapshot = read_json(snapshot_path)
    except FileNotFoundError as e:
        logger.error(f"ì½ê¸° ì‹¤íŒ¨: ë²„ì „ {version or '(latest)'}ì˜ ë©”íƒ€ë°ì´í„°/ìŠ¤ëƒ…ìƒ·ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        raise e

    # --- 2. ëª¨ë“œì— ë”°ë¼ 'ìŠ¤ë ˆë“œ ë²„ì „'ì˜ ì¬ì¡°ë¦½ í•¨ìˆ˜ í˜¸ì¶œ ---
    
    # [ë³µì› ëª¨ë“œ]
    if mode == 'restore':
        if not destination_path:
            raise ValueError("mode='restore'ë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ destination_pathë¥¼ ë°˜ë“œì‹œ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤.")
        logger.info(f"v{version_id} ëª¨ë¸ì„ '{destination_path}' ê²½ë¡œì— ë³‘ë ¬ë¡œ ë³µì›í•©ë‹ˆë‹¤...")
        result_path = _reassemble_from_chunks_threaded(table_path, snapshot, destination_path, max_workers, show_progress)
        logger.info(f"âœ… ë³µì› ì™„ë£Œ: {result_path}")
        return result_path

    # [ìë™ ì¸ë©”ëª¨ë¦¬ ë¡œë”© ëª¨ë“œ]
    elif mode == 'auto':
        logger.info(f"v{version_id} ëª¨ë¸ì„ ë©”ëª¨ë¦¬ë¡œ ë³‘ë ¬ ë¡œë”©í•©ë‹ˆë‹¤...")
        
        is_pytorch = len(snapshot['files']) == 1 and snapshot['files'][0]['path'].endswith(('.pth', '.pt'))
        
        if is_pytorch:
            if not _PYTORCH_AVAILABLE:
                raise ImportError("PyTorch ëª¨ë¸ì„ ë¡œë“œí•˜ë ¤ë©´ 'torch' ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            
            in_memory_file = _reassemble_from_chunks_threaded(table_path, snapshot, None, max_workers, show_progress)
            model_obj = torch.load(in_memory_file)
            logger.info("âœ… PyTorch ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")
            return model_obj
        else: # TensorFlow
            if not _TENSORFLOW_AVAILABLE:
                raise ImportError("TensorFlow ëª¨ë¸ì„ ë¡œë“œí•˜ë ¤ë©´ 'tensorflow' ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•©ë‹ˆë‹¤.")

            temp_dir = _reassemble_from_chunks_threaded(table_path, snapshot, None, max_workers, show_progress)
            try:
                model_obj = tf.saved_model.load(temp_dir)
                logger.info("âœ… TensorFlow ëª¨ë¸ ë¡œë”© ì™„ë£Œ.")
                return model_obj
            finally:
                shutil.rmtree(temp_dir)
    else:
        raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” modeì…ë‹ˆë‹¤: '{mode}'. 'auto' ë˜ëŠ” 'restore'ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.")