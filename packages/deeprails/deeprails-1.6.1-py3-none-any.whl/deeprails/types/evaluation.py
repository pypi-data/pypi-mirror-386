# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

from typing import Dict, List, Optional
from datetime import datetime
from typing_extensions import Literal

from pydantic import Field as FieldInfo

from .._models import BaseModel

__all__ = ["Evaluation", "ModelInput"]


class ModelInput(BaseModel):
    ground_truth: Optional[str] = None
    """The ground truth for evaluating Ground Truth Adherence guardrail."""

    system_prompt: Optional[str] = None
    """The system prompt used to generate the output."""

    user_prompt: Optional[str] = None
    """The user prompt used to generate the output."""


class Evaluation(BaseModel):
    eval_id: str
    """A unique evaluation ID."""

    evaluation_status: Literal["in_progress", "completed", "canceled", "queued", "failed"]
    """Status of the evaluation."""

    api_model_input: ModelInput = FieldInfo(alias="model_input")
    """A dictionary of inputs sent to the LLM to generate output.

    The dictionary must contain at least one of `user_prompt` or `system_prompt`.
    For ground_truth_aherence guadrail metric, `ground_truth` should be provided.
    """

    api_model_output: str = FieldInfo(alias="model_output")
    """Output generated by the LLM to be evaluated."""

    run_mode: Literal["precision_plus", "precision", "smart", "economy"]
    """Run mode for the evaluation.

    The run mode allows the user to optimize for speed, accuracy, and cost by
    determining which models are used to evaluate the event.
    """

    created_at: Optional[datetime] = None
    """The time the evaluation was created in UTC."""

    end_timestamp: Optional[datetime] = None
    """The time the evaluation completed in UTC."""

    error_message: Optional[str] = None
    """Description of the error causing the evaluation to fail, if any."""

    error_timestamp: Optional[datetime] = None
    """The time the error causing the evaluation to fail was recorded."""

    evaluation_result: Optional[Dict[str, object]] = None
    """
    Evaluation result consisting of average scores and rationales for each of the
    evaluated guardrail metrics.
    """

    evaluation_total_cost: Optional[float] = None
    """Total cost of the evaluation."""

    guardrail_metrics: Optional[
        List[
            Literal[
                "correctness",
                "completeness",
                "instruction_adherence",
                "context_adherence",
                "ground_truth_adherence",
                "comprehensive_safety",
            ]
        ]
    ] = None
    """
    An array of guardrail metrics that the model input and output pair will be
    evaluated on.
    """

    api_model_used: Optional[str] = FieldInfo(alias="model_used", default=None)
    """Model ID used to generate the output, like `gpt-4o` or `o3`."""

    modified_at: Optional[datetime] = None
    """The most recent time the evaluation was modified in UTC."""

    nametag: Optional[str] = None
    """An optional, user-defined tag for the evaluation."""

    progress: Optional[int] = None
    """Evaluation progress.

    Values range between 0 and 100; 100 corresponds to a completed
    `evaluation_status`.
    """

    start_timestamp: Optional[datetime] = None
    """The time the evaluation started in UTC."""
