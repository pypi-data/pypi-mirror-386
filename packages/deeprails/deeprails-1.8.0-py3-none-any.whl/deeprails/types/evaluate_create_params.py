# File generated from our OpenAPI spec by Stainless. See CONTRIBUTING.md for details.

from __future__ import annotations

from typing import List
from typing_extensions import Literal, Required, TypedDict

__all__ = ["EvaluateCreateParams", "ModelInput"]


class EvaluateCreateParams(TypedDict, total=False):
    model_input: Required[ModelInput]
    """A dictionary of inputs sent to the LLM to generate output.

    The dictionary must contain at least `user_prompt` or `system_prompt` field. For
    ground_truth_adherence guardrail metric, `ground_truth` should be provided.
    """

    model_output: Required[str]
    """Output generated by the LLM to be evaluated."""

    run_mode: Required[Literal["precision_plus", "precision", "smart", "economy"]]
    """Run mode for the evaluation.

    The run mode allows the user to optimize for speed, accuracy, and cost by
    determining which models are used to evaluate the event. Available run modes
    include `precision_plus`, `precision`, `smart`, and `economy`. Defaults to
    `smart`.
    """

    guardrail_metrics: List[
        Literal[
            "correctness",
            "completeness",
            "instruction_adherence",
            "context_adherence",
            "ground_truth_adherence",
            "comprehensive_safety",
        ]
    ]
    """
    An array of guardrail metrics that the model input and output pair will be
    evaluated on. For non-enterprise users, these will be limited to the allowed
    guardrail metrics.
    """

    model_used: str
    """Model ID used to generate the output, like `gpt-4o` or `o3`."""

    nametag: str
    """An optional, user-defined tag for the evaluation."""


class ModelInput(TypedDict, total=False):
    ground_truth: str
    """The ground truth for evaluating Ground Truth Adherence guardrail."""

    system_prompt: str
    """The system prompt used to generate the output."""

    user_prompt: str
    """The user prompt used to generate the output."""
