# Cargo Build Configuration for Astrora
#
# This configuration optimizes builds for SIMD performance and enables
# CPU-specific optimizations for local development and benchmarking.

# ============================================================================
# Build Configuration
# ============================================================================

[build]
# Incremental compilation for faster development builds
incremental = true

# ============================================================================
# Target-Specific Configuration
# ============================================================================

# Local development and benchmarking: Enable native CPU optimizations
# This enables SIMD instructions specific to your CPU (AVX2, AVX-512, NEON, etc.)
#
# IMPORTANT: This is ONLY for local development and benchmarking.
# When building wheels for distribution, DO NOT use target-cpu=native
# as it would produce binaries incompatible with other CPUs.
#
# To use native optimizations for local development:
#   1. Uncomment the [target.'cfg(not(target_env = "msvc"))'] section below
#   2. Run: cargo build --release  (or maturin develop --release)
#   3. Benchmarks will automatically use these settings
#
# For distribution builds (via maturin build):
#   - Keep this section commented out
#   - maturin will build portable wheels with baseline CPU features

# UNCOMMENT FOR LOCAL DEVELOPMENT/BENCHMARKING (Unix/Linux/macOS):
# [target.'cfg(not(target_env = "msvc"))']
# rustflags = [
#     "-C", "target-cpu=native",              # Enable all CPU features (SIMD)
#     "-C", "opt-level=3",                     # Maximum optimization
# ]

# UNCOMMENT FOR LOCAL DEVELOPMENT/BENCHMARKING (Windows MSVC):
# [target.'cfg(target_env = "msvc")']
# rustflags = [
#     "-C", "target-cpu=native",              # Enable all CPU features (SIMD)
#     "-C", "opt-level=3",                     # Maximum optimization
# ]

# ============================================================================
# Profile-Specific Settings
# ============================================================================

# These settings are applied in addition to the profiles in Cargo.toml

[profile.release]
# Additional release optimizations beyond Cargo.toml
# (Cargo.toml already has opt-level=3, lto="fat", codegen-units=1)

[profile.bench]
# Benchmark profile inherits from release (set in Cargo.toml)
# No additional flags needed here

# ============================================================================
# SIMD Optimization Notes
# ============================================================================
#
# ## What SIMD Optimizations Are Already Active?
#
# Even WITHOUT target-cpu=native, our code benefits from SIMD because:
#
# 1. **nalgebra** (v0.33): Automatically uses SIMD for f32 and f64 operations
#    - Vector operations (dot, cross, normalize) use SIMD when available
#    - Matrix operations delegate to matrixmultiply crate
#
# 2. **matrixmultiply** (used by ndarray and nalgebra):
#    - Automatically detects and uses AVX/AVX2/FMA on x86_64
#    - Automatically uses NEON on ARM64 (Apple Silicon, etc.)
#    - Works at compile-time and runtime (CPU feature detection)
#
# 3. **Compiler Auto-Vectorization**:
#    - LLVM automatically vectorizes loops where possible
#    - Works best with contiguous array access patterns
#    - Our batch operations (using rayon) are SIMD-friendly
#
# ## Performance Gains from target-cpu=native
#
# Enabling target-cpu=native provides ADDITIONAL benefits:
#
# - **AVX2/AVX-512**: ~2-4x faster vector operations (if CPU supports it)
# - **FMA (Fused Multiply-Add)**: ~1.5-2x faster in numerical integration
# - **Better instruction selection**: Compiler can use newer CPU features
# - **Reduced overhead**: Removes runtime CPU detection checks
#
# Typical speedup: 1.2-2x for our workloads (coordinate transforms, propagation)
#
# ## When to Use target-cpu=native
#
# ✅ **DO use for**:
# - Local development (faster iteration)
# - Benchmarking (measure peak performance)
# - Production deployment on known hardware (e.g., AWS instance type)
#
# ❌ **DON'T use for**:
# - Building Python wheels for PyPI (must be portable)
# - CI/CD artifacts (unknown deployment targets)
# - Docker images for general distribution
#
# ## Verifying SIMD is Active
#
# To check what CPU features are being used:
#
# ```bash
# # Check baseline features
# rustc --print cfg | grep target_feature
#
# # Check with native CPU
# rustc --print cfg -C target-cpu=native | grep target_feature
# ```
#
# Common SIMD features:
# - x86_64: sse, sse2, sse3, ssse3, sse4.1, sse4.2, avx, avx2, fma, avx512f
# - ARM64: neon, fp, asimd
#
# ## Code Patterns for Best SIMD Performance
#
# Our code already follows these best practices:
#
# 1. **Contiguous arrays**: nalgebra::Vector3/Vector6 are stack-allocated and contiguous
# 2. **Batch operations**: Using rayon for parallel + SIMD (excellent scaling)
# 3. **Aligned data**: nalgebra ensures proper alignment for SIMD
# 4. **No aliasing**: Immutable borrows allow aggressive optimization
#
# ## References
#
# - Rust SIMD Performance Guide: https://rust-lang.github.io/packed_simd/perf-guide/
# - nalgebra SIMD: https://www.rustsim.org/blog/2020/03/23/simd-aosoa-in-nalgebra/
# - matrixmultiply: https://github.com/bluss/matrixmultiply (auto-detects SIMD)
# - Rust Performance Book: https://nnethercote.github.io/perf-book/build-configuration.html
