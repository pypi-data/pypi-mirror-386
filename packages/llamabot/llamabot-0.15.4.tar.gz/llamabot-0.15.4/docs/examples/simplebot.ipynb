{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "keep_output": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "keep_output": true
   },
   "source": [
    "# LLaMaBot's SimpleBot in under 5 minutes"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "keep_output": true
   },
   "source": [
    "Let's say we have the text of a blog..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "keep_output": true
   },
   "outputs": [],
   "source": [
    "with open(\"../../data/blog_text.txt\", \"r+\") as f:\n",
    "    blog_text = f.read()\n",
    "blog_text[0:100] + \"...\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "keep_output": true
   },
   "source": [
    "And we'd like to create a function that takes in the text and gives us a draft LinkedIn post,\n",
    "complete with emojis,\n",
    "that is designed to entice others to read the blog post.\n",
    "LLaMaBot's `SimpleBot` lets us build that function easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "keep_output": true
   },
   "outputs": [],
   "source": [
    "from llamabot import SimpleBot\n",
    "\n",
    "system_prompt = \"\"\"You are a LinkedIn post generator bot.\n",
    "A human will give you the text of a blog post that they've authored,\n",
    "and you will compose a LinkedIn post that advertises it.\n",
    "The post is intended to hook a reader into reading the blog post.\n",
    "The LinkedIn post should be written with one line per sentence.\n",
    "Each sentence should begin with an emoji appropriate to that sentence.\n",
    "The post should be written in professional English and in first-person tone for the human.\n",
    "\"\"\"\n",
    "\n",
    "linkedin = SimpleBot(\n",
    "    system_prompt=system_prompt,\n",
    "    stream_target=\"stdout\",  # this is the default!,\n",
    "    model_name=\"gpt-4-0125-preview\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that SimpleBot by default will always stream. \n",
    "All that you need to configure is where you want to stream to."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "keep_output": true
   },
   "source": [
    "With `linkedin`, we can now pass in the blog text and - voila! - get back a draft LinkedIn post."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "keep_output": true
   },
   "outputs": [],
   "source": [
    "linkedin_post = linkedin(blog_text)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "keep_output": true
   },
   "source": [
    "Now, you can edit it to your hearts content! :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next up, we have streaming that is compatible with Panel's Chat interface,\n",
    "which expects the text to be returned in its entirety as it is being built up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_panel = SimpleBot(\n",
    "    system_prompt=system_prompt,\n",
    "    stream_target=\"panel\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_post = linkedin_panel(blog_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for post in linkedin_post:\n",
    "    print(post)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we have streaming via the API. We return a generator that yields individual parts of text as they are being generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkedin_api = SimpleBot(\n",
    "    system_prompt=system_prompt,\n",
    "    stream_target=\"api\",\n",
    ")\n",
    "\n",
    "linkedin_post = linkedin_api(blog_text)\n",
    "for post in linkedin_post:\n",
    "    print(post, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have an Ollama server running, you can hit the API using SimpleBot.\n",
    "The pre-requisite is that you have already run `ollama pull <modelname>` \n",
    "to download the model to the Ollama server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(system_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "linkedin_ollama = SimpleBot(\n",
    "    model_name=\"ollama/mistral\",  # Specifying Ollama via the model_name argument is necessary!s\n",
    "    system_prompt=system_prompt,\n",
    "    stream_target=\"stdout\",  # this is the default!\n",
    "    api_base=f\"http://{os.getenv('OLLAMA_SERVER')}:11434\",\n",
    ")\n",
    "linkedin_post = linkedin_ollama(blog_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llamabot",
   "keep_output": true,
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
