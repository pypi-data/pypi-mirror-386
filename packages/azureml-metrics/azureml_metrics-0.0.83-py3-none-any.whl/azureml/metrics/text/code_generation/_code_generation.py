# ---------------------------------------------------------
# Copyright (c) Microsoft Corporation. All rights reserved.
# ---------------------------------------------------------

"""Definitions for Language Modeling metrics."""
import logging
import re
import codecs

from abc import abstractmethod

from typing import Any, List, Optional
from azureml.metrics.common._metric_base import Metric, ScalarMetric
from azureml.metrics.common.utilities import retry
from azureml.metrics import constants
from azureml.metrics.common.exceptions import MissingDependencies

logger = logging.getLogger(__name__)


class CodeGenerationMetric(Metric):
    """Base class for Code Generation metric"""

    def __init__(
        self,
        y_test: List[Any],
        y_pred: List[str],
        test_cases: List[str],
        allow_code_eval: Optional[bool],
        no_of_candidates: Optional[List[int]],
        num_workers: Optional[int],
        timeout: Optional[int],
        dataset: Optional[str],
    ) -> None:
        """
        :param y_test: Actual list of test cases
        :param y_pred: list of list of predictions or code candidates
            generated by model.
        :param metrics: pass@k metric to be computed for code generation task
        :param allow_code_eval: Boolean to indicate whether to execute untrusted model generated code
        :param no_of_candidates: number of code candidates to consider in the evaluation.
        :param num_workers: number of workers used to evaluate the candidate programs
        :param timeout: The maximum time taken to produce a prediction before it is considered a “timeout”.
        """
        self.y_test = y_test
        self.y_pred = y_pred
        self.test_cases = test_cases
        self.allow_code_eval = allow_code_eval
        self.no_of_candidates = no_of_candidates
        self.num_workers = num_workers
        self.timeout = timeout
        self.dataset = dataset
        super().__init__()

    @abstractmethod
    def compute(self) -> Any:
        """Compute the score for the metric"""
        ...


class CodeEval(CodeGenerationMetric, ScalarMetric):
    """Code Eval metric for Code Generation Tasks"""

    hf_code_eval = None

    def compute(self) -> Any:
        """Compute the score for Code Eval metric"""
        if self.test_cases is None:
            logger.warning(
                "code_eval metric is not applicable as it needs test_cases "
                "for every example."
            )
            return {"pass@k": "not-applicable"}

        if self.dataset == "human_eval":
            logger.info("Computing pass@k for openai_humaneval dataset.")
            self.process_humaneval_dataset()

        self.load_code_eval()

        import os

        # setting the environment variable to allow the execution of untrusted
        # remote code generated by the model.
        if self.allow_code_eval is True:
            logger.warning(
                "Setting the environment variable as required by Hugging Face "
                "Evaluate for the execution of untrusted model generated code."
            )
            os.environ["HF_ALLOW_CODE_EVAL"] = "1"

        code_eval_args = {
            "k": self.no_of_candidates,
            "num_workers": self.num_workers,
            "timeout": self.timeout,
        }
        pass_at_k, results = CodeEval.hf_code_eval.compute(
            references=self.test_cases, predictions=self.y_pred, **code_eval_args
        )
        return pass_at_k

    @retry(
        max_attempts=constants.RetryConstants.MAX_ATTEMPTS,
        delay=constants.RetryConstants.DELAY_TIME,
    )
    def load_code_eval(self):
        try:
            import evaluate
        except ImportError:
            safe_message = (
                "evaluate package is not available. "
                "Please run pip install azureml-metrics[evaluate]"
            )

            raise MissingDependencies(safe_message, safe_message=safe_message)

        if CodeEval.hf_code_eval is None:
            CodeEval.hf_code_eval = evaluate.load("code_eval")

    def process_humaneval_dataset(self):
        """process the test_cases"""
        regex_exp = """((?:.*?def.*?FUNCNAME.*?))(?=(?:
        \\S|$))"""

        for i, _ in enumerate(self.y_pred):
            function_entrypoints = list(
                map(
                    lambda prediction_funct: self.find_entrypoint(prediction_funct),
                    self.y_pred[i],
                )
            )
            self.y_pred[i] = list(
                map(
                    lambda prediction: self.apply_regex_expr(
                        prediction, function_entrypoints[i], regex_exp
                    ),
                    self.y_pred[i],
                )
            )

    def find_entrypoint(self, code_str: str):
        regex_exp = r"\bdef\s+(\w+)\s*\("
        match = re.search(regex_exp, code_str)
        if match:
            return match.group(1)
        else:
            return None

    def apply_regex_expr(
        self, text: str, funct_entry_point: str, regex_exp: str = None
    ):
        if regex_exp:
            regex_exp.replace("FUNCNAME", funct_entry_point)
            regex_exp = codecs.decode(regex_exp, "unicode_escape")
            matches = re.search(regex_exp, text, flags=re.DOTALL)
            if matches is None or len(matches.groups()) == 0:
                return text
            return matches.group(1)
        return text
