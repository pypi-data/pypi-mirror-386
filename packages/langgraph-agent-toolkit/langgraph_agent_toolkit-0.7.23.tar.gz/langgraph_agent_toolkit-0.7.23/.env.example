# Web server configuration
HOST=0.0.0.0
PORT=8080

# Skip logging for redirection responses
SKIP_REDIRECTION_LOGGING=true

# Application mode. If the value is "dev", it will enable uvicorn reload
ENV_MODE=development

# Authentication secret, HTTP bearer token header is required if set
AUTH_SECRET=

# Observability backend
# OBSERVABILITY_BACKEND=langfuse

# Langfuse configuration
# LANGFUSE_SECRET_KEY=
# LANGFUSE_PUBLIC_KEY=
# LANGFUSE_HOST=http://langfuse-web:3000

# Langsmith configuration
# LANGSMITH_TRACING=true
# LANGSMITH_API_KEY=
# LANGSMITH_PROJECT=default
# LANGSMITH_ENDPOINT=https://api.smith.langchain.com

# Database type.
# If the value is "postgres", then it will require Postgresql related environment variables.
# If the value is "sqlite", then you can configure optional file path via SQLITE_DB_PATH
MEMORY_BACKEND=postgres

# If DATABASE_TYPE=sqlite (Optional)
SQLITE_DB_PATH=

# If DATABASE_TYPE=postgres
POSTGRES_USER=
POSTGRES_PASSWORD=
POSTGRES_HOST=
POSTGRES_PORT=
POSTGRES_DB=agents
POSTGRES_SCHEMA=checkpoints
# Connection pool settings (optional)
POSTGRES_POOL_SIZE=10
POSTGRES_MIN_SIZE=3
POSTGRES_MAX_IDLE=5

# Agent URL: used in Streamlit app - if not set, defaults to http://{HOST}:{PORT}
# AGENT_URL=http://0.0.0.0:8080

# Use a fake model for testing
USE_FAKE_MODEL=false

# OpenAI Settings
OPENAI_API_KEY=
OPENAI_API_BASE_URL=
OPENAI_API_VERSION=
OPENAI_MODEL_NAME=

# Azure OpenAI Settings
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_ENDPOINT=
AZURE_OPENAI_API_VERSION=
AZURE_OPENAI_MODEL_NAME=
AZURE_OPENAI_DEPLOYMENT_NAME=

# Anthropic Settings
ANTHROPIC_API_KEY=
ANTHROPIC_MODEL_NAME=

 # Google VertexAI Settings
GOOGLE_VERTEXAI_MODEL_NAME=
GOOGLE_VERTEXAI_API_KEY=

# Google GenAI Settings
GOOGLE_GENAI_MODEL_NAME=
GOOGLE_GENAI_API_KEY=

# Bedrock Settings
AWS_BEDROCK_MODEL_NAME=

# DeepSeek Settings
DEEPSEEK_MODEL_NAME=
DEEPSEEK_API_KEY=

# Ollama Settings
OLLAMA_MODEL_NAME=
OLLAMA_BASE_URL=

# Openrouter
OPENROUTER_API_KEY=

# Model configurations - use multiline format with line continuation for readability
# It can be useful if you want to use different models for different agents or use different models from different providers.
MODEL_CONFIGS={"router":{"provider":"azure_openai","model_name":"gpt-4o","openai_api_key":"your-azure-key-here","azure_endpoint":"https://your-resource.openai.azure.com/","openai_api_version":"2024-12-01-preview","deployment_name":"gpt-4o-deployment"},"assistant":{"provider":"azure_openai","model_name":"gpt-4o-mini","openai_api_key":"your-azure-key-here","azure_endpoint":"https://your-resource.openai.azure.com/","openai_api_version":"2024-12-01-preview","deployment_name":"gpt-4o-mini-deployment"},"analyzer":{"provider":"google_genai","model_name":"gemini-pro","api_key":"your-google-key-here","temperature":0.7}}
MODEL_CONFIGS_BASE64=
MODEL_CONFIGS_PATH=

# Amazon Bedrock Knowledge Base ID
AWS_KB_ID=
