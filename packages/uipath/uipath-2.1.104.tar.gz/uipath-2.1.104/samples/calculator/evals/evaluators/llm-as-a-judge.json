{
  "fileName": "llm-as-a-judge.json",
  "id": "llm-as-a-judge",
  "name": "LLMAsAJudge Evaluator",
  "description": "An evaluator that judges the agent based on it's run history and expected behavior",
  "category": 3,
  "type": 7,
  "prompt": "As an expert evaluator, determine how well the agent did on a scale of 0-100. Focus on if the simulation was successful and if the agent behaved according to the expected output accounting for alternative valid expressions, and reasonable variations in language while maintaining high standards for accuracy and completeness. Provide your score with a justification, explaining briefly and concisely why you gave that score.\n----\nUserOrSyntheticInputGivenToAgent:\n{{UserOrSyntheticInput}}\n----\nSimulationInstructions:\n{{SimulationInstructions}}\n----\nExpectedAgentBehavior:\n{{ExpectedAgentBehavior}}\n----\nAgentRunHistory:\n{{AgentRunHistory}}\n",
  "targetOutputKey": "*",
  "model":"gpt-4.1-2025-04-14",
  "createdAt": "2025-08-21T15:12:58.695Z",
  "updatedAt": "2025-08-21T15:12:58.695Z"
}
