"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from cribl_control_plane import utils
from cribl_control_plane.types import BaseModel
from cribl_control_plane.utils import validate_open_enum
from enum import Enum
import pydantic
from pydantic.functional_validators import PlainValidator
from typing import List, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class OutputCloudwatchType(str, Enum):
    CLOUDWATCH = "cloudwatch"


class OutputCloudwatchAuthenticationMethod(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    AUTO = "auto"
    MANUAL = "manual"
    SECRET = "secret"


class OutputCloudwatchBackpressureBehavior(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""How to handle events when all receivers are exerting backpressure"""

    BLOCK = "block"
    DROP = "drop"
    QUEUE = "queue"


class OutputCloudwatchCompression(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    NONE = "none"
    GZIP = "gzip"


class OutputCloudwatchQueueFullBehavior(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""

    BLOCK = "block"
    DROP = "drop"


class OutputCloudwatchMode(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""

    ERROR = "error"
    BACKPRESSURE = "backpressure"
    ALWAYS = "always"


class OutputCloudwatchPqControlsTypedDict(TypedDict):
    pass


class OutputCloudwatchPqControls(BaseModel):
    pass


class OutputCloudwatchTypedDict(TypedDict):
    type: OutputCloudwatchType
    log_group_name: str
    r"""CloudWatch log group to associate events with"""
    log_stream_name: str
    r"""Prefix for CloudWatch log stream name. This prefix will be used to generate a unique log stream name per cribl instance, for example: myStream_myHost_myOutputId"""
    region: str
    r"""Region where the CloudWatchLogs is located"""
    id: NotRequired[str]
    r"""Unique ID for this output"""
    pipeline: NotRequired[str]
    r"""Pipeline to process data before sending out to this output"""
    system_fields: NotRequired[List[str]]
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    aws_authentication_method: NotRequired[OutputCloudwatchAuthenticationMethod]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_secret_key: NotRequired[str]
    endpoint: NotRequired[str]
    r"""CloudWatchLogs service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to CloudWatchLogs-compatible endpoint."""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access CloudWatchLogs"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    max_queue_size: NotRequired[float]
    r"""Maximum number of queued batches before blocking"""
    max_record_size_kb: NotRequired[float]
    r"""Maximum size (KB) of each individual record before compression. For non compressible data 1MB is the max recommended size"""
    flush_period_sec: NotRequired[float]
    r"""Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size."""
    on_backpressure: NotRequired[OutputCloudwatchBackpressureBehavior]
    r"""How to handle events when all receivers are exerting backpressure"""
    description: NotRequired[str]
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""
    pq_max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)"""
    pq_max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    pq_path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""
    pq_compress: NotRequired[OutputCloudwatchCompression]
    r"""Codec to use to compress the persisted data"""
    pq_on_backpressure: NotRequired[OutputCloudwatchQueueFullBehavior]
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""
    pq_mode: NotRequired[OutputCloudwatchMode]
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""
    pq_controls: NotRequired[OutputCloudwatchPqControlsTypedDict]


class OutputCloudwatch(BaseModel):
    type: OutputCloudwatchType

    log_group_name: Annotated[str, pydantic.Field(alias="logGroupName")]
    r"""CloudWatch log group to associate events with"""

    log_stream_name: Annotated[str, pydantic.Field(alias="logStreamName")]
    r"""Prefix for CloudWatch log stream name. This prefix will be used to generate a unique log stream name per cribl instance, for example: myStream_myHost_myOutputId"""

    region: str
    r"""Region where the CloudWatchLogs is located"""

    id: Optional[str] = None
    r"""Unique ID for this output"""

    pipeline: Optional[str] = None
    r"""Pipeline to process data before sending out to this output"""

    system_fields: Annotated[
        Optional[List[str]], pydantic.Field(alias="systemFields")
    ] = None
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[OutputCloudwatchAuthenticationMethod],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = OutputCloudwatchAuthenticationMethod.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    endpoint: Optional[str] = None
    r"""CloudWatchLogs service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to CloudWatchLogs-compatible endpoint."""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use Assume Role credentials to access CloudWatchLogs"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    max_queue_size: Annotated[Optional[float], pydantic.Field(alias="maxQueueSize")] = 5
    r"""Maximum number of queued batches before blocking"""

    max_record_size_kb: Annotated[
        Optional[float], pydantic.Field(alias="maxRecordSizeKB")
    ] = 1024
    r"""Maximum size (KB) of each individual record before compression. For non compressible data 1MB is the max recommended size"""

    flush_period_sec: Annotated[
        Optional[float], pydantic.Field(alias="flushPeriodSec")
    ] = 1
    r"""Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size."""

    on_backpressure: Annotated[
        Annotated[
            Optional[OutputCloudwatchBackpressureBehavior],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="onBackpressure"),
    ] = OutputCloudwatchBackpressureBehavior.BLOCK
    r"""How to handle events when all receivers are exerting backpressure"""

    description: Optional[str] = None

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    pq_max_file_size: Annotated[
        Optional[str], pydantic.Field(alias="pqMaxFileSize")
    ] = "1 MB"
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)"""

    pq_max_size: Annotated[Optional[str], pydantic.Field(alias="pqMaxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    pq_path: Annotated[Optional[str], pydantic.Field(alias="pqPath")] = (
        "$CRIBL_HOME/state/queues"
    )
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""

    pq_compress: Annotated[
        Annotated[
            Optional[OutputCloudwatchCompression],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="pqCompress"),
    ] = OutputCloudwatchCompression.NONE
    r"""Codec to use to compress the persisted data"""

    pq_on_backpressure: Annotated[
        Annotated[
            Optional[OutputCloudwatchQueueFullBehavior],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="pqOnBackpressure"),
    ] = OutputCloudwatchQueueFullBehavior.BLOCK
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""

    pq_mode: Annotated[
        Annotated[
            Optional[OutputCloudwatchMode], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="pqMode"),
    ] = OutputCloudwatchMode.ERROR
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""

    pq_controls: Annotated[
        Optional[OutputCloudwatchPqControls], pydantic.Field(alias="pqControls")
    ] = None
