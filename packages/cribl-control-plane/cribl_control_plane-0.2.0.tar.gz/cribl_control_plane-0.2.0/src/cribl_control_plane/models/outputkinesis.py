"""Code generated by Speakeasy (https://speakeasy.com). DO NOT EDIT."""

from __future__ import annotations
from cribl_control_plane import utils
from cribl_control_plane.types import BaseModel
from cribl_control_plane.utils import validate_open_enum
from enum import Enum
import pydantic
from pydantic.functional_validators import PlainValidator
from typing import List, Optional
from typing_extensions import Annotated, NotRequired, TypedDict


class OutputKinesisType(str, Enum):
    KINESIS = "kinesis"


class OutputKinesisAuthenticationMethod(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    AUTO = "auto"
    MANUAL = "manual"
    SECRET = "secret"


class OutputKinesisSignatureVersion(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Signature version to use for signing Kinesis stream requests"""

    V2 = "v2"
    V4 = "v4"


class OutputKinesisCompression(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Compression type to use for records"""

    NONE = "none"
    GZIP = "gzip"


class OutputKinesisBackpressureBehavior(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""How to handle events when all receivers are exerting backpressure"""

    BLOCK = "block"
    DROP = "drop"
    QUEUE = "queue"


class OutputKinesisPqCompressCompression(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""Codec to use to compress the persisted data"""

    NONE = "none"
    GZIP = "gzip"


class OutputKinesisQueueFullBehavior(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""

    BLOCK = "block"
    DROP = "drop"


class OutputKinesisMode(str, Enum, metaclass=utils.OpenEnumMeta):
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""

    ERROR = "error"
    BACKPRESSURE = "backpressure"
    ALWAYS = "always"


class OutputKinesisPqControlsTypedDict(TypedDict):
    pass


class OutputKinesisPqControls(BaseModel):
    pass


class OutputKinesisTypedDict(TypedDict):
    type: OutputKinesisType
    stream_name: str
    r"""Kinesis stream name to send events to."""
    region: str
    r"""Region where the Kinesis stream is located"""
    id: NotRequired[str]
    r"""Unique ID for this output"""
    pipeline: NotRequired[str]
    r"""Pipeline to process data before sending out to this output"""
    system_fields: NotRequired[List[str]]
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""
    environment: NotRequired[str]
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""
    streamtags: NotRequired[List[str]]
    r"""Tags for filtering and grouping in @{product}"""
    aws_authentication_method: NotRequired[OutputKinesisAuthenticationMethod]
    r"""AWS authentication method. Choose Auto to use IAM roles."""
    aws_secret_key: NotRequired[str]
    endpoint: NotRequired[str]
    r"""Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint."""
    signature_version: NotRequired[OutputKinesisSignatureVersion]
    r"""Signature version to use for signing Kinesis stream requests"""
    reuse_connections: NotRequired[bool]
    r"""Reuse connections between requests, which can improve performance"""
    reject_unauthorized: NotRequired[bool]
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""
    enable_assume_role: NotRequired[bool]
    r"""Use Assume Role credentials to access Kinesis stream"""
    assume_role_arn: NotRequired[str]
    r"""Amazon Resource Name (ARN) of the role to assume"""
    assume_role_external_id: NotRequired[str]
    r"""External ID to use when assuming role"""
    duration_seconds: NotRequired[float]
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""
    concurrency: NotRequired[float]
    r"""Maximum number of ongoing put requests before blocking."""
    max_record_size_kb: NotRequired[float]
    r"""Maximum size (KB) of each individual record before compression. For uncompressed or non-compressible data 1MB is the max recommended size"""
    flush_period_sec: NotRequired[float]
    r"""Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size."""
    compression: NotRequired[OutputKinesisCompression]
    r"""Compression type to use for records"""
    use_list_shards: NotRequired[bool]
    r"""Provides higher stream rate limits, improving delivery speed and reliability by minimizing throttling. See the [ListShards API](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html) documentation for details."""
    as_ndjson: NotRequired[bool]
    r"""Batch events into a single record as NDJSON"""
    on_backpressure: NotRequired[OutputKinesisBackpressureBehavior]
    r"""How to handle events when all receivers are exerting backpressure"""
    description: NotRequired[str]
    aws_api_key: NotRequired[str]
    aws_secret: NotRequired[str]
    r"""Select or create a stored secret that references your access key and secret key"""
    pq_max_file_size: NotRequired[str]
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)"""
    pq_max_size: NotRequired[str]
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""
    pq_path: NotRequired[str]
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""
    pq_compress: NotRequired[OutputKinesisPqCompressCompression]
    r"""Codec to use to compress the persisted data"""
    pq_on_backpressure: NotRequired[OutputKinesisQueueFullBehavior]
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""
    pq_mode: NotRequired[OutputKinesisMode]
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""
    pq_controls: NotRequired[OutputKinesisPqControlsTypedDict]


class OutputKinesis(BaseModel):
    type: OutputKinesisType

    stream_name: Annotated[str, pydantic.Field(alias="streamName")]
    r"""Kinesis stream name to send events to."""

    region: str
    r"""Region where the Kinesis stream is located"""

    id: Optional[str] = None
    r"""Unique ID for this output"""

    pipeline: Optional[str] = None
    r"""Pipeline to process data before sending out to this output"""

    system_fields: Annotated[
        Optional[List[str]], pydantic.Field(alias="systemFields")
    ] = None
    r"""Fields to automatically add to events, such as cribl_pipe. Supports wildcards."""

    environment: Optional[str] = None
    r"""Optionally, enable this config only on a specified Git branch. If empty, will be enabled everywhere."""

    streamtags: Optional[List[str]] = None
    r"""Tags for filtering and grouping in @{product}"""

    aws_authentication_method: Annotated[
        Annotated[
            Optional[OutputKinesisAuthenticationMethod],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="awsAuthenticationMethod"),
    ] = OutputKinesisAuthenticationMethod.AUTO
    r"""AWS authentication method. Choose Auto to use IAM roles."""

    aws_secret_key: Annotated[Optional[str], pydantic.Field(alias="awsSecretKey")] = (
        None
    )

    endpoint: Optional[str] = None
    r"""Kinesis stream service endpoint. If empty, defaults to the AWS Region-specific endpoint. Otherwise, it must point to Kinesis stream-compatible endpoint."""

    signature_version: Annotated[
        Annotated[
            Optional[OutputKinesisSignatureVersion],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="signatureVersion"),
    ] = OutputKinesisSignatureVersion.V4
    r"""Signature version to use for signing Kinesis stream requests"""

    reuse_connections: Annotated[
        Optional[bool], pydantic.Field(alias="reuseConnections")
    ] = True
    r"""Reuse connections between requests, which can improve performance"""

    reject_unauthorized: Annotated[
        Optional[bool], pydantic.Field(alias="rejectUnauthorized")
    ] = True
    r"""Reject certificates that cannot be verified against a valid CA, such as self-signed certificates"""

    enable_assume_role: Annotated[
        Optional[bool], pydantic.Field(alias="enableAssumeRole")
    ] = False
    r"""Use Assume Role credentials to access Kinesis stream"""

    assume_role_arn: Annotated[Optional[str], pydantic.Field(alias="assumeRoleArn")] = (
        None
    )
    r"""Amazon Resource Name (ARN) of the role to assume"""

    assume_role_external_id: Annotated[
        Optional[str], pydantic.Field(alias="assumeRoleExternalId")
    ] = None
    r"""External ID to use when assuming role"""

    duration_seconds: Annotated[
        Optional[float], pydantic.Field(alias="durationSeconds")
    ] = 3600
    r"""Duration of the assumed role's session, in seconds. Minimum is 900 (15 minutes), default is 3600 (1 hour), and maximum is 43200 (12 hours)."""

    concurrency: Optional[float] = 5
    r"""Maximum number of ongoing put requests before blocking."""

    max_record_size_kb: Annotated[
        Optional[float], pydantic.Field(alias="maxRecordSizeKB")
    ] = 1024
    r"""Maximum size (KB) of each individual record before compression. For uncompressed or non-compressible data 1MB is the max recommended size"""

    flush_period_sec: Annotated[
        Optional[float], pydantic.Field(alias="flushPeriodSec")
    ] = 1
    r"""Maximum time between requests. Small values could cause the payload size to be smaller than the configured Max record size."""

    compression: Annotated[
        Optional[OutputKinesisCompression], PlainValidator(validate_open_enum(False))
    ] = OutputKinesisCompression.GZIP
    r"""Compression type to use for records"""

    use_list_shards: Annotated[
        Optional[bool], pydantic.Field(alias="useListShards")
    ] = False
    r"""Provides higher stream rate limits, improving delivery speed and reliability by minimizing throttling. See the [ListShards API](https://docs.aws.amazon.com/kinesis/latest/APIReference/API_ListShards.html) documentation for details."""

    as_ndjson: Annotated[Optional[bool], pydantic.Field(alias="asNdjson")] = True
    r"""Batch events into a single record as NDJSON"""

    on_backpressure: Annotated[
        Annotated[
            Optional[OutputKinesisBackpressureBehavior],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="onBackpressure"),
    ] = OutputKinesisBackpressureBehavior.BLOCK
    r"""How to handle events when all receivers are exerting backpressure"""

    description: Optional[str] = None

    aws_api_key: Annotated[Optional[str], pydantic.Field(alias="awsApiKey")] = None

    aws_secret: Annotated[Optional[str], pydantic.Field(alias="awsSecret")] = None
    r"""Select or create a stored secret that references your access key and secret key"""

    pq_max_file_size: Annotated[
        Optional[str], pydantic.Field(alias="pqMaxFileSize")
    ] = "1 MB"
    r"""The maximum size to store in each queue file before closing and optionally compressing (KB, MB, etc.)"""

    pq_max_size: Annotated[Optional[str], pydantic.Field(alias="pqMaxSize")] = "5GB"
    r"""The maximum disk space that the queue can consume (as an average per Worker Process) before queueing stops. Enter a numeral with units of KB, MB, etc."""

    pq_path: Annotated[Optional[str], pydantic.Field(alias="pqPath")] = (
        "$CRIBL_HOME/state/queues"
    )
    r"""The location for the persistent queue files. To this field's value, the system will append: /<worker-id>/<output-id>."""

    pq_compress: Annotated[
        Annotated[
            Optional[OutputKinesisPqCompressCompression],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="pqCompress"),
    ] = OutputKinesisPqCompressCompression.NONE
    r"""Codec to use to compress the persisted data"""

    pq_on_backpressure: Annotated[
        Annotated[
            Optional[OutputKinesisQueueFullBehavior],
            PlainValidator(validate_open_enum(False)),
        ],
        pydantic.Field(alias="pqOnBackpressure"),
    ] = OutputKinesisQueueFullBehavior.BLOCK
    r"""How to handle events when the queue is exerting backpressure (full capacity or low disk). 'Block' is the same behavior as non-PQ blocking. 'Drop new data' throws away incoming data, while leaving the contents of the PQ unchanged."""

    pq_mode: Annotated[
        Annotated[
            Optional[OutputKinesisMode], PlainValidator(validate_open_enum(False))
        ],
        pydantic.Field(alias="pqMode"),
    ] = OutputKinesisMode.ERROR
    r"""In Error mode, PQ writes events to the filesystem if the Destination is unavailable. In Backpressure mode, PQ writes events to the filesystem when it detects backpressure from the Destination. In Always On mode, PQ always writes events to the filesystem."""

    pq_controls: Annotated[
        Optional[OutputKinesisPqControls], pydantic.Field(alias="pqControls")
    ] = None
