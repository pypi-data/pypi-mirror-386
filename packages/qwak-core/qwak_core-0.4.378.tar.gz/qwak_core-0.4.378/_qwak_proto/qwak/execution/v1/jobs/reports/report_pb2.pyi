"""
@generated by mypy-protobuf.  Do not edit manually!
isort:skip_file
"""
import builtins
import collections.abc
import google.protobuf.descriptor
import google.protobuf.internal.containers
import google.protobuf.internal.enum_type_wrapper
import google.protobuf.message
import google.protobuf.timestamp_pb2
import qwak.execution.v1.backfill_pb2
import sys
import typing

if sys.version_info >= (3, 10):
    import typing as typing_extensions
else:
    import typing_extensions

DESCRIPTOR: google.protobuf.descriptor.FileDescriptor

class _JobStatus:
    ValueType = typing.NewType("ValueType", builtins.int)
    V: typing_extensions.TypeAlias = ValueType

class _JobStatusEnumTypeWrapper(google.protobuf.internal.enum_type_wrapper._EnumTypeWrapper[_JobStatus.ValueType], builtins.type):  # noqa: F821
    DESCRIPTOR: google.protobuf.descriptor.EnumDescriptor
    JOB_STATUS_INVALID: _JobStatus.ValueType  # 0
    """reserved"""
    JOB_STATUS_SUCCESS: _JobStatus.ValueType  # 1
    """job successful"""
    JOB_STATUS_FAILURE: _JobStatus.ValueType  # 2
    """job failed"""
    JOB_STATUS_TOO_EARLY: _JobStatus.ValueType  # 3
    """job is too early"""

class JobStatus(_JobStatus, metaclass=_JobStatusEnumTypeWrapper): ...

JOB_STATUS_INVALID: JobStatus.ValueType  # 0
"""reserved"""
JOB_STATUS_SUCCESS: JobStatus.ValueType  # 1
"""job successful"""
JOB_STATUS_FAILURE: JobStatus.ValueType  # 2
"""job failed"""
JOB_STATUS_TOO_EARLY: JobStatus.ValueType  # 3
"""job is too early"""
global___JobStatus = JobStatus

class FailureInformation(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    ERROR_MESSAGE_FIELD_NUMBER: builtins.int
    error_message: builtins.str
    """holds the full stacktrace of the exception"""
    def __init__(
        self,
        *,
        error_message: builtins.str = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["error_message", b"error_message"]) -> None: ...

global___FailureInformation = FailureInformation

class BatchIngestionJobMetadata(google.protobuf.message.Message):
    """batchV1 job ingestion details"""

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    BATCH_EXECUTION_DATE_FIELD_NUMBER: builtins.int
    INGESTION_WINDOW_START_TIME_FIELD_NUMBER: builtins.int
    INGESTION_WINDOW_END_TIME_FIELD_NUMBER: builtins.int
    @property
    def batch_execution_date(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """LOGICAL batch execution date (cron tick)"""
    @property
    def ingestion_window_start_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """LOGICAL ingestion window start time"""
    @property
    def ingestion_window_end_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """LOGICAL ingestion window end time"""
    def __init__(
        self,
        *,
        batch_execution_date: google.protobuf.timestamp_pb2.Timestamp | None = ...,
        ingestion_window_start_time: google.protobuf.timestamp_pb2.Timestamp | None = ...,
        ingestion_window_end_time: google.protobuf.timestamp_pb2.Timestamp | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["batch_execution_date", b"batch_execution_date", "ingestion_window_end_time", b"ingestion_window_end_time", "ingestion_window_start_time", b"ingestion_window_start_time"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["batch_execution_date", b"batch_execution_date", "ingestion_window_end_time", b"ingestion_window_end_time", "ingestion_window_start_time", b"ingestion_window_start_time"]) -> None: ...

global___BatchIngestionJobMetadata = BatchIngestionJobMetadata

class BatchBackfillJobMetadata(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    NUM_INTERVAL_ROWS_FIELD_NUMBER: builtins.int
    BACKFILL_SPEC_FIELD_NUMBER: builtins.int
    OFFLINE_PROCESSING_START_TIME_FIELD_NUMBER: builtins.int
    OFFLINE_PROCESSING_END_TIME_FIELD_NUMBER: builtins.int
    ONLINE_PROCESSING_START_TIME_FIELD_NUMBER: builtins.int
    ONLINE_PROCESSING_END_TIME_FIELD_NUMBER: builtins.int
    LAST_BATCH_LOGICAL_DATE_FIELD_NUMBER: builtins.int
    num_interval_rows: builtins.int
    """number of incoming rows in the interval"""
    @property
    def backfill_spec(self) -> qwak.execution.v1.backfill_pb2.BackfillSpec:
        """number of rows affected by the backfill"""
    @property
    def offline_processing_start_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """SYSTEM time where offline processing started"""
    @property
    def offline_processing_end_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """SYSTEM time where offline processing ended"""
    @property
    def online_processing_start_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """SYSTEM time when online processing started"""
    @property
    def online_processing_end_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """SYSTEM time when online processing ended"""
    @property
    def last_batch_logical_date(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """Cron tick of last ingestion (window end time)"""
    def __init__(
        self,
        *,
        num_interval_rows: builtins.int = ...,
        backfill_spec: qwak.execution.v1.backfill_pb2.BackfillSpec | None = ...,
        offline_processing_start_time: google.protobuf.timestamp_pb2.Timestamp | None = ...,
        offline_processing_end_time: google.protobuf.timestamp_pb2.Timestamp | None = ...,
        online_processing_start_time: google.protobuf.timestamp_pb2.Timestamp | None = ...,
        online_processing_end_time: google.protobuf.timestamp_pb2.Timestamp | None = ...,
        last_batch_logical_date: google.protobuf.timestamp_pb2.Timestamp | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["backfill_spec", b"backfill_spec", "last_batch_logical_date", b"last_batch_logical_date", "offline_processing_end_time", b"offline_processing_end_time", "offline_processing_start_time", b"offline_processing_start_time", "online_processing_end_time", b"online_processing_end_time", "online_processing_start_time", b"online_processing_start_time"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["backfill_spec", b"backfill_spec", "last_batch_logical_date", b"last_batch_logical_date", "num_interval_rows", b"num_interval_rows", "offline_processing_end_time", b"offline_processing_end_time", "offline_processing_start_time", b"offline_processing_start_time", "online_processing_end_time", b"online_processing_end_time", "online_processing_start_time", b"online_processing_start_time"]) -> None: ...

global___BatchBackfillJobMetadata = BatchBackfillJobMetadata

class StreamingJobMetadata(google.protobuf.message.Message):
    """row-level streaming"""

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    BATCH_ID_FIELD_NUMBER: builtins.int
    batch_id: builtins.int
    """Offline feature-store batch id"""
    def __init__(
        self,
        *,
        batch_id: builtins.int = ...,
    ) -> None: ...
    def ClearField(self, field_name: typing_extensions.Literal["batch_id", b"batch_id"]) -> None: ...

global___StreamingJobMetadata = StreamingJobMetadata

class StreamingAggregationJobMetadata(google.protobuf.message.Message):
    """streaming aggregation"""

    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    def __init__(
        self,
    ) -> None: ...

global___StreamingAggregationJobMetadata = StreamingAggregationJobMetadata

class JobReport(google.protobuf.message.Message):
    DESCRIPTOR: google.protobuf.descriptor.Descriptor

    class JobLabelsEntry(google.protobuf.message.Message):
        DESCRIPTOR: google.protobuf.descriptor.Descriptor

        KEY_FIELD_NUMBER: builtins.int
        VALUE_FIELD_NUMBER: builtins.int
        key: builtins.str
        value: builtins.str
        def __init__(
            self,
            *,
            key: builtins.str = ...,
            value: builtins.str = ...,
        ) -> None: ...
        def ClearField(self, field_name: typing_extensions.Literal["key", b"key", "value", b"value"]) -> None: ...

    BATCH_INGESTION_METADATA_FIELD_NUMBER: builtins.int
    STREAMING_JOB_METADATA_FIELD_NUMBER: builtins.int
    STREAMING_AGGR_JOB_METADATA_FIELD_NUMBER: builtins.int
    BATCH_BACKFILL_JOB_METADATA_FIELD_NUMBER: builtins.int
    NUM_AFFECTED_RECORDS_FIELD_NUMBER: builtins.int
    RUN_START_TIME_FIELD_NUMBER: builtins.int
    RUN_END_TIME_FIELD_NUMBER: builtins.int
    LEGACY_RUN_ID_FIELD_NUMBER: builtins.int
    EXECUTION_ID_FIELD_NUMBER: builtins.int
    ENVIRONMENT_ID_FIELD_NUMBER: builtins.int
    FEATURESET_ID_FIELD_NUMBER: builtins.int
    FEATURESET_NAME_FIELD_NUMBER: builtins.int
    STATUS_FIELD_NUMBER: builtins.int
    JOB_LABELS_FIELD_NUMBER: builtins.int
    JOB_ID_FIELD_NUMBER: builtins.int
    INFO_FIELD_NUMBER: builtins.int
    @property
    def batch_ingestion_metadata(self) -> global___BatchIngestionJobMetadata: ...
    @property
    def streaming_job_metadata(self) -> global___StreamingJobMetadata: ...
    @property
    def streaming_aggr_job_metadata(self) -> global___StreamingAggregationJobMetadata: ...
    @property
    def batch_backfill_job_metadata(self) -> global___BatchBackfillJobMetadata: ...
    num_affected_records: builtins.int
    """number of affected records"""
    @property
    def run_start_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """SYSTEM start time of the job"""
    @property
    def run_end_time(self) -> google.protobuf.timestamp_pb2.Timestamp:
        """SYSTEM end time of the job"""
    legacy_run_id: builtins.int
    """legacy run id (what used to be called execution_id in the old report"""
    execution_id: builtins.str
    """Execution Manager Execution ID"""
    environment_id: builtins.str
    """environment_id"""
    featureset_id: builtins.str
    """featureset id"""
    featureset_name: builtins.str
    """featureset name"""
    status: global___JobStatus.ValueType
    """job status"""
    @property
    def job_labels(self) -> google.protobuf.internal.containers.ScalarMap[builtins.str, builtins.str]:
        """general tags/labels"""
    job_id: builtins.str
    """Job ID"""
    @property
    def info(self) -> global___FailureInformation: ...
    def __init__(
        self,
        *,
        batch_ingestion_metadata: global___BatchIngestionJobMetadata | None = ...,
        streaming_job_metadata: global___StreamingJobMetadata | None = ...,
        streaming_aggr_job_metadata: global___StreamingAggregationJobMetadata | None = ...,
        batch_backfill_job_metadata: global___BatchBackfillJobMetadata | None = ...,
        num_affected_records: builtins.int = ...,
        run_start_time: google.protobuf.timestamp_pb2.Timestamp | None = ...,
        run_end_time: google.protobuf.timestamp_pb2.Timestamp | None = ...,
        legacy_run_id: builtins.int = ...,
        execution_id: builtins.str = ...,
        environment_id: builtins.str = ...,
        featureset_id: builtins.str = ...,
        featureset_name: builtins.str = ...,
        status: global___JobStatus.ValueType = ...,
        job_labels: collections.abc.Mapping[builtins.str, builtins.str] | None = ...,
        job_id: builtins.str = ...,
        info: global___FailureInformation | None = ...,
    ) -> None: ...
    def HasField(self, field_name: typing_extensions.Literal["batch_backfill_job_metadata", b"batch_backfill_job_metadata", "batch_ingestion_metadata", b"batch_ingestion_metadata", "execution_id", b"execution_id", "execution_identifier", b"execution_identifier", "failure_info", b"failure_info", "info", b"info", "job_metadata_type", b"job_metadata_type", "legacy_run_id", b"legacy_run_id", "run_end_time", b"run_end_time", "run_start_time", b"run_start_time", "streaming_aggr_job_metadata", b"streaming_aggr_job_metadata", "streaming_job_metadata", b"streaming_job_metadata"]) -> builtins.bool: ...
    def ClearField(self, field_name: typing_extensions.Literal["batch_backfill_job_metadata", b"batch_backfill_job_metadata", "batch_ingestion_metadata", b"batch_ingestion_metadata", "environment_id", b"environment_id", "execution_id", b"execution_id", "execution_identifier", b"execution_identifier", "failure_info", b"failure_info", "featureset_id", b"featureset_id", "featureset_name", b"featureset_name", "info", b"info", "job_id", b"job_id", "job_labels", b"job_labels", "job_metadata_type", b"job_metadata_type", "legacy_run_id", b"legacy_run_id", "num_affected_records", b"num_affected_records", "run_end_time", b"run_end_time", "run_start_time", b"run_start_time", "status", b"status", "streaming_aggr_job_metadata", b"streaming_aggr_job_metadata", "streaming_job_metadata", b"streaming_job_metadata"]) -> None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing_extensions.Literal["execution_identifier", b"execution_identifier"]) -> typing_extensions.Literal["legacy_run_id", "execution_id"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing_extensions.Literal["failure_info", b"failure_info"]) -> typing_extensions.Literal["info"] | None: ...
    @typing.overload
    def WhichOneof(self, oneof_group: typing_extensions.Literal["job_metadata_type", b"job_metadata_type"]) -> typing_extensions.Literal["batch_ingestion_metadata", "streaming_job_metadata", "streaming_aggr_job_metadata", "batch_backfill_job_metadata"] | None: ...

global___JobReport = JobReport
