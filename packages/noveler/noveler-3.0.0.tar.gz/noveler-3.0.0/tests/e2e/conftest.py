#!/usr/bin/env python3
"""E2Eãƒ†ã‚¹ãƒˆç”¨pytestè¨­å®š

E2Eãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚ã®å…±é€šè¨­å®šã€ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£ã€ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—æ©Ÿèƒ½
"""

import os
import shutil
import tempfile
import time
from pathlib import Path

import psutil
import pytest


pytestmark = pytest.mark.slow

# --- Minimal-progress print override for LLM runs ---
import os as _llm_os, sys as _llm_sys

_def_builtin_print = (lambda: (__builtins__['print'] if isinstance(__builtins__, dict) else __builtins__.print))()

def _llm_e2e_print(*args, **kwargs):
    """Route E2E progress prints to stderr, or suppress when LLM_SILENT_PROGRESS=1."""
    if (_llm_os.getenv("LLM_SILENT_PROGRESS") or "").strip().lower() in {"1","true","on","yes"}:
        return
    kwargs.setdefault('file', _llm_sys.stderr)
    return _def_builtin_print(*args, **kwargs)

# Shadow built-in print in this module only
print = _llm_e2e_print


class E2ETestEnvironment:
    """E2Eãƒ†ã‚¹ãƒˆç’°å¢ƒç®¡ç†"""

    def __init__(self) -> None:
        self.test_start_time = time.time()
        self.baseline_memory = psutil.Process().memory_info().rss / 1024 / 1024  # MB
        self.temp_dirs: dict[str, Path] = {}
        self.cleanup_callbacks = []

        # ãƒ—ãƒ­ã‚»ã‚¹æƒ…å ±ã®è¨˜éŒ²
        self.process = psutil.Process()
        self.initial_cpu_times = self.process.cpu_times()
        self.initial_memory = self.process.memory_info()

        print(f"ğŸ§ª E2Eãƒ†ã‚¹ãƒˆç’°å¢ƒåˆæœŸåŒ– - ãƒ¡ãƒ¢ãƒª: {self.baseline_memory:.1f}MB")

    def create_isolated_temp_dir(self, prefix: str = "e2e_test_") -> Path:
        """åˆ†é›¢ã•ã‚ŒãŸãƒ†ãƒ³ãƒãƒ©ãƒªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ"""
        temp_dir = Path(tempfile.mkdtemp(prefix=prefix))
        self.temp_dirs[prefix] = temp_dir

        # é©åˆ‡ãªæ¨©é™è¨­å®š
        temp_dir.chmod(0o755)

        print(f"ğŸ“ ãƒ†ã‚¹ãƒˆç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ: {temp_dir}")
        return temp_dir

    def register_cleanup(self, callback):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã®ç™»éŒ²"""
        self.cleanup_callbacks.append(callback)

    def cleanup(self):
        """å…¨ä½“ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã®å®Ÿè¡Œ"""
        cleanup_start = time.time()

        # ã‚«ã‚¹ã‚¿ãƒ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã®å®Ÿè¡Œ
        for callback in self.cleanup_callbacks:
            try:
                callback()
            except Exception as e:
                print(f"âš ï¸ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")

        # ãƒ†ãƒ³ãƒãƒ©ãƒªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®å‰Šé™¤
        for temp_dir in self.temp_dirs.values():
            if temp_dir.exists():
                try:
                    shutil.rmtree(temp_dir)
                    print(f"ğŸ—‘ï¸ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {temp_dir}")
                except Exception as e:
                    print(f"âš ï¸ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå‰Šé™¤ã‚¨ãƒ©ãƒ¼: {temp_dir} - {e}")

        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã®å‡ºåŠ›
        self._output_performance_stats()

        cleanup_time = time.time() - cleanup_start
        print(f"âœ… E2Eãƒ†ã‚¹ãƒˆç’°å¢ƒã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº† ({cleanup_time:.2f}s)")

    def _output_performance_stats(self) -> None:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆã®å‡ºåŠ›"""
        try:
            test_duration = time.time() - self.test_start_time
            final_memory = self.process.memory_info().rss / 1024 / 1024
            memory_increase = final_memory - self.baseline_memory

            final_cpu_times = self.process.cpu_times()
            cpu_user_time = final_cpu_times.user - self.initial_cpu_times.user
            cpu_system_time = final_cpu_times.system - self.initial_cpu_times.system

            print("ğŸ“Š E2Eãƒ†ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆ:")
            print(f"   å®Ÿè¡Œæ™‚é–“: {test_duration:.2f}ç§’")
            print(f"   ãƒ¡ãƒ¢ãƒªä½¿ç”¨: {self.baseline_memory:.1f}MB â†’ {final_memory:.1f}MB (+{memory_increase:.1f}MB)")
            print(f"   CPUæ™‚é–“: User {cpu_user_time:.2f}s, System {cpu_system_time:.2f}s")

            # ç•°å¸¸å€¤ã®è­¦å‘Š
            if memory_increase > 100:
                print(f"âš ï¸ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãŒå¤§å¹…ã«å¢—åŠ ã—ã¦ã„ã¾ã™: +{memory_increase:.1f}MB")

            if test_duration > 300:  # 5åˆ†ä»¥ä¸Š
                print(f"âš ï¸ ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚é–“ãŒé•·ã™ãã¾ã™: {test_duration:.1f}ç§’")

        except Exception as e:
            print(f"âš ï¸ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹çµ±è¨ˆå‡ºåŠ›ã‚¨ãƒ©ãƒ¼: {e}")


# ã‚°ãƒ­ãƒ¼ãƒãƒ«ãƒ†ã‚¹ãƒˆç’°å¢ƒã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹
_test_environment = None


@pytest.fixture(scope="session", autouse=True)
def e2e_test_environment():
    """E2Eãƒ†ã‚¹ãƒˆç’°å¢ƒã®ã‚»ãƒƒã‚·ãƒ§ãƒ³ãƒ¬ãƒ™ãƒ«ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£"""
    global _test_environment

    # ç’°å¢ƒã®åˆæœŸåŒ–
    _test_environment = E2ETestEnvironment()

    # å¿…è¦ãªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆ
    project_root = Path(__file__).parent.parent.parent
    temp_dir = project_root / "temp"
    temp_dir.mkdir(exist_ok=True)

    (temp_dir / "cache").mkdir(exist_ok=True)
    (temp_dir / "logs").mkdir(exist_ok=True)
    (temp_dir / "reports").mkdir(exist_ok=True)

    # ç’°å¢ƒå¤‰æ•°ã®è¨­å®š
    original_env = dict(os.environ)

    # ãƒ†ã‚¹ãƒˆå°‚ç”¨ã®ç’°å¢ƒå¤‰æ•°
    os.environ["PYTEST_CURRENT_TEST"] = "e2e"
    os.environ["NOVEL_TEST_MODE"] = "true"
    os.environ["NOVEL_CACHE_DIR"] = str(temp_dir / "cache")
    os.environ["NOVEL_LOG_LEVEL"] = "WARNING"  # ãƒ­ã‚°ã‚’æŠ‘åˆ¶

    print("ğŸŒ E2Eãƒ†ã‚¹ãƒˆç’°å¢ƒè¨­å®šå®Œäº†")

    yield _test_environment

    # ç’°å¢ƒå¤‰æ•°ã®å¾©å…ƒ
    os.environ.clear()
    os.environ.update(original_env)

    # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã®å®Ÿè¡Œ
    _test_environment.cleanup()


@pytest.fixture
def isolated_temp_dir(e2e_test_environment):
    """ãƒ†ã‚¹ãƒˆç”¨ã®åˆ†é›¢ã•ã‚ŒãŸãƒ†ãƒ³ãƒãƒ©ãƒªãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"""
    test_name = os.environ.get("PYTEST_CURRENT_TEST", "unknown").split("::")[-1]
    safe_test_name = "".join(c if c.isalnum() else "_" for c in test_name)

    return e2e_test_environment.create_isolated_temp_dir(f"e2e_{safe_test_name}_")


    # å€‹åˆ¥ãƒ†ã‚¹ãƒˆå¾Œã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã¯ç’°å¢ƒç®¡ç†ã«å§”è­²


@pytest.fixture
def project_root():
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹ã®å–å¾—"""
    return Path(__file__).parent.parent.parent




@pytest.fixture
def test_environment_vars():
    """ãƒ†ã‚¹ãƒˆç”¨ç’°å¢ƒå¤‰æ•°ã®è¨­å®š"""
    original_env = dict(os.environ)

    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’å–å¾—
    project_root = Path(__file__).parent.parent.parent
    src_path = project_root / "src"

    # ãƒ†ã‚¹ãƒˆç”¨ç’°å¢ƒå¤‰æ•°
    test_env_vars = {
        "NOVEL_TEST_ISOLATION": "true",
        "NOVEL_DISABLE_CACHE": "true",
        "NOVEL_SUPPRESS_PROMPTS": "true",
        "NOVEL_LOG_FORMAT": "simple",
        "PYTHONPATH": f"{src_path}:{os.environ.get('PYTHONPATH', '')}"  # src/ãƒ‘ã‚¹è¿½åŠ 
    }

    os.environ.update(test_env_vars)

    yield test_env_vars

    # ç’°å¢ƒå¤‰æ•°ã®å¾©å…ƒ
    os.environ.clear()
    os.environ.update(original_env)


@pytest.fixture
def performance_monitor():
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£"""
    class PerformanceMonitor:
        def __init__(self) -> None:
            self.start_time = time.time()
            self.process = psutil.Process()
            self.start_memory = self.process.memory_info().rss / 1024 / 1024
            self.measurements = []

        def measure(self, label: str):
            """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šç‚¹ã®è¨˜éŒ²"""
            current_time = time.time()
            current_memory = self.process.memory_info().rss / 1024 / 1024

            measurement = {
                "label": label,
                "time": current_time - self.start_time,
                "memory": current_memory,
                "memory_delta": current_memory - self.start_memory
            }

            self.measurements.append(measurement)
            return measurement

        def get_summary(self):
            """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ¸¬å®šã‚µãƒãƒªãƒ¼"""
            if not self.measurements:
                return {}

            total_time = self.measurements[-1]["time"]
            max_memory = max(m["memory"] for m in self.measurements)
            total_memory_delta = self.measurements[-1]["memory_delta"]

            return {
                "total_time": total_time,
                "max_memory": max_memory,
                "total_memory_delta": total_memory_delta,
                "measurement_count": len(self.measurements)
            }

    monitor = PerformanceMonitor()
    yield monitor

    # ãƒ†ã‚¹ãƒˆçµ‚äº†æ™‚ã«ã‚µãƒãƒªãƒ¼ã‚’å‡ºåŠ›
    summary = monitor.get_summary()
    if summary:
        print(f"â±ï¸ ãƒ†ã‚¹ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹: "
              f"{summary['total_time']:.2f}s, "
              f"æœ€å¤§ãƒ¡ãƒ¢ãƒª: {summary['max_memory']:.1f}MB, "
              f"ãƒ¡ãƒ¢ãƒªå¢—åŠ : {summary['total_memory_delta']:+.1f}MB")


# pytest ãƒ•ãƒƒã‚¯é–¢æ•°

def pytest_configure(config):
    """pytestè¨­å®šæ™‚ã®å‡¦ç†"""
    print("ğŸ”§ E2Eãƒ†ã‚¹ãƒˆè¨­å®šã®åˆæœŸåŒ–...")

    # ã‚«ã‚¹ã‚¿ãƒ ãƒãƒ¼ã‚«ãƒ¼ã®ç™»éŒ²
    config.addinivalue_line("markers", "e2e_critical: é‡è¦ãªE2Eãƒ†ã‚¹ãƒˆ")
    config.addinivalue_line("markers", "e2e_smoke: ã‚¹ãƒ¢ãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆç”¨E2E")
    config.addinivalue_line("markers", "e2e_regression: å›å¸°ãƒ†ã‚¹ãƒˆç”¨E2E")


def pytest_sessionstart(session):
    """ãƒ†ã‚¹ãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹æ™‚ã®å‡¦ç†"""
    print("ğŸš€ E2Eãƒ†ã‚¹ãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³é–‹å§‹")

    # ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®å‡ºåŠ›
    print(f"ğŸ–¥ï¸ ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±: CPU {psutil.cpu_count()}ã‚³ã‚¢, "
          f"ãƒ¡ãƒ¢ãƒª {psutil.virtual_memory().total / 1024 / 1024 / 1024:.1f}GB")


def pytest_sessionfinish(session, exitstatus):
    """ãƒ†ã‚¹ãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã®å‡¦ç†"""
    print(f"ğŸ E2Eãƒ†ã‚¹ãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº† (çµ‚äº†ã‚³ãƒ¼ãƒ‰: {exitstatus})")


def pytest_runtest_setup(item):
    """å„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œå‰ã®å‡¦ç†"""
    # ãƒ†ã‚¹ãƒˆåã‚’ç’°å¢ƒå¤‰æ•°ã«è¨­å®š
    os.environ["PYTEST_CURRENT_TEST"] = item.nodeid


def pytest_runtest_teardown(item, nextitem):
    """å„ãƒ†ã‚¹ãƒˆå®Ÿè¡Œå¾Œã®å‡¦ç†"""
    # ã‚¬ãƒ™ãƒ¼ã‚¸ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³å®Ÿè¡Œï¼ˆãƒ¡ãƒ¢ãƒªè§£æ”¾ï¼‰
    import gc
    gc.collect()


def pytest_collection_modifyitems(config, items):
    """ãƒ†ã‚¹ãƒˆåé›†å¾Œã®å‡¦ç†"""

    # E2Eãƒ†ã‚¹ãƒˆã®å®Ÿè¡Œé †åºã‚’æœ€é©åŒ–
    priority_markers = {
        "e2e_smoke": 1,      # ã‚¹ãƒ¢ãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆã‚’æœ€åˆã«
        "e2e_critical": 2,   # é‡è¦ãƒ†ã‚¹ãƒˆã‚’æ¬¡ã«
        "workflow": 3,       # ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ãƒ†ã‚¹ãƒˆ
        "quality": 4,        # å“è³ªãƒ†ã‚¹ãƒˆ
        "performance": 5,    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆï¼ˆæœ€å¾Œï¼‰
        "stress": 6          # ã‚¹ãƒˆãƒ¬ã‚¹ãƒ†ã‚¹ãƒˆï¼ˆæœ€å¾Œï¼‰
    }

    def get_priority(item):
        for marker, priority in priority_markers.items():
            if item.get_closest_marker(marker):
                return priority
        return 10  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå„ªå…ˆåº¦

    # å„ªå…ˆåº¦é †ã§ã‚½ãƒ¼ãƒˆ
    items.sort(key=get_priority)

    skipped_count = sum(1 for item in items if item.get_closest_marker("skip"))
    active_count = len(items) - skipped_count

    print(f"ğŸ“‹ E2Eãƒ†ã‚¹ãƒˆè¨­å®šå®Œäº†: {active_count}ãƒ†ã‚¹ãƒˆæœ‰åŠ¹, {skipped_count}ãƒ†ã‚¹ãƒˆç„¡åŠ¹åŒ– (CLIå»ƒæ­¢ã®ãŸã‚)")


# ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°

def pytest_exception_interact(node, call, report):
    """ä¾‹å¤–ç™ºç”Ÿæ™‚ã®å‡¦ç†"""
    if report.failed:
        # ãƒ†ã‚¹ãƒˆå¤±æ•—æ™‚ã®è©³ç´°æƒ…å ±å‡ºåŠ›
        test_name = node.name
        failure_info = {
            "test": test_name,
            "stage": call.when,
            "duration": getattr(call, "duration", 0),
            "memory": psutil.Process().memory_info().rss / 1024 / 1024
        }

        print(f"âŒ ãƒ†ã‚¹ãƒˆå¤±æ•—è©³ç´°: {failure_info}")


# ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™ã¨ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ

def pytest_runtest_protocol(item, nextitem):
    """ãƒ†ã‚¹ãƒˆå®Ÿè¡Œãƒ—ãƒ­ãƒˆã‚³ãƒ«"""
    # ãƒ†ã‚¹ãƒˆã”ã¨ã®ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™è¨­å®š
    try:
        import resource

        # ãƒ¡ãƒ¢ãƒªåˆ¶é™ï¼ˆ1GBï¼‰
        resource.setrlimit(resource.RLIMIT_AS, (1024 * 1024 * 1024, resource.RLIM_INFINITY))

        # CPUæ™‚é–“åˆ¶é™ï¼ˆ10åˆ†ï¼‰
        resource.setrlimit(resource.RLIMIT_CPU, (600, resource.RLIM_INFINITY))

    except ImportError:
        # Windowsã§ã¯ resource ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãŒåˆ©ç”¨ã§ããªã„
        pass
    except Exception as e:
        print(f"âš ï¸ ãƒªã‚½ãƒ¼ã‚¹åˆ¶é™è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")



# ãƒ†ã‚¹ãƒˆæƒ…å ±ã®åé›†

@pytest.fixture(autouse=True)
def test_metadata(request):
    """ãƒ†ã‚¹ãƒˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã®åé›†"""
    test_info = {
        "name": request.node.name,
        "file": str(request.node.fspath),
        "markers": [marker.name for marker in request.node.iter_markers()],
        "start_time": time.time()
    }

    yield test_info

    # ãƒ†ã‚¹ãƒˆçµ‚äº†æ™‚ã®æƒ…å ±æ›´æ–°
    test_info["duration"] = time.time() - test_info["start_time"]
    test_info["status"] = "completed"

    # é•·æ™‚é–“å®Ÿè¡Œã®è­¦å‘Š
    if test_info["duration"] > 60:  # 1åˆ†ä»¥ä¸Š
        print(f"â° é•·æ™‚é–“å®Ÿè¡Œãƒ†ã‚¹ãƒˆ: {test_info['name']} ({test_info['duration']:.1f}s)")


# ã‚«ã‚¹ã‚¿ãƒ ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³

def pytest_assertrepr_compare(config, op, left, right):
    """ã‚«ã‚¹ã‚¿ãƒ ã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³è¡¨ç¤º"""
    if op == "==" and isinstance(left, Path) and isinstance(right, Path):
        return [
            "ãƒ‘ã‚¹æ¯”è¼ƒå¤±æ•—:",
            f"  å·¦è¾º: {left}",
            f"  å³è¾º: {right}",
            f"  å·¦è¾ºå­˜åœ¨: {left.exists()}",
            f"  å³è¾ºå­˜åœ¨: {right.exists()}"
        ]
    return None
