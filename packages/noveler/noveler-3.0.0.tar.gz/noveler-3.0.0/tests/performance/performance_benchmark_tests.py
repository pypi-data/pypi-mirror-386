#!/usr/bin/env python3
"""ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ

æœ€é©åŒ–å‰å¾Œã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã‚’å®šé‡çš„ã«æ¸¬å®š
30%ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ”¹å–„ã€50%ã®ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å‰Šæ¸›ã‚’æ¤œè¨¼
"""

import asyncio
import gc
import json
import tempfile
import time
from pathlib import Path
from typing import Any

import pytest

from noveler.infrastructure.performance.comprehensive_performance_optimizer import (
    ComprehensivePerformanceOptimizer,
    FileIOOptimizer,
    MemoryOptimizer,
    YAMLOptimizer,
)

# CLIãŒå»ƒæ­¢ã•ã‚ŒãŸãŸã‚ã€æ¨™æº–ã®printæ–‡ã‚’ä½¿ç”¨


class PerformanceBenchmark:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‚¯ãƒ©ã‚¹"""

    def __init__(self) -> None:
        self.optimizer = ComprehensivePerformanceOptimizer()
        self.file_io_optimizer = FileIOOptimizer()
        self.yaml_optimizer = YAMLOptimizer()
        self.memory_optimizer = MemoryOptimizer()
        self.benchmark_results: list[dict[str, Any]] = []

    def measure_execution_time(self, func, *args, **kwargs) -> tuple[Any, float]:
        """å®Ÿè¡Œæ™‚é–“æ¸¬å®š"""
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        return result, end_time - start_time

    def measure_memory_usage(self, func, *args, **kwargs) -> tuple[Any, float, float]:
        """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š"""
        import psutil

        process = psutil.Process()

        gc.collect()
        start_memory = process.memory_info().rss / 1024 / 1024  # MB

        result = func(*args, **kwargs)

        gc.collect()
        end_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_delta = end_memory - start_memory

        return result, start_memory, memory_delta

    def create_test_yaml_data(self, size: str = "medium") -> dict[str, Any]:
        """ãƒ†ã‚¹ãƒˆç”¨YAMLãƒ‡ãƒ¼ã‚¿ç”Ÿæˆ"""
        sizes = {"small": 100, "medium": 1000, "large": 10000}

        items_count = sizes.get(size, 1000)

        test_data = {
            "metadata": {
                "created_at": "2025-08-27T00:00:00Z",
                "version": "1.0.0",
                "description": f"Test data with {items_count} items",
            },
            "items": [],
        }

        for i in range(items_count):
            test_data["items"].append(
                {
                    "id": f"item_{i:04d}",
                    "name": f"Test Item {i}",
                    "description": f"This is test item number {i} for performance testing",
                    "properties": {
                        "category": f"category_{i % 10}",
                        "priority": i % 5,
                        "active": i % 2 == 0,
                        "tags": [f"tag_{j}" for j in range(i % 5 + 1)],
                    },
                }
            )

        return test_data


class FileIOPerformanceTests:
    """ãƒ•ã‚¡ã‚¤ãƒ«I/Oæœ€é©åŒ–ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""

    def __init__(self, benchmark: PerformanceBenchmark) -> None:
        self.benchmark = benchmark
        self.test_dir = Path(tempfile.mkdtemp(prefix="performance_test_"))

    def test_file_reading_performance(self) -> dict[str, Any]:
        """ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        test_files = []
        for i in range(50):  # 50ãƒ•ã‚¡ã‚¤ãƒ«
            test_file = self.test_dir / f"test_file_{i}.txt"
            test_content = f"Test content for file {i}\n" * 100  # 100è¡Œ
            test_file.write_text(test_content, encoding="utf-8")
            test_files.append(test_file)

        # æ¨™æº–çš„ãªèª­ã¿è¾¼ã¿æ¸¬å®š
        def standard_read():
            contents = []
            for file_path in test_files:
                with open(file_path, encoding="utf-8") as f:
                    contents.append(f.read())
            return contents

        # æœ€é©åŒ–ã•ã‚ŒãŸèª­ã¿è¾¼ã¿æ¸¬å®š
        def optimized_read():
            contents = []
            for file_path in test_files:
                content = self.benchmark.file_io_optimizer.optimized_read_text(file_path)
                contents.append(content)
            return contents

        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        _, standard_time = self.benchmark.measure_execution_time(standard_read)
        _, optimized_time = self.benchmark.measure_execution_time(optimized_read)

        # 2å›ç›®å®Ÿè¡Œï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœæ¸¬å®šï¼‰
        _, optimized_time_cached = self.benchmark.measure_execution_time(optimized_read)

        improvement_percent = ((standard_time - optimized_time) / standard_time) * 100
        cache_improvement = ((optimized_time - optimized_time_cached) / optimized_time) * 100

        return {
            "test_name": "file_reading_performance",
            "files_count": len(test_files),
            "standard_time": standard_time,
            "optimized_time": optimized_time,
            "optimized_time_cached": optimized_time_cached,
            "improvement_percent": improvement_percent,
            "cache_improvement_percent": cache_improvement,
            "target_improvement": 30.0,  # 30%æ”¹å–„ç›®æ¨™
            "target_achieved": improvement_percent >= 30.0,
        }

    def test_batch_file_writing_performance(self) -> dict[str, Any]:
        """ãƒãƒƒãƒãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        test_data = [f"Test data line {i}\n" for i in range(1000)]

        # æ¨™æº–çš„ãªæ›¸ãè¾¼ã¿æ¸¬å®š
        def standard_write() -> None:
            for i, data in enumerate(test_data):
                file_path = self.test_dir / f"standard_{i}.txt"
                file_path.write_text(data, encoding="utf-8")

        # ãƒãƒƒãƒæ›¸ãè¾¼ã¿æ¸¬å®š
        def batch_write() -> None:
            for i, data in enumerate(test_data):
                file_path = self.test_dir / f"batch_{i}.txt"
                self.benchmark.file_io_optimizer.batch_write_text(file_path, data)
            self.benchmark.file_io_optimizer.flush_batch_operations()

        _, standard_time = self.benchmark.measure_execution_time(standard_write)
        _, batch_time = self.benchmark.measure_execution_time(batch_write)

        improvement_percent = ((standard_time - batch_time) / standard_time) * 100

        return {
            "test_name": "batch_file_writing_performance",
            "operations_count": len(test_data),
            "standard_time": standard_time,
            "batch_time": batch_time,
            "improvement_percent": improvement_percent,
            "target_improvement": 40.0,  # 40%æ”¹å–„ç›®æ¨™
            "target_achieved": improvement_percent >= 40.0,
        }

    def cleanup(self):
        """ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        import shutil

        if self.test_dir.exists():
            shutil.rmtree(self.test_dir)


class YAMLPerformanceTests:
    """YAMLå‡¦ç†æœ€é©åŒ–ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""

    def __init__(self, benchmark: PerformanceBenchmark) -> None:
        self.benchmark = benchmark
        self.test_dir = Path(tempfile.mkdtemp(prefix="yaml_performance_test_"))

    def test_yaml_processing_performance(self) -> dict[str, Any]:
        """YAMLå‡¦ç†ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        # ãƒ†ã‚¹ãƒˆYAMLãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
        test_files = []
        for size in ["small", "medium", "large"]:
            for i in range(10):  # å„ã‚µã‚¤ã‚º10ãƒ•ã‚¡ã‚¤ãƒ«
                yaml_file = self.test_dir / f"test_{size}_{i}.yaml"
                test_data = self.benchmark.create_test_yaml_data(size)

                import yaml

                with open(yaml_file, "w", encoding="utf-8") as f:
                    yaml.dump(test_data, f, allow_unicode=True, default_flow_style=False)

                test_files.append(yaml_file)

        # æ¨™æº–çš„ãªYAMLèª­ã¿è¾¼ã¿æ¸¬å®š
        def standard_yaml_load():
            results = []
            for file_path in test_files:
                import yaml

                with open(file_path, encoding="utf-8") as f:
                    data = yaml.safe_load(f)
                    results.append(data)
            return results

        # æœ€é©åŒ–ã•ã‚ŒãŸYAMLèª­ã¿è¾¼ã¿æ¸¬å®š
        def optimized_yaml_load():
            results = []
            for file_path in test_files:
                data = self.benchmark.yaml_optimizer.optimized_yaml_load(file_path)
                results.append(data)
            return results

        # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
        _, standard_time = self.benchmark.measure_execution_time(standard_yaml_load)
        _, optimized_time = self.benchmark.measure_execution_time(optimized_yaml_load)

        # 2å›ç›®å®Ÿè¡Œï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æœï¼‰
        _, optimized_time_cached = self.benchmark.measure_execution_time(optimized_yaml_load)

        improvement_percent = ((standard_time - optimized_time) / standard_time) * 100
        cache_improvement = ((optimized_time - optimized_time_cached) / optimized_time) * 100

        return {
            "test_name": "yaml_processing_performance",
            "files_count": len(test_files),
            "standard_time": standard_time,
            "optimized_time": optimized_time,
            "optimized_time_cached": optimized_time_cached,
            "improvement_percent": improvement_percent,
            "cache_improvement_percent": cache_improvement,
            "target_improvement": 50.0,  # 50%æ”¹å–„ç›®æ¨™
            "target_achieved": improvement_percent >= 50.0,
        }

    def test_yaml_writing_performance(self) -> dict[str, Any]:
        """YAMLæ›¸ãè¾¼ã¿ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""
        test_data_sets = [self.benchmark.create_test_yaml_data("medium") for _ in range(20)]

        # æ¨™æº–çš„ãªYAMLæ›¸ãè¾¼ã¿
        def standard_yaml_write() -> None:
            import yaml

            for i, data in enumerate(test_data_sets):
                file_path = self.test_dir / f"standard_output_{i}.yaml"
                with open(file_path, "w", encoding="utf-8") as f:
                    yaml.dump(data, f, allow_unicode=True, default_flow_style=False)

        # æœ€é©åŒ–ã•ã‚ŒãŸYAMLæ›¸ãè¾¼ã¿
        def optimized_yaml_write() -> None:
            for i, data in enumerate(test_data_sets):
                file_path = self.test_dir / f"optimized_output_{i}.yaml"
                self.benchmark.yaml_optimizer.optimized_yaml_dump(data, file_path)

        _, standard_time = self.benchmark.measure_execution_time(standard_yaml_write)
        _, optimized_time = self.benchmark.measure_execution_time(optimized_yaml_write)

        improvement_percent = ((standard_time - optimized_time) / standard_time) * 100

        return {
            "test_name": "yaml_writing_performance",
            "datasets_count": len(test_data_sets),
            "standard_time": standard_time,
            "optimized_time": optimized_time,
            "improvement_percent": improvement_percent,
            "target_improvement": 25.0,  # 25%æ”¹å–„ç›®æ¨™
            "target_achieved": improvement_percent >= 25.0,
        }

    def cleanup(self):
        """ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        import shutil

        if self.test_dir.exists():
            shutil.rmtree(self.test_dir)


class MemoryPerformanceTests:
    """ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆ"""

    def __init__(self, benchmark: PerformanceBenchmark) -> None:
        self.benchmark = benchmark

    def test_large_data_processing_memory(self) -> dict[str, Any]:
        """å¤§å®¹é‡ãƒ‡ãƒ¼ã‚¿å‡¦ç†ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ†ã‚¹ãƒˆ"""
        # å¤§ããªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”Ÿæˆ
        large_dataset = [f"Large data item {i}" * 100 for i in range(10000)]

        # æ¨™æº–çš„ãªå‡¦ç†ï¼ˆå…¨ã¦ãƒ¡ãƒ¢ãƒªã«ãƒ­ãƒ¼ãƒ‰ï¼‰
        def standard_processing():
            results = []
            for item in large_dataset:
                processed = item.upper().replace(" ", "_")
                results.append(processed)
            return results

        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªå‡¦ç†
        def optimized_processing():
            results = []
            with self.benchmark.memory_optimizer.memory_efficient_processing():
                for chunk in self.benchmark.memory_optimizer.optimize_large_data_processing(large_dataset):
                    processed = chunk.upper().replace(" ", "_")
                    results.append(processed)
            return results

        # ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æ¸¬å®š
        _, start_mem1, mem_delta1 = self.benchmark.measure_memory_usage(standard_processing)
        _, start_mem2, mem_delta2 = self.benchmark.measure_memory_usage(optimized_processing)

        memory_savings_percent = ((mem_delta1 - mem_delta2) / mem_delta1) * 100 if mem_delta1 > 0 else 0

        return {
            "test_name": "large_data_processing_memory",
            "dataset_size": len(large_dataset),
            "standard_memory_delta": mem_delta1,
            "optimized_memory_delta": mem_delta2,
            "memory_savings_percent": memory_savings_percent,
            "target_savings": 50.0,  # 50%å‰Šæ¸›ç›®æ¨™
            "target_achieved": memory_savings_percent >= 50.0,
        }

    def test_caching_memory_efficiency(self) -> dict[str, Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ¡ãƒ¢ãƒªåŠ¹ç‡ãƒ†ã‚¹ãƒˆ"""
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹ç‡æ¸¬å®šç”¨ã®ãƒ‡ãƒ¼ã‚¿
        cache_keys = [f"key_{i}" for i in range(1000)]
        cache_values = [f"value_{i}" * 50 for i in range(1000)]

        def test_caching():
            cache = self.benchmark.optimizer.cache_manager

            # ãƒ‡ãƒ¼ã‚¿æ›¸ãè¾¼ã¿
            for key, value in zip(cache_keys, cache_values, strict=False):
                cache.set(key, value)

            # ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆï¼‰
            results = []
            for key in cache_keys:
                result = cache.get(key)
                results.append(result)

            return results, cache.get_hit_rate()

        _, start_mem, mem_delta = self.benchmark.measure_memory_usage(test_caching)
        _, hit_rate = test_caching()

        return {
            "test_name": "caching_memory_efficiency",
            "cache_items": len(cache_keys),
            "memory_usage": mem_delta,
            "hit_rate": hit_rate,
            "target_hit_rate": 0.95,  # 95%ãƒ’ãƒƒãƒˆç‡ç›®æ¨™
            "target_achieved": hit_rate >= 0.95,
        }


class ComprehensivePerformanceBenchmarkRunner:
    """åŒ…æ‹¬çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self) -> None:
        self.benchmark = PerformanceBenchmark()
        self.results: dict[str, Any] = {}

    async def run_all_benchmarks(self) -> dict[str, Any]:
        """å…¨ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ"""
        print("ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯é–‹å§‹...")
        start_time = time.time()

        # 1. ãƒ•ã‚¡ã‚¤ãƒ«I/Oãƒ†ã‚¹ãƒˆ
        print("ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«I/Oãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        file_io_tests = FileIOPerformanceTests(self.benchmark)
        file_io_results = [
            file_io_tests.test_file_reading_performance(),
            file_io_tests.test_batch_file_writing_performance(),
        ]
        file_io_tests.cleanup()

        # 2. YAMLãƒ†ã‚¹ãƒˆ
        print("ğŸ“„ YAMLå‡¦ç†ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        yaml_tests = YAMLPerformanceTests(self.benchmark)
        yaml_results = [yaml_tests.test_yaml_processing_performance(), yaml_tests.test_yaml_writing_performance()]
        yaml_tests.cleanup()

        # 3. ãƒ¡ãƒ¢ãƒªãƒ†ã‚¹ãƒˆ
        print("ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè¡Œä¸­...")
        memory_tests = MemoryPerformanceTests(self.benchmark)
        memory_results = [
            memory_tests.test_large_data_processing_memory(),
            memory_tests.test_caching_memory_efficiency(),
        ]

        total_time = time.time() - start_time

        # çµæœçµ±åˆ
        all_results = file_io_results + yaml_results + memory_results

        # å…¨ä½“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡
        overall_assessment = self._assess_overall_performance(all_results)

        self.results = {
            "benchmark_timestamp": time.time(),
            "total_execution_time": total_time,
            "file_io_results": file_io_results,
            "yaml_results": yaml_results,
            "memory_results": memory_results,
            "overall_assessment": overall_assessment,
            "targets_achieved": self._count_targets_achieved(all_results),
            "performance_summary": self._generate_performance_summary(all_results),
        }

        print(f"âœ… ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Œäº† ({total_time:.2f}ç§’)")
        return self.results

    def _assess_overall_performance(self, all_results: list[dict[str, Any]]) -> dict[str, Any]:
        """å…¨ä½“ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹è©•ä¾¡"""
        targets_met = sum(1 for result in all_results if result.get("target_achieved", False))
        total_tests = len(all_results)

        # å„ã‚«ãƒ†ã‚´ãƒªã®å¹³å‡æ”¹å–„ç‡è¨ˆç®—
        improvements = []
        memory_savings = []

        for result in all_results:
            if "improvement_percent" in result:
                improvements.append(result["improvement_percent"])
            if "memory_savings_percent" in result:
                memory_savings.append(result["memory_savings_percent"])

        avg_improvement = sum(improvements) / len(improvements) if improvements else 0
        avg_memory_savings = sum(memory_savings) / len(memory_savings) if memory_savings else 0

        # å…¨ä½“è©•ä¾¡
        if targets_met / total_tests >= 0.8 and avg_improvement >= 30:
            grade = "Excellent"
            recommendation = "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æœ€é©åŒ–ç›®æ¨™ã‚’ä¸Šå›ã‚‹æˆæœ"
        elif targets_met / total_tests >= 0.6 and avg_improvement >= 20:
            grade = "Good"
            recommendation = "è‰¯å¥½ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã‚’é”æˆ"
        elif targets_met / total_tests >= 0.4:
            grade = "Fair"
            recommendation = "éƒ¨åˆ†çš„ãªæ”¹å–„ã€è¿½åŠ æœ€é©åŒ–ã‚’æ¨å¥¨"
        else:
            grade = "Poor"
            recommendation = "æœ€é©åŒ–æˆ¦ç•¥ã®è¦‹ç›´ã—ãŒå¿…è¦"

        return {
            "grade": grade,
            "targets_met_ratio": targets_met / total_tests,
            "average_improvement_percent": avg_improvement,
            "average_memory_savings_percent": avg_memory_savings,
            "recommendation": recommendation,
        }

    def _count_targets_achieved(self, all_results: list[dict[str, Any]]) -> dict[str, int]:
        """ç›®æ¨™é”æˆã‚«ã‚¦ãƒ³ãƒˆ"""
        return {
            "total_tests": len(all_results),
            "targets_achieved": sum(1 for result in all_results if result.get("target_achieved", False)),
            "targets_missed": sum(1 for result in all_results if not result.get("target_achieved", False)),
        }

    def _generate_performance_summary(self, all_results: list[dict[str, Any]]) -> dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        return {
            "best_improvement": max(all_results, key=lambda x: x.get("improvement_percent", 0)),
            "worst_performance": min(all_results, key=lambda x: x.get("improvement_percent", float("inf"))),
            "recommendations": [
                "ç¶™ç¶šçš„ãªãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›£è¦–ã‚’å®Ÿè£…",
                "ã‚­ãƒ£ãƒƒã‚·ãƒ¥æˆ¦ç•¥ã®æ›´ãªã‚‹æœ€é©åŒ–",
                "éåŒæœŸå‡¦ç†ã®æ´»ç”¨æ‹¡å¤§",
                "ãƒ¡ãƒ¢ãƒªåŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿æ§‹é€ ã®æ¡ç”¨",
            ],
        }

    def print_benchmark_results(self):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœè¡¨ç¤º"""
        if not self.results:
            print("âš ï¸ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœãŒã‚ã‚Šã¾ã›ã‚“")
            return

        results = self.results

        print("\n" + "=" * 80)
        print("ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœ")
        print("=" * 80)

        # å…¨ä½“è©•ä¾¡
        assessment = results["overall_assessment"]
        print(f"\nğŸ¯ å…¨ä½“è©•ä¾¡: {assessment['grade']}")
        print(f"ğŸ“ˆ å¹³å‡æ”¹å–„ç‡: {assessment['average_improvement_percent']:.1f}%")
        print(f"ğŸ’¾ å¹³å‡ãƒ¡ãƒ¢ãƒªå‰Šæ¸›: {assessment['average_memory_savings_percent']:.1f}%")
        print(f"ğŸª ç›®æ¨™é”æˆç‡: {assessment['targets_met_ratio']:.1%}")
        print(f"ğŸ’¡ æ¨å¥¨: {assessment['recommendation']}")

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµæœ
        categories = [
            ("file_io_results", "ğŸ“ ãƒ•ã‚¡ã‚¤ãƒ«I/Oæœ€é©åŒ–"),
            ("yaml_results", "ğŸ“„ YAMLå‡¦ç†æœ€é©åŒ–"),
            ("memory_results", "ğŸ’¾ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡æœ€é©åŒ–"),
        ]

        for category_key, category_name in categories:
            print(f"\n{category_name}:")
            print("-" * 60)

            for result in results[category_key]:
                test_name = result["test_name"]
                target_achieved = "âœ…" if result.get("target_achieved", False) else "âŒ"

                print(f"  {target_achieved} {test_name}")

                if "improvement_percent" in result:
                    print(f"      æ”¹å–„ç‡: {result['improvement_percent']:.1f}%")
                if "memory_savings_percent" in result:
                    print(f"      ãƒ¡ãƒ¢ãƒªå‰Šæ¸›: {result['memory_savings_percent']:.1f}%")
                if "hit_rate" in result:
                    print(f"      ãƒ’ãƒƒãƒˆç‡: {result['hit_rate']:.1%}")

        # æ¨å¥¨äº‹é …
        print("\nğŸ’¡ ä»Šå¾Œã®æ¨å¥¨äº‹é …:")
        for i, rec in enumerate(results["performance_summary"]["recommendations"], 1):
            print(f"  {i}. {rec}")

        print("=" * 80)

    def export_benchmark_results(self, output_path: Path):
        """ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        if not self.results:
            print("âš ï¸ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¯¾è±¡ã®çµæœãŒã‚ã‚Šã¾ã›ã‚“")
            return

        output_path.parent.mkdir(parents=True, exist_ok=True)

        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(self.results, f, ensure_ascii=False, indent=2, default=str)

        print(f"ğŸ“„ ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ: {output_path}")


# pytest ãƒ†ã‚¹ãƒˆé–¢æ•°
@pytest.mark.asyncio
async def test_comprehensive_performance_benchmark():
    """åŒ…æ‹¬çš„ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ"""
    runner = ComprehensivePerformanceBenchmarkRunner()
    results = await runner.run_all_benchmarks()

    # åŸºæœ¬çš„ãªã‚¢ã‚µãƒ¼ã‚·ãƒ§ãƒ³
    assert results is not None
    assert "overall_assessment" in results
    assert "file_io_results" in results
    assert "yaml_results" in results
    assert "memory_results" in results

    # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ç›®æ¨™ã®ç¢ºèª
    assessment = results["overall_assessment"]
    assert assessment["average_improvement_percent"] > 0

    # çµæœè¡¨ç¤º
    runner.print_benchmark_results()

    return results


async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    runner = ComprehensivePerformanceBenchmarkRunner()

    # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯å®Ÿè¡Œ
    await runner.run_all_benchmarks()

    # çµæœè¡¨ç¤º
    runner.print_benchmark_results()

    # çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
    output_path = Path("temp/performance_benchmark_results.json")
    runner.export_benchmark_results(output_path)


if __name__ == "__main__":
    asyncio.run(main())
