#!/usr/bin/env python3
"""æ®µéšçš„å“è³ªãƒã‚§ãƒƒã‚¯MCPãƒ„ãƒ¼ãƒ«ã®çµ±åˆãƒ†ã‚¹ãƒˆ

MCPã‚µãƒ¼ãƒãƒ¼ã®æ®µéšçš„å“è³ªãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½ã‚’ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ã§ãƒ†ã‚¹ãƒˆã€‚
B20æº–æ‹ : å®Ÿéš›ã®MCPã‚µãƒ¼ãƒãƒ¼çµŒç”±ã§ã®æ®µéšçš„å®Ÿè¡Œã‚’æ¤œè¨¼ã€‚

ä»•æ§˜æ›¸: SPEC-MCP-PROGRESSIVE-CHECK-001
"""

import json
import pytest
import asyncio
from pathlib import Path
from tempfile import TemporaryDirectory
from unittest.mock import AsyncMock, Mock, patch

import src.mcp_servers.noveler.main as mcp_main
from noveler.presentation.mcp import dispatcher as mcp_dispatcher

# MCPã‚µãƒ¼ãƒãƒ¼ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆãƒ†ã‚¹ãƒˆ
try:
    from src.mcp_servers.noveler.main import server as mcp_server
    MCP_SERVER_AVAILABLE = True
except ImportError:
    MCP_SERVER_AVAILABLE = False
    mcp_server = None


class TestProgressiveCheckMCPTools:
    """æ®µéšçš„å“è³ªãƒã‚§ãƒƒã‚¯MCPãƒ„ãƒ¼ãƒ«ã®ãƒ†ã‚¹ãƒˆ"""

    @pytest.fixture
    async def mcp_server_instance(self):
        """ãƒ†ã‚¹ãƒˆç”¨MCPã‚µãƒ¼ãƒãƒ¼"""
        if not MCP_SERVER_AVAILABLE:
            pytest.skip("MCPã‚µãƒ¼ãƒãƒ¼ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“")
        return mcp_server

    @pytest.fixture
    def temp_project(self):
        """ãƒ†ã‚¹ãƒˆç”¨ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆç’°å¢ƒ"""
        with TemporaryDirectory() as temp_dir:
            project_root = Path(temp_dir)

            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ ä½œæˆ
            (project_root / "manuscripts").mkdir()
            (project_root / ".noveler" / "checks").mkdir(parents=True)

            # ãƒ†ã‚¹ãƒˆåŸç¨¿ä½œæˆ
            test_manuscript = project_root / "manuscripts" / "episode_001.md"
            test_manuscript.write_text(
                "ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆç”¨ã®åŸç¨¿ã§ã™ã€‚èª¤å­—ã‚„æ–‡æ³•ã®å•é¡ŒãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€‚",
                encoding="utf-8"
            )

            yield project_root

    @pytest.mark.asyncio
    async def test_get_check_tasks_tool(self, mcp_server_instance, temp_project):
        """get_check_tasksãƒ„ãƒ¼ãƒ«ã®ãƒ†ã‚¹ãƒˆ"""

        session_id = "EP001_202509270900"
        tasks_payload = {
            "session_id": session_id,
            "episode_number": 1,
            "current_step": 1,
            "current_task": {
                "id": 1,
                "name": "èª¤å­—è„±å­—ãƒã‚§ãƒƒã‚¯",
                "phase": "basic_quality",
                "description": "åŸºæœ¬çš„ãªèª¤å­—è„±å­—ã®æ¤œå‡º",
            },
            "executable_tasks": [
                {
                    "id": 1,
                    "name": "èª¤å­—è„±å­—ãƒã‚§ãƒƒã‚¯",
                    "phase": "basic_quality",
                    "description": "åŸºæœ¬çš„ãªèª¤å­—è„±å­—ã®æ¤œå‡º",
                    "estimated_duration": "3-5åˆ†",
                },
                {
                    "id": 2,
                    "name": "æ–‡æ³•ãƒ»è¡¨è¨˜çµ±ä¸€ãƒã‚§ãƒƒã‚¯",
                    "phase": "basic_quality",
                    "description": "æ–‡æ³•ã®æ­£ç¢ºæ€§ã¨è¡¨è¨˜ã®ä¸€è²«æ€§ã‚’ç¢ºèª",
                    "estimated_duration": "5-8åˆ†",
                },
            ],
            "progress": {"completed": 0, "total": 12, "percentage": 0.0},
            "llm_instruction": "LLMã«ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆã‚’æç¤ºã—ã¦ãã ã•ã„",
            "next_action": "execute_check_step",
            "phase_info": {"phase": "basic_quality"},
        }

        with patch('noveler.domain.services.progressive_check_manager.ProgressiveCheckManager') as mock_manager:
            mock_instance = Mock()
            mock_manager.return_value = mock_instance
            mock_instance.get_check_tasks.return_value = tasks_payload

            result = await mcp_dispatcher.dispatch(
                "get_check_tasks",
                {
                    "episode_number": 1,
                    "project_root": str(temp_project),
                },
            )

        mock_instance.get_check_tasks.assert_called_once()
        assert result["success"] is True
        assert result["session_id"] == session_id
        assert result["execution_method"] == "progressive_check_manager"
        assert result["tasks_info"]["session_id"] == session_id
        assert len(result["tasks_info"]["executable_tasks"]) == 2

    @pytest.mark.asyncio
    async def test_execute_check_step_tool(self, mcp_server_instance, temp_project):
        """execute_check_stepãƒ„ãƒ¼ãƒ«ã®ãƒ†ã‚¹ãƒˆ"""
        with patch('noveler.domain.services.progressive_check_manager.ProgressiveCheckManager') as mock_manager:
            mock_instance = Mock()
            mock_manager.return_value = mock_instance
            mock_instance.session_id = "EP001_202509270901"
            mock_instance.execute_check_step.return_value = {
                "step_id": 1,
                "success": True,
                "execution_time": 3.2,
                "issues_found": 2,
                "quality_score": 88.5,
                "corrections": [
                    "1è¡Œç›®: ã€Œã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€â†’ã€Œã‹ã‚‚ã—ã‚Œã¾ã›ã‚“ã€ï¼ˆåŠ©è©ã®çµ±ä¸€ï¼‰",
                    "1è¡Œç›®: ã€Œèª¤å­—ã€â†’ã€Œèª¤å­—ã€ï¼ˆè¡¨è¨˜ç¢ºèªå®Œäº†ï¼‰",
                ],
                "next_step": 2,
                "files": {
                    "input_file": "EP001_202501091430_step_1_input.json",
                    "output_file": "EP001_202501091430_step_1_output.json",
                },
            }

            result = await mcp_dispatcher.dispatch(
                "execute_check_step",
                {
                    "episode_number": 1,
                    "step_id": 1,
                    "input_data": {
                        "manuscript_content": "ãƒ†ã‚¹ãƒˆåŸç¨¿å†…å®¹",
                        "check_focus": "typo_detection",
                    },
                    "project_root": str(temp_project),
                },
            )

        mock_instance.execute_check_step.assert_called_once_with(1, {
            "manuscript_content": "ãƒ†ã‚¹ãƒˆåŸç¨¿å†…å®¹",
            "check_focus": "typo_detection",
        }, False)
        assert result["success"] is True
        assert result["execution_result"]["step_id"] == 1
        assert result["execution_result"]["session_id"] == "EP001_202509270901"
        assert result["execution_result"]["next_step"] == 2
        assert len(result["execution_result"]["corrections"]) == 2

    @pytest.mark.asyncio
    async def test_execute_check_step_command_returns_session_info(self, temp_project):
        """execute_check_stepã‚³ãƒãƒ³ãƒ‰ã®ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã«sessionæƒ…å ±ãŒå«ã¾ã‚Œã‚‹ã‹ã‚’ç¢ºèª"""

        with patch('noveler.domain.services.progressive_check_manager.ProgressiveCheckManager') as mock_manager:
            mock_instance = Mock()
            mock_manager.return_value = mock_instance
            mock_instance.execute_check_step.return_value = {
                "success": False,
                "session_id": "QC_EP001_TEST",
                "error": "Session corrupted",
                "session_reset_required": True,
            }

            result = await mcp_dispatcher.dispatch(
                "execute_check_step",
                {
                    "episode_number": 1,
                    "step_id": 5,
                    "input_data": {},
                    "project_root": str(temp_project),
                },
            )

        assert result["execution_method"] == "progressive_check_manager"
        assert result["execution_result"].get("session_id") == "QC_EP001_TEST"
        assert result["execution_result"].get("session_reset_required") is not None
        assert result["success"] is False

    @pytest.mark.asyncio
    async def test_get_check_status_tool(self, mcp_server, temp_project):
        """get_check_statusãƒ„ãƒ¼ãƒ«ã®ãƒ†ã‚¹ãƒˆ"""
        with patch('noveler.domain.services.progressive_check_manager.ProgressiveCheckManager') as mock_manager:
            mock_instance = Mock()
            mock_manager.return_value = mock_instance

            # ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹æƒ…å ±ã®ãƒ¢ãƒƒã‚¯
            mock_instance.get_check_status.return_value = {
                "session_id": "EP001_202501091430",
                "episode_number": 1,
                "total_steps": 12,
                "completed_steps": 3,
                "progress_percentage": 25.0,
                "current_phase": "basic_quality",
                "last_completed_step": 3,
                "next_step": 4,
                "estimated_remaining_time": "45-60åˆ†",
                "session_start_time": "2025-01-09T14:30:00+09:00"
            }
            mock_instance.session_id = "EP001_202501091430"

            result = await mcp_dispatcher.dispatch(
                "get_check_status",
                {
                    "episode_number": 1,
                    "project_root": str(temp_project),
                },
            )

        mock_instance.get_check_status.assert_called_once()
        assert result["success"] is True
        assert result["session_id"] == "EP001_202501091430"
        status = result["status_info"]
        assert status["episode_number"] == 1
        assert status["total_steps"] == 12
        assert status["session_id"] == "EP001_202501091430"
        assert status["completed_steps"] == 3
        assert status["progress_percentage"] == 25.0
        assert status["current_phase"] == "basic_quality"
        assert status["next_step"] == 4
        assert result["session_id"].startswith("EP001_")

    @pytest.mark.asyncio
    async def test_check_basic_failure_contains_error_log(self, temp_project):
        """check_basicãŒCLIå¤±æ•—æ™‚ã«error_logã‚’è¿”å´ã™ã‚‹ã‹ç¢ºèª"""

        failure_payload = {
            "success": False,
            "error": "CLI execution failed",
            "error_log": "Traceback: simulated error",
        }

        with patch(
            'src.mcp_servers.noveler.main.execute_novel_command',
            new=AsyncMock(return_value=failure_payload),
        ) as mock_execute:
            response = await mcp_main.call_tool(
                "check_basic",
                {"episode_number": 1, "project_root": str(temp_project)},
            )

        mock_execute.assert_awaited_once()
        assert response, "call_tool should return a response"
        payload = json.loads(response[0].text)
        assert payload["success"] is False
        assert payload["error_log"] == "Traceback: simulated error"
        assert payload["error"] == "CLI execution failed"

    @pytest.mark.asyncio
    async def test_get_check_history_tool(self, mcp_server, temp_project):
        """get_check_historyãƒ„ãƒ¼ãƒ«ã®ãƒ†ã‚¹ãƒˆ"""
        # Arrange
        episode_number = 1

        with patch('src.mcp_servers.noveler.main.ProgressiveCheckManager') as mock_manager:
            mock_instance = Mock()
            mock_manager.return_value = mock_instance

            # å±¥æ­´ãƒ‡ãƒ¼ã‚¿ã®ãƒ¢ãƒƒã‚¯
            mock_instance.get_execution_history.return_value = [
                {
                    "step_id": 1,
                    "step_name": "èª¤å­—è„±å­—ãƒã‚§ãƒƒã‚¯",
                    "executed_at": "2025-01-09T14:31:00+09:00",
                    "execution_time": 3.2,
                    "success": True,
                    "issues_found": 2,
                    "quality_score": 88.5
                },
                {
                    "step_id": 2,
                    "step_name": "æ–‡æ³•ãƒ»è¡¨è¨˜çµ±ä¸€ãƒã‚§ãƒƒã‚¯",
                    "executed_at": "2025-01-09T14:35:00+09:00",
                    "execution_time": 4.1,
                    "success": True,
                    "issues_found": 1,
                    "quality_score": 92.0
                },
                {
                    "step_id": 3,
                    "step_name": "èª­ã¿ã‚„ã™ã•åŸºç¤ãƒã‚§ãƒƒã‚¯",
                    "executed_at": "2025-01-09T14:40:00+09:00",
                    "execution_time": 5.8,
                    "success": True,
                    "issues_found": 0,
                    "quality_score": 95.0
                }
            ]

            # Act: å±¥æ­´å–å¾—
            result = mock_instance.get_execution_history()

        # Assert
        assert len(result) == 3

        # å„å±¥æ­´é …ç›®ã®æ¤œè¨¼
        for i, record in enumerate(result, 1):
            assert record["step_id"] == i
            assert record["success"] is True
            assert record["execution_time"] > 0
            assert record["quality_score"] > 80
            assert "step_name" in record
            assert "executed_at" in record

    @pytest.mark.asyncio
    async def test_step_by_step_workflow(self, mcp_server, temp_project):
        """ã‚¹ãƒ†ãƒƒãƒ—ãƒã‚¤ã‚¹ãƒ†ãƒƒãƒ—ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®çµ±åˆãƒ†ã‚¹ãƒˆ"""
        # Arrange
        episode_number = 1

        with patch('src.mcp_servers.noveler.main.ProgressiveCheckManager') as mock_manager:
            mock_instance = Mock()
            mock_manager.return_value = mock_instance

            # æ®µéšçš„å®Ÿè¡Œã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            execution_sequence = []

            # ã‚¹ãƒ†ãƒƒãƒ—1: èª¤å­—è„±å­—ãƒã‚§ãƒƒã‚¯
            mock_instance.execute_check_step.side_effect = [
                {
                    "step_id": 1,
                    "success": True,
                    "execution_time": 3.2,
                    "issues_found": 2,
                    "quality_score": 88.5,
                    "next_step": 2,
                    "phase": "basic_quality"
                },
                {
                    "step_id": 2,
                    "success": True,
                    "execution_time": 4.1,
                    "issues_found": 1,
                    "quality_score": 92.0,
                    "next_step": 3,
                    "phase": "basic_quality"
                },
                {
                    "step_id": 3,
                    "success": True,
                    "execution_time": 5.8,
                    "issues_found": 0,
                    "quality_score": 95.0,
                    "next_step": 4,
                    "phase": "story_quality"  # æ¬¡ã®ãƒ•ã‚§ãƒ¼ã‚ºã«é€²ã‚€
                }
            ]

            # Act: åŸºæœ¬å“è³ªãƒ•ã‚§ãƒ¼ã‚ºï¼ˆã‚¹ãƒ†ãƒƒãƒ—1-3ï¼‰ã‚’æ®µéšçš„ã«å®Ÿè¡Œ
            results = []
            for step_id in range(1, 4):
                result = mock_instance.execute_check_step(step_id, {
                    "episode_number": episode_number,
                    "step_focus": f"step_{step_id}"
                })
                results.append(result)
                execution_sequence.append(step_id)

        # Assert
        assert len(results) == 3
        assert len(execution_sequence) == 3

        # å„ã‚¹ãƒ†ãƒƒãƒ—ã®æˆåŠŸã‚’ç¢ºèª
        for i, result in enumerate(results, 1):
            assert result["step_id"] == i
            assert result["success"] is True
            assert result["quality_score"] > 80

        # æ®µéšçš„ãªå“è³ªå‘ä¸Šã‚’ç¢ºèª
        scores = [r["quality_score"] for r in results]
        assert scores[0] < scores[1] < scores[2]  # å“è³ªã‚¹ã‚³ã‚¢ãŒæ®µéšçš„ã«å‘ä¸Š

        # ãƒ•ã‚§ãƒ¼ã‚ºé·ç§»ã®ç¢ºèª
        assert results[0]["phase"] == "basic_quality"
        assert results[1]["phase"] == "basic_quality"
        assert results[2]["phase"] == "story_quality"  # ãƒ•ã‚§ãƒ¼ã‚ºå¤‰æ›´

    @pytest.mark.asyncio
    async def test_file_io_integration(self, mcp_server, temp_project):
        """ãƒ•ã‚¡ã‚¤ãƒ«å…¥å‡ºåŠ›çµ±åˆãƒ†ã‚¹ãƒˆ"""
        # Arrange
        episode_number = 1

        with patch('src.mcp_servers.noveler.main.ProgressiveCheckManager') as mock_manager:
            mock_instance = Mock()
            mock_manager.return_value = mock_instance

            # ãƒ•ã‚¡ã‚¤ãƒ«I/Oã®ãƒ¢ãƒƒã‚¯
            expected_session_id = "EP001_202501091430"
            mock_instance.session_id = expected_session_id

            input_file_path = temp_project / ".noveler" / "checks" / expected_session_id / f"{expected_session_id}_step_1_input.json"
            output_file_path = temp_project / ".noveler" / "checks" / expected_session_id / f"{expected_session_id}_step_1_output.json"

            mock_instance.save_step_input.return_value = input_file_path
            mock_instance.save_step_output.return_value = output_file_path

            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            input_file_path.parent.mkdir(parents=True, exist_ok=True)

            # ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ
            input_data = {
                "step_id": 1,
                "episode_number": episode_number,
                "manuscript_content": "ãƒ†ã‚¹ãƒˆåŸç¨¿",
                "timestamp": "2025-01-09T14:30:00+09:00"
            }

            output_data = {
                "step_id": 1,
                "issues_found": 2,
                "quality_score": 88.5,
                "corrections": ["ä¿®æ­£1", "ä¿®æ­£2"],
                "execution_time": 3.2
            }

            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã®ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³
            input_file_path.write_text(json.dumps(input_data, ensure_ascii=False, indent=2), encoding="utf-8")
            output_file_path.write_text(json.dumps(output_data, ensure_ascii=False, indent=2), encoding="utf-8")

            # Act
            saved_input_file = mock_instance.save_step_input(1, input_data)
            saved_output_file = mock_instance.save_step_output(1, output_data)

        # Assert
        assert saved_input_file == input_file_path
        assert saved_output_file == output_file_path

        # ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®ç¢ºèª
        assert input_file_path.exists()
        assert output_file_path.exists()

        loaded_input = json.loads(input_file_path.read_text(encoding="utf-8"))
        loaded_output = json.loads(output_file_path.read_text(encoding="utf-8"))

        assert loaded_input == input_data
        assert loaded_output == output_data

    @pytest.mark.asyncio
    async def test_error_recovery_workflow(self, mcp_server, temp_project):
        """ã‚¨ãƒ©ãƒ¼å¾©æ—§ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ãƒ†ã‚¹ãƒˆ"""
        # Arrange
        episode_number = 1

        with patch('src.mcp_servers.noveler.main.ProgressiveCheckManager') as mock_manager:
            mock_instance = Mock()
            mock_manager.return_value = mock_instance

            # ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿã‚±ãƒ¼ã‚¹ã®ãƒ¢ãƒƒã‚¯
            mock_instance.execute_check_step.side_effect = [
                # ã‚¹ãƒ†ãƒƒãƒ—1: æˆåŠŸ
                {
                    "step_id": 1,
                    "success": True,
                    "quality_score": 88.5,
                    "next_step": 2
                },
                # ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ
                {
                    "step_id": 2,
                    "success": False,
                    "error": "å‡¦ç†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ",
                    "error_type": "processing_error",
                    "recoverable": True
                },
                # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒªãƒˆãƒ©ã‚¤æˆåŠŸ
                {
                    "step_id": 2,
                    "success": True,
                    "quality_score": 90.0,
                    "next_step": 3,
                    "retry_count": 1
                }
            ]

            # Act: ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿã¨ãƒªãƒˆãƒ©ã‚¤ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼
            results = []

            # ã‚¹ãƒ†ãƒƒãƒ—1: æˆåŠŸ
            result1 = mock_instance.execute_check_step(1, {"test": "data"})
            results.append(result1)

            # ã‚¹ãƒ†ãƒƒãƒ—2: ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ
            result2_error = mock_instance.execute_check_step(2, {"test": "data"})
            results.append(result2_error)

            # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒªãƒˆãƒ©ã‚¤
            if not result2_error["success"] and result2_error.get("recoverable"):
                result2_retry = mock_instance.execute_check_step(2, {"test": "data", "retry": True})
                results.append(result2_retry)

        # Assert
        assert len(results) == 3

        # ã‚¹ãƒ†ãƒƒãƒ—1: æˆåŠŸ
        assert results[0]["success"] is True

        # ã‚¹ãƒ†ãƒƒãƒ—2: æœ€åˆã¯ã‚¨ãƒ©ãƒ¼
        assert results[1]["success"] is False
        assert "error" in results[1]
        assert results[1]["recoverable"] is True

        # ã‚¹ãƒ†ãƒƒãƒ—2: ãƒªãƒˆãƒ©ã‚¤å¾Œã¯æˆåŠŸ
        assert results[2]["success"] is True
        assert results[2]["retry_count"] == 1
        assert results[2]["quality_score"] > 0

    @pytest.mark.asyncio
    async def test_backward_compatibility(self, mcp_server, temp_project):
        """å¾Œæ–¹äº’æ›æ€§ãƒ†ã‚¹ãƒˆï¼ˆå¾“æ¥ã®ä¸€æ‹¬å®Ÿè¡Œã¨ã®ä½µç”¨ï¼‰"""
        # Arrange
        with patch('src.mcp_servers.noveler.main.server._execute_progressive_check') as mock_progressive:
            # æ®µéšçš„å®Ÿè¡Œã‚¬ã‚¤ãƒ€ãƒ³ã‚¹ã®ãƒ¢ãƒƒã‚¯
            mock_progressive.return_value = """
ğŸ¯ æ®µéšçš„å“è³ªãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã§ã™
æ–°ã—ã„æ®µéšçš„ãƒã‚§ãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ ã®ä½¿ç”¨æ–¹æ³•:
1. get_check_tasks(episode_number=1)
2. execute_check_step(episode_number=1, step_id=1)
ğŸ’¡ æ®µéšçš„æŒ‡å°ã§å“è³ªå‘ä¸Š
"""

            # Act: æ—¢å­˜checkãƒ„ãƒ¼ãƒ«ã§progressive=Trueã‚’ä½¿ç”¨
            # ã“ã‚Œã¯å®Ÿéš›ã®MCPã‚µãƒ¼ãƒãƒ¼ã®json_conversion_server.pyã®checkãƒ„ãƒ¼ãƒ«ã‚’ã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ãƒˆ
            guidance_result = mock_progressive(1, "all", str(temp_project))

        # Assert
        assert "æ®µéšçš„å“è³ªãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½ãŒåˆ©ç”¨å¯èƒ½ã§ã™" in guidance_result
        assert "get_check_tasks" in guidance_result
        assert "execute_check_step" in guidance_result
        assert "æ®µéšçš„æŒ‡å°ã§å“è³ªå‘ä¸Š" in guidance_result


@pytest.mark.e2e
class TestProgressiveCheckE2E:
    """æ®µéšçš„å“è³ªãƒã‚§ãƒƒã‚¯ã®ã‚¨ãƒ³ãƒ‰ãƒ„ãƒ¼ã‚¨ãƒ³ãƒ‰ãƒ†ã‚¹ãƒˆ"""

    @pytest.mark.asyncio
    async def test_complete_12_step_workflow(self, temp_project):
        """å®Œå…¨ãª12ã‚¹ãƒ†ãƒƒãƒ—ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®ãƒ†ã‚¹ãƒˆï¼ˆç°¡ç•¥ç‰ˆï¼‰"""
        # Arrange
        with patch('src.mcp_servers.noveler.main.ProgressiveCheckManager') as mock_manager:
            mock_instance = Mock()
            mock_manager.return_value = mock_instance

            # 12ã‚¹ãƒ†ãƒƒãƒ—å…¨ä½“ã®å®Ÿè¡Œçµæœã‚’ãƒ¢ãƒƒã‚¯
            all_steps_results = []
            phases = [
                ("basic_quality", [1, 2, 3]),
                ("story_quality", [4, 5, 6]),
                ("structure_quality", [7, 8, 9]),
                ("expression_quality", [10, 11, 12])
            ]

            step_results = []
            for phase_name, step_ids in phases:
                for step_id in step_ids:
                    step_results.append({
                        "step_id": step_id,
                        "success": True,
                        "phase": phase_name,
                        "quality_score": 80 + step_id,  # æ®µéšçš„å‘ä¸Š
                        "next_step": step_id + 1 if step_id < 12 else None,
                        "execution_time": 2.0 + step_id * 0.5
                    })

            mock_instance.execute_check_step.side_effect = step_results

            # æœ€çµ‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹
            mock_instance.get_execution_status.return_value = {
                "episode_number": 1,
                "total_steps": 12,
                "completed_steps": 12,
                "progress_percentage": 100.0,
                "current_phase": "expression_quality",
                "final_quality_score": 92.0,
                "session_completed": True
            }

        # Act: 12ã‚¹ãƒ†ãƒƒãƒ—ã™ã¹ã¦ã‚’å®Ÿè¡Œï¼ˆç°¡ç•¥ç‰ˆï¼‰
        execution_results = []
        for step_id in range(1, 13):
            result = mock_instance.execute_check_step(step_id, {"step_test": step_id})
            execution_results.append(result)

        final_status = mock_instance.get_execution_status()

        # Assert
        assert len(execution_results) == 12

        # å„ãƒ•ã‚§ãƒ¼ã‚ºã®å®Œäº†ç¢ºèª
        basic_steps = execution_results[0:3]  # ã‚¹ãƒ†ãƒƒãƒ—1-3
        story_steps = execution_results[3:6]  # ã‚¹ãƒ†ãƒƒãƒ—4-6
        structure_steps = execution_results[6:9]  # ã‚¹ãƒ†ãƒƒãƒ—7-9
        expression_steps = execution_results[9:12]  # ã‚¹ãƒ†ãƒƒãƒ—10-12

        for steps in [basic_steps, story_steps, structure_steps, expression_steps]:
            for step in steps:
                assert step["success"] is True

        # å“è³ªã‚¹ã‚³ã‚¢ã®æ®µéšçš„å‘ä¸Šç¢ºèª
        scores = [r["quality_score"] for r in execution_results]
        assert all(scores[i] <= scores[i+1] for i in range(len(scores)-1))  # å˜èª¿å¢—åŠ 

        # æœ€çµ‚ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ç¢ºèª
        assert final_status["completed_steps"] == 12
        assert final_status["progress_percentage"] == 100.0
        assert final_status["session_completed"] is True
        assert final_status["final_quality_score"] > 90


if __name__ == "__main__":
    pytest.main([__file__, "-v", "--tb=short"])
