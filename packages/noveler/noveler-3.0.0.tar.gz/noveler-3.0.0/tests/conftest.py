#!/usr/bin/env python3
"""çµ±åˆpytestè¨­å®šãƒ•ã‚¡ã‚¤ãƒ« (pytest 8.xæœ€é©åŒ–)
å…¨ãƒ†ã‚¹ãƒˆã‚«ãƒ†ã‚´ãƒªï¼ˆunit/integration/e2e/architecture/contractsï¼‰ã§ã®å…±é€šè¨­å®š
DDDæº–æ‹ ãƒ»åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆç’°å¢ƒå¯¾å¿œ
src/noveler/tests ã¨ãƒ«ãƒ¼ãƒˆtests/ã®çµ±åˆç‰ˆ
"""
import sys
import tempfile
from pathlib import Path
import os
# -----------------------------------------------------------------
# Compatibility shim: support Path.Path("w").open(encoding=...) pattern
# Several legacy tests and utilities rely on this chainable helper.
# This adds a lightweight proxy so that Path objects accept a .Path(...) call
# that returns an object exposing .open(...) with default mode/encoding.
# -----------------------------------------------------------------
try:
    import pathlib as _pathlib
    def _compat_Path(self, mode: str | None = None, encoding: str | None = None, **default_kwargs):
        class _OpenProxy:
            def __init__(self, p: _pathlib.Path, m: str | None, enc: str | None, dkw: dict):
                self._p = p
                self._m = m
                self._enc = enc
                self._dkw = dict(dkw) if dkw else {}
            def open(self, mode: str | None = None, encoding: str | None = None, **kwargs):
                use_mode = mode or self._m or "r"
                use_kwargs = {**self._dkw, **kwargs}
                if encoding is None:
                    encoding = self._enc
                if encoding is not None:
                    use_kwargs["encoding"] = encoding
                return self._p.open(use_mode, **use_kwargs)
        return _OpenProxy(self, mode, encoding, default_kwargs)
    for _cls_name in ("Path", "PosixPath", "WindowsPath"):
        _cls = getattr(_pathlib, _cls_name, None)
        if _cls is not None and not hasattr(_cls, "Path"):
            setattr(_cls, "Path", _compat_Path)
except Exception:
    # Best-effort compatibility; ignore if environment disallows monkeypatching
    pass
import pytest
import yaml
pytest_plugins = ("pytester",)
# -----------------------------------------------------------------
# Soft hint: suggest unified runner when called via raw pytest
# -----------------------------------------------------------------
def pytest_sessionstart(session):
    """Print a one-line hint when tests are invoked bypassing the unified runner.
    This is a soft warning and does not affect test outcomes.
    """
    import os as _os, sys as _sys
    if (_os.getenv("LLM_TEST_RUNNER") or "").strip().lower() in {"1", "true", "yes", "on"}:
        return
    try:
        _sys.stderr.write("[hint] ãƒ†ã‚¹ãƒˆã¯ scripts/run_pytest.py ã¾ãŸã¯ `make test` ã‚’ä½¿ã†ã¨ç’°å¢ƒ/å‡ºåŠ›ãŒçµ±ä¸€ã•ã‚Œã¾ã™\n")
    except Exception:
        pass
from tests._utils.slow_marker_utils import extract_slow_marker_tokens
# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’Pythonãƒ‘ã‚¹ã«è¿½åŠ 
project_root = Path(__file__).parent.parent
src_root = project_root / "src"
sys.path.insert(0, str(project_root))
sys.path.insert(0, str(src_root))
# -----------------------------------------------------------------
# MCP stdout ã‚¬ãƒ¼ãƒ‰ã‚’ãƒ†ã‚¹ãƒˆå…¨ä½“ã§æœ‰åŠ¹åŒ–
# - stdoutã¯MCPå°‚ç”¨ã€ãƒ­ã‚°ã¯stderrã¸ï¼ˆMCP_STDIO_SAFE=1ï¼‰
# - SDKå‡ºåŠ›ä»¥å¤–ã®stdoutæ›¸ãè¾¼ã¿ã‚’æ¤œå‡ºï¼ˆMCP_STRICT_STDOUT=1ï¼‰
# -----------------------------------------------------------------
@pytest.fixture(scope="session", autouse=True)
def enable_mcp_stdout_guard():
    import os as _os
    saved = {k: _os.environ.get(k) for k in ("MCP_STRICT_STDOUT", "MCP_STDIO_SAFE", "PYTHONUNBUFFERED")}
    _os.environ["MCP_STRICT_STDOUT"] = "1"
    # Ensure server-side consoles default to stderr to avoid protocol noise
    _os.environ.setdefault("MCP_STDIO_SAFE", "1")
    _os.environ.setdefault("PYTHONUNBUFFERED", "1")
    try:
        yield
    finally:
        for k, v in saved.items():
            if v is None:
                _os.environ.pop(k, None)
            else:
                _os.environ[k] = v
# -----------------------------------------------------------------
# AsyncIO compatibility for Python 3.11+
# Many legacy calls use asyncio.get_event_loop() in sync context.
# Ensure a default loop is present on MainThread during tests.
# -----------------------------------------------------------------
@pytest.fixture(scope="session", autouse=True)
def ensure_default_event_loop():
    import asyncio as _asyncio
    try:
        _asyncio.get_running_loop()
    except RuntimeError:
        _asyncio.set_event_loop(_asyncio.new_event_loop())
    yield


# -----------------------------------------------------------------
# Reset shared path-service cache at session start to avoid cross-test
# contamination between projects/roots (flake prevention)
# -----------------------------------------------------------------
@pytest.fixture(scope="session", autouse=True)
def _reset_common_path_service_cache():
    try:
        from noveler.presentation.shared.shared_utilities import (
            reset_common_path_service as _reset_cps,
        )
        _reset_cps()
    except Exception:
        # Not fatal in environments without presentation layer available
        pass
    yield
# é…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆã§circular importå›žé¿
def get_path_service():
    """ãƒ‘ã‚¹ã‚µãƒ¼ãƒ“ã‚¹å–å¾—ï¼ˆé…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼‰"""
    try:
        from noveler.presentation.shared.shared_utilities import get_common_path_service
        return get_common_path_service()
    except ImportError:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬çš„ãªãƒ‘ã‚¹æ“ä½œ
        class FallbackPathService:
            @property
            def project_root(self):
                return project_root
            def get_manuscript_dir(self):
                # è¨­å®šãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¹å–å¾—ã‚’è©¦è¡Œ
                try:
                    from noveler.infrastructure.factories.configuration_service_factory import get_configuration_manager
                    config_manager = get_configuration_manager()
                    project_config = config_manager.get_project_configuration(project_root)
                    if project_config and "paths" in project_config:
                        manuscripts_path = project_config["paths"].get("manuscripts", "40_åŽŸç¨¿")
                    else:
                        manuscripts_path = "40_åŽŸç¨¿"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                    return project_root / f"temp/test_data/{manuscripts_path}"
                except Exception:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ§‹é€ 
                    return project_root / "temp/test_data/40_åŽŸç¨¿"
            def get_management_dir(self):
                return project_root / "50_ç®¡ç†è³‡æ–™"
            def get_plots_dir(self):
                return project_root / "50_ç®¡ç†è³‡æ–™" / "plots"
            def get_settings_dir(self):
                return project_root / "config"
            def get_management_dir_1(self):
                return project_root / "50_ç®¡ç†è³‡æ–™"
            @property
            def manuscript_dir(self):
                # è¨­å®šãƒ™ãƒ¼ã‚¹ã®ãƒ‘ã‚¹å–å¾—ã‚’è©¦è¡Œ
                try:
                    from noveler.infrastructure.factories.configuration_service_factory import get_configuration_manager
                    config_manager = get_configuration_manager()
                    project_config = config_manager.get_project_configuration(project_root)
                    if project_config and "paths" in project_config:
                        manuscripts_path = project_config["paths"].get("manuscripts", "40_åŽŸç¨¿")
                    else:
                        manuscripts_path = "40_åŽŸç¨¿"  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ
                    return project_root / f"temp/test_data/{manuscripts_path}"
                except Exception:
                    # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆæ§‹é€ 
                    return project_root / "temp/test_data/40_åŽŸç¨¿"
            @property
            def config_dir(self):
                return project_root / "config"
        return FallbackPathService()
def get_test_manuscripts_path(project_root=None):
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®šã«åŸºã¥ããƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®manuscriptsãƒ‘ã‚¹å–å¾—"""
    try:
        if project_root:
            from noveler.infrastructure.ai_integration.repositories.yaml_project_config_repository import (
                YamlProjectConfigRepository,
            )
            from pathlib import Path
            import os
            repository = YamlProjectConfigRepository(Path(project_root))
            path_config = repository.get_path_config(Path(project_root))
            manuscripts_path = path_config.get("manuscripts", "40_åŽŸç¨¿")
            return f"temp/test_data/{manuscripts_path}"
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¾ãŸã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return "temp/test_data/40_åŽŸç¨¿"
    except Exception:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return "temp/test_data/40_åŽŸç¨¿"
def get_test_management_path(project_root=None):
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®šã«åŸºã¥ããƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®managementãƒ‘ã‚¹å–å¾—"""
    try:
        if project_root:
            from noveler.infrastructure.ai_integration.repositories.yaml_project_config_repository import (
                YamlProjectConfigRepository,
            )
            from pathlib import Path
            import os
            repository = YamlProjectConfigRepository(Path(project_root))
            path_config = repository.get_path_config(Path(project_root))
            management_path = path_config.get("management", "50_ç®¡ç†è³‡æ–™")
            return f"temp/test_data/{management_path}"
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¾ãŸã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return "temp/test_data/50_ç®¡ç†è³‡æ–™"
    except Exception:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        return "temp/test_data/50_ç®¡ç†è³‡æ–™"
def get_cleanup_manager():
    """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ãƒžãƒãƒ¼ã‚¸ãƒ£ãƒ¼å–å¾—ï¼ˆé…å»¶ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼‰"""
    try:
        from noveler.infrastructure.shared.test_cleanup_manager import TestCleanupManager
        return TestCleanupManager(project_root)
    except ImportError:
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        class FallbackCleanupManager:
            def __init__(self, project_root=None):
                self.project_root = project_root or Path.cwd()
            def cleanup_test_files(self, test_dir):
                if test_dir.exists():
                    import shutil
                    shutil.rmtree(test_dir)
            def cleanup_test_artifacts(self, dry_run=False):
                return {
                    "files_deleted": [],
                    "dirs_deleted": [],
                    "errors": []
                }
        return FallbackCleanupManager(project_root)
# -----------------------------------------------------------------
# Sessionãƒ¬ãƒ™ãƒ«ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£(é‡ã„I/Oæ“ä½œã‚’ä¸€åº¦ã ã‘å®Ÿè¡Œ)
# -----------------------------------------------------------------
@pytest.fixture(scope="session")
def temp_project_dir():
    """ãƒ†ã‚¹ãƒˆç”¨ä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª(sessionã‚¹ã‚³ãƒ¼ãƒ—)"""
    path_service = get_path_service()
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)
        # åŸºæœ¬ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ ã‚’ä½œæˆ
        try:
            (temp_path / str(path_service.get_manuscript_dir())).mkdir(parents=True, exist_ok=True)
            (temp_path / str(path_service.get_management_dir())).mkdir(parents=True, exist_ok=True)
            (temp_path / str(path_service.get_plots_dir())).mkdir(parents=True, exist_ok=True)
            (temp_path / str(path_service.get_settings_dir())).mkdir(parents=True, exist_ok=True)
        except AttributeError:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            manuscripts_path = get_test_manuscripts_path()
            (temp_path / manuscripts_path).mkdir(parents=True, exist_ok=True)
            (temp_path / "50_ç®¡ç†è³‡æ–™").mkdir(exist_ok=True)
            (temp_path / "50_ç®¡ç†è³‡æ–™" / "plots").mkdir(parents=True, exist_ok=True)
            (temp_path / "config").mkdir(exist_ok=True)
        # è¿½åŠ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆå¾“æ¥ã®tests/conftest.pyã¨ã®äº’æ›æ€§ï¼‰
        (temp_path / "plots").mkdir(exist_ok=True)
        (temp_path / "quality").mkdir(exist_ok=True)
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆï¼ˆsrc/noveler/tests/conftest.pyæ©Ÿèƒ½ï¼‰
        project_config = {
            "project": {
                "title": "ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ",
                "author": "ãƒ†ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼",
                "genre": "ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼",
                "ncode": "N0000XX",
            },
        }
        with open(temp_path / "ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®š.yaml", "w", encoding="utf-8") as f:
            yaml.dump(project_config, f, allow_unicode=True, default_flow_style=False, sort_keys=False)
        yield temp_path
@pytest.fixture(scope="session")
def default_project_settings() -> dict[str, object]:
    """ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆè¨­å®š(sessionã‚¹ã‚³ãƒ¼ãƒ—)"""
    return {
        "project": {
            "title": "ãƒ†ã‚¹ãƒˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ",
            "author": "ãƒ†ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼",
            "genre": "ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼",
            "target_audience": "ä¸€èˆ¬",
            "writing_status": "é€£è¼‰ä¸­",
            "ncode": "N0000XX",
        },
        "quality": {
            "min_score": 70,
            "target_score": 85,
            "max_warnings": 5,
        },
    }
@pytest.fixture(scope="session")
def test_manuscript_file(temp_project_dir):
    """ãƒ†ã‚¹ãƒˆç”¨åŽŸç¨¿ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ(sessionã‚¹ã‚³ãƒ¼ãƒ—)"""
    manuscripts_path = get_test_manuscripts_path(temp_project_dir)
    manuscript_dir = temp_project_dir / manuscripts_path
    manuscript_file = manuscript_dir / "ç¬¬001è©±_ãƒ†ã‚¹ãƒˆç‰©èªž.md"
    test_content = """# ç¬¬1è©± ãƒ†ã‚¹ãƒˆç‰©èªž
ã“ã‚Œã¯ãƒ†ã‚¹ãƒˆç”¨ã®åŽŸç¨¿ã§ã™ã€‚
æ–‡ç« ã®å“è³ªãƒã‚§ãƒƒã‚¯ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ãŸã‚ã®å†…å®¹ã‚’å«ã‚“ã§ã„ã¾ã™ã€‚
## ã‚·ãƒ¼ãƒ³1
ä¸»äººå…¬ãŒå†’é™ºã‚’å§‹ã‚ã¾ã™ã€‚
é•·ã„æ–‡ç« ã§ã‚‚ãƒ†ã‚¹ãƒˆã§ãã‚‹ã‚ˆã†ã«ã€è¤‡æ•°ã®æ–‡ã‚’å«ã‚€æ®µè½ã‚’ç”¨æ„ã—ã¦ã„ã¾ã™ã€‚
## ã‚·ãƒ¼ãƒ³2
å±•é–‹éƒ¨åˆ†ã§ã®å‡ºæ¥äº‹ã€‚
"""
    manuscript_file.write_text(test_content, encoding="utf-8")
    return manuscript_file
@pytest.fixture(scope="session")
def test_config_file(temp_project_dir):
    """ãƒ†ã‚¹ãƒˆç”¨è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ(sessionã‚¹ã‚³ãƒ¼ãƒ—)"""
    config_dir = temp_project_dir / "config"
    config_file = config_dir / "novel_config.yaml"
    test_config = {
        "system": {
            "project_name": "test_novel",
            "author": "ãƒ†ã‚¹ãƒˆãƒ¦ãƒ¼ã‚¶ãƒ¼"
        },
        "paths": {
            "manuscripts": get_test_manuscripts_path().replace("temp/test_data/", ""),
            "plots": "plots",
            "quality": "quality"
        },
        "defaults": {
            "episode": {
                "target_words": 3000
            }
        }
    }
    config_file.write_text(yaml.dump(test_config, allow_unicode=True), encoding="utf-8")
    return config_file
# -----------------------------------------------------------------
# Packageãƒ¬ãƒ™ãƒ«ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£(ãƒ†ã‚¹ãƒˆãƒ‘ãƒƒã‚±ãƒ¼ã‚¸å˜ä½ã§å†åˆ©ç”¨)
# -----------------------------------------------------------------
@pytest.fixture(scope="package")
def sample_episode_content() -> str:
    """ã‚µãƒ³ãƒ—ãƒ«ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„(packageã‚¹ã‚³ãƒ¼ãƒ—)"""
    return """
# ç¬¬001è©± ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒˆãƒ«
 æœã®æ—¥å·®ã—ãŒéƒ¨å±‹ã«å·®ã—è¾¼ã‚“ã§ãã‚‹ã€‚ç§ã¯ãƒ™ãƒƒãƒ‰ã‹ã‚‰èº«ã‚’èµ·ã“ã—ã€å¤§ããä¼¸ã³ã‚’ã—ãŸã€‚
ã€ŒãŠã¯ã‚ˆã†ã€ä¸–ç•Œã€
 ä»Šæ—¥ã‚‚æ–°ã—ã„ä¸€æ—¥ã®å§‹ã¾ã‚Šã ã€‚çª“ã®å¤–ã«ã¯é’ã„ç©ºãŒåºƒãŒã£ã¦ã„ã‚‹ã€‚é›²ãŒã‚†ã£ãã‚Šã¨æµã‚Œã¦ã„ãã®ã‚’è¦‹ã¦ã„ã‚‹ã¨ã€å¿ƒãŒè½ã¡ç€ã„ã¦ãã‚‹ã€‚
 ç§ã¯ã‚­ãƒƒãƒãƒ³ã«å‘ã‹ã„ã€ã‚³ãƒ¼ãƒ’ãƒ¼ã‚’æ·¹ã‚ŒãŸã€‚é¦™ã°ã—ã„é¦™ã‚ŠãŒéƒ¨å±‹å…¨ä½“ã«åºƒãŒã£ã¦ã„ãã€‚ã“ã®ç¿’æ…£ã‚’å§‹ã‚ã¦ã‹ã‚‰ã€æ¯Žæ—¥ãŒå°‘ã—ã ã‘ç‰¹åˆ¥ã«æ„Ÿã˜ã‚‰ã‚Œã‚‹ã‚ˆã†ã«ãªã£ãŸã€‚
 ãƒžã‚°ã‚«ãƒƒãƒ—ã‚’æ‰‹ã«å–ã‚Šã€ãƒãƒ«ã‚³ãƒ‹ãƒ¼ã«å‡ºã‚‹ã€‚æ¸©ã‹ã„é¢¨ãŒé °ã‚’æ’«ã§ã¦ã„ãã€‚é ãã®å±±ã€…ãŒæœéœ§ã«åŒ…ã¾ã‚Œã¦ã€å¹»æƒ³çš„ãªç¾Žã—ã•ã‚’é†¸ã—å‡ºã—ã¦ã„ã‚‹ã€‚
 ã“ã®å¹³å®‰ãªæœãŒã€ã„ã¤ã¾ã§ã‚‚ç¶šã‘ã°ã„ã„ã®ã«ã€‚ãã‚“ãªã“ã¨ã‚’è€ƒãˆãªãŒã‚‰ã€ç§ã¯ã‚³ãƒ¼ãƒ’ãƒ¼ã‚’ä¸€å£é£²ã‚“ã ã€‚
""".strip()
@pytest.fixture(scope="package")
def sample_yaml_metadata() -> dict[str, object]:
    """ã‚µãƒ³ãƒ—ãƒ«YAMLãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿(packageã‚¹ã‚³ãƒ¼ãƒ—)"""
    return {
        "episode": {
            "number": 1,
            "title": "ãƒ†ã‚¹ãƒˆã‚¿ã‚¤ãƒˆãƒ«",
            "status": "draft",
            "target_words": 3000,
            "created_at": "2025-07-16T10:00:00",
            "tags": ["ãƒ•ã‚¡ãƒ³ã‚¿ã‚¸ãƒ¼", "æ—¥å¸¸"],
        },
        "quality": {
            "last_checked": None,
            "score": None,
            "issues": [],
        },
    }
# -----------------------------------------------------------------
# Functionãƒ¬ãƒ™ãƒ«ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£(å„ãƒ†ã‚¹ãƒˆã§ç‹¬ç«‹ã—ãŸãƒªã‚½ãƒ¼ã‚¹)
# -----------------------------------------------------------------
@pytest.fixture
def isolated_temp_dir():
    """ãƒ†ã‚¹ãƒˆæ¯Žã«ç‹¬ç«‹ã—ãŸä¸€æ™‚ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª"""
    with tempfile.TemporaryDirectory() as temp_dir:
        temp_path = Path(temp_dir)
        yield temp_path
@pytest.fixture
def mock_episode_file(isolated_temp_dir):
    """ãƒ†ã‚¹ãƒˆç”¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ãƒ•ã‚¡ã‚¤ãƒ«ä½œæˆ"""
    manuscripts_path = get_test_manuscripts_path()
    manuscript_dir = isolated_temp_dir / manuscripts_path
    manuscript_dir.mkdir(parents=True, exist_ok=True)
    episode_file = manuscript_dir / "ç¬¬001è©±_ãƒ†ã‚¹ãƒˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰.md"
    episode_content = """# ç¬¬1è©± ãƒ†ã‚¹ãƒˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰
ãƒ†ã‚¹ãƒˆç”¨ã®ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰å†…å®¹ã§ã™ã€‚
å“è³ªãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½ã‚’ãƒ†ã‚¹ãƒˆã™ã‚‹ãŸã‚ã€æ§˜ã€…ãªæ–‡ç« ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’å«ã‚“ã§ã„ã¾ã™ã€‚
é•·ã„æ–‡ç« ã®ãƒ†ã‚¹ãƒˆã®ãŸã‚ã«ã€æ„å›³çš„ã«è¤‡æ•°ã®è¦ç´ ã‚’å«ã‚€è¤‡é›‘ãªæ§‹é€ ã®æ–‡ç« ã‚’ä½œæˆã—ã¦ã€èª­ã¿ã‚„ã™ã•ã‚„æ–‡æ³•ã®æ¤œè¨¼ãŒã§ãã‚‹ã‚ˆã†ã«ã—ã¦ã„ã¾ã™ã€‚
çŸ­ã„æ–‡ã€‚
## ä¼šè©±ã‚·ãƒ¼ãƒ³
ã€Œã“ã‚“ã«ã¡ã¯ã€ã¨å½¼ã¯è¨€ã£ãŸã€‚
ã€Œã¯ã„ã€ã“ã‚“ã«ã¡ã¯ã€ã¨å½¼å¥³ã¯ç­”ãˆãŸã€‚
"""
    episode_file.write_text(episode_content, encoding="utf-8")
    return episode_file
@pytest.fixture
def cleanup_test_files():
    """ãƒ†ã‚¹ãƒˆå¾Œã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
    created_files = []
    def track_file(file_path):
        created_files.append(Path(file_path))
    yield track_file
    # ãƒ†ã‚¹ãƒˆå¾Œã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    cleanup_manager = get_cleanup_manager()
    for file_path in created_files:
        try:
            if file_path.is_dir():
                cleanup_manager.cleanup_test_files(file_path)
            elif file_path.exists():
                file_path.unlink()
        except Exception:
            # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼ã¯ç„¡è¦–ï¼ˆãƒ†ã‚¹ãƒˆçµæžœã«å½±éŸ¿ã•ã›ãªã„ï¼‰
            pass
# -----------------------------------------------------------------
# ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹æ¸¬å®šç”¨ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£
# -----------------------------------------------------------------
@pytest.fixture
def benchmark_episode_content() -> str:
    """ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ç”¨ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰ã‚³ãƒ³ãƒ†ãƒ³ãƒ„"""
    # 3000æ–‡å­—ç¨‹åº¦ã®ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚’ç”Ÿæˆ
    base_text = "ã“ã‚Œã¯ãƒ™ãƒ³ãƒãƒžãƒ¼ã‚¯ç”¨ã®ãƒ†ã‚¹ãƒˆãƒ†ã‚­ã‚¹ãƒˆã§ã™ã€‚"
    return base_text * 100  # ç´„ 3000æ–‡å­—
# -----------------------------------------------------------------
# E2Eãƒ†ã‚¹ãƒˆå°‚ç”¨ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£
# -----------------------------------------------------------------
@pytest.fixture
def e2e_test_environment(temp_project_dir, test_config_file, test_manuscript_file):
    """E2Eãƒ†ã‚¹ãƒˆç”¨ç’°å¢ƒã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—"""
    return {
        "project_root": temp_project_dir,
        "config_file": test_config_file,
        "manuscript_file": test_manuscript_file,
        "manuscript_dir": temp_project_dir / get_test_manuscripts_path(temp_project_dir)
    }
# -----------------------------------------------------------------
# Architecture/Integration ãƒ†ã‚¹ãƒˆå°‚ç”¨ãƒ•ã‚£ã‚¯ã‚¹ãƒãƒ£
# -----------------------------------------------------------------
@pytest.fixture
def mock_dependencies():
    """ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ†ã‚¹ãƒˆç”¨ã®ãƒ¢ãƒƒã‚¯ä¾å­˜é–¢ä¿‚"""
    from unittest.mock import Mock
    return {
        "logger": Mock(),
        "path_service": Mock(),
        "repository": Mock(),
        "use_case": Mock()
    }
# -----------------------------------------------------------------
# ãƒ†ã‚¹ãƒˆãƒžãƒ¼ã‚«ãƒ¼ç”¨ãƒ•ãƒƒã‚¯
# -----------------------------------------------------------------
def pytest_configure(config: pytest.Config) -> None:
    """pytestè¨­å®šãƒ•ãƒƒã‚¯(pytest 8.xæœ€é©åŒ–) - é«˜é€ŸåŒ–ç‰ˆ
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã®è‡ªå‹•ã‚¹ã‚­ãƒƒãƒ—ã€
    éžåŒæœŸãƒ†ã‚¹ãƒˆã®è‡ªå‹•è¨­å®šç­‰ã‚’å®Ÿè£…ã€‚
    """
    # MCPè»½é‡å‡ºåŠ›ã‚’æ—¢å®šONï¼ˆB20æº–æ‹ ï¼‰
    os.environ.setdefault("MCP_LIGHTWEIGHT_DEFAULT", "1")
    # ç¶™ç¶šCIç’°å¢ƒã§ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚¹ã‚­ãƒƒãƒ—
    if config.getoption("--maxfail") == 1:
        config.addinivalue_line("markers", "performance: skip in fail-fast mode")
    # ãƒžãƒ¼ã‚«ãƒ¼ä¸€æ‹¬ç™»éŒ²ï¼ˆé«˜é€ŸåŒ–ï¼‰
    markers = [
        "spec(id): ä»•æ§˜æ›¸IDã¨ã®ç´ä»˜ã‘",
        "requirement(id): è¦ä»¶IDã¨ã®ç´ä»˜ã‘",
        "spec(spec_id): ãƒ†ã‚¹ãƒˆä»•æ§˜ID",
        "slow: å®Ÿè¡Œæ™‚é–“ã®é•·ã„ãƒ†ã‚¹ãƒˆ",
        "integration: çµ±åˆãƒ†ã‚¹ãƒˆ",
        "e2e: E2Eãƒ†ã‚¹ãƒˆ",
        "unit: ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ",
        "architecture: ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ãƒ†ã‚¹ãƒˆ",
        "contracts: ã‚³ãƒ³ãƒˆãƒ©ã‚¯ãƒˆãƒ†ã‚¹ãƒˆ",
        "plot_episode: ãƒ—ãƒ­ãƒƒãƒˆã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰é ˜åŸŸã®ãƒ†ã‚¹ãƒˆ",
        "quality_domain: å“è³ªãƒ‰ãƒ¡ã‚¤ãƒ³é–¢é€£ã®ãƒ†ã‚¹ãƒˆ",
        "vo_smoke: å€¤ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã®ã‚¹ãƒ¢ãƒ¼ã‚¯ãƒ†ã‚¹ãƒˆ"
    ]
    for marker in markers:
        config.addinivalue_line("markers", marker)
def pytest_collection_modifyitems(config: pytest.Config, items: list[pytest.Item]) -> None:
    """ãƒ†ã‚¹ãƒˆã‚¢ã‚¤ãƒ†ãƒ ã®å‹•çš„ä¿®æ­£(pytest 8.xæœ€é©åŒ–) - é«˜é€ŸåŒ–ç‰ˆ
    - fail-fastãƒ¢ãƒ¼ãƒ‰ã§ã¯ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã‚’ã‚¹ã‚­ãƒƒãƒ—
    - éžåŒæœŸãƒ†ã‚¹ãƒˆã«è‡ªå‹•ãƒžãƒ¼ã‚«ãƒ¼ä»˜ä¸Ž
    - æ–‡å­—åˆ—æ“ä½œã‚’æœ€é©åŒ–
    """
    # ãƒžãƒ¼ã‚«ãƒ¼ã‚’äº‹å‰ä½œæˆï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥åŠ¹æžœï¼‰
    skip_performance = pytest.mark.skip(reason="Performance tests skipped in fail-fast mode")
    # maxfailã‚ªãƒ—ã‚·ãƒ§ãƒ³ã‚’ä¸€åº¦ã ã‘ãƒã‚§ãƒƒã‚¯
    should_skip_performance = config.getoption("--maxfail") == 1
    # æ–‡å­—åˆ—ãƒã‚§ãƒƒã‚¯ç”¨ã®é«˜é€ŸåŒ–
    slow_keywords = {"slow", "heavy", "stress", "benchmark", "performance"}
    for item in items:
        # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒžãƒ³ã‚¹ãƒ†ã‚¹ãƒˆã®æ¡ä»¶ä»˜ãã‚¹ã‚­ãƒƒãƒ—
        if should_skip_performance and "performance" in item.keywords:
            item.add_marker(skip_performance)
        # asyncãƒ†ã‚¹ãƒˆã®è‡ªå‹•èªè­˜
        if "async" in item.name or item.get_closest_marker("asyncio"):
            item.add_marker(pytest.mark.asyncio)
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãƒ™ãƒ¼ã‚¹ã®è‡ªå‹•ãƒžãƒ¼ã‚­ãƒ³ã‚°ï¼ˆæœ€é©åŒ–: ä¸€åº¦ã®æ–‡å­—åˆ—å¤‰æ›ï¼‰
        fspath_str = str(item.fspath)
        if "e2e" in fspath_str:
            item.add_marker(pytest.mark.e2e)
            item.add_marker(pytest.mark.slow)
        elif "integration" in fspath_str:
            item.add_marker(pytest.mark.integration)
        elif "architecture" in fspath_str:
            item.add_marker(pytest.mark.architecture)
        elif "contracts" in fspath_str:
            item.add_marker(pytest.mark.contracts)
        elif "unit" in fspath_str:
            item.add_marker(pytest.mark.unit)
        # å®Ÿè¡Œæ™‚é–“ãƒ™ãƒ¼ã‚¹ã®ãƒžãƒ¼ã‚­ãƒ³ã‚°ï¼ˆæœ€é©åŒ–: set intersectionï¼‰
        item_words = extract_slow_marker_tokens(item.name)
        if item_words & slow_keywords:
            item.add_marker(pytest.mark.slow)
def pytest_sessionfinish(session: pytest.Session, exitstatus: int) -> None:
    """ãƒ†ã‚¹ãƒˆã‚»ãƒƒã‚·ãƒ§ãƒ³çµ‚äº†æ™‚ã®è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
    ãƒ†ã‚¹ãƒˆå®Ÿè¡Œã§ç”Ÿæˆã•ã‚ŒãŸä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ã‚µãƒ³ãƒ—ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•å‰Šé™¤ã€‚
    """
    # Fast cleanup mode for LLM runs: skip heavy post-session cleanup if enabled
    try:
        fast_flag = os.environ.get("LLM_FAST_CLEANUP", "")
        _fast = fast_flag.strip().lower() in {"1", "true", "on", "yes"}
    except Exception:
        _fast = False

    if _fast and exitstatus == 0:
        # Skip heavy cleanup to speed up LLM-facing runs
        return

    try:
        # ãƒ†ã‚¹ãƒˆå¤±æ•—æ™‚ã¯ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ãªã„(ãƒ‡ãƒãƒƒã‚°ç”¨ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’æ®‹ã™)
        if exitstatus != 0:
            print()
            print("âš ï¸ ãƒ†ã‚¹ãƒˆã«å¤±æ•—ã—ãŸãŸã‚ã€ãƒ‡ãƒãƒƒã‚°ç”¨ã«ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿æŒã—ã¾ã™")
            print("   æ‰‹å‹•ã§ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã™ã‚‹å ´åˆ: python scripts/tools/test_cleanup_manager.py")
            return
        # è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Ÿè¡Œ
        print()
        print("ðŸ§¹ ãƒ†ã‚¹ãƒˆå¾Œã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œä¸­...")
        cleanup_manager = get_cleanup_manager()
        result = cleanup_manager.cleanup_test_artifacts(dry_run=False)
        if result["files_deleted"] or result["dirs_deleted"]:
            print(
                f"âœ… ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†: {len(result['files_deleted'])} ãƒ•ã‚¡ã‚¤ãƒ«, {len(result['dirs_deleted'])} ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’å‰Šé™¤"
            )
        else:
            print("âœ… ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å¯¾è±¡ãªã—")
        if result["errors"]:
            print(f"âš ï¸ {len(result['errors'])} ä»¶ã®ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
    except Exception as e:
        print(f"âš ï¸ è‡ªå‹•ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿ: {e}")
        print("   æ‰‹å‹•ã§ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã—ã¦ãã ã•ã„: python scripts/tools/test_cleanup_manager.py")
# -----------------------------------------------------------------
# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œæ™‚ãƒ•ãƒƒã‚¯
# -----------------------------------------------------------------
@pytest.fixture(autouse=True)
def setup_test_logging():
    """å„ãƒ†ã‚¹ãƒˆã§ã®ãƒ­ã‚°è¨­å®š"""
    from noveler.infrastructure.logging.unified_logger import configure_logging, LogFormat
    configure_logging(preset="development", console_format=LogFormat.RICH)
    yield
    # ãƒ†ã‚¹ãƒˆå¾Œã¯ãƒ­ã‚®ãƒ³ã‚°è¨­å®šã‚’åˆæœŸåŒ–
    from noveler.infrastructure.logging.unified_logger import configure_logging
    configure_logging(preset="development")


# -----------------------------------------------------------------
# Fail-only NDJSON streaming (opt-in): write a line when a test phase fails
# Enable with: LLM_REPORT_STREAM_FAIL=1
# -----------------------------------------------------------------


# -----------------------------------------------------------------
# Fail-only NDJSON streaming (opt-in): write per-worker lines on phase failure
# Enable with: LLM_REPORT_STREAM_FAIL=1
# -----------------------------------------------------------------

import json as _json
from datetime import datetime as _dt
from pathlib import Path as _Path


def _llm_fail_enabled() -> bool:
    try:
        import os as _os
        return (_os.getenv("LLM_REPORT_STREAM_FAIL") or "").strip().lower() in {"1", "true", "on", "yes"}
    except Exception:
        return False


def _llm_worker_id() -> str:
    try:
        import os as _os
        return _os.getenv("PYTEST_XDIST_WORKER", "main")
    except Exception:
        return "main"


def _llm_fail_path() -> _Path:
    base = _Path("reports") / "stream"
    base.mkdir(parents=True, exist_ok=True)
    return base / f"llm_fail_{_llm_worker_id()}.ndjson"


def pytest_runtest_logreport(report):
    if not _llm_fail_enabled():
        return
    try:
        outcome = getattr(report, "outcome", "") or ""
        if outcome not in {"failed", "error", "crashed"}:
            return
        nodeid = getattr(report, "nodeid", "")
        when = getattr(report, "when", "")
        duration = float(getattr(report, "duration", 0.0) or 0.0)
        payload = {
            "ts": _dt.utcnow().isoformat(timespec="milliseconds") + "Z",
            "event": "test_phase",
            "test_id": nodeid,
            "phase": when,
            "outcome": outcome,
            "duration_s": duration,
            "worker_id": _llm_worker_id(),
        }
        path = _llm_fail_path()
        with path.open("a", encoding="utf-8") as f:
            f.write(_json.dumps(payload, ensure_ascii=False) + "\n")
    except Exception:
        return


def _llm_merge_fail_logs() -> None:
    if not _llm_fail_enabled():
        return
    base = _Path("reports") / "stream"
    if not base.exists():
        return
    files = sorted(base.glob("llm_fail_*.ndjson"))
    if not files:
        return
    # Merge and compute summary
    merged = _Path("reports") / "llm_fail.ndjson"
    failed = error = crashed = 0
    workers = set()
    with merged.open("w", encoding="utf-8") as out:
        for f in files:
            try:
                workers.add(f.stem.split("_")[-1])
                for line in f.read_text(encoding="utf-8").splitlines():
                    if not line.strip():
                        continue
                    out.write(line + "\n")
                    try:
                        rec = _json.loads(line)
                        if rec.get("outcome") == "failed":
                            failed += 1
                        elif rec.get("outcome") == "error":
                            error += 1
                        elif rec.get("outcome") == "crashed":
                            crashed += 1
                    except Exception:
                        continue
            except Exception:
                continue
        summary = {
            "ts": _dt.utcnow().isoformat(timespec="milliseconds") + "Z",
            "event": "session_summary",
            "final_outcome": "failed" if (failed or error or crashed) else "passed",
            "failed_events": failed,
            "error_events": error,
            "crashed_events": crashed,
            "workers": sorted(workers),
            "files": len(files),
        }
        out.write(_json.dumps(summary, ensure_ascii=False) + "\n")
