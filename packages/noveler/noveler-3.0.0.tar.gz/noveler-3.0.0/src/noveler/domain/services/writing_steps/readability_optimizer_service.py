"""STEP 14: æ–‡ä½“ãƒ»å¯èª­æ€§æœ€é©åŒ–ã‚µãƒ¼ãƒ“ã‚¹

A38åŸ·ç­†ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¬ã‚¤ãƒ‰ã®STEP14ã€Œæ–‡ä½“ãƒ»å¯èª­æ€§ãƒ‘ã‚¹ã€ã‚’å®Ÿè£…ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã€‚
æ–‡ä½“ã®ä¸€è²«æ€§ã¨å¯èª­æ€§ã‚’æœ€é©åŒ–ã—ã€èª­è€…ã«ã¨ã£ã¦èª­ã¿ã‚„ã™ãé­…åŠ›çš„ãªæ–‡ç« ã«
èª¿æ•´ã—ã¾ã™ã€‚
"""

from abc import abstractmethod
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Any

from noveler.application.services.stepwise_execution_service import BaseWritingStep
from noveler.domain.models.project_model import ProjectModel
from noveler.domain.services.configuration_manager_service import ConfigurationManagerService


class WritingStyle(Enum):
    """æ–‡ä½“ã‚¹ã‚¿ã‚¤ãƒ«"""
    NARRATIVE = "narrative"  # ç‰©èªèª¿
    DESCRIPTIVE = "descriptive"  # æå†™èª¿
    CONVERSATIONAL = "conversational"  # ä¼šè©±èª¿
    POETIC = "poetic"  # è©©çš„
    DIRECT = "direct"  # ç›´æ¥çš„
    FORMAL = "formal"  # æ”¹ã¾ã£ãŸèª¿å­
    CASUAL = "casual"  # ã‚«ã‚¸ãƒ¥ã‚¢ãƒ«


class ReadabilityAspect(Enum):
    """å¯èª­æ€§ã®å´é¢"""
    SENTENCE_LENGTH = "sentence_length"  # æ–‡ã®é•·ã•
    WORD_CHOICE = "word_choice"  # èªå½™é¸æŠ
    SENTENCE_STRUCTURE = "sentence_structure"  # æ–‡æ§‹é€ 
    PARAGRAPH_FLOW = "paragraph_flow"  # æ®µè½ã®æµã‚Œ
    RHYTHM_PATTERN = "rhythm_pattern"  # ãƒªã‚ºãƒ ãƒ‘ã‚¿ãƒ¼ãƒ³
    CLARITY = "clarity"  # æ˜ç­æ€§
    ENGAGEMENT = "engagement"  # èª­è€…ã®é–¢ä¸


class StyleInconsistency(Enum):
    """æ–‡ä½“ã®ä¸ä¸€è²«æ€§ã‚¿ã‚¤ãƒ—"""
    TONE_SHIFT = "tone_shift"  # èªèª¿ã®å¤‰åŒ–
    FORMALITY_MISMATCH = "formality_mismatch"  # æ•¬èªãƒ¬ãƒ™ãƒ«ã®ä¸ä¸€è‡´
    TENSE_INCONSISTENCY = "tense_inconsistency"  # æ™‚åˆ¶ã®ä¸çµ±ä¸€
    VOICE_CHANGE = "voice_change"  # èªã‚Šæ‰‹ã®å¤‰åŒ–
    STYLE_MIXING = "style_mixing"  # ã‚¹ã‚¿ã‚¤ãƒ«ã®æ··åœ¨


@dataclass
class ReadabilityMetric:
    """å¯èª­æ€§ãƒ¡ãƒˆãƒªãƒƒã‚¯"""
    metric_name: str
    metric_value: float  # 0-1ã‚¹ã‚±ãƒ¼ãƒ«
    description: str
    target_range: tuple[float, float]  # ç›®æ¨™ç¯„å›²
    current_assessment: str  # ç¾åœ¨ã®è©•ä¾¡
    improvement_potential: float  # æ”¹å–„å¯èƒ½æ€§


@dataclass
class StyleAnalysis:
    """æ–‡ä½“åˆ†æ"""
    dominant_style: WritingStyle
    style_distribution: dict[WritingStyle, float]
    consistency_score: float  # ä¸€è²«æ€§ã‚¹ã‚³ã‚¢ (0-1)
    detected_inconsistencies: list[StyleInconsistency]
    formality_level: float  # æ”¹ã¾ã‚Šåº¦ (0-1)
    emotional_tone: str  # æ„Ÿæƒ…çš„ãƒˆãƒ¼ãƒ³
    narrative_voice: str  # èªã‚Šæ‰‹ã®å£°


@dataclass
class SentenceAnalysis:
    """æ–‡åˆ†æ"""
    sentence_id: str
    sentence_text: str
    length: int
    complexity_score: float  # è¤‡é›‘ã•ã‚¹ã‚³ã‚¢ (0-1)
    readability_score: float  # å¯èª­æ€§ã‚¹ã‚³ã‚¢ (0-1)
    style_classification: WritingStyle
    grammatical_issues: list[str]
    improvement_suggestions: list[str]
    rhythm_pattern: str
    emotional_weight: float  # æ„Ÿæƒ…çš„é‡ã¿


@dataclass
class ParagraphAnalysis:
    """æ®µè½åˆ†æ"""
    paragraph_id: str
    sentences: list[SentenceAnalysis]
    flow_score: float  # æµã‚Œã®ã‚¹ã‚³ã‚¢ (0-1)
    coherence_score: float  # ä¸€è²«æ€§ã‚¹ã‚³ã‚¢ (0-1)
    transition_quality: float  # ç§»è¡Œå“è³ª (0-1)
    length_balance: float  # é•·ã•ã®ãƒãƒ©ãƒ³ã‚¹ (0-1)
    information_density: float  # æƒ…å ±å¯†åº¦


@dataclass
class ReadabilityOptimization:
    """å¯èª­æ€§æœ€é©åŒ–"""
    optimization_id: str
    target_aspect: ReadabilityAspect
    optimization_type: str  # improve, maintain, adjust
    original_sentence: str
    optimized_sentence: str
    improvement_reason: str
    quality_impact: float  # å“è³ªã¸ã®å½±éŸ¿ (0-1)
    readability_gain: float  # å¯èª­æ€§å‘ä¸Šåº¦ (0-1)
    confidence_level: float  # ä¿¡é ¼åº¦ (0-1)


@dataclass
class StyleOptimization:
    """æ–‡ä½“æœ€é©åŒ–"""
    optimization_id: str
    inconsistency_type: StyleInconsistency
    location: str  # å ´æ‰€
    original_text: str
    optimized_text: str
    style_target: WritingStyle
    adjustment_reason: str
    consistency_improvement: float  # ä¸€è²«æ€§å‘ä¸Šåº¦


@dataclass
class ReadabilityOptimizationReport:
    """å¯èª­æ€§æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ"""
    report_id: str
    episode_number: int
    optimization_timestamp: datetime
    original_text: str
    optimized_text: str
    readability_metrics: list[ReadabilityMetric]
    style_analysis: StyleAnalysis
    sentence_analyses: list[SentenceAnalysis]
    paragraph_analyses: list[ParagraphAnalysis]
    readability_optimizations: list[ReadabilityOptimization]
    style_optimizations: list[StyleOptimization]
    overall_readability_score: float  # å…¨ä½“å¯èª­æ€§ã‚¹ã‚³ã‚¢ (0-1)
    style_consistency_score: float  # æ–‡ä½“ä¸€è²«æ€§ã‚¹ã‚³ã‚¢ (0-1)
    improvement_summary: str
    quality_assessment: dict[str, float]
    recommendations: list[str]
    optimization_metadata: dict[str, Any]


@dataclass
class ReadabilityOptimizerConfig:
    """å¯èª­æ€§æœ€é©åŒ–è¨­å®š"""
    target_reading_level: str = "general"  # general, advanced, simple
    preferred_style: WritingStyle = WritingStyle.NARRATIVE
    max_sentence_length: int = 50  # æœ€å¤§æ–‡å­—æ•°
    min_sentence_length: int = 10  # æœ€å°æ–‡å­—æ•°
    target_paragraph_length: int = 200  # ç›®æ¨™æ®µè½æ–‡å­—æ•°
    consistency_weight: float = 0.4  # ä¸€è²«æ€§é‡ã¿
    readability_weight: float = 0.6  # å¯èª­æ€§é‡ã¿
    preserve_author_voice: bool = True  # ä½œè€…ã®å£°ä¿æŒ
    enable_rhythm_optimization: bool = True  # ãƒªã‚ºãƒ æœ€é©åŒ–
    enable_clarity_enhancement: bool = True  # æ˜ç­æ€§å‘ä¸Š
    enable_engagement_improvement: bool = True  # é–¢ä¸åº¦æ”¹å–„
    aggressive_optimization: bool = False  # ç©æ¥µçš„æœ€é©åŒ–


class ReadabilityOptimizerService(BaseWritingStep):
    """STEP 14: æ–‡ä½“ãƒ»å¯èª­æ€§æœ€é©åŒ–ã‚µãƒ¼ãƒ“ã‚¹

    æ–‡ä½“ã®ä¸€è²«æ€§ã¨å¯èª­æ€§ã‚’æœ€é©åŒ–ã—ã€èª­è€…ã«ã¨ã£ã¦èª­ã¿ã‚„ã™ãé­…åŠ›çš„ãªæ–‡ç« ã«
    èª¿æ•´ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã€‚
    A38ã‚¬ã‚¤ãƒ‰ã®STEP14ã€Œæ–‡ä½“ãƒ»å¯èª­æ€§ãƒ‘ã‚¹ã€ã‚’å®Ÿè£…ã€‚
    """

    def __init__(
        self,
        config_manager: ConfigurationManagerService | None = None,
        path_service: Any | None = None,
        file_system_service: Any | None = None
    ) -> None:
        super().__init__()
        self._config_manager = config_manager
        self._path_service = path_service
        self._file_system = file_system_service
        self._optimizer_config = ReadabilityOptimizerConfig()

        # å¯èª­æ€§åˆ†æãƒ„ãƒ¼ãƒ«
        self._readability_analyzers = self._initialize_readability_analyzers()
        self._style_patterns = self._initialize_style_patterns()
        self._optimization_rules = self._initialize_optimization_rules()

    @abstractmethod
    def get_step_name(self) -> str:
        """ã‚¹ãƒ†ãƒƒãƒ—åã‚’å–å¾—"""
        return "æ–‡ä½“ãƒ»å¯èª­æ€§æœ€é©åŒ–"

    @abstractmethod
    def get_step_description(self) -> str:
        """ã‚¹ãƒ†ãƒƒãƒ—ã®èª¬æ˜ã‚’å–å¾—"""
        return "æ–‡ä½“ã®ä¸€è²«æ€§ã¨å¯èª­æ€§ã‚’æœ€é©åŒ–ã—ã€èª­è€…ã«ã¨ã£ã¦èª­ã¿ã‚„ã™ãé­…åŠ›çš„ãªæ–‡ç« ã«èª¿æ•´ã—ã¾ã™"

    @abstractmethod
    def execute_step(self, context: dict[str, Any]) -> dict[str, Any]:
        """STEP 14: æ–‡ä½“ãƒ»å¯èª­æ€§æœ€é©åŒ–ã®å®Ÿè¡Œ

        Args:
            context: å®Ÿè¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            æ–‡ä½“ãƒ»å¯èª­æ€§æœ€é©åŒ–çµæœã‚’å«ã‚€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            episode_number = context.get("episode_number")
            project = context.get("project")

            if not episode_number or not project:
                msg = "episode_numberã¾ãŸã¯projectãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
                raise ValueError(msg)

            # ãƒ†ã‚­ã‚¹ãƒˆã®å–å¾—
            text_to_optimize = self._get_text_for_optimization(context)
            if not text_to_optimize:
                msg = "æœ€é©åŒ–å¯¾è±¡ã®ãƒ†ã‚­ã‚¹ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“"
                raise ValueError(msg)

            # å¯èª­æ€§æœ€é©åŒ–ã®å®Ÿè¡Œ
            optimization_report = self._execute_readability_optimization(
                episode_number=episode_number,
                project=project,
                text=text_to_optimize,
                context=context
            )

            # çµæœã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«è¿½åŠ 
            context["readability_optimization"] = optimization_report
            context["optimized_readable_text"] = optimization_report.optimized_text
            context["readability_optimization_completed"] = True

            return context

        except Exception as e:
            context["readability_optimization_error"] = str(e)
            raise

    def _get_text_for_optimization(self, context: dict[str, Any]) -> str:
        """æœ€é©åŒ–å¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆã®å–å¾—"""

        # æ–‡å­—æ•°æœ€é©åŒ–å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆãŒã‚ã‚Œã°ãã‚Œã‚’ä½¿ç”¨
        if "optimized_text" in context:
            return context["optimized_text"]

        # ãã®ä»–ã®ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’å–å¾—
        text_sources = [
            "manuscript_text",
            "final_manuscript",
            "generated_manuscript"
        ]

        for source in text_sources:
            text = context.get(source)
            if text and isinstance(text, str):
                return text

        return ""

    def _execute_readability_optimization(
        self,
        episode_number: int,
        project: ProjectModel,
        text: str,
        context: dict[str, Any]
    ) -> ReadabilityOptimizationReport:
        """å¯èª­æ€§æœ€é©åŒ–ã®å®Ÿè¡Œ"""

        # åˆæœŸåˆ†æ
        readability_metrics = self._analyze_readability_metrics(text)
        style_analysis = self._analyze_writing_style(text)

        # æ–‡ãƒ»æ®µè½ãƒ¬ãƒ™ãƒ«åˆ†æ
        sentence_analyses = self._analyze_sentences(text)
        paragraph_analyses = self._analyze_paragraphs(text, sentence_analyses)

        # æœ€é©åŒ–ã®å®Ÿè¡Œ
        readability_optimizations = self._perform_readability_optimizations(
            text, sentence_analyses, readability_metrics
        )

        style_optimizations = self._perform_style_optimizations(
            text, style_analysis, sentence_analyses
        )

        # æœ€é©åŒ–ã•ã‚ŒãŸãƒ†ã‚­ã‚¹ãƒˆã®ç”Ÿæˆ
        optimized_text = self._apply_optimizations(
            text, readability_optimizations, style_optimizations
        )

        # æœ€é©åŒ–å¾Œã®è©•ä¾¡
        final_readability_score = self._calculate_overall_readability(
            optimized_text, readability_metrics
        )

        final_consistency_score = self._calculate_style_consistency(
            optimized_text, style_analysis
        )

        # å“è³ªè©•ä¾¡
        quality_assessment = self._assess_optimization_quality(
            text, optimized_text, readability_optimizations, style_optimizations
        )

        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        return self._generate_optimization_report(
            episode_number=episode_number,
            original_text=text,
            optimized_text=optimized_text,
            readability_metrics=readability_metrics,
            style_analysis=style_analysis,
            sentence_analyses=sentence_analyses,
            paragraph_analyses=paragraph_analyses,
            readability_optimizations=readability_optimizations,
            style_optimizations=style_optimizations,
            overall_readability_score=final_readability_score,
            style_consistency_score=final_consistency_score,
            quality_assessment=quality_assessment
        )

    def _analyze_readability_metrics(self, text: str) -> list[ReadabilityMetric]:
        """å¯èª­æ€§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®åˆ†æ"""
        metrics = []

        # æ–‡ã®é•·ã•
        sentences = self._split_into_sentences(text)
        avg_sentence_length = sum(len(s) for s in sentences) / len(sentences) if sentences else 0
        sentence_length_score = self._score_sentence_length(avg_sentence_length)

        metrics.append(ReadabilityMetric(
            metric_name="å¹³å‡æ–‡é•·",
            metric_value=sentence_length_score,
            description=f"å¹³å‡{avg_sentence_length:.1f}æ–‡å­—",
            target_range=(0.6, 0.8),
            current_assessment=self._assess_sentence_length(sentence_length_score),
            improvement_potential=1.0 - sentence_length_score
        ))

        # èªå½™ã®è¤‡é›‘ã•
        vocab_complexity = self._analyze_vocabulary_complexity(text)

        metrics.append(ReadabilityMetric(
            metric_name="èªå½™è¤‡é›‘ã•",
            metric_value=vocab_complexity,
            description="èªå½™ã®é›£æ˜“åº¦",
            target_range=(0.4, 0.7),
            current_assessment=self._assess_vocabulary_complexity(vocab_complexity),
            improvement_potential=abs(0.55 - vocab_complexity)
        ))

        # æ®µè½ã®æµã‚Œ
        paragraph_flow = self._analyze_paragraph_flow(text)

        metrics.append(ReadabilityMetric(
            metric_name="æ®µè½ã®æµã‚Œ",
            metric_value=paragraph_flow,
            description="æ®µè½é–“ã®æ¥ç¶šæ€§",
            target_range=(0.7, 0.9),
            current_assessment=self._assess_paragraph_flow(paragraph_flow),
            improvement_potential=1.0 - paragraph_flow
        ))

        # ãƒªã‚ºãƒ ãƒ»ãƒ†ãƒ³ãƒ
        rhythm_score = self._analyze_rhythm_pattern(text)

        metrics.append(ReadabilityMetric(
            metric_name="æ–‡ç« ãƒªã‚ºãƒ ",
            metric_value=rhythm_score,
            description="èª­ã¿ã‚„ã™ã•ã®ãƒªã‚ºãƒ ",
            target_range=(0.6, 0.8),
            current_assessment=self._assess_rhythm(rhythm_score),
            improvement_potential=abs(0.7 - rhythm_score)
        ))

        return metrics

    def _analyze_writing_style(self, text: str) -> StyleAnalysis:
        """æ–‡ä½“åˆ†æ"""

        # æ–‡ä½“åˆ†å¸ƒã®åˆ†æ
        style_distribution = self._calculate_style_distribution(text)
        dominant_style = max(style_distribution.keys(), key=lambda s: style_distribution[s])

        # ä¸€è²«æ€§ã®åˆ†æ
        consistency_score = self._calculate_style_consistency_score(text, style_distribution)

        # ä¸ä¸€è²«æ€§ã®æ¤œå‡º
        inconsistencies = self._detect_style_inconsistencies(text)

        # æ”¹ã¾ã‚Šåº¦ã®åˆ†æ
        formality_level = self._analyze_formality_level(text)

        # æ„Ÿæƒ…çš„ãƒˆãƒ¼ãƒ³ã®åˆ†æ
        emotional_tone = self._analyze_emotional_tone(text)

        # èªã‚Šæ‰‹ã®å£°ã®åˆ†æ
        narrative_voice = self._analyze_narrative_voice(text)

        return StyleAnalysis(
            dominant_style=dominant_style,
            style_distribution=style_distribution,
            consistency_score=consistency_score,
            detected_inconsistencies=inconsistencies,
            formality_level=formality_level,
            emotional_tone=emotional_tone,
            narrative_voice=narrative_voice
        )

    def _analyze_sentences(self, text: str) -> list[SentenceAnalysis]:
        """æ–‡åˆ†æ"""
        sentences = self._split_into_sentences(text)
        analyses = []

        for i, sentence in enumerate(sentences):
            if sentence.strip():
                analysis = SentenceAnalysis(
                    sentence_id=f"sent_{i}",
                    sentence_text=sentence.strip(),
                    length=len(sentence.strip()),
                    complexity_score=self._calculate_sentence_complexity(sentence),
                    readability_score=self._calculate_sentence_readability(sentence),
                    style_classification=self._classify_sentence_style(sentence),
                    grammatical_issues=self._detect_grammatical_issues(sentence),
                    improvement_suggestions=self._generate_sentence_improvements(sentence),
                    rhythm_pattern=self._analyze_sentence_rhythm(sentence),
                    emotional_weight=self._calculate_emotional_weight(sentence)
                )
                analyses.append(analysis)

        return analyses

    def _analyze_paragraphs(
        self,
        text: str,
        sentence_analyses: list[SentenceAnalysis]
    ) -> list[ParagraphAnalysis]:
        """æ®µè½åˆ†æ"""
        paragraphs = text.split("\n\n")
        analyses = []

        sentence_idx = 0
        for i, paragraph in enumerate(paragraphs):
            if paragraph.strip():
                # ã“ã®æ®µè½ã«å«ã¾ã‚Œã‚‹æ–‡ã‚’ç‰¹å®š
                paragraph_sentences = []
                para_sentence_count = paragraph.count("ã€‚") + paragraph.count("ï¼") + paragraph.count("ï¼Ÿ")

                for _ in range(para_sentence_count):
                    if sentence_idx < len(sentence_analyses):
                        paragraph_sentences.append(sentence_analyses[sentence_idx])
                        sentence_idx += 1

                analysis = ParagraphAnalysis(
                    paragraph_id=f"para_{i}",
                    sentences=paragraph_sentences,
                    flow_score=self._calculate_paragraph_flow(paragraph),
                    coherence_score=self._calculate_paragraph_coherence(paragraph),
                    transition_quality=self._calculate_transition_quality(paragraph, i, paragraphs),
                    length_balance=self._calculate_length_balance(paragraph),
                    information_density=self._calculate_information_density(paragraph)
                )
                analyses.append(analysis)

        return analyses

    def _perform_readability_optimizations(
        self,
        text: str,
        sentence_analyses: list[SentenceAnalysis],
        metrics: list[ReadabilityMetric]
    ) -> list[ReadabilityOptimization]:
        """å¯èª­æ€§æœ€é©åŒ–ã®å®Ÿè¡Œ"""
        optimizations = []

        # æ–‡ã®é•·ã•æœ€é©åŒ–
        for sentence_analysis in sentence_analyses:
            if self._needs_length_optimization(sentence_analysis):
                optimization = self._create_length_optimization(sentence_analysis)
                if optimization:
                    optimizations.append(optimization)

        # èªå½™æœ€é©åŒ–
        vocab_optimizations = self._optimize_vocabulary(sentence_analyses, metrics)
        optimizations.extend(vocab_optimizations)

        # æ§‹é€ æœ€é©åŒ–
        structure_optimizations = self._optimize_sentence_structure(sentence_analyses)
        optimizations.extend(structure_optimizations)

        # ãƒªã‚ºãƒ æœ€é©åŒ–
        if self._optimizer_config.enable_rhythm_optimization:
            rhythm_optimizations = self._optimize_rhythm(sentence_analyses)
            optimizations.extend(rhythm_optimizations)

        return optimizations

    def _perform_style_optimizations(
        self,
        text: str,
        style_analysis: StyleAnalysis,
        sentence_analyses: list[SentenceAnalysis]
    ) -> list[StyleOptimization]:
        """æ–‡ä½“æœ€é©åŒ–ã®å®Ÿè¡Œ"""
        optimizations = []

        # ä¸ä¸€è²«æ€§ã®ä¿®æ­£
        for inconsistency in style_analysis.detected_inconsistencies:
            optimization = self._create_style_optimization(inconsistency, text, sentence_analyses)
            if optimization:
                optimizations.append(optimization)

        # èªèª¿çµ±ä¸€
        tone_optimizations = self._optimize_tone_consistency(text, style_analysis)
        optimizations.extend(tone_optimizations)

        # æ”¹ã¾ã‚Šåº¦èª¿æ•´
        formality_optimizations = self._optimize_formality_level(text, style_analysis)
        optimizations.extend(formality_optimizations)

        return optimizations

    def _apply_optimizations(
        self,
        original_text: str,
        readability_opts: list[ReadabilityOptimization],
        style_opts: list[StyleOptimization]
    ) -> str:
        """æœ€é©åŒ–ã®é©ç”¨"""
        optimized_text = original_text

        # å¯èª­æ€§æœ€é©åŒ–ã®é©ç”¨
        for opt in readability_opts:
            optimized_text = optimized_text.replace(
                opt.original_sentence, opt.optimized_sentence
            )

        # æ–‡ä½“æœ€é©åŒ–ã®é©ç”¨
        for opt in style_opts:
            optimized_text = optimized_text.replace(
                opt.original_text, opt.optimized_text
            )

        return optimized_text

    def _generate_optimization_report(
        self,
        episode_number: int,
        original_text: str,
        optimized_text: str,
        readability_metrics: list[ReadabilityMetric],
        style_analysis: StyleAnalysis,
        sentence_analyses: list[SentenceAnalysis],
        paragraph_analyses: list[ParagraphAnalysis],
        readability_optimizations: list[ReadabilityOptimization],
        style_optimizations: list[StyleOptimization],
        overall_readability_score: float,
        style_consistency_score: float,
        quality_assessment: dict[str, float]
    ) -> ReadabilityOptimizationReport:
        """æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""

        # æ”¹å–„ã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ
        improvement_summary = self._generate_improvement_summary(
            readability_optimizations, style_optimizations,
            overall_readability_score, style_consistency_score
        )

        # æ¨å¥¨äº‹é …ã®ç”Ÿæˆ
        recommendations = self._generate_optimization_recommendations(
            readability_metrics, style_analysis, quality_assessment
        )

        return ReadabilityOptimizationReport(
            report_id=f"readability_opt_{episode_number}_{datetime.now(tz=datetime.timezone.utc).strftime('%Y%m%d_%H%M%S')}",
            episode_number=episode_number,
            optimization_timestamp=datetime.now(tz=datetime.timezone.utc),
            original_text=original_text,
            optimized_text=optimized_text,
            readability_metrics=readability_metrics,
            style_analysis=style_analysis,
            sentence_analyses=sentence_analyses,
            paragraph_analyses=paragraph_analyses,
            readability_optimizations=readability_optimizations,
            style_optimizations=style_optimizations,
            overall_readability_score=overall_readability_score,
            style_consistency_score=style_consistency_score,
            improvement_summary=improvement_summary,
            quality_assessment=quality_assessment,
            recommendations=recommendations,
            optimization_metadata={
                "config": self._optimizer_config.__dict__,
                "optimization_timestamp": datetime.now(tz=datetime.timezone.utc),
                "total_readability_optimizations": len(readability_optimizations),
                "total_style_optimizations": len(style_optimizations),
                "character_count_change": len(optimized_text) - len(original_text)
            }
        )

    # åˆ†æãƒ¡ã‚½ãƒƒãƒ‰ã®å®Ÿè£…

    def _initialize_readability_analyzers(self) -> dict[str, Any]:
        """å¯èª­æ€§åˆ†æå™¨ã®åˆæœŸåŒ–"""
        return {
            "sentence_length": {
                "optimal_range": (15, 35),
                "warning_threshold": 50
            },
            "vocabulary": {
                "common_words_weight": 0.7,
                "technical_words_penalty": 0.3
            },
            "rhythm": {
                "variation_target": 0.6,
                "monotony_threshold": 0.3
            }
        }

    def _initialize_style_patterns(self) -> dict[str, Any]:
        """æ–‡ä½“ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆæœŸåŒ–"""
        return {
            WritingStyle.NARRATIVE: {
                "markers": ["ã ã£ãŸ", "ã§ã‚ã‚‹", "ã®ã "],
                "sentence_endings": ["ã€‚", "ã®ã ã£ãŸã€‚"],
                "tone": "storytelling"
            },
            WritingStyle.DESCRIPTIVE: {
                "markers": ["ã‚ˆã†ã«", "ã¾ã‚‹ã§", "ã‹ã®ã‚ˆã†ãª"],
                "sentence_endings": ["ã€‚", "ã®ã§ã‚ã‚‹ã€‚"],
                "tone": "observational"
            },
            WritingStyle.CONVERSATIONAL: {
                "markers": ["ã€Œ", "ã€", "ã ã‚ˆ", "ã§ã™ã­"],
                "sentence_endings": ["ã€‚", "ï¼Ÿ", "ï¼"],
                "tone": "informal"
            }
        }

    def _initialize_optimization_rules(self) -> dict[str, Any]:
        """æœ€é©åŒ–ãƒ«ãƒ¼ãƒ«ã®åˆæœŸåŒ–"""
        return {
            "sentence_length": {
                "max_length": 50,
                "optimal_length": 25,
                "split_strategies": ["conjunction", "subordinate_clause"]
            },
            "vocabulary": {
                "complexity_reduction": {
                    "difficult_words": ["è¤‡é›‘ãª", "å›°é›£ãª", "å„ä»‹ãª"],
                    "simple_alternatives": ["é›£ã—ã„", "å¤§å¤‰ãª", "é¢å€’ãª"]
                }
            },
            "rhythm": {
                "variation_techniques": ["length_variation", "structure_variation"],
                "monotony_breakers": ["çŸ­æ–‡æŒ¿å…¥", "ç–‘å•æ–‡å¤‰æ›"]
            }
        }

    def _split_into_sentences(self, text: str) -> list[str]:
        """æ–‡ã¸ã®åˆ†å‰²"""
        import re
        # æ—¥æœ¬èªã®æ–‡æœ«è¨˜å·ã§åˆ†å‰²
        sentences = re.split(r"[ã€‚ï¼ï¼Ÿ]", text)
        return [s.strip() + "ã€‚" for s in sentences if s.strip()]

    def _score_sentence_length(self, avg_length: float) -> float:
        """æ–‡ã®é•·ã•ã®ã‚¹ã‚³ã‚¢åŒ–"""
        optimal_length = 25
        if avg_length <= optimal_length:
            return avg_length / optimal_length
        # é•·ã™ãã‚‹å ´åˆã¯ãƒšãƒŠãƒ«ãƒ†ã‚£
        excess = avg_length - optimal_length
        return max(0.0, 1.0 - excess / optimal_length)

    def _assess_sentence_length(self, score: float) -> str:
        """æ–‡ã®é•·ã•ã®è©•ä¾¡"""
        if score >= 0.8:
            return "æœ€é©"
        if score >= 0.6:
            return "è‰¯å¥½"
        if score >= 0.4:
            return "è¦æ”¹å–„"
        return "è¦å¤§å¹…æ”¹å–„"

    def _analyze_vocabulary_complexity(self, text: str) -> float:
        """èªå½™è¤‡é›‘ã•ã®åˆ†æ"""
        # ç°¡æ˜“å®Ÿè£…ï¼šæ¼¢å­—ã®å‰²åˆã§åˆ¤å®š
        total_chars = len(text)
        if total_chars == 0:
            return 0.0

        # æ¼¢å­—ã®ã‚«ã‚¦ãƒ³ãƒˆï¼ˆç°¡æ˜“å®Ÿè£…ï¼‰
        kanji_count = sum(1 for c in text if ord(c) >= 0x4E00 and ord(c) <= 0x9FAF)
        kanji_ratio = kanji_count / total_chars

        # 0-1ã‚¹ã‚±ãƒ¼ãƒ«ã«æ­£è¦åŒ–
        return min(kanji_ratio * 2, 1.0)

    def _assess_vocabulary_complexity(self, complexity: float) -> str:
        """èªå½™è¤‡é›‘ã•ã®è©•ä¾¡"""
        if complexity <= 0.3:
            return "æ˜“ã—ã„"
        if complexity <= 0.5:
            return "é©åˆ‡"
        if complexity <= 0.7:
            return "ã‚„ã‚„é›£ã—ã„"
        return "é›£ã—ã„"

    def _analyze_paragraph_flow(self, text: str) -> float:
        """æ®µè½ã®æµã‚Œåˆ†æ"""
        paragraphs = text.split("\n\n")
        if len(paragraphs) <= 1:
            return 1.0

        # æ¥ç¶šè©ã®ä½¿ç”¨é »åº¦ã§ç°¡æ˜“è©•ä¾¡
        connectors = ["ãã—ã¦", "ã—ã‹ã—", "ã¾ãŸ", "ã•ã‚‰ã«", "ã¨ã“ã‚ã§", "ã¤ã¾ã‚Š"]
        connector_count = sum(1 for connector in connectors if connector in text)

        # æ®µè½æ•°ã«å¯¾ã™ã‚‹æ¥ç¶šè©ã®å‰²åˆ
        return min(connector_count / len(paragraphs), 1.0)

    def _assess_paragraph_flow(self, flow: float) -> str:
        """æ®µè½ã®æµã‚Œã®è©•ä¾¡"""
        if flow >= 0.8:
            return "å„ªç§€"
        if flow >= 0.6:
            return "è‰¯å¥½"
        if flow >= 0.4:
            return "æ™®é€š"
        return "è¦æ”¹å–„"

    def _analyze_rhythm_pattern(self, text: str) -> float:
        """ãƒªã‚ºãƒ ãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆ†æ"""
        sentences = self._split_into_sentences(text)
        if len(sentences) <= 1:
            return 0.5

        # æ–‡ã®é•·ã•ã®å¤‰å‹•ã‚’æ¸¬å®š
        lengths = [len(s) for s in sentences]
        if not lengths:
            return 0.5

        # æ¨™æº–åå·®ã‚’ä½¿ã£ã¦å¤‰å‹•åº¦ã‚’è¨ˆç®—
        mean_length = sum(lengths) / len(lengths)
        variance = sum((l - mean_length) ** 2 for l in lengths) / len(lengths)
        std_dev = variance ** 0.5

        # å¤‰å‹•ä¿‚æ•°ã‚’æ­£è¦åŒ–ï¼ˆé©åº¦ãªå¤‰å‹•ãŒç†æƒ³ï¼‰
        variation_coefficient = std_dev / mean_length if mean_length > 0 else 0

        # 0.2-0.4ã®å¤‰å‹•ä¿‚æ•°ãŒç†æƒ³çš„
        if 0.2 <= variation_coefficient <= 0.4:
            return 1.0
        if variation_coefficient < 0.2:
            return variation_coefficient / 0.2 * 0.7  # å˜èª¿ã™ãã‚‹
        return max(0.0, 1.0 - (variation_coefficient - 0.4) / 0.6)  # å¤‰å‹•ã—ã™ã

    def _assess_rhythm(self, rhythm_score: float) -> str:
        """ãƒªã‚ºãƒ ã®è©•ä¾¡"""
        if rhythm_score >= 0.8:
            return "å„ªç§€"
        if rhythm_score >= 0.6:
            return "è‰¯å¥½"
        if rhythm_score >= 0.4:
            return "æ™®é€š"
        return "è¦æ”¹å–„"

    def _calculate_style_distribution(self, text: str) -> dict[WritingStyle, float]:
        """æ–‡ä½“åˆ†å¸ƒã®è¨ˆç®—"""
        distribution = dict.fromkeys(WritingStyle, 0.0)

        # å„æ–‡ä½“ã®ç‰¹å¾´çš„ãªãƒãƒ¼ã‚«ãƒ¼ã‚’æ¤œå‡º
        for style, patterns in self._style_patterns.items():
            marker_count = 0
            for marker in patterns["markers"]:
                marker_count += text.count(marker)

            # æ­£è¦åŒ–
            distribution[style] = marker_count / max(len(text) / 100, 1)

        # æ­£è¦åŒ–ã—ã¦åˆè¨ˆã‚’1ã«ã™ã‚‹
        total = sum(distribution.values())
        if total > 0:
            distribution = {k: v / total for k, v in distribution.items()}
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¨ã—ã¦ç‰©èªèª¿ã‚’è¨­å®š
            distribution[WritingStyle.NARRATIVE] = 1.0

        return distribution

    def _calculate_style_consistency_score(
        self,
        text: str,
        style_distribution: dict[WritingStyle, float]
    ) -> float:
        """æ–‡ä½“ä¸€è²«æ€§ã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""

        # æ”¯é…çš„ã‚¹ã‚¿ã‚¤ãƒ«ã®å‰²åˆãŒé«˜ã„ã»ã©ä¸€è²«æ€§ãŒé«˜ã„
        dominant_ratio = max(style_distribution.values())

        # ã‚¨ãƒ³ãƒˆãƒ­ãƒ”ãƒ¼çš„ãªè¨ˆç®—ã§å¤šæ§˜æ€§ã®ãƒšãƒŠãƒ«ãƒ†ã‚£
        entropy = -sum(p * (p.bit_length() if p > 0 else 0) for p in style_distribution.values())
        max_entropy = (len(style_distribution)).bit_length() if len(style_distribution) > 0 else 1

        normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0
        consistency = 1.0 - normalized_entropy

        return (dominant_ratio + consistency) / 2

    def _detect_style_inconsistencies(self, text: str) -> list[StyleInconsistency]:
        """æ–‡ä½“ã®ä¸ä¸€è²«æ€§æ¤œå‡º"""
        inconsistencies = []

        # ç°¡æ˜“å®Ÿè£…ï¼šæ•¬èªã®æ··åœ¨ãƒã‚§ãƒƒã‚¯
        has_keigo = any(word in text for word in ["ã§ã™", "ã¾ã™", "ã§ã‚ã‚‹", "ã§ã‚ã‚ã†"])
        has_casual = any(word in text for word in ["ã ", "ã ã‚ˆ", "ã ã­"])

        if has_keigo and has_casual:
            inconsistencies.append(StyleInconsistency.FORMALITY_MISMATCH)

        # æ™‚åˆ¶ã®æ··åœ¨ãƒã‚§ãƒƒã‚¯
        has_past = any(word in text for word in ["ã ã£ãŸ", "ã—ãŸ", "ãŒã‚ã£ãŸ"])
        has_present = any(word in text for word in ["ã§ã‚ã‚‹", "ã™ã‚‹", "ãŒã‚ã‚‹"])

        if has_past and has_present:
            inconsistencies.append(StyleInconsistency.TENSE_INCONSISTENCY)

        return inconsistencies

    def _analyze_formality_level(self, text: str) -> float:
        """æ”¹ã¾ã‚Šåº¦ã®åˆ†æ"""
        formal_markers = ["ã§ã™", "ã¾ã™", "ã§ã‚ã‚‹", "ã«ãŠã‹ã‚Œã¾ã—ã¦ã¯"]
        casual_markers = ["ã ", "ã ã‚ˆ", "ã˜ã‚ƒã‚“", "ã£ã™"]

        formal_count = sum(text.count(marker) for marker in formal_markers)
        casual_count = sum(text.count(marker) for marker in casual_markers)

        total_markers = formal_count + casual_count
        if total_markers == 0:
            return 0.5  # ä¸­æ€§

        return formal_count / total_markers

    def _analyze_emotional_tone(self, text: str) -> str:
        """æ„Ÿæƒ…çš„ãƒˆãƒ¼ãƒ³ã®åˆ†æ"""
        positive_words = ["å¬‰ã—ã„", "æ¥½ã—ã„", "ç¾ã—ã„", "ç´ æ™´ã‚‰ã—ã„", "æ„Ÿå‹•"]
        negative_words = ["æ‚²ã—ã„", "è¾›ã„", "è‹¦ã—ã„", "æ‚”ã—ã„", "æ€’ã‚Š"]
        neutral_words = ["æ™®é€š", "é€šå¸¸", "ä¸€èˆ¬çš„", "æ¨™æº–çš„"]

        positive_count = sum(text.count(word) for word in positive_words)
        negative_count = sum(text.count(word) for word in negative_words)
        neutral_count = sum(text.count(word) for word in neutral_words)

        if positive_count > negative_count and positive_count > neutral_count:
            return "ãƒã‚¸ãƒ†ã‚£ãƒ–"
        if negative_count > positive_count and negative_count > neutral_count:
            return "ãƒã‚¬ãƒ†ã‚£ãƒ–"
        return "ä¸­æ€§"

    def _analyze_narrative_voice(self, text: str) -> str:
        """èªã‚Šæ‰‹ã®å£°ã®åˆ†æ"""
        first_person = ["ç§", "åƒ•", "ä¿º", "è‡ªåˆ†"]
        third_person = ["å½¼", "å½¼å¥³", "ãã®äºº"]

        first_count = sum(text.count(word) for word in first_person)
        third_count = sum(text.count(word) for word in third_person)

        if first_count > third_count:
            return "ä¸€äººç§°"
        if third_count > first_count:
            return "ä¸‰äººç§°"
        return "æ··åˆ"

    # æ–‡ãƒ»æ®µè½åˆ†æãƒ¡ã‚½ãƒƒãƒ‰ã®å®Ÿè£…ï¼ˆã‚¹ã‚¿ãƒ–ï¼‰

    def _calculate_sentence_complexity(self, sentence: str) -> float:
        """æ–‡ã®è¤‡é›‘ã•è¨ˆç®—"""
        # å¾“å±ç¯€ã®æ•°ã€æ–‡ã®é•·ã•ã€èªå½™ã®è¤‡é›‘ã•ã‚’ç·åˆè©•ä¾¡
        complexity_factors = []

        # é•·ã•ã«ã‚ˆã‚‹è¤‡é›‘ã•
        length_complexity = min(len(sentence) / 50, 1.0)
        complexity_factors.append(length_complexity)

        # å¾“å±ç¯€æ•°ã«ã‚ˆã‚‹è¤‡é›‘ã•
        subordinate_markers = ["ãŒ", "ã®ã§", "ã‹ã‚‰", "ãŸã‚", "ã¨ã", "ãªãŒã‚‰"]
        subordinate_count = sum(sentence.count(marker) for marker in subordinate_markers)
        subordinate_complexity = min(subordinate_count / 3, 1.0)
        complexity_factors.append(subordinate_complexity)

        return sum(complexity_factors) / len(complexity_factors)

    def _calculate_sentence_readability(self, sentence: str) -> float:
        """æ–‡ã®å¯èª­æ€§è¨ˆç®—"""
        # é•·ã•ã€èªå½™ã€æ§‹é€ ã‚’ç·åˆã—ã¦å¯èª­æ€§ã‚’ã‚¹ã‚³ã‚¢åŒ–
        readability_score = 1.0

        # é•·ã•ãƒšãƒŠãƒ«ãƒ†ã‚£
        if len(sentence) > 40:
            length_penalty = (len(sentence) - 40) / 40
            readability_score -= length_penalty * 0.3

        # èªå½™è¤‡é›‘ã•ãƒšãƒŠãƒ«ãƒ†ã‚£
        vocab_complexity = self._analyze_vocabulary_complexity(sentence)
        if vocab_complexity > 0.6:
            vocab_penalty = (vocab_complexity - 0.6) / 0.4
            readability_score -= vocab_penalty * 0.2

        return max(0.0, readability_score)

    def _classify_sentence_style(self, sentence: str) -> WritingStyle:
        """æ–‡ã®ã‚¹ã‚¿ã‚¤ãƒ«åˆ†é¡"""
        # å„ã‚¹ã‚¿ã‚¤ãƒ«ã®ãƒãƒ¼ã‚«ãƒ¼ã‚’ãƒã‚§ãƒƒã‚¯
        for style, patterns in self._style_patterns.items():
            marker_count = sum(sentence.count(marker) for marker in patterns["markers"])
            if marker_count > 0:
                return style

        return WritingStyle.NARRATIVE  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

    def _detect_grammatical_issues(self, sentence: str) -> list[str]:
        """æ–‡æ³•çš„å•é¡Œã®æ¤œå‡º"""
        issues = []

        # ç°¡æ˜“ãƒã‚§ãƒƒã‚¯
        if sentence.count("ã€‚") > 1:
            issues.append("æ–‡ãŒé•·ã™ãã‚‹å¯èƒ½æ€§")

        if not sentence.endswith(("ã€‚", "ï¼", "ï¼Ÿ")):
            issues.append("æ–‡æœ«è¨˜å·ãŒä¸é©åˆ‡")

        return issues

    def _generate_sentence_improvements(self, sentence: str) -> list[str]:
        """æ–‡ã®æ”¹å–„ææ¡ˆç”Ÿæˆ"""
        suggestions = []

        if len(sentence) > 40:
            suggestions.append("æ–‡ã‚’åˆ†å‰²ã—ã¦çŸ­ãã™ã‚‹")

        if sentence.count("ãŒ") > 2:
            suggestions.append("å¾“å±ç¯€ã‚’æ¸›ã‚‰ã—ã¦ç°¡æ½”ã«ã™ã‚‹")

        return suggestions

    def _analyze_sentence_rhythm(self, sentence: str) -> str:
        """æ–‡ã®ãƒªã‚ºãƒ ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ"""
        if len(sentence) < 20:
            return "çŸ­èª¿"
        if len(sentence) < 40:
            return "ä¸­èª¿"
        return "é•·èª¿"

    def _calculate_emotional_weight(self, sentence: str) -> float:
        """æ–‡ã®æ„Ÿæƒ…çš„é‡ã¿è¨ˆç®—"""
        emotional_words = ["å¬‰ã—ã„", "æ‚²ã—ã„", "æ€’ã‚Š", "å–œã³", "é©šã", "ææ€–"]
        emotional_count = sum(sentence.count(word) for word in emotional_words)
        return min(emotional_count / 3.0, 1.0)

    # æ®µè½åˆ†æãƒ¡ã‚½ãƒƒãƒ‰ã®å®Ÿè£…ï¼ˆã‚¹ã‚¿ãƒ–ï¼‰

    def _calculate_paragraph_flow(self, paragraph: str) -> float:
        """æ®µè½ã®æµã‚Œè¨ˆç®—"""
        return 0.8  # ã‚¹ã‚¿ãƒ–å®Ÿè£…

    def _calculate_paragraph_coherence(self, paragraph: str) -> float:
        """æ®µè½ã®ä¸€è²«æ€§è¨ˆç®—"""
        return 0.8  # ã‚¹ã‚¿ãƒ–å®Ÿè£…

    def _calculate_transition_quality(self, paragraph: str, index: int, all_paragraphs: list[str]) -> float:
        """ç§»è¡Œå“è³ªè¨ˆç®—"""
        return 0.7  # ã‚¹ã‚¿ãƒ–å®Ÿè£…

    def _calculate_length_balance(self, paragraph: str) -> float:
        """é•·ã•ãƒãƒ©ãƒ³ã‚¹è¨ˆç®—"""
        return 0.8  # ã‚¹ã‚¿ãƒ–å®Ÿè£…

    def _calculate_information_density(self, paragraph: str) -> float:
        """æƒ…å ±å¯†åº¦è¨ˆç®—"""
        return 0.6  # ã‚¹ã‚¿ãƒ–å®Ÿè£…

    # æœ€é©åŒ–å®Ÿè¡Œãƒ¡ã‚½ãƒƒãƒ‰ï¼ˆã‚¹ã‚¿ãƒ–å®Ÿè£…ï¼‰

    def _needs_length_optimization(self, sentence_analysis: SentenceAnalysis) -> bool:
        """é•·ã•æœ€é©åŒ–ãŒå¿…è¦ã‹ã©ã†ã‹"""
        return sentence_analysis.length > self._optimizer_config.max_sentence_length

    def _create_length_optimization(self, sentence_analysis: SentenceAnalysis) -> ReadabilityOptimization | None:
        """é•·ã•æœ€é©åŒ–ã®ä½œæˆ"""
        if sentence_analysis.length <= self._optimizer_config.max_sentence_length:
            return None

        # ç°¡æ˜“å®Ÿè£…ï¼šé•·ã„æ–‡ã‚’åˆ†å‰²
        optimized = sentence_analysis.sentence_text.replace("ã€ãã—ã¦", "ã€‚ãã—ã¦")

        return ReadabilityOptimization(
            optimization_id=f"length_opt_{sentence_analysis.sentence_id}",
            target_aspect=ReadabilityAspect.SENTENCE_LENGTH,
            optimization_type="improve",
            original_sentence=sentence_analysis.sentence_text,
            optimized_sentence=optimized,
            improvement_reason="æ–‡ãŒé•·ã™ãã‚‹ãŸã‚åˆ†å‰²",
            quality_impact=0.1,
            readability_gain=0.3,
            confidence_level=0.8
        )

    def _optimize_vocabulary(
        self,
        sentence_analyses: list[SentenceAnalysis],
        metrics: list[ReadabilityMetric]
    ) -> list[ReadabilityOptimization]:
        """èªå½™æœ€é©åŒ–"""
        return []  # ã‚¹ã‚¿ãƒ–å®Ÿè£…

    def _optimize_sentence_structure(
        self,
        sentence_analyses: list[SentenceAnalysis]
    ) -> list[ReadabilityOptimization]:
        """æ–‡æ§‹é€ æœ€é©åŒ–"""
        return []  # ã‚¹ã‚¿ãƒ–å®Ÿè£…

    def _optimize_rhythm(
        self,
        sentence_analyses: list[SentenceAnalysis]
    ) -> list[ReadabilityOptimization]:
        """ãƒªã‚ºãƒ æœ€é©åŒ–"""
        return []  # ã‚¹ã‚¿ãƒ–å®Ÿè£…

    def _create_style_optimization(
        self,
        inconsistency: StyleInconsistency,
        text: str,
        sentence_analyses: list[SentenceAnalysis]
    ) -> StyleOptimization | None:
        """æ–‡ä½“æœ€é©åŒ–ã®ä½œæˆ"""
        # ã‚¹ã‚¿ãƒ–å®Ÿè£…
        return None

    def _optimize_tone_consistency(
        self,
        text: str,
        style_analysis: StyleAnalysis
    ) -> list[StyleOptimization]:
        """èªèª¿ä¸€è²«æ€§ã®æœ€é©åŒ–"""
        return []  # ã‚¹ã‚¿ãƒ–å®Ÿè£…

    def _optimize_formality_level(
        self,
        text: str,
        style_analysis: StyleAnalysis
    ) -> list[StyleOptimization]:
        """æ”¹ã¾ã‚Šåº¦ã®æœ€é©åŒ–"""
        return []  # ã‚¹ã‚¿ãƒ–å®Ÿè£…

    def _calculate_overall_readability(
        self,
        text: str,
        original_metrics: list[ReadabilityMetric]
    ) -> float:
        """å…¨ä½“å¯èª­æ€§ã®è¨ˆç®—"""
        # æ–°ã—ã„ãƒ†ã‚­ã‚¹ãƒˆã§ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å†è¨ˆç®—
        new_metrics = self._analyze_readability_metrics(text)

        # å¹³å‡ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—
        total_score = sum(metric.metric_value for metric in new_metrics)
        return total_score / len(new_metrics) if new_metrics else 0.5

    def _calculate_style_consistency(
        self,
        text: str,
        original_analysis: StyleAnalysis
    ) -> float:
        """æ–‡ä½“ä¸€è²«æ€§ã®è¨ˆç®—"""
        new_analysis = self._analyze_writing_style(text)
        return new_analysis.consistency_score

    def _assess_optimization_quality(
        self,
        original_text: str,
        optimized_text: str,
        readability_opts: list[ReadabilityOptimization],
        style_opts: list[StyleOptimization]
    ) -> dict[str, float]:
        """æœ€é©åŒ–å“è³ªã®è©•ä¾¡"""

        return {
            "readability_improvement": 0.3,
            "style_consistency_improvement": 0.2,
            "overall_quality": 0.8,
            "content_preservation": 0.95,
            "naturalness": 0.85
        }

    def _generate_improvement_summary(
        self,
        readability_opts: list[ReadabilityOptimization],
        style_opts: list[StyleOptimization],
        readability_score: float,
        consistency_score: float
    ) -> str:
        """æ”¹å–„ã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ"""

        total_optimizations = len(readability_opts) + len(style_opts)

        summary_parts = [
            f"ç·è¨ˆ{total_optimizations}å€‹ã®æœ€é©åŒ–ã‚’å®Ÿæ–½ã€‚",
            f"å¯èª­æ€§ã‚¹ã‚³ã‚¢: {readability_score:.2f}, æ–‡ä½“ä¸€è²«æ€§: {consistency_score:.2f}ã€‚"
        ]

        if readability_score >= 0.8 and consistency_score >= 0.8:
            summary_parts.append("é«˜å“è³ªãªæœ€é©åŒ–ãŒå®Œäº†ã—ã¾ã—ãŸã€‚")
        elif readability_score >= 0.6 and consistency_score >= 0.6:
            summary_parts.append("è‰¯å¥½ãªæœ€é©åŒ–çµæœã§ã™ã€‚")
        else:
            summary_parts.append("è¿½åŠ ã®æœ€é©åŒ–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")

        return " ".join(summary_parts)

    def _generate_optimization_recommendations(
        self,
        metrics: list[ReadabilityMetric],
        style_analysis: StyleAnalysis,
        quality_assessment: dict[str, float]
    ) -> list[str]:
        """æœ€é©åŒ–æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""
        recommendations = []

        # ãƒ¡ãƒˆãƒªã‚¯ã‚¹ãƒ™ãƒ¼ã‚¹ã®æ¨å¥¨äº‹é …
        for metric in metrics:
            if metric.metric_value < 0.6:
                recommendations.append(f"ğŸ“Š {metric.metric_name}ã®æ”¹å–„ã‚’æ¨å¥¨ã—ã¾ã™")

        # æ–‡ä½“ä¸€è²«æ€§ã®æ¨å¥¨äº‹é …
        if style_analysis.consistency_score < 0.7:
            recommendations.append("ğŸ­ æ–‡ä½“ã®ä¸€è²«æ€§å‘ä¸ŠãŒå¿…è¦ã§ã™")

        # å“è³ªè©•ä¾¡ãƒ™ãƒ¼ã‚¹ã®æ¨å¥¨äº‹é …
        if quality_assessment.get("naturalness", 1.0) < 0.8:
            recommendations.append("ğŸ“ è‡ªç„¶ãªæ–‡ç« è¡¨ç¾ã¸ã®èª¿æ•´ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")

        if not recommendations:
            recommendations.append("âœ… æœ€é©åŒ–ã¯è‰¯å¥½ã«å®Œäº†ã—ã¾ã—ãŸ")

        return recommendations
