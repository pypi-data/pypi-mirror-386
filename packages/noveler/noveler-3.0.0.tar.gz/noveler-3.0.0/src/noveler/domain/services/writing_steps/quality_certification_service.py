"""STEP 16: DOD & KPIçµ±åˆå“è³ªèªå®šã‚µãƒ¼ãƒ“ã‚¹

A38åŸ·ç­†ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚¬ã‚¤ãƒ‰ã®STEP16ã€ŒDOD & KPIçµ±åˆå“è³ªèªå®šã€ã‚’å®Ÿè£…ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã€‚
Definition of Done (DOD) ã¨Key Performance Indicators (KPI) ã‚’ç”¨ã„ã¦
ç·åˆçš„ãªå“è³ªèªå®šã‚’è¡Œã„ã€å…¬é–‹å¯èƒ½å“è³ªã®ç¢ºä¿ã‚’ä¿è¨¼ã—ã¾ã™ã€‚
"""

from abc import abstractmethod
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Any

from noveler.application.services.stepwise_execution_service import BaseWritingStep
from noveler.domain.models.project_model import ProjectModel
from noveler.domain.services.configuration_manager_service import ConfigurationManagerService


class QualityLevel(Enum):
    """å“è³ªãƒ¬ãƒ™ãƒ«"""
    DRAFT = "draft"  # è‰ç¨¿ãƒ¬ãƒ™ãƒ«
    REVIEW_READY = "review_ready"  # ãƒ¬ãƒ“ãƒ¥ãƒ¼å¯èƒ½ãƒ¬ãƒ™ãƒ«
    PUBLICATION_READY = "publication_ready"  # å…¬é–‹å¯èƒ½ãƒ¬ãƒ™ãƒ«
    PREMIUM = "premium"  # ãƒ—ãƒ¬ãƒŸã‚¢ãƒ å“è³ª


class DODCategory(Enum):
    """DODã‚«ãƒ†ã‚´ãƒªãƒ¼"""
    CONTENT_COMPLETENESS = "content_completeness"  # å†…å®¹å®Œæˆåº¦
    TECHNICAL_QUALITY = "technical_quality"  # æŠ€è¡“å“è³ª
    NARRATIVE_QUALITY = "narrative_quality"  # ç‰©èªå“è³ª
    READABILITY = "readability"  # å¯èª­æ€§
    CONSISTENCY = "consistency"  # ä¸€è²«æ€§
    ENGAGEMENT = "engagement"  # ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ


class KPIType(Enum):
    """KPIã‚¿ã‚¤ãƒ—"""
    QUANTITATIVE = "quantitative"  # å®šé‡çš„KPI
    QUALITATIVE = "qualitative"  # å®šæ€§çš„KPI
    READER_EXPERIENCE = "reader_experience"  # èª­è€…ä½“é¨“KPI
    TECHNICAL_METRICS = "technical_metrics"  # æŠ€è¡“çš„ãƒ¡ãƒˆãƒªã‚¯ã‚¹


class CertificationStatus(Enum):
    """èªå®šã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹"""
    PASSED = "passed"  # åˆæ ¼
    FAILED = "failed"  # ä¸åˆæ ¼
    CONDITIONAL = "conditional"  # æ¡ä»¶ä»˜ãåˆæ ¼
    REQUIRES_REVISION = "requires_revision"  # ä¿®æ­£è¦


@dataclass
class DODCriterion:
    """DODåŸºæº–"""
    criterion_id: str
    category: DODCategory
    name: str
    description: str
    measurement_method: str
    pass_threshold: float  # åˆæ ¼é–¾å€¤ (0-1)
    weight: float  # é‡ã¿
    mandatory: bool  # å¿…é ˆã‹ã©ã†ã‹
    validation_rules: list[str]


@dataclass
class KPIMetric:
    """KPIãƒ¡ãƒˆãƒªãƒƒã‚¯"""
    metric_id: str
    kpi_type: KPIType
    name: str
    description: str
    target_value: float | str
    current_value: float | str
    unit: str
    measurement_method: str
    benchmark: float | None = None  # ãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯
    trend: str | None = None  # ãƒˆãƒ¬ãƒ³ãƒ‰


@dataclass
class QualityCheckResult:
    """å“è³ªãƒã‚§ãƒƒã‚¯çµæœ"""
    check_id: str
    criterion: DODCriterion
    status: CertificationStatus
    score: float  # ã‚¹ã‚³ã‚¢ (0-1)
    measured_value: float | str
    pass_threshold: float
    details: str
    evidence: list[str]  # æ ¹æ‹ 
    recommendations: list[str]  # æ¨å¥¨äº‹é …


@dataclass
class KPIAssessment:
    """KPIè©•ä¾¡"""
    assessment_id: str
    metric: KPIMetric
    achievement_rate: float  # é”æˆç‡ (0-1)
    performance_grade: str  # A, B, C, D
    analysis: str
    improvement_suggestions: list[str]


@dataclass
class QualityGate:
    """å“è³ªã‚²ãƒ¼ãƒˆ"""
    gate_id: str
    gate_name: str
    quality_level: QualityLevel
    required_criteria: list[str]  # å¿…è¦ãªDODåŸºæº–ID
    required_kpis: list[str]  # å¿…è¦ãªKPI ID
    pass_rate_threshold: float  # åˆæ ¼ç‡é–¾å€¤
    mandatory_pass_criteria: list[str]  # å¿…é ˆåˆæ ¼åŸºæº–


@dataclass
class CertificationReport:
    """èªå®šãƒ¬ãƒãƒ¼ãƒˆ"""
    report_id: str
    episode_number: int
    certification_timestamp: datetime
    target_quality_level: QualityLevel
    overall_status: CertificationStatus
    overall_score: float  # ç·åˆã‚¹ã‚³ã‚¢ (0-1)

    dod_results: list[QualityCheckResult]
    kpi_assessments: list[KPIAssessment]
    quality_gates: list[QualityGate]

    passed_gates: list[str]
    failed_gates: list[str]
    conditional_gates: list[str]

    strengths: list[str]  # å¼·ã¿
    weaknesses: list[str]  # å¼±ç‚¹
    critical_issues: list[str]  # é‡è¦ãªå•é¡Œ
    improvement_priorities: list[str]  # æ”¹å–„å„ªå…ˆåº¦

    certification_summary: str
    next_steps: list[str]
    certification_metadata: dict[str, Any]


@dataclass
class QualityCertificationConfig:
    """å“è³ªèªå®šè¨­å®š"""
    target_quality_level: QualityLevel = QualityLevel.PUBLICATION_READY
    strict_certification: bool = True  # å³æ ¼èªå®š
    enable_all_dod_categories: bool = True
    enable_all_kpi_types: bool = True

    # DODé‡ã¿è¨­å®š
    dod_category_weights: dict[DODCategory, float] = None

    # KPIé‡ã¿è¨­å®š
    kpi_type_weights: dict[KPIType, float] = None

    # å“è³ªã‚²ãƒ¼ãƒˆè¨­å®š
    enable_quality_gates: bool = True
    gate_pass_threshold: float = 0.8

    # èªå®šåŸºæº–
    overall_pass_threshold: float = 0.75
    mandatory_criteria_pass_rate: float = 1.0

    # ãƒ¬ãƒãƒ¼ãƒˆè¨­å®š
    include_detailed_analysis: bool = True
    include_improvement_roadmap: bool = True


class QualityCertificationService(BaseWritingStep):
    """STEP 16: DOD & KPIçµ±åˆå“è³ªèªå®šã‚µãƒ¼ãƒ“ã‚¹

    Definition of Done (DOD) ã¨Key Performance Indicators (KPI) ã‚’ç”¨ã„ã¦
    ç·åˆçš„ãªå“è³ªèªå®šã‚’è¡Œã„ã€å…¬é–‹å¯èƒ½å“è³ªã®ç¢ºä¿ã‚’ä¿è¨¼ã™ã‚‹ã‚µãƒ¼ãƒ“ã‚¹ã€‚
    A38ã‚¬ã‚¤ãƒ‰ã®STEP16ã€ŒDOD & KPIçµ±åˆå“è³ªèªå®šã€ã‚’å®Ÿè£…ã€‚
    """

    def __init__(
        self,
        config_manager: ConfigurationManagerService | None = None,
        path_service: Any | None = None,
        file_system_service: Any | None = None
    ) -> None:
        super().__init__()
        self._config_manager = config_manager
        self._path_service = path_service
        self._file_system = file_system_service

        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé‡ã¿è¨­å®š
        self._certification_config = QualityCertificationConfig()
        if self._certification_config.dod_category_weights is None:
            self._certification_config.dod_category_weights = {
                DODCategory.CONTENT_COMPLETENESS: 0.25,
                DODCategory.NARRATIVE_QUALITY: 0.25,
                DODCategory.TECHNICAL_QUALITY: 0.20,
                DODCategory.READABILITY: 0.15,
                DODCategory.CONSISTENCY: 0.10,
                DODCategory.ENGAGEMENT: 0.05
            }

        if self._certification_config.kpi_type_weights is None:
            self._certification_config.kpi_type_weights = {
                KPIType.READER_EXPERIENCE: 0.40,
                KPIType.TECHNICAL_METRICS: 0.30,
                KPIType.QUANTITATIVE: 0.20,
                KPIType.QUALITATIVE: 0.10
            }

        # DODåŸºæº–ã¨KPIãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®åˆæœŸåŒ–
        self._dod_criteria = self._initialize_dod_criteria()
        self._kpi_metrics = self._initialize_kpi_metrics()
        self._quality_gates = self._initialize_quality_gates()

    @abstractmethod
    def get_step_name(self) -> str:
        """ã‚¹ãƒ†ãƒƒãƒ—åã‚’å–å¾—"""
        return "DOD & KPIçµ±åˆå“è³ªèªå®š"

    @abstractmethod
    def get_step_description(self) -> str:
        """ã‚¹ãƒ†ãƒƒãƒ—ã®èª¬æ˜ã‚’å–å¾—"""
        return "DODã¨KPIã‚’ç”¨ã„ã¦ç·åˆçš„ãªå“è³ªèªå®šã‚’è¡Œã„ã€å…¬é–‹å¯èƒ½å“è³ªã®ç¢ºä¿ã‚’ä¿è¨¼ã—ã¾ã™"

    @abstractmethod
    def execute_step(self, context: dict[str, Any]) -> dict[str, Any]:
        """STEP 16: DOD & KPIçµ±åˆå“è³ªèªå®šã®å®Ÿè¡Œ

        Args:
            context: å®Ÿè¡Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ

        Returns:
            å“è³ªèªå®šçµæœã‚’å«ã‚€ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        """
        try:
            episode_number = context.get("episode_number")
            project = context.get("project")

            if not episode_number or not project:
                msg = "episode_numberã¾ãŸã¯projectãŒæŒ‡å®šã•ã‚Œã¦ã„ã¾ã›ã‚“"
                raise ValueError(msg)

            # DOD & KPIçµ±åˆå“è³ªèªå®šã®å®Ÿè¡Œ
            certification_report = self._execute_quality_certification(
                episode_number=episode_number,
                project=project,
                context=context
            )

            # çµæœã‚’ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«è¿½åŠ 
            context["quality_certification"] = certification_report
            context["certification_status"] = certification_report.overall_status
            context["quality_score"] = certification_report.overall_score
            context["quality_certification_completed"] = True

            # å“è³ªã‚²ãƒ¼ãƒˆé€šéçŠ¶æ³ã‚’è¨­å®š
            context["passed_quality_gates"] = certification_report.passed_gates
            context["failed_quality_gates"] = certification_report.failed_gates

            return context

        except Exception as e:
            context["quality_certification_error"] = str(e)
            raise

    def _execute_quality_certification(
        self,
        episode_number: int,
        project: ProjectModel,
        context: dict[str, Any]
    ) -> CertificationReport:
        """å“è³ªèªå®šã®å®Ÿè¡Œ"""

        # æœ€çµ‚åŸç¨¿ã®å–å¾—
        final_manuscript = self._get_final_manuscript(context)

        # DODåŸºæº–ã«ã‚ˆã‚‹å“è³ªãƒã‚§ãƒƒã‚¯
        dod_results = self._perform_dod_checks(final_manuscript, context)

        # KPIãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ã‚ˆã‚‹è©•ä¾¡
        kpi_assessments = self._perform_kpi_assessments(final_manuscript, context)

        # å“è³ªã‚²ãƒ¼ãƒˆã®è©•ä¾¡
        gate_evaluations = self._evaluate_quality_gates(dod_results, kpi_assessments)

        # ç·åˆè©•ä¾¡ã®è¨ˆç®—
        overall_score = self._calculate_overall_score(dod_results, kpi_assessments)
        overall_status = self._determine_overall_status(
            overall_score, gate_evaluations, dod_results
        )

        # å¼·ã¿ãƒ»å¼±ç‚¹ã®åˆ†æ
        strengths, weaknesses = self._analyze_strengths_and_weaknesses(
            dod_results, kpi_assessments
        )

        # é‡è¦ãªå•é¡Œã¨æ”¹å–„å„ªå…ˆåº¦ã®ç‰¹å®š
        critical_issues = self._identify_critical_issues(dod_results, kpi_assessments)
        improvement_priorities = self._determine_improvement_priorities(
            dod_results, kpi_assessments, critical_issues
        )

        # ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        return self._generate_certification_report(
            episode_number=episode_number,
            overall_status=overall_status,
            overall_score=overall_score,
            dod_results=dod_results,
            kpi_assessments=kpi_assessments,
            gate_evaluations=gate_evaluations,
            strengths=strengths,
            weaknesses=weaknesses,
            critical_issues=critical_issues,
            improvement_priorities=improvement_priorities
        )

    def _get_final_manuscript(self, context: dict[str, Any]) -> str:
        """æœ€çµ‚åŸç¨¿ã®å–å¾—"""

        # å¯èª­æ€§æœ€é©åŒ–å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆãŒæœ€å„ªå…ˆ
        if "optimized_readable_text" in context:
            return context["optimized_readable_text"]

        # æ–‡å­—æ•°æœ€é©åŒ–å¾Œã®ãƒ†ã‚­ã‚¹ãƒˆ
        if "optimized_text" in context:
            return context["optimized_text"]

        # ãã®ä»–ã®ã‚½ãƒ¼ã‚¹
        text_sources = [
            "final_manuscript",
            "manuscript_text",
            "generated_manuscript"
        ]

        for source in text_sources:
            text = context.get(source)
            if text and isinstance(text, str):
                return text

        return ""

    def _perform_dod_checks(
        self,
        manuscript: str,
        context: dict[str, Any]
    ) -> list[QualityCheckResult]:
        """DODåŸºæº–ã«ã‚ˆã‚‹å“è³ªãƒã‚§ãƒƒã‚¯"""

        results = []

        for criterion in self._dod_criteria:
            if not self._is_criterion_enabled(criterion):
                continue

            # åŸºæº–ã«å¯¾ã™ã‚‹æ¸¬å®šå®Ÿè¡Œ
            measured_value, score = self._measure_criterion(
                criterion, manuscript, context
            )

            # åˆæ ¼åˆ¤å®š
            status = CertificationStatus.PASSED if score >= criterion.pass_threshold else CertificationStatus.FAILED

            # è©³ç´°åˆ†æ
            details = self._analyze_criterion_details(criterion, measured_value, score)
            evidence = self._collect_criterion_evidence(criterion, manuscript, context)
            recommendations = self._generate_criterion_recommendations(criterion, score)

            result = QualityCheckResult(
                check_id=f"dod_{criterion.criterion_id}",
                criterion=criterion,
                status=status,
                score=score,
                measured_value=measured_value,
                pass_threshold=criterion.pass_threshold,
                details=details,
                evidence=evidence,
                recommendations=recommendations
            )

            results.append(result)

        return results

    def _perform_kpi_assessments(
        self,
        manuscript: str,
        context: dict[str, Any]
    ) -> list[KPIAssessment]:
        """KPIãƒ¡ãƒˆãƒªã‚¯ã‚¹ã«ã‚ˆã‚‹è©•ä¾¡"""

        assessments = []

        for metric in self._kpi_metrics:
            if not self._is_kpi_enabled(metric):
                continue

            # KPIã®æ¸¬å®š
            current_value = self._measure_kpi_metric(metric, manuscript, context)

            # é”æˆç‡ã®è¨ˆç®—
            achievement_rate = self._calculate_achievement_rate(metric, current_value)

            # æˆç¸¾è©•ä¾¡
            performance_grade = self._calculate_performance_grade(achievement_rate)

            # åˆ†æã¨æ”¹å–„ææ¡ˆ
            analysis = self._analyze_kpi_performance(metric, current_value, achievement_rate)
            improvement_suggestions = self._generate_kpi_improvements(metric, achievement_rate)

            # KPIãƒ¡ãƒˆãƒªãƒƒã‚¯ã®ç¾åœ¨å€¤ã‚’æ›´æ–°
            updated_metric = metric
            updated_metric.current_value = current_value

            assessment = KPIAssessment(
                assessment_id=f"kpi_{metric.metric_id}",
                metric=updated_metric,
                achievement_rate=achievement_rate,
                performance_grade=performance_grade,
                analysis=analysis,
                improvement_suggestions=improvement_suggestions
            )

            assessments.append(assessment)

        return assessments

    def _evaluate_quality_gates(
        self,
        dod_results: list[QualityCheckResult],
        kpi_assessments: list[KPIAssessment]
    ) -> dict[str, Any]:
        """å“è³ªã‚²ãƒ¼ãƒˆã®è©•ä¾¡"""

        gate_evaluations = {
            "passed_gates": [],
            "failed_gates": [],
            "conditional_gates": []
        }

        for gate in self._quality_gates:
            gate_status = self._evaluate_single_quality_gate(
                gate, dod_results, kpi_assessments
            )

            if gate_status == CertificationStatus.PASSED:
                gate_evaluations["passed_gates"].append(gate.gate_id)
            elif gate_status == CertificationStatus.CONDITIONAL:
                gate_evaluations["conditional_gates"].append(gate.gate_id)
            else:
                gate_evaluations["failed_gates"].append(gate.gate_id)

        return gate_evaluations

    def _calculate_overall_score(
        self,
        dod_results: list[QualityCheckResult],
        kpi_assessments: list[KPIAssessment]
    ) -> float:
        """ç·åˆã‚¹ã‚³ã‚¢ã®è¨ˆç®—"""

        # DODã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        dod_score = 0.0
        dod_total_weight = 0.0

        for result in dod_results:
            category_weight = self._certification_config.dod_category_weights.get(
                result.criterion.category, 0.1
            )
            criterion_weight = result.criterion.weight
            total_weight = category_weight * criterion_weight

            dod_score += result.score * total_weight
            dod_total_weight += total_weight

        normalized_dod_score = dod_score / dod_total_weight if dod_total_weight > 0 else 0.0

        # KPIã‚¹ã‚³ã‚¢ã®è¨ˆç®—
        kpi_score = 0.0
        kpi_total_weight = 0.0

        for assessment in kpi_assessments:
            type_weight = self._certification_config.kpi_type_weights.get(
                assessment.metric.kpi_type, 0.1
            )

            kpi_score += assessment.achievement_rate * type_weight
            kpi_total_weight += type_weight

        normalized_kpi_score = kpi_score / kpi_total_weight if kpi_total_weight > 0 else 0.0

        # DODã¨KPIã®é‡ã¿ä»˜ã‘çµ±åˆ (DOD 70%, KPI 30%)
        overall_score = normalized_dod_score * 0.7 + normalized_kpi_score * 0.3

        return min(1.0, max(0.0, overall_score))

    def _determine_overall_status(
        self,
        overall_score: float,
        gate_evaluations: dict[str, Any],
        dod_results: list[QualityCheckResult]
    ) -> CertificationStatus:
        """ç·åˆã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®åˆ¤å®š"""

        # å¿…é ˆåŸºæº–ã®ç¢ºèª
        mandatory_failed = any(
            result.status == CertificationStatus.FAILED and result.criterion.mandatory
            for result in dod_results
        )

        if mandatory_failed:
            return CertificationStatus.FAILED

        # å“è³ªã‚²ãƒ¼ãƒˆã®ç¢ºèª
        failed_gates = gate_evaluations.get("failed_gates", [])
        if failed_gates:
            return CertificationStatus.REQUIRES_REVISION

        # ç·åˆã‚¹ã‚³ã‚¢ã«ã‚ˆã‚‹åˆ¤å®š
        if overall_score >= self._certification_config.overall_pass_threshold:
            conditional_gates = gate_evaluations.get("conditional_gates", [])
            if conditional_gates:
                return CertificationStatus.CONDITIONAL
            return CertificationStatus.PASSED
        return CertificationStatus.FAILED

    def _analyze_strengths_and_weaknesses(
        self,
        dod_results: list[QualityCheckResult],
        kpi_assessments: list[KPIAssessment]
    ) -> tuple[list[str], list[str]]:
        """å¼·ã¿ãƒ»å¼±ç‚¹ã®åˆ†æ"""

        strengths = []
        weaknesses = []

        # DODåŸºæº–ã‹ã‚‰ã®å¼·ã¿ãƒ»å¼±ç‚¹
        for result in dod_results:
            if result.score >= 0.8:
                strengths.append(
                    f"{result.criterion.name}: å„ªç§€ ({result.score:.2f})"
                )
            elif result.score < 0.6:
                weaknesses.append(
                    f"{result.criterion.name}: è¦æ”¹å–„ ({result.score:.2f})"
                )

        # KPIè©•ä¾¡ã‹ã‚‰ã®å¼·ã¿ãƒ»å¼±ç‚¹
        for assessment in kpi_assessments:
            if assessment.performance_grade in ["A", "B"]:
                strengths.append(
                    f"{assessment.metric.name}: {assessment.performance_grade}è©•ä¾¡"
                )
            elif assessment.performance_grade in ["D"]:
                weaknesses.append(
                    f"{assessment.metric.name}: {assessment.performance_grade}è©•ä¾¡"
                )

        return strengths, weaknesses

    def _identify_critical_issues(
        self,
        dod_results: list[QualityCheckResult],
        kpi_assessments: list[KPIAssessment]
    ) -> list[str]:
        """é‡è¦ãªå•é¡Œã®ç‰¹å®š"""

        critical_issues = []

        # å¿…é ˆåŸºæº–ã®ä¸åˆæ ¼
        for result in dod_results:
            if result.criterion.mandatory and result.status == CertificationStatus.FAILED:
                critical_issues.append(
                    f"å¿…é ˆåŸºæº–ä¸åˆæ ¼: {result.criterion.name}"
                )

        # è‘—ã—ãä½ã„ã‚¹ã‚³ã‚¢
        for result in dod_results:
            if result.score < 0.3:
                critical_issues.append(
                    f"æ·±åˆ»ãªå“è³ªå•é¡Œ: {result.criterion.name} ({result.score:.2f})"
                )

        # KPIã®å¤§å¹…æœªé”
        for assessment in kpi_assessments:
            if assessment.achievement_rate < 0.5:
                critical_issues.append(
                    f"KPIå¤§å¹…æœªé”: {assessment.metric.name} ({assessment.achievement_rate:.2f})"
                )

        return critical_issues

    def _determine_improvement_priorities(
        self,
        dod_results: list[QualityCheckResult],
        kpi_assessments: list[KPIAssessment],
        critical_issues: list[str]
    ) -> list[str]:
        """æ”¹å–„å„ªå…ˆåº¦ã®æ±ºå®š"""

        priorities = []

        # 1. å¿…é ˆåŸºæº–ã®ä¿®æ­£
        mandatory_fixes = [
            result.criterion.name
            for result in dod_results
            if result.criterion.mandatory and result.status == CertificationStatus.FAILED
        ]

        if mandatory_fixes:
            priorities.append(f"ğŸ”´ æœ€å„ªå…ˆ: å¿…é ˆåŸºæº–ä¿®æ­£ ({', '.join(mandatory_fixes)})")

        # 2. é‡å¤§ãªå“è³ªå•é¡Œ
        major_issues = [
            result.criterion.name
            for result in dod_results
            if result.score < 0.5 and not result.criterion.mandatory
        ]

        if major_issues:
            priorities.append(f"ğŸŸ¡ é«˜å„ªå…ˆ: ä¸»è¦å“è³ªå•é¡Œ ({', '.join(major_issues)})")

        # 3. KPIæ”¹å–„
        kpi_improvements = [
            assessment.metric.name
            for assessment in kpi_assessments
            if assessment.achievement_rate < 0.7
        ]

        if kpi_improvements:
            priorities.append(f"ğŸŸ¢ ä¸­å„ªå…ˆ: KPIæ”¹å–„ ({', '.join(kpi_improvements)})")

        return priorities

    def _generate_certification_report(
        self,
        episode_number: int,
        overall_status: CertificationStatus,
        overall_score: float,
        dod_results: list[QualityCheckResult],
        kpi_assessments: list[KPIAssessment],
        gate_evaluations: dict[str, Any],
        strengths: list[str],
        weaknesses: list[str],
        critical_issues: list[str],
        improvement_priorities: list[str]
    ) -> CertificationReport:
        """èªå®šãƒ¬ãƒãƒ¼ãƒˆã®ç”Ÿæˆ"""

        # èªå®šã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ
        certification_summary = self._generate_certification_summary(
            overall_status, overall_score, dod_results, kpi_assessments
        )

        # æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ç”Ÿæˆ
        next_steps = self._generate_next_steps(
            overall_status, critical_issues, improvement_priorities
        )

        return CertificationReport(
            report_id=f"cert_{episode_number}_{datetime.now(tz=datetime.timezone.utc).strftime('%Y%m%d_%H%M%S')}",
            episode_number=episode_number,
            certification_timestamp=datetime.now(tz=datetime.timezone.utc),
            target_quality_level=self._certification_config.target_quality_level,
            overall_status=overall_status,
            overall_score=overall_score,
            dod_results=dod_results,
            kpi_assessments=kpi_assessments,
            quality_gates=self._quality_gates,
            passed_gates=gate_evaluations.get("passed_gates", []),
            failed_gates=gate_evaluations.get("failed_gates", []),
            conditional_gates=gate_evaluations.get("conditional_gates", []),
            strengths=strengths,
            weaknesses=weaknesses,
            critical_issues=critical_issues,
            improvement_priorities=improvement_priorities,
            certification_summary=certification_summary,
            next_steps=next_steps,
            certification_metadata={
                "config": self._certification_config.__dict__,
                "certification_timestamp": datetime.now(tz=datetime.timezone.utc),
                "total_dod_criteria": len(dod_results),
                "total_kpi_metrics": len(kpi_assessments),
                "passed_dod_criteria": len([r for r in dod_results if r.status == CertificationStatus.PASSED]),
                "passed_kpi_metrics": len([a for a in kpi_assessments if a.performance_grade in ["A", "B"]])
            }
        )

    # åˆæœŸåŒ–ãƒ¡ã‚½ãƒƒãƒ‰ã®å®Ÿè£…

    def _initialize_dod_criteria(self) -> list[DODCriterion]:
        """DODåŸºæº–ã®åˆæœŸåŒ–"""
        criteria = []

        # å†…å®¹å®Œæˆåº¦åŸºæº–
        criteria.append(DODCriterion(
            criterion_id="content_word_count",
            category=DODCategory.CONTENT_COMPLETENESS,
            name="æ–‡å­—æ•°è¦ä»¶",
            description="ç›®æ¨™æ–‡å­—æ•°ã®é”æˆåº¦",
            measurement_method="æ–‡å­—æ•°ã‚«ã‚¦ãƒ³ãƒˆ",
            pass_threshold=0.9,  # 90%ä»¥ä¸Š
            weight=1.0,
            mandatory=True,
            validation_rules=["æœ€ä½3000æ–‡å­—", "æœ€å¤§5000æ–‡å­—"]
        ))

        criteria.append(DODCriterion(
            criterion_id="plot_completeness",
            category=DODCategory.CONTENT_COMPLETENESS,
            name="ãƒ—ãƒ­ãƒƒãƒˆå®Œæˆåº¦",
            description="èµ·æ‰¿è»¢çµã®å®Œæˆåº¦",
            measurement_method="æ§‹é€ åˆ†æ",
            pass_threshold=0.8,
            weight=1.0,
            mandatory=True,
            validation_rules=["å°å…¥éƒ¨å­˜åœ¨", "å±•é–‹éƒ¨å­˜åœ¨", "ã‚¯ãƒ©ã‚¤ãƒãƒƒã‚¯ã‚¹å­˜åœ¨", "è§£æ±ºéƒ¨å­˜åœ¨"]
        ))

        # æŠ€è¡“å“è³ªåŸºæº–
        criteria.append(DODCriterion(
            criterion_id="grammar_accuracy",
            category=DODCategory.TECHNICAL_QUALITY,
            name="æ–‡æ³•æ­£ç¢ºæ€§",
            description="æ–‡æ³•ã‚¨ãƒ©ãƒ¼ã®å°‘ãªã•",
            measurement_method="æ–‡æ³•ãƒã‚§ãƒƒã‚¯",
            pass_threshold=0.95,
            weight=0.8,
            mandatory=False,
            validation_rules=["èª¤å­—è„±å­—æœ€å°åŒ–", "æ–‡æ³•ã‚¨ãƒ©ãƒ¼æœ€å°åŒ–"]
        ))

        # ç‰©èªå“è³ªåŸºæº–
        criteria.append(DODCriterion(
            criterion_id="character_consistency",
            category=DODCategory.NARRATIVE_QUALITY,
            name="ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ä¸€è²«æ€§",
            description="ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ã®ä¸€è²«æ€§",
            measurement_method="ä¸€è²«æ€§åˆ†æ",
            pass_threshold=0.8,
            weight=0.9,
            mandatory=False,
            validation_rules=["ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼è¨­å®šä¸€è²«æ€§", "å¯¾è©±ã®è‡ªç„¶æ€§"]
        ))

        # å¯èª­æ€§åŸºæº–
        criteria.append(DODCriterion(
            criterion_id="readability_score",
            category=DODCategory.READABILITY,
            name="å¯èª­æ€§ã‚¹ã‚³ã‚¢",
            description="èª­ã¿ã‚„ã™ã•ã®ç·åˆè©•ä¾¡",
            measurement_method="å¯èª­æ€§åˆ†æ",
            pass_threshold=0.7,
            weight=0.8,
            mandatory=False,
            validation_rules=["é©åˆ‡ãªæ–‡é•·", "èªå½™ã®é©åˆ‡æ€§"]
        ))

        return criteria

    def _initialize_kpi_metrics(self) -> list[KPIMetric]:
        """KPIãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®åˆæœŸåŒ–"""
        metrics = []

        # èª­è€…ä½“é¨“KPI
        metrics.append(KPIMetric(
            metric_id="reader_engagement",
            kpi_type=KPIType.READER_EXPERIENCE,
            name="èª­è€…ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆ",
            description="èª­è€…ã®é–¢ä¸åº¦",
            target_value=0.8,
            current_value=0.0,
            unit="ã‚¹ã‚³ã‚¢",
            measurement_method="ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆåˆ†æ"
        ))

        # æŠ€è¡“ãƒ¡ãƒˆãƒªã‚¯ã‚¹KPI
        metrics.append(KPIMetric(
            metric_id="text_quality_index",
            kpi_type=KPIType.TECHNICAL_METRICS,
            name="æ–‡ç« å“è³ªæŒ‡æ•°",
            description="æŠ€è¡“çš„ãªæ–‡ç« å“è³ª",
            target_value=0.85,
            current_value=0.0,
            unit="æŒ‡æ•°",
            measurement_method="å¤šæ¬¡å…ƒå“è³ªåˆ†æ"
        ))

        # å®šé‡KPI
        metrics.append(KPIMetric(
            metric_id="story_pacing",
            kpi_type=KPIType.QUANTITATIVE,
            name="ç‰©èªãƒšãƒ¼ã‚¹",
            description="ç‰©èªå±•é–‹ã®ãƒšãƒ¼ã‚¹",
            target_value=0.75,
            current_value=0.0,
            unit="ãƒšãƒ¼ã‚¹ã‚¹ã‚³ã‚¢",
            measurement_method="ãƒšãƒ¼ã‚¹åˆ†æ"
        ))

        return metrics

    def _initialize_quality_gates(self) -> list[QualityGate]:
        """å“è³ªã‚²ãƒ¼ãƒˆã®åˆæœŸåŒ–"""
        gates = []

        # åŸºæœ¬å“è³ªã‚²ãƒ¼ãƒˆ
        gates.append(QualityGate(
            gate_id="basic_quality",
            gate_name="åŸºæœ¬å“è³ªã‚²ãƒ¼ãƒˆ",
            quality_level=QualityLevel.REVIEW_READY,
            required_criteria=["content_word_count", "grammar_accuracy"],
            required_kpis=["text_quality_index"],
            pass_rate_threshold=0.8,
            mandatory_pass_criteria=["content_word_count"]
        ))

        # å…¬é–‹å“è³ªã‚²ãƒ¼ãƒˆ
        gates.append(QualityGate(
            gate_id="publication_quality",
            gate_name="å…¬é–‹å“è³ªã‚²ãƒ¼ãƒˆ",
            quality_level=QualityLevel.PUBLICATION_READY,
            required_criteria=["plot_completeness", "character_consistency", "readability_score"],
            required_kpis=["reader_engagement", "story_pacing"],
            pass_rate_threshold=0.75,
            mandatory_pass_criteria=["plot_completeness"]
        ))

        return gates

    # æ¸¬å®šãƒ»åˆ†æãƒ¡ã‚½ãƒƒãƒ‰ã®å®Ÿè£…ï¼ˆã‚¹ã‚¿ãƒ–ï¼‰

    def _is_criterion_enabled(self, criterion: DODCriterion) -> bool:
        """åŸºæº–ãŒæœ‰åŠ¹ã‹ã©ã†ã‹"""
        return True  # ç°¡æ˜“å®Ÿè£…

    def _measure_criterion(
        self,
        criterion: DODCriterion,
        manuscript: str,
        context: dict[str, Any]
    ) -> tuple[float | str, float]:
        """åŸºæº–ã®æ¸¬å®š"""

        if criterion.criterion_id == "content_word_count":
            word_count = len(manuscript)
            target_count = context.get("target_word_count", 4000)
            score = min(word_count / target_count, 1.0) if target_count > 0 else 0.0
            return word_count, score

        if criterion.criterion_id == "plot_completeness":
            # ç°¡æ˜“ãƒ—ãƒ­ãƒƒãƒˆå®Œæˆåº¦è©•ä¾¡
            has_intro = "ã¯ã˜ã‚" in manuscript or len(manuscript) > 100
            has_development = len(manuscript) > 1000
            has_climax = "ã‚¯ãƒ©ã‚¤ãƒãƒƒã‚¯ã‚¹" in manuscript or "çµæœ«" in manuscript
            has_resolution = manuscript.endswith(("ã€‚", "ï¼"))

            completeness = (has_intro + has_development + has_climax + has_resolution) / 4.0
            return f"{completeness:.2%}", completeness

        if criterion.criterion_id == "grammar_accuracy":
            # ç°¡æ˜“æ–‡æ³•æ­£ç¢ºæ€§è©•ä¾¡
            error_indicators = ["ã€‚ã€‚", "ï¼ï¼", "ï¼Ÿï¼Ÿ"]
            error_count = sum(manuscript.count(indicator) for indicator in error_indicators)
            accuracy = max(0.0, 1.0 - error_count / 100.0)
            return f"{accuracy:.2%}", accuracy

        if criterion.criterion_id == "character_consistency":
            # ã‚­ãƒ£ãƒ©ã‚¯ã‚¿ãƒ¼ä¸€è²«æ€§ã®ç°¡æ˜“è©•ä¾¡
            consistency_score = 0.8  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
            return f"{consistency_score:.2%}", consistency_score

        if criterion.criterion_id == "readability_score":
            # å¯èª­æ€§ã®ç°¡æ˜“è©•ä¾¡
            readability = context.get("readability_optimization", {})
            score = readability.get("overall_readability_score", 0.7) if isinstance(readability, dict) else 0.7
            return f"{score:.2f}", score

        return "æœªæ¸¬å®š", 0.5

    def _is_kpi_enabled(self, metric: KPIMetric) -> bool:
        """KPIãŒæœ‰åŠ¹ã‹ã©ã†ã‹"""
        return True  # ç°¡æ˜“å®Ÿè£…

    def _measure_kpi_metric(
        self,
        metric: KPIMetric,
        manuscript: str,
        context: dict[str, Any]
    ) -> float | str:
        """KPIãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®æ¸¬å®š"""

        if metric.metric_id == "reader_engagement":
            # èª­è€…ã‚¨ãƒ³ã‚²ãƒ¼ã‚¸ãƒ¡ãƒ³ãƒˆã®ç°¡æ˜“è©•ä¾¡
            engagement_factors = []

            # å¯¾è©±ã®å‰²åˆ
            dialogue_ratio = manuscript.count("ã€Œ") / max(len(manuscript) / 100, 1)
            engagement_factors.append(min(dialogue_ratio / 5, 1.0))

            # æ„Ÿæƒ…è¡¨ç¾ã®è±Šã‹ã•
            emotional_words = ["å¬‰ã—ã„", "æ‚²ã—ã„", "é©šã", "æ€’ã‚Š", "æ„›"]
            emotion_count = sum(manuscript.count(word) for word in emotional_words)
            emotion_score = min(emotion_count / 10, 1.0)
            engagement_factors.append(emotion_score)

            return sum(engagement_factors) / len(engagement_factors)

        if metric.metric_id == "text_quality_index":
            # æ–‡ç« å“è³ªæŒ‡æ•°ã®è¨ˆç®—
            quality_factors = []

            # æ–‡ç« ã®å¤šæ§˜æ€§
            sentences = manuscript.split("ã€‚")
            avg_length = sum(len(s) for s in sentences) / len(sentences) if sentences else 0
            variety_score = min(avg_length / 30, 1.0)
            quality_factors.append(variety_score)

            # èªå½™ã®è±Šã‹ã•
            vocab_score = 0.8  # ç°¡æ˜“å®Ÿè£…
            quality_factors.append(vocab_score)

            return sum(quality_factors) / len(quality_factors)

        if metric.metric_id == "story_pacing":
            # ç‰©èªãƒšãƒ¼ã‚¹ã®è©•ä¾¡
            pacing_score = 0.75  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤

            # æ§‹é€ çš„ãƒãƒ©ãƒ³ã‚¹
            structure_data = context.get("story_structure", {})
            if isinstance(structure_data, dict):
                balance_score = structure_data.get("balance_score", 0.75)
                pacing_score = balance_score

            return pacing_score

        return 0.5

    def _calculate_achievement_rate(
        self,
        metric: KPIMetric,
        current_value: float | str
    ) -> float:
        """é”æˆç‡ã®è¨ˆç®—"""

        if isinstance(current_value, str):
            return 0.5  # æ–‡å­—åˆ—å€¤ã®å ´åˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ

        target_value = metric.target_value
        if isinstance(target_value, str) or target_value == 0:
            return 0.5

        return min(current_value / target_value, 1.0)

    def _calculate_performance_grade(self, achievement_rate: float) -> str:
        """æˆç¸¾è©•ä¾¡ã®è¨ˆç®—"""
        if achievement_rate >= 0.9:
            return "A"
        if achievement_rate >= 0.8:
            return "B"
        if achievement_rate >= 0.7:
            return "C"
        return "D"

    def _evaluate_single_quality_gate(
        self,
        gate: QualityGate,
        dod_results: list[QualityCheckResult],
        kpi_assessments: list[KPIAssessment]
    ) -> CertificationStatus:
        """å˜ä¸€å“è³ªã‚²ãƒ¼ãƒˆã®è©•ä¾¡"""

        # å¿…é ˆåˆæ ¼åŸºæº–ã®ãƒã‚§ãƒƒã‚¯
        for mandatory_criterion in gate.mandatory_pass_criteria:
            result = next(
                (r for r in dod_results if r.criterion.criterion_id == mandatory_criterion),
                None
            )
            if result and result.status != CertificationStatus.PASSED:
                return CertificationStatus.FAILED

        # å¿…è¦åŸºæº–ã®åˆæ ¼ç‡ãƒã‚§ãƒƒã‚¯
        required_results = [
            r for r in dod_results if r.criterion.criterion_id in gate.required_criteria
        ]
        passed_count = len([r for r in required_results if r.status == CertificationStatus.PASSED])
        pass_rate = passed_count / len(required_results) if required_results else 0.0

        if pass_rate >= gate.pass_rate_threshold:
            return CertificationStatus.PASSED
        if pass_rate >= gate.pass_rate_threshold * 0.8:  # 80%ä»¥ä¸Šãªã‚‰æ¡ä»¶ä»˜ã
            return CertificationStatus.CONDITIONAL
        return CertificationStatus.FAILED

    # åˆ†æãƒ»ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆãƒ¡ã‚½ãƒƒãƒ‰ã®å®Ÿè£…ï¼ˆã‚¹ã‚¿ãƒ–ï¼‰

    def _analyze_criterion_details(
        self,
        criterion: DODCriterion,
        measured_value: float | str,
        score: float
    ) -> str:
        """åŸºæº–è©³ç´°ã®åˆ†æ"""

        if score >= criterion.pass_threshold:
            return f"åˆæ ¼åŸºæº–ã‚’æº€ãŸã—ã¦ã„ã¾ã™ (ã‚¹ã‚³ã‚¢: {score:.2f})"
        gap = criterion.pass_threshold - score
        return f"åˆæ ¼åŸºæº–ã« {gap:.2f} ä¸è¶³ã—ã¦ã„ã¾ã™"

    def _collect_criterion_evidence(
        self,
        criterion: DODCriterion,
        manuscript: str,
        context: dict[str, Any]
    ) -> list[str]:
        """åŸºæº–æ ¹æ‹ ã®åé›†"""
        return [f"{criterion.name}ã®æ¸¬å®šæ ¹æ‹ "]

    def _generate_criterion_recommendations(
        self,
        criterion: DODCriterion,
        score: float
    ) -> list[str]:
        """åŸºæº–æ¨å¥¨äº‹é …ã®ç”Ÿæˆ"""

        if score >= criterion.pass_threshold:
            return ["ç¾åœ¨ã®å“è³ªã‚’ç¶­æŒã—ã¦ãã ã•ã„"]
        return [f"{criterion.name}ã®æ”¹å–„ãŒå¿…è¦ã§ã™", "å…·ä½“çš„ãªæ”¹å–„æ–½ç­–ã‚’æ¤œè¨ã—ã¦ãã ã•ã„"]

    def _analyze_kpi_performance(
        self,
        metric: KPIMetric,
        current_value: float | str,
        achievement_rate: float
    ) -> str:
        """KPIãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã®åˆ†æ"""

        if achievement_rate >= 0.9:
            return f"å„ªç§€ãªæˆæœã§ã™ã€‚ç›®æ¨™ã‚’{achievement_rate:.1%}é”æˆã—ã¾ã—ãŸã€‚"
        if achievement_rate >= 0.7:
            return f"è‰¯å¥½ãªæˆæœã§ã™ã€‚ç›®æ¨™ã®{achievement_rate:.1%}ã‚’é”æˆã—ã¾ã—ãŸã€‚"
        return f"æ”¹å–„ãŒå¿…è¦ã§ã™ã€‚ç›®æ¨™ã®{achievement_rate:.1%}ã®é”æˆã«ç•™ã¾ã£ã¦ã„ã¾ã™ã€‚"

    def _generate_kpi_improvements(
        self,
        metric: KPIMetric,
        achievement_rate: float
    ) -> list[str]:
        """KPIæ”¹å–„ææ¡ˆã®ç”Ÿæˆ"""

        if achievement_rate >= 0.8:
            return ["ç¾åœ¨ã®ãƒ¬ãƒ™ãƒ«ã‚’ç¶­æŒã—ã¦ãã ã•ã„"]
        if achievement_rate >= 0.6:
            return [f"{metric.name}ã®ã•ã‚‰ãªã‚‹å‘ä¸Šã‚’ç›®æŒ‡ã—ã¦ãã ã•ã„"]
        return [f"{metric.name}ã®æ ¹æœ¬çš„ãªæ”¹å–„ãŒå¿…è¦ã§ã™", "æ”¹å–„è¨ˆç”»ã®ç­–å®šã‚’æ¨å¥¨ã—ã¾ã™"]

    def _generate_certification_summary(
        self,
        overall_status: CertificationStatus,
        overall_score: float,
        dod_results: list[QualityCheckResult],
        kpi_assessments: list[KPIAssessment]
    ) -> str:
        """èªå®šã‚µãƒãƒªãƒ¼ã®ç”Ÿæˆ"""

        status_text = {
            CertificationStatus.PASSED: "åˆæ ¼",
            CertificationStatus.CONDITIONAL: "æ¡ä»¶ä»˜ãåˆæ ¼",
            CertificationStatus.FAILED: "ä¸åˆæ ¼",
            CertificationStatus.REQUIRES_REVISION: "ä¿®æ­£è¦"
        }.get(overall_status, "æœªåˆ¤å®š")

        summary_parts = [
            f"ç·åˆåˆ¤å®š: {status_text}",
            f"ç·åˆã‚¹ã‚³ã‚¢: {overall_score:.2f}",
            f"DODåŸºæº–: {len([r for r in dod_results if r.status == CertificationStatus.PASSED])}/{len(dod_results)}åˆæ ¼",
            f"KPIè©•ä¾¡: å¹³å‡{sum(a.achievement_rate for a in kpi_assessments)/len(kpi_assessments):.1%}é”æˆ"
        ]

        return " | ".join(summary_parts)

    def _generate_next_steps(
        self,
        overall_status: CertificationStatus,
        critical_issues: list[str],
        improvement_priorities: list[str]
    ) -> list[str]:
        """æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã®ç”Ÿæˆ"""

        next_steps = []

        if overall_status == CertificationStatus.PASSED:
            next_steps.append("âœ… å…¬é–‹æº–å‚™ã«é€²ã‚“ã§ãã ã•ã„")
            next_steps.append("ğŸ“‹ æœ€çµ‚ãƒã‚§ãƒƒã‚¯ãƒªã‚¹ãƒˆã®ç¢ºèª")

        elif overall_status == CertificationStatus.CONDITIONAL:
            next_steps.append("ğŸ”¶ æ¡ä»¶ä»˜ãåˆæ ¼ï¼šè»½å¾®ãªä¿®æ­£å¾Œã«å…¬é–‹å¯èƒ½")
            next_steps.extend(improvement_priorities[:2])  # ä¸Šä½2ã¤ã®å„ªå…ˆäº‹é …

        elif overall_status == CertificationStatus.REQUIRES_REVISION:
            next_steps.append("ğŸ”„ ä¿®æ­£ãŒå¿…è¦ï¼šå“è³ªå‘ä¸Šå¾Œã«å†èªå®š")
            next_steps.extend(improvement_priorities)

        else:  # FAILED
            next_steps.append("âŒ å¤§å¹…ãªå“è³ªæ”¹å–„ãŒå¿…è¦")
            if critical_issues:
                next_steps.append("ğŸš¨ é‡è¦å•é¡Œã®è§£æ±ºãŒæœ€å„ªå…ˆ")
                next_steps.extend(critical_issues[:3])  # ä¸Šä½3ã¤ã®é‡è¦å•é¡Œ

        return next_steps
