"""Tools.prompt_validation_tool
Where: Tool validating prompts for quality and constraints.
What: Runs prompt validation logic and reports issues to users.
Why: Ensures generated prompts meet defined quality standards.
"""

import argparse
import json
import os
import statistics
import sys
from pathlib import Path
from typing import Any

from noveler.presentation.shared.shared_utilities import console

"\nãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå“è³ªæ¤œè¨¼ãƒ„ãƒ¼ãƒ«\n\nSPEC-PROMPT-SAVE-001: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä¿å­˜æ©Ÿèƒ½ä»•æ§˜æ›¸æº–æ‹ \nãƒ‡ãƒ—ãƒ­ã‚¤å¾Œã®å“è³ªç›£è¦–ãƒ»æ¤œè¨¼ç”¨ãƒ„ãƒ¼ãƒ«\n"

from noveler.domain.entities.episode_prompt import EpisodePrompt
from noveler.infrastructure.repositories.episode_prompt_repository import EpisodePromptRepository
from noveler.presentation.shared.shared_utilities import get_common_path_service


class PromptValidationReport:
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå“è³ªæ¤œè¨¼ãƒ¬ãƒãƒ¼ãƒˆ"""

    def __init__(self, logger_service: Any | None = None, console_service: Any | None = None) -> None:
        self.total_prompts: int = 0
        self.quality_scores: list[float] = []
        self.file_sizes: list[int] = []
        self.validation_errors: list[str] = []
        self.naming_violations: list[str] = []
        self.high_quality_count: int = 0
        self.logger_service = logger_service
        self.console_service = console_service

    def add_prompt_result(self, prompt: EpisodePrompt, file_path: Path, quality_score: float) -> None:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ¤œè¨¼çµæœè¿½åŠ """
        self.total_prompts += 1
        self.quality_scores.append(quality_score)
        if file_path.exists():
            self.file_sizes.append(file_path.stat().st_size)
        if quality_score >= 0.8:
            self.high_quality_count += 1

    def add_validation_error(self, error: str) -> None:
        """æ¤œè¨¼ã‚¨ãƒ©ãƒ¼è¿½åŠ """
        self.validation_errors.append(error)

    def add_naming_violation(self, violation: str) -> None:
        """å‘½åè¦å‰‡é•åè¿½åŠ """
        self.naming_violations.append(violation)

    def generate_summary(self) -> str:
        """æ¤œè¨¼ã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        if not self.quality_scores:
            return "âŒ æ¤œè¨¼å¯¾è±¡ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ"
        avg_quality = statistics.mean(self.quality_scores)
        avg_file_size = statistics.mean(self.file_sizes) if self.file_sizes else 0
        high_quality_rate = self.high_quality_count / self.total_prompts * 100
        summary = f"\nğŸ” ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå“è³ªæ¤œè¨¼çµæœ\n\nğŸ“Š åŸºæœ¬çµ±è¨ˆ:\n  - ç·ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ•°: {self.total_prompts}\n  - å¹³å‡å“è³ªã‚¹ã‚³ã‚¢: {avg_quality:.3f}\n  - é«˜å“è³ªç‡(â‰¥0.8): {high_quality_rate:.1f}% ({self.high_quality_count}/{self.total_prompts})\n  - å¹³å‡ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚º: {avg_file_size:,.0f} bytes\n\nğŸ¯ å“è³ªåˆ†æ:\n  - æœ€é«˜å“è³ª: {max(self.quality_scores):.3f}\n  - æœ€ä½å“è³ª: {min(self.quality_scores):.3f}\n  - å“è³ªæ¨™æº–åå·®: {statistics.stdev(self.quality_scores):.3f}\n\n{('âœ… å“è³ªç›®æ¨™é”æˆ' if avg_quality >= 0.8 else 'âš ï¸ å“è³ªæ”¹å–„ãŒå¿…è¦')}\n"
        if self.validation_errors:
            summary += f"\nâŒ æ¤œè¨¼ã‚¨ãƒ©ãƒ¼ ({len(self.validation_errors)}ä»¶):\n"
            for error in self.validation_errors[:5]:
                summary += f"  - {error}\n"
        if self.naming_violations:
            summary += f"\nâš ï¸ å‘½åè¦å‰‡é•å ({len(self.naming_violations)}ä»¶):\n"
            for violation in self.naming_violations[:5]:
                summary += f"  - {violation}\n"
        return summary


class PromptValidationTool:
    """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå“è³ªæ¤œè¨¼ãƒ„ãƒ¼ãƒ«"""

    def __init__(
        self, project_root: Path | None = None, logger_service: Any | None = None, console_service: Any | None = None
    ) -> None:
        """ãƒ„ãƒ¼ãƒ«åˆæœŸåŒ–"""

        self.project_root = project_root or self._get_project_root()
        self.path_service = get_common_path_service(self.project_root)
        self.repository = EpisodePromptRepository()

        self.prompt_dir = self.path_service.get_prompts_dir() / "è©±åˆ¥ãƒ—ãƒ­ãƒƒãƒˆ"
        self.logger_service = logger_service
        self.console_service = console_service

    def validate_all_prompts(self) -> PromptValidationReport:
        """å…¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å“è³ªæ¤œè¨¼"""
        report = PromptValidationReport()
        if not self.prompt_dir.exists():
            report.add_validation_error(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ã¾ã›ã‚“: {self.prompt_dir}")
            return report
        prompt_files = self.repository.list_prompts(self.prompt_dir)
        for file_path in prompt_files:
            try:
                prompt = self.repository.load_prompt(file_path)
                if not prompt:
                    report.add_validation_error(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆèª­ã¿è¾¼ã¿å¤±æ•—: {file_path.name}")
                    continue
                quality_score = self._validate_prompt_quality(prompt, file_path, report)
                self._validate_file_naming(prompt, file_path, report)
                report.add_prompt_result(prompt, file_path, quality_score)
            except Exception as e:
                report.add_validation_error(f"ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ¤œè¨¼ã‚¨ãƒ©ãƒ¼ {file_path.name}: {e!s}")
        return report

    def _validate_prompt_quality(self, prompt: EpisodePrompt, file_path: Path, report: PromptValidationReport) -> float:
        """ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå“è³ªæ¤œè¨¼"""
        quality_score = prompt.get_content_quality_score()
        if len(prompt.prompt_content) < 1000:
            report.add_validation_error(f"{file_path.name}: ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆé•·ã•ä¸è¶³ ({len(prompt.prompt_content)}æ–‡å­—)")
        if len(prompt.content_sections) < 3:
            report.add_validation_error(
                f"{file_path.name}: ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚»ã‚¯ã‚·ãƒ§ãƒ³ä¸è¶³ ({len(prompt.content_sections)}å€‹)"
            )
        yaml_content = prompt.get_yaml_content()
        metadata = yaml_content.get("metadata", {})
        required_metadata = ["spec_id", "episode_number", "title", "generation_timestamp"]
        for field in required_metadata:
            if field not in metadata:
                report.add_validation_error(f"{file_path.name}: å¿…é ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ä¸è¶³ - {field}")
        return quality_score

    def _validate_file_naming(self, prompt: EpisodePrompt, file_path: Path, report: PromptValidationReport) -> None:
        """ãƒ•ã‚¡ã‚¤ãƒ«å‘½åè¦å‰‡æ¤œè¨¼"""
        try:
            expected_filename = prompt.get_file_name().to_filename()
            if file_path.name != expected_filename:
                report.add_naming_violation(f"ãƒ•ã‚¡ã‚¤ãƒ«åä¸ä¸€è‡´: å®Ÿéš›={file_path.name}, æœŸå¾…={expected_filename}")
        except Exception as e:
            report.add_validation_error(f"ãƒ•ã‚¡ã‚¤ãƒ«åæ¤œè¨¼ã‚¨ãƒ©ãƒ¼ {file_path.name}: {e!s}")

    def analyze_quality_trends(self) -> dict[str, Any]:
        """å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ"""
        prompt_files = self.repository.list_prompts(self.prompt_dir)
        quality_by_episode = {}
        size_by_episode = {}
        for file_path in prompt_files:
            prompt = self.repository.load_prompt(file_path)
            if prompt:
                episode_num = prompt.episode_number
                quality_score = prompt.get_content_quality_score()
                file_size = file_path.stat().st_size if file_path.exists() else 0
                quality_by_episode[episode_num] = quality_score
                size_by_episode[episode_num] = file_size
        return {
            "quality_by_episode": quality_by_episode,
            "size_by_episode": size_by_episode,
            "episode_count": len(quality_by_episode),
            "quality_trend": "improving" if self._is_improving_trend(quality_by_episode) else "stable",
        }

    def _is_improving_trend(self, quality_by_episode: dict[int, float]) -> bool:
        """å“è³ªæ”¹å–„ãƒˆãƒ¬ãƒ³ãƒ‰åˆ¤å®š"""
        if len(quality_by_episode) < 3:
            return False
        sorted_episodes = sorted(quality_by_episode.items())
        recent_scores = [score for (_, score) in sorted_episodes[-3:]]
        early_scores = [score for (_, score) in sorted_episodes[:3]]
        return statistics.mean(recent_scores) > statistics.mean(early_scores)

    def _get_project_root(self) -> Path:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆå–å¾—"""

        env_project_root = os.environ.get("PROJECT_ROOT")
        if env_project_root:
            return Path(env_project_root)
        return Path.cwd()


def main() -> None:
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    from noveler.infrastructure.adapters.console_service_adapter import get_console_service  # noqa: PLC0415

    parser = argparse.ArgumentParser(description="ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå“è³ªæ¤œè¨¼ãƒ„ãƒ¼ãƒ«")
    parser.add_argument("--project-root", type=Path, help="ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‘ã‚¹ï¼ˆçœç•¥æ™‚ã¯ç’°å¢ƒå¤‰æ•°PROJECT_ROOTä½¿ç”¨ï¼‰")
    parser.add_argument("--detailed", action="store_true", help="è©³ç´°ãªåˆ†æçµæœã‚’è¡¨ç¤º")
    parser.add_argument("--format", choices=["text", "json"], default="text", help="å‡ºåŠ›å½¢å¼")
    args = parser.parse_args()
    get_console_service()
    try:
        tool = PromptValidationTool(project_root=args.project_root)
        report = tool.validate_all_prompts()
        if args.format == "json":

            result = {
                "total_prompts": report.total_prompts,
                "average_quality": statistics.mean(report.quality_scores) if report.quality_scores else 0,
                "high_quality_count": report.high_quality_count,
                "validation_errors": report.validation_errors,
                "naming_violations": report.naming_violations,
            }
            console.print(json.dumps(result, ensure_ascii=False, indent=2))
        else:
            console.print(report.generate_summary())
            if args.detailed:
                trends = tool.analyze_quality_trends()
                console.print("\nğŸ“ˆ å“è³ªãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ:")
                console.print(f"  - ã‚¨ãƒ”ã‚½ãƒ¼ãƒ‰æ•°: {trends['episode_count']}")
                console.print(f"  - å“è³ªå‚¾å‘: {trends['quality_trend']}")
    except Exception as e:
        console.print(f"âŒ æ¤œè¨¼ãƒ„ãƒ¼ãƒ«å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e!s}")
        return 1
    return 0


if __name__ == "__main__":
    sys.exit(main())
