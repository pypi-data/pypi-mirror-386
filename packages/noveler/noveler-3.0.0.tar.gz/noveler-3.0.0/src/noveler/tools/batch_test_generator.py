"""Tools.batch_test_generator
Where: Tool generating tests in batch from specifications.
What: Creates test cases based on spec inputs to expand coverage quickly.
Why: Accelerates test creation and reduces manual work.
"""

from noveler.presentation.shared.shared_utilities import console

"ä¸€æ‹¬ãƒ†ã‚¹ãƒˆç”Ÿæˆãƒ„ãƒ¼ãƒ«\n\næœªãƒ†ã‚¹ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã«å¯¾ã—ã¦SPECæº–æ‹ ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¸€æ‹¬ç”Ÿæˆ\nDDDæº–æ‹ ãƒ»é–‹ç™ºè€…ä½“é¨“æœ€é©åŒ–å¯¾å¿œ\n"
import argparse
import ast
import json
import re
import sys
from dataclasses import asdict, dataclass
from pathlib import Path
from typing import Any

from noveler.domain.value_objects.project_time import project_now

try:
    from noveler.infrastructure.logging.unified_logger import get_logger
except ImportError:  # pragma: no cover - fallback for lightweight environments
    from noveler.domain.interfaces.logger_interface import NullLogger

    def get_logger(_: str) -> NullLogger:
        """Return a NullLogger when unified logging infrastructure is unavailable."""
        return NullLogger()


logger = get_logger(__name__)


@dataclass
class TestGenerationConfig:
    """ãƒ†ã‚¹ãƒˆç”Ÿæˆè¨­å®š"""

    project_root: Path
    source_patterns: list[str]
    test_patterns: list[str]
    template_style: str = "ddd_compliant"
    spec_prefix: str = "SPEC"
    max_tests_per_class: int = 15
    include_async_tests: bool = True
    include_performance_tests: bool = True
    mock_external_dependencies: bool = True


@dataclass
class ClassInfo:
    """ã‚¯ãƒ©ã‚¹æƒ…å ±"""

    name: str
    file_path: Path
    methods: list[str]
    dependencies: list[str]
    is_async: bool
    docstring: str | None
    complexity_score: int


@dataclass
class TestGenerationResult:
    """ãƒ†ã‚¹ãƒˆç”Ÿæˆçµæœ"""

    generated_files: list[str]
    skipped_files: list[str]
    errors: list[str]
    total_test_cases: int
    execution_time_seconds: float
    quality_metrics: dict[str, Any]


class BatchTestGenerator:
    """ä¸€æ‹¬ãƒ†ã‚¹ãƒˆç”Ÿæˆå™¨

    è²¬å‹™:
    - æœªãƒ†ã‚¹ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã®è‡ªå‹•æ¤œå‡º
    - SPECæº–æ‹ ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆ
    - DDDæº–æ‹ ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆé©ç”¨
    - ä¾å­˜é–¢ä¿‚ãƒ¢ãƒƒã‚¯è‡ªå‹•ç”Ÿæˆ
    """

    def __init__(self, config: TestGenerationConfig, dry_run: bool = False) -> None:
        """åˆæœŸåŒ–

        Args:
            config: ãƒ†ã‚¹ãƒˆç”Ÿæˆè¨­å®š
            dry_run: True ã®å ´åˆã€å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆã‚’è¡Œã‚ãªã„
        """
        self.config = config
        self.project_root = config.project_root
        self.generated_count = 0
        self.dry_run = dry_run
        self.templates = {
            "ddd_compliant": self._get_ddd_template(),
            "basic": self._get_basic_template(),
            "advanced": self._get_advanced_template(),
        }

    def generate_tests_batch(self, target_modules: list[str] | None = None) -> TestGenerationResult:
        """ä¸€æ‹¬ãƒ†ã‚¹ãƒˆç”Ÿæˆå®Ÿè¡Œ

        Args:
            target_modules: å¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒªã‚¹ãƒˆï¼ˆNoneã§å…¨å¯¾è±¡ï¼‰

        Returns:
            TestGenerationResult: ç”Ÿæˆçµæœ
        """
        start_time = project_now().datetime
        logger.info("ä¸€æ‹¬ãƒ†ã‚¹ãƒˆç”Ÿæˆé–‹å§‹")
        try:
            if target_modules is None:
                target_classes = self._discover_untested_modules()
            else:
                target_classes = self._analyze_specified_modules(target_modules)
            logger.info(f"å¯¾è±¡ã‚¯ãƒ©ã‚¹æ•°: {len(target_classes)}")
            generated_files = []
            skipped_files = []
            errors = []
            total_test_cases = 0
            for class_info in target_classes:
                try:
                    if self.dry_run:
                        test_file_path = self._get_corresponding_test_file(class_info.file_path)
                        test_count = len(class_info.methods) + 2
                        generated_files.append(str(test_file_path))
                        total_test_cases += test_count
                        logger.info(f"[DRY-RUN] ç”Ÿæˆå¯¾è±¡: {test_file_path} (ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹æ•°: {test_count})")
                    else:
                        result = self._generate_test_for_class(class_info)
                        if result["success"]:
                            generated_files.append(result["file_path"])
                            total_test_cases += result["test_count"]
                            logger.info(f"ç”Ÿæˆå®Œäº†: {result['file_path']}")
                        else:
                            skipped_files.append(class_info.file_path)
                            errors.extend(result.get("errors", []))
                except Exception as e:
                    errors.append(f"{class_info.file_path}: {e!s}")
                    logger.exception("ç”Ÿæˆã‚¨ãƒ©ãƒ¼ %s", class_info.file_path)
            quality_metrics = self._calculate_quality_metrics(generated_files, total_test_cases)
            end_time = project_now().datetime
            execution_time = (end_time - start_time).total_seconds()
            result = TestGenerationResult(
                generated_files=generated_files,
                skipped_files=skipped_files,
                errors=errors,
                total_test_cases=total_test_cases,
                execution_time_seconds=execution_time,
                quality_metrics=quality_metrics,
            )
            logger.info("ä¸€æ‹¬ãƒ†ã‚¹ãƒˆç”Ÿæˆå®Œäº†: %sãƒ•ã‚¡ã‚¤ãƒ«ã€%sãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹", len(generated_files), total_test_cases)
            return result
        except Exception:
            logger.exception("ä¸€æ‹¬ãƒ†ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼")
            raise

    def _discover_untested_modules(self) -> list[ClassInfo]:
        """æœªãƒ†ã‚¹ãƒˆãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ç™ºè¦‹"""
        untested_classes = []
        for pattern in self.config.source_patterns:
            for source_file in self.project_root.rglob(pattern):
                if self._should_skip_file(source_file):
                    continue
                test_file = self._get_corresponding_test_file(source_file)
                if not test_file.exists():
                    classes = self._extract_class_info(source_file)
                    untested_classes.extend(classes)
        return untested_classes

    def _analyze_specified_modules(self, module_paths: list[str]) -> list[ClassInfo]:
        """æŒ‡å®šãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«è§£æ"""
        specified_classes = []
        for module_path in module_paths:
            file_path = Path(module_path)
            if not file_path.is_absolute():
                file_path = self.project_root / file_path
            if file_path.exists() and file_path.is_file():
                if self._should_skip_file(file_path):
                    logger.info(f"ã‚¹ã‚­ãƒƒãƒ—: {file_path}")
                    continue
                classes = self._extract_class_info(file_path)
                specified_classes.extend(classes)
            else:
                logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚‰ãªã„ã‹ã€ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã§ã™: {module_path}")
        return specified_classes

    def _extract_class_info(self, file_path: Path) -> list[ClassInfo]:
        """ã‚¯ãƒ©ã‚¹æƒ…å ±æŠ½å‡º"""
        try:
            with file_path.open(encoding="utf-8") as f:
                content = f.read()
            tree = ast.parse(content)
            classes = []
            for node in ast.walk(tree):
                if isinstance(node, ast.ClassDef):
                    class_info = self._analyze_class_node(node, file_path, content)
                    classes.append(class_info)
            return classes
        except Exception as e:
            logger.warning(f"ã‚¯ãƒ©ã‚¹æƒ…å ±æŠ½å‡ºã‚¨ãƒ©ãƒ¼ {file_path}: {e}")
            return []

    def _analyze_class_node(self, node: ast.ClassDef, file_path: Path, content: str) -> ClassInfo:
        """ã‚¯ãƒ©ã‚¹ãƒãƒ¼ãƒ‰è§£æ"""
        methods = []
        dependencies = set()
        is_async = False
        for item in node.body:
            if isinstance(item, ast.FunctionDef):
                methods.append(item.name)
                if isinstance(item, ast.AsyncFunctionDef):
                    is_async = True
            elif isinstance(item, ast.AsyncFunctionDef):
                methods.append(item.name)
                is_async = True
        for item in ast.walk(node):
            if isinstance(item, ast.Import):
                for alias in item.names:
                    dependencies.add(alias.name)
            elif isinstance(item, ast.ImportFrom):
                if item.module:
                    dependencies.add(item.module)
        complexity_score = self._calculate_complexity_score(node)
        return ClassInfo(
            name=node.name,
            file_path=file_path,
            methods=methods,
            dependencies=list(dependencies),
            is_async=is_async,
            docstring=ast.get_docstring(node),
            complexity_score=complexity_score,
        )

    def _calculate_complexity_score(self, node: ast.ClassDef) -> int:
        """è¤‡é›‘åº¦ã‚¹ã‚³ã‚¢è¨ˆç®—"""
        score = 0
        methods = [item for item in node.body if isinstance(item, ast.FunctionDef | ast.AsyncFunctionDef)]
        score += len(methods) * 2
        for method in methods:
            score += self._calculate_nesting_depth(method)
        for item in ast.walk(node):
            if isinstance(item, ast.If | ast.While | ast.For | ast.Try):
                score += 1
        return score

    def _calculate_nesting_depth(self, node: ast.AST, current_depth: int = 0) -> int:
        """ãƒã‚¹ãƒˆãƒ¬ãƒ™ãƒ«è¨ˆç®—"""
        max_depth = current_depth
        for child in ast.iter_child_nodes(node):
            if isinstance(child, ast.If | ast.While | ast.For | ast.With | ast.Try):
                child_depth = self._calculate_nesting_depth(child, current_depth + 1)
                max_depth = max(max_depth, child_depth)
        return max_depth

    def _generate_test_for_class(self, class_info: ClassInfo) -> dict[str, Any]:
        """ã‚¯ãƒ©ã‚¹ç”¨ãƒ†ã‚¹ãƒˆç”Ÿæˆ"""
        try:
            test_file_path = self._get_corresponding_test_file(class_info.file_path)
            test_file_path.parent.mkdir(parents=True, exist_ok=True)
            template = self.templates[self.config.template_style]
            test_content = self._generate_test_content(class_info, template)
            with test_file_path.open("w", encoding="utf-8") as f:
                f.write(test_content)
            test_count = len(class_info.methods) + 2
            return {"success": True, "file_path": str(test_file_path), "test_count": test_count}
        except Exception as e:
            return {"success": False, "errors": [str(e)]}

    def _generate_test_content(self, class_info: ClassInfo, template: str) -> str:
        """ãƒ†ã‚¹ãƒˆã‚³ãƒ³ãƒ†ãƒ³ãƒ„ç”Ÿæˆ"""
        variables = {
            "class_name": class_info.name,
            "module_path": self._get_module_path(class_info.file_path),
            "test_class_name": f"Test{class_info.name}",
            "docstring": class_info.docstring or f"{class_info.name}ã®ãƒ†ã‚¹ãƒˆ",
            "dependencies": self._generate_dependency_mocks(class_info.dependencies),
            "method_tests": self._generate_method_tests(class_info),
            "async_decorator": "@pytest.mark.asyncio" if class_info.is_async else "",
            "imports": self._generate_imports(class_info),
            "spec_prefix": self._generate_spec_prefix(class_info.name),
        }
        return template.format(**variables)

    def _generate_dependency_mocks(self, dependencies: list[str]) -> str:
        """ä¾å­˜é–¢ä¿‚ãƒ¢ãƒƒã‚¯ç”Ÿæˆ"""
        if not dependencies or not self.config.mock_external_dependencies:
            return ""
        mock_fixtures = []
        for dep in dependencies[:5]:
            clean_name = re.sub("[^\\w]", "_", dep.split(".")[-1].lower())
            mock_fixtures.append(
                f'\n    @pytest.fixture\n    def mock_{clean_name}(self):\n        """${dep} ã®ãƒ¢ãƒƒã‚¯"""\n        return Mock()'
            )
        return "\n".join(mock_fixtures)

    def _generate_method_tests(self, class_info: ClassInfo) -> str:
        """ãƒ¡ã‚½ãƒƒãƒ‰ãƒ†ã‚¹ãƒˆç”Ÿæˆ"""
        method_tests = []
        spec_counter = 2
        method_tests.append(
            f'\n    @mark.spec("{self._generate_spec_prefix(class_info.name)}-001")\n    def test_initialization(self):\n        """åˆæœŸåŒ–ãƒ†ã‚¹ãƒˆ"""\n        # Arrange & Act & Assert ãƒ‘ã‚¿ãƒ¼ãƒ³å®Ÿè£…\n        assert True  # TODO: å®Ÿè£…ã™ã‚‹'
        )
        for method in class_info.methods[: self.config.max_tests_per_class - 1]:
            if method.startswith("_"):
                continue
            spec_id = f"{self._generate_spec_prefix(class_info.name)}-{spec_counter:03d}"
            async_decorator = "@pytest.mark.asyncio\n    " if class_info.is_async and "async" in method else ""
            method_tests.append(
                f'\n    @mark.spec("{spec_id}")\n    {async_decorator}def test_{method}(self):\n        """${method}ãƒ†ã‚¹ãƒˆ"""\n        # Arrange\n        # TODO: ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿æº–å‚™\n\n        # Act\n        # TODO: ãƒ¡ã‚½ãƒƒãƒ‰å®Ÿè¡Œ\n\n        # Assert\n        # TODO: çµæœæ¤œè¨¼\n        assert True  # TODO: å®Ÿè£…ã™ã‚‹'
            )
            spec_counter += 1
        return "\n".join(method_tests)

    def _generate_imports(self, class_info: ClassInfo) -> str:
        """ã‚¤ãƒ³ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        imports = [
            "import pytest",
            "from pathlib import Path",
            "from unittest.mock import Mock, patch, AsyncMock"
            if class_info.is_async
            else "from unittest.mock import Mock, patch",
            "from pytest import mark",
        ]
        module_path = self._get_module_path(class_info.file_path)
        imports.append(f"\nfrom {module_path} import {class_info.name}")
        return "\n".join(imports)

    def _generate_spec_prefix(self, class_name: str) -> str:
        """SPEC ID ãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ç”Ÿæˆ"""
        abbreviation = "".join([c for c in class_name if c.isupper()])[:3]
        if len(abbreviation) < 2:
            abbreviation = class_name[:3].upper()
        return f"SPEC-{abbreviation}"

    def _get_module_path(self, file_path: Path) -> str:
        """ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ‘ã‚¹å–å¾—"""
        relative_path = file_path.relative_to(self.project_root)
        return str(relative_path.with_suffix("")).replace("/", ".")

    def _get_corresponding_test_file(self, source_file: Path) -> Path:
        """å¯¾å¿œã™ã‚‹ãƒ†ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹å–å¾—"""
        if source_file.is_absolute():
            try:
                relative_path = source_file.relative_to(self.project_root / "src" / "noveler")
            except ValueError:
                logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«ãŒãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå¤–: {source_file}")
                return self.project_root / "tests" / "unit" / "unknown" / f"test_{source_file.name}"
        else:
            relative_path = Path(str(source_file).replace("noveler/", ""))
        test_file_name = f"test_{source_file.name}"
        return self.project_root / "tests" / "unit" / relative_path.parent / test_file_name

    def _should_skip_file(self, file_path: Path) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«ã‚¹ã‚­ãƒƒãƒ—åˆ¤å®š"""
        skip_patterns = [
            "__pycache__",
            "__init__.py",
            "test_",
            ".pyc",
            "migrations/",
            "noveler/tools/",
            "backup/",
            "archive/",
            "temp/",
        ]
        file_str = str(file_path)
        return any(pattern in file_str for pattern in skip_patterns)

    def _calculate_quality_metrics(self, generated_files: list[str], test_count: int) -> dict[str, Any]:
        """å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        return {
            "files_generated": len(generated_files),
            "total_test_cases": test_count,
            "average_tests_per_file": test_count / len(generated_files) if generated_files else 0,
            "spec_compliance_rate": 1.0,
            "coverage_improvement_estimate": len(generated_files) * 15,
            "generation_efficiency": test_count / max(1, self.generated_count),
        }

    def _get_ddd_template(self) -> str:
        """DDDæº–æ‹ ãƒ†ã‚¹ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ"""
        return '#!/usr/bin/env python3\n"""{docstring}\n\n{class_name}ã®å‹•ä½œã‚’æ¤œè¨¼ã™ã‚‹ãƒ¦ãƒ‹ãƒƒãƒˆãƒ†ã‚¹ãƒˆ\nDDDæº–æ‹ ãƒ»åŒ…æ‹¬çš„ãƒ†ã‚¹ãƒˆã‚«ãƒãƒ¬ãƒƒã‚¸å¯¾å¿œç‰ˆ\n"""\n\n{imports}\n\n\nclass {test_class_name}:\n    """{class_name} ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""\n{dependencies}\n{method_tests}'

    def _get_basic_template(self) -> str:
        """åŸºæœ¬ãƒ†ã‚¹ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ"""
        return '#!/usr/bin/env python3\n"""{docstring}\n\n{class_name}ã®åŸºæœ¬ãƒ†ã‚¹ãƒˆ\n"""\n\n{imports}\n\n\nclass {test_class_name}:\n    """{class_name} ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""\n{method_tests}'

    def _get_advanced_template(self) -> str:
        """é«˜åº¦ãƒ†ã‚¹ãƒˆãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ"""
        return '#!/usr/bin/env python3\n"""{docstring}\n\n{class_name}ã®è©³ç´°ãƒ†ã‚¹ãƒˆ\nãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ»ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ»ã‚¨ãƒ©ãƒ¼ãƒãƒ³ãƒ‰ãƒªãƒ³ã‚°å«ã‚€\n"""\n\n{imports}\nimport time\nimport psutil\nfrom datetime import datetime\n\n\nclass {test_class_name}:\n    """{class_name} ãƒ†ã‚¹ãƒˆã‚¯ãƒ©ã‚¹"""\n{dependencies}\n{method_tests}\n\n    @mark.spec("{spec_prefix}-999")\n    def test_performance_baseline(self):\n        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ãƒ†ã‚¹ãƒˆ"""\n        start_time = time.time()\n        # TODO: ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒ†ã‚¹ãƒˆå®Ÿè£…\n        execution_time = time.time() - start_time\n        assert execution_time < 1.0  # 1ç§’ä»¥å†…ã®å®Ÿè¡Œ'

    def export_generation_report(self, result: TestGenerationResult, output_path: Path | None = None) -> None:
        """ç”Ÿæˆãƒ¬ãƒãƒ¼ãƒˆã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        if output_path is None:
            output_path = self.project_root / "temp" / "test_generation_report.json"
        report_data = {
            "generation_timestamp": project_now().datetime.isoformat(),
            "configuration": asdict(self.config),
            "results": asdict(result),
            "recommendations": self._generate_recommendations(result),
        }
        if self.dry_run:
            logger.info(f"[DRY-RUN] ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›äºˆå®š: {output_path}")
            console.print("\n=== DRY-RUN ãƒ¬ãƒãƒ¼ãƒˆå†…å®¹ ===")
            report_json = json.dumps(report_data, ensure_ascii=False, indent=2, default=str)
            console.print(report_json[:1000])
            if len(report_json) > 1000:
                console.print("... (ä»¥ä¸‹çœç•¥)")
        else:
            output_path.parent.mkdir(parents=True, exist_ok=True)
            with output_path.open("w", encoding="utf-8") as f:
                json.dump(report_data, f, ensure_ascii=False, indent=2, default=str)
            logger.info(f"ç”Ÿæˆãƒ¬ãƒãƒ¼ãƒˆã‚’ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ: {output_path}")

    def _generate_recommendations(self, result: TestGenerationResult) -> list[str]:
        """æ”¹å–„æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []
        if result.quality_metrics["average_tests_per_file"] < 10:
            recommendations.append("ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹æ•°ã‚’å¢—ã‚„ã—ã¦ç¶²ç¾…æ€§ã‚’å‘ä¸Š")
        if len(result.errors) > 0:
            recommendations.append("ç”Ÿæˆã‚¨ãƒ©ãƒ¼ã®åŸå› èª¿æŸ»ã¨ä¿®æ­£")
        if result.quality_metrics["files_generated"] > 20:
            recommendations.append("ä¸¦è¡Œãƒ†ã‚¹ãƒˆå®Ÿè¡Œã§CI/CDåŠ¹ç‡åŒ–")
        recommendations.append("ç”Ÿæˆã•ã‚ŒãŸãƒ†ã‚¹ãƒˆã®æ‰‹å‹•ãƒ¬ãƒ“ãƒ¥ãƒ¼ã¨èª¿æ•´å®Ÿæ–½")
        recommendations.append("ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã¨ãƒ¢ãƒƒã‚¯ã®å…·ä½“çš„ãªå®Ÿè£…")
        return recommendations


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    parser = argparse.ArgumentParser(description="ä¸€æ‹¬ãƒ†ã‚¹ãƒˆç”Ÿæˆãƒ„ãƒ¼ãƒ«")
    parser.add_argument("--project-root", type=Path, default=Path.cwd(), help="ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª")
    parser.add_argument("--pattern", action="append", default=["noveler/**/*.py"], help="å¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¿ãƒ¼ãƒ³")
    parser.add_argument(
        "--template",
        choices=["ddd_compliant", "basic", "advanced"],
        default="ddd_compliant",
        help="ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆã‚¹ã‚¿ã‚¤ãƒ«",
    )
    parser.add_argument("--max-tests", type=int, default=15, help="ã‚¯ãƒ©ã‚¹å½“ãŸã‚Šæœ€å¤§ãƒ†ã‚¹ãƒˆæ•°")
    parser.add_argument("--targets", nargs="*", help="å¯¾è±¡ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«æŒ‡å®š")
    parser.add_argument("--output-report", type=Path, help="ãƒ¬ãƒãƒ¼ãƒˆå‡ºåŠ›å…ˆ")
    parser.add_argument("--dry-run", action="store_true", help="å®Ÿéš›ã®ãƒ•ã‚¡ã‚¤ãƒ«ç”Ÿæˆã‚’è¡Œã‚ãšã«å¯¾è±¡ã®ç¢ºèªã®ã¿")
    args = parser.parse_args()
    config = TestGenerationConfig(
        project_root=args.project_root,
        source_patterns=args.pattern,
        test_patterns=["tests/**/test_*.py"],
        template_style=args.template,
        max_tests_per_class=args.max_tests,
        include_async_tests=True,
        include_performance_tests=args.template == "advanced",
        mock_external_dependencies=True,
    )
    try:
        generator = BatchTestGenerator(config, dry_run=args.dry_run)
        result = generator.generate_tests_batch(args.targets)
        if args.output_report:
            generator.export_generation_report(result, args.output_report)
        else:
            generator.export_generation_report(result)
        if args.dry_run:
            console.print("\nğŸ” [DRY-RUN MODE] ä¸€æ‹¬ãƒ†ã‚¹ãƒˆç”Ÿæˆã‚·ãƒŸãƒ¥ãƒ¬ãƒ¼ã‚·ãƒ§ãƒ³å®Œäº†!")
            console.print(f"ğŸ“ ç”Ÿæˆäºˆå®šãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(result.generated_files)}")
            console.print(f"ğŸ§ª ç”Ÿæˆäºˆå®šãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹æ•°: {result.total_test_cases}")
            console.print(f"â±ï¸  åˆ†ææ™‚é–“: {result.execution_time_seconds:.2f}ç§’")
            if result.generated_files:
                console.print("\nç”Ÿæˆäºˆå®šãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§:")
                for file in result.generated_files[:10]:
                    console.print(f"  - {file}")
                if len(result.generated_files) > 10:
                    console.print(f"  ... ä»– {len(result.generated_files) - 10} ãƒ•ã‚¡ã‚¤ãƒ«")
        else:
            console.print("\nğŸ‰ ä¸€æ‹¬ãƒ†ã‚¹ãƒˆç”Ÿæˆå®Œäº†!")
            console.print(f"ğŸ“ ç”Ÿæˆãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(result.generated_files)}")
            console.print(f"ğŸ§ª ãƒ†ã‚¹ãƒˆã‚±ãƒ¼ã‚¹æ•°: {result.total_test_cases}")
            console.print(f"â±ï¸  å®Ÿè¡Œæ™‚é–“: {result.execution_time_seconds:.2f}ç§’")
        if result.errors:
            console.print(f"âš ï¸  ã‚¨ãƒ©ãƒ¼æ•°: {len(result.errors)}")
            for error in result.errors[:3]:
                console.print(f"   - {error}")
        console.print("\nğŸ“Š å“è³ªãƒ¡ãƒˆãƒªã‚¯ã‚¹:")
        for key, value in result.quality_metrics.items():
            console.print(f"   - {key}: {value}")
        return 0 if not result.errors else 1
    except Exception:
        logger.exception("ä¸€æ‹¬ãƒ†ã‚¹ãƒˆç”Ÿæˆã‚¨ãƒ©ãƒ¼")
        return 1


if __name__ == "__main__":
    sys.exit(main())
