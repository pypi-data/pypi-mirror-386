# File: src/noveler/infrastructure/logging/log_analyzer.py
# Purpose: Analyze aggregated logs for patterns, performance issues, and insights
# Context: Phase 3 log analysis utilities for debugging and optimization

"""ãƒ­ã‚°åˆ†æãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£

é›†ç´„ã•ã‚ŒãŸãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‚’åˆ†æã—ã€ãƒ‘ã‚¿ãƒ¼ãƒ³èªè­˜ã€
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œã®ç‰¹å®šã€æœ€é©åŒ–ææ¡ˆã‚’è¡Œã†ã€‚
"""

import re
from collections import Counter, defaultdict
from dataclasses import dataclass
from typing import Any, Dict, List, Optional, Tuple

import numpy as np
from noveler.infrastructure.logging.log_aggregator_service import LogEntry


@dataclass
class PerformanceBottleneck:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯æƒ…å ±"""
    operation: str
    avg_time_ms: float
    p95_time_ms: float
    p99_time_ms: float
    frequency: int
    impact_score: float  # é »åº¦ Ã— å¹³å‡æ™‚é–“
    recommendations: List[str]


@dataclass
class ErrorPattern:
    """ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³æƒ…å ±"""
    error_type: str
    frequency: int
    affected_operations: List[str]
    sample_messages: List[str]
    first_occurrence: float
    last_occurrence: float
    trend: str  # increasing, decreasing, stable


@dataclass
class UserSessionAnalysis:
    """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ†æ"""
    session_id: str
    duration_seconds: float
    request_count: int
    error_count: int
    operations_performed: List[str]
    avg_response_time_ms: float
    successful: bool


class LogAnalyzer:
    """ãƒ­ã‚°åˆ†æã‚¯ãƒ©ã‚¹

    ãƒ­ã‚°ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æœ‰ç”¨ãªæ´å¯Ÿã‚’æŠ½å‡ºã—ã€
    ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã¨å•é¡Œè§£æ±ºã®ææ¡ˆã‚’è¡Œã†ã€‚
    """

    def __init__(self):
        """åˆæœŸåŒ–"""
        self.performance_thresholds = {
            "fast": 100,      # 100msä»¥ä¸‹
            "normal": 500,    # 500msä»¥ä¸‹
            "slow": 1000,     # 1000msä»¥ä¸‹
            "very_slow": 3000 # 3000msä»¥ä¸Š
        }

    def analyze_performance_bottlenecks(
        self,
        logs: List[LogEntry],
        min_frequency: int = 10
    ) -> List[PerformanceBottleneck]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã‚’åˆ†æ

        Args:
            logs: ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªã®ãƒªã‚¹ãƒˆ
            min_frequency: åˆ†æå¯¾è±¡ã¨ã™ã‚‹æœ€å°é »åº¦

        Returns:
            ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã®ãƒªã‚¹ãƒˆï¼ˆå½±éŸ¿åº¦é †ï¼‰
        """
        operation_times = defaultdict(list)

        for log in logs:
            if log.operation and log.execution_time_ms is not None:
                operation_times[log.operation].append(log.execution_time_ms)

        bottlenecks = []

        for operation, times in operation_times.items():
            if len(times) < min_frequency:
                continue

            times_array = np.array(times)
            avg_time = np.mean(times_array)
            p95_time = np.percentile(times_array, 95)
            p99_time = np.percentile(times_array, 99)
            frequency = len(times)
            impact_score = frequency * avg_time

            recommendations = self._generate_performance_recommendations(
                operation, avg_time, p95_time, p99_time
            )

            bottlenecks.append(PerformanceBottleneck(
                operation=operation,
                avg_time_ms=avg_time,
                p95_time_ms=p95_time,
                p99_time_ms=p99_time,
                frequency=frequency,
                impact_score=impact_score,
                recommendations=recommendations
            ))

        # å½±éŸ¿åº¦ã‚¹ã‚³ã‚¢ã§é™é †ã‚½ãƒ¼ãƒˆ
        bottlenecks.sort(key=lambda x: x.impact_score, reverse=True)
        return bottlenecks

    def _generate_performance_recommendations(
        self,
        operation: str,
        avg_time: float,
        p95_time: float,
        p99_time: float
    ) -> List[str]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã®ææ¡ˆã‚’ç”Ÿæˆ

        Args:
            operation: æ“ä½œå
            avg_time: å¹³å‡å®Ÿè¡Œæ™‚é–“
            p95_time: 95ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«
            p99_time: 99ãƒ‘ãƒ¼ã‚»ãƒ³ã‚¿ã‚¤ãƒ«

        Returns:
            æ”¹å–„ææ¡ˆã®ãƒªã‚¹ãƒˆ
        """
        recommendations = []

        # çµ¶å¯¾çš„ãªé…ã•
        if avg_time > self.performance_thresholds["very_slow"]:
            recommendations.append("âš ï¸ éå¸¸ã«é…ã„å‡¦ç†ã§ã™ã€‚ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã®è¦‹ç›´ã—ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")
        elif avg_time > self.performance_thresholds["slow"]:
            recommendations.append("å‡¦ç†æ™‚é–“ãŒé•·ã„ã§ã™ã€‚éåŒæœŸå‡¦ç†ã‚„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®å°å…¥ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")

        # ã°ã‚‰ã¤ããŒå¤§ãã„
        if p99_time > avg_time * 3:
            recommendations.append("å®Ÿè¡Œæ™‚é–“ã®ã°ã‚‰ã¤ããŒå¤§ãã„ã§ã™ã€‚å¤–éƒ¨ä¾å­˜ã‚„I/Oã‚’ç¢ºèªã—ã¦ãã ã•ã„")

        # ç‰¹å®šãƒ‘ã‚¿ãƒ¼ãƒ³ã®æ¤œå‡º
        if "llm" in operation.lower() or "claude" in operation.lower():
            recommendations.append("LLMå‘¼ã³å‡ºã—ã§ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæœ€é©åŒ–ã‚„ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")

        if "database" in operation.lower() or "query" in operation.lower():
            recommendations.append("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹æ“ä½œã§ã™ã€‚ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚„ã‚¯ã‚¨ãƒªæœ€é©åŒ–ã‚’ç¢ºèªã—ã¦ãã ã•ã„")

        if "file" in operation.lower() or "io" in operation.lower():
            recommendations.append("ãƒ•ã‚¡ã‚¤ãƒ«I/Oæ“ä½œã§ã™ã€‚ãƒãƒƒãƒ•ã‚¡ãƒªãƒ³ã‚°ã‚„ãƒãƒƒãƒå‡¦ç†ã‚’æ¤œè¨ã—ã¦ãã ã•ã„")

        return recommendations

    def analyze_error_patterns(
        self,
        logs: List[LogEntry],
        time_window_seconds: float = 3600
    ) -> List[ErrorPattern]:
        """ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’åˆ†æ

        Args:
            logs: ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªã®ãƒªã‚¹ãƒˆ
            time_window_seconds: ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æã®æ™‚é–“çª“ï¼ˆç§’ï¼‰

        Returns:
            ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã®ãƒªã‚¹ãƒˆ
        """
        error_groups = defaultdict(lambda: {
            "logs": [],
            "operations": set(),
            "messages": []
        })

        for log in logs:
            if log.error_type:
                error_groups[log.error_type]["logs"].append(log)
                if log.operation:
                    error_groups[log.error_type]["operations"].add(log.operation)
                error_groups[log.error_type]["messages"].append(log.message)

        patterns = []

        for error_type, data in error_groups.items():
            if not data["logs"]:
                continue

            timestamps = [log.timestamp for log in data["logs"]]
            first_occurrence = min(timestamps)
            last_occurrence = max(timestamps)

            # ãƒˆãƒ¬ãƒ³ãƒ‰åˆ†æ
            trend = self._analyze_error_trend(timestamps, time_window_seconds)

            # ã‚µãƒ³ãƒ—ãƒ«ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ï¼ˆæœ€å¤§5ä»¶ã€é‡è¤‡æ’é™¤ï¼‰
            unique_messages = list(set(data["messages"]))[:5]

            patterns.append(ErrorPattern(
                error_type=error_type,
                frequency=len(data["logs"]),
                affected_operations=list(data["operations"]),
                sample_messages=unique_messages,
                first_occurrence=first_occurrence,
                last_occurrence=last_occurrence,
                trend=trend
            ))

        # é »åº¦ã§é™é †ã‚½ãƒ¼ãƒˆ
        patterns.sort(key=lambda x: x.frequency, reverse=True)
        return patterns

    def _analyze_error_trend(
        self,
        timestamps: List[float],
        window_seconds: float
    ) -> str:
        """ã‚¨ãƒ©ãƒ¼ã®ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’åˆ†æ

        Args:
            timestamps: ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿæ™‚åˆ»ã®ãƒªã‚¹ãƒˆ
            window_seconds: åˆ†æçª“ã®ç§’æ•°

        Returns:
            ãƒˆãƒ¬ãƒ³ãƒ‰ï¼ˆincreasing, decreasing, stableï¼‰
        """
        if len(timestamps) < 2:
            return "stable"

        timestamps.sort()
        mid_point = timestamps[len(timestamps) // 2]

        recent_count = sum(1 for t in timestamps if t > mid_point)
        older_count = sum(1 for t in timestamps if t <= mid_point)

        if recent_count > older_count * 1.5:
            return "increasing"
        elif recent_count < older_count * 0.67:
            return "decreasing"
        else:
            return "stable"

    def analyze_user_sessions(
        self,
        logs: List[LogEntry]
    ) -> List[UserSessionAnalysis]:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ†æ

        Args:
            logs: ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªã®ãƒªã‚¹ãƒˆ

        Returns:
            ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ†æçµæœã®ãƒªã‚¹ãƒˆ
        """
        sessions = defaultdict(lambda: {
            "logs": [],
            "operations": [],
            "errors": 0,
            "total_time": 0,
            "request_count": 0
        })

        for log in logs:
            if log.session_id:
                session_data = sessions[log.session_id]
                session_data["logs"].append(log)

                if log.operation:
                    session_data["operations"].append(log.operation)

                if log.error_type:
                    session_data["errors"] += 1

                if log.execution_time_ms:
                    session_data["total_time"] += log.execution_time_ms
                    session_data["request_count"] += 1

        analyses = []

        for session_id, data in sessions.items():
            if not data["logs"]:
                continue

            timestamps = [log.timestamp for log in data["logs"]]
            duration_seconds = max(timestamps) - min(timestamps) if len(timestamps) > 1 else 0

            avg_response_time = (
                data["total_time"] / data["request_count"]
                if data["request_count"] > 0 else 0
            )

            analyses.append(UserSessionAnalysis(
                session_id=session_id,
                duration_seconds=duration_seconds,
                request_count=len(data["logs"]),
                error_count=data["errors"],
                operations_performed=data["operations"],
                avg_response_time_ms=avg_response_time,
                successful=data["errors"] == 0
            ))

        # ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“ã§é™é †ã‚½ãƒ¼ãƒˆ
        analyses.sort(key=lambda x: x.duration_seconds, reverse=True)
        return analyses

    def find_correlated_events(
        self,
        logs: List[LogEntry],
        time_threshold_seconds: float = 1.0
    ) -> List[Tuple[str, str, int]]:
        """ç›¸é–¢ã®ã‚ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆã‚’æ¤œå‡º

        Args:
            logs: ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªã®ãƒªã‚¹ãƒˆ
            time_threshold_seconds: ç›¸é–¢ã¨è¦‹ãªã™æ™‚é–“é–¾å€¤

        Returns:
            (ã‚¤ãƒ™ãƒ³ãƒˆ1, ã‚¤ãƒ™ãƒ³ãƒˆ2, å…±èµ·å›æ•°)ã®ã‚¿ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆ
        """
        # ã‚¿ã‚¤ãƒ ã‚¹ã‚¿ãƒ³ãƒ—ã§ã‚½ãƒ¼ãƒˆ
        sorted_logs = sorted(logs, key=lambda x: x.timestamp)

        event_pairs = []

        for i in range(len(sorted_logs) - 1):
            log1 = sorted_logs[i]
            log2 = sorted_logs[i + 1]

            # æ™‚é–“é–¾å€¤å†…ã®ã‚¤ãƒ™ãƒ³ãƒˆãƒšã‚¢ã‚’è¨˜éŒ²
            if log2.timestamp - log1.timestamp <= time_threshold_seconds:
                if log1.operation and log2.operation:
                    event_pairs.append((log1.operation, log2.operation))

        # é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ
        pair_counts = Counter(event_pairs)

        # é »åº¦é †ã«ã‚½ãƒ¼ãƒˆ
        correlated = [
            (pair[0], pair[1], count)
            for pair, count in pair_counts.items()
            if count >= 3  # æœ€ä½3å›ä»¥ä¸Šã®å…±èµ·
        ]

        correlated.sort(key=lambda x: x[2], reverse=True)
        return correlated

    def generate_optimization_report(
        self,
        logs: List[LogEntry]
    ) -> str:
        """æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ

        Args:
            logs: ãƒ­ã‚°ã‚¨ãƒ³ãƒˆãƒªã®ãƒªã‚¹ãƒˆ

        Returns:
            Markdownå½¢å¼ã®ãƒ¬ãƒãƒ¼ãƒˆ
        """
        # å„ç¨®åˆ†æã‚’å®Ÿè¡Œ
        bottlenecks = self.analyze_performance_bottlenecks(logs)
        error_patterns = self.analyze_error_patterns(logs)
        sessions = self.analyze_user_sessions(logs)
        correlations = self.find_correlated_events(logs)

        report = """# ãƒ­ã‚°åˆ†æãƒ»æœ€é©åŒ–ãƒ¬ãƒãƒ¼ãƒˆ

## ğŸ“Š æ¦‚è¦çµ±è¨ˆ
"""
        # åŸºæœ¬çµ±è¨ˆ
        total_logs = len(logs)
        error_logs = sum(1 for log in logs if log.error_type)
        error_rate = (error_logs / total_logs * 100) if total_logs > 0 else 0

        report += f"""- ç·ãƒ­ã‚°æ•°: {total_logs:,}
- ã‚¨ãƒ©ãƒ¼æ•°: {error_logs:,}
- ã‚¨ãƒ©ãƒ¼ç‡: {error_rate:.1f}%

## âš¡ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯

"""
        if bottlenecks:
            report += "| æ“ä½œ | å¹³å‡æ™‚é–“ | P95 | P99 | é »åº¦ | å½±éŸ¿åº¦ |\n"
            report += "|------|----------|-----|-----|------|--------|\n"

            for b in bottlenecks[:10]:  # ä¸Šä½10ä»¶
                report += (
                    f"| {b.operation} | "
                    f"{b.avg_time_ms:.1f}ms | "
                    f"{b.p95_time_ms:.1f}ms | "
                    f"{b.p99_time_ms:.1f}ms | "
                    f"{b.frequency} | "
                    f"{b.impact_score:.0f} |\n"
                )

                if b.recommendations:
                    report += f"\n**æ¨å¥¨äº‹é …:**\n"
                    for rec in b.recommendations:
                        report += f"- {rec}\n"
                    report += "\n"
        else:
            report += "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\n\n"

        report += """## ğŸ”¥ ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³

"""
        if error_patterns:
            for pattern in error_patterns[:5]:  # ä¸Šä½5ä»¶
                trend_icon = {
                    "increasing": "ğŸ“ˆ",
                    "decreasing": "ğŸ“‰",
                    "stable": "â¡ï¸"
                }.get(pattern.trend, "")

                report += f"""### {pattern.error_type} {trend_icon}
- ç™ºç”Ÿå›æ•°: {pattern.frequency}
- ãƒˆãƒ¬ãƒ³ãƒ‰: {pattern.trend}
- å½±éŸ¿ã‚’å—ã‘ã‚‹æ“ä½œ: {', '.join(pattern.affected_operations[:5])}

"""
        else:
            report += "ã‚¨ãƒ©ãƒ¼ãƒ‘ã‚¿ãƒ¼ãƒ³ã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\n\n"

        report += """## ğŸ‘¥ ã‚»ãƒƒã‚·ãƒ§ãƒ³åˆ†æ

"""
        if sessions:
            successful_sessions = sum(1 for s in sessions if s.successful)
            success_rate = (successful_sessions / len(sessions) * 100) if sessions else 0

            report += f"""- ç·ã‚»ãƒƒã‚·ãƒ§ãƒ³æ•°: {len(sessions)}
- æˆåŠŸç‡: {success_rate:.1f}%
- å¹³å‡ã‚»ãƒƒã‚·ãƒ§ãƒ³æ™‚é–“: {np.mean([s.duration_seconds for s in sessions]):.1f}ç§’

"""

        report += """## ğŸ”— ç›¸é–¢ã‚¤ãƒ™ãƒ³ãƒˆ

"""
        if correlations:
            report += "é »ç¹ã«é€£ç¶šã—ã¦ç™ºç”Ÿã™ã‚‹ã‚¤ãƒ™ãƒ³ãƒˆãƒšã‚¢:\n\n"
            for event1, event2, count in correlations[:5]:
                report += f"- {event1} â†’ {event2} ({count}å›)\n"
        else:
            report += "ç›¸é–¢ã‚¤ãƒ™ãƒ³ãƒˆã¯æ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚\n"

        return report