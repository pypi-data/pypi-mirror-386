#!/usr/bin/env python3
"""
ä»•æ§˜æ›¸è‡ªå‹•åˆ†æã‚¹ã‚¯ãƒªãƒ—ãƒˆ

277å€‹ã®ä»•æ§˜æ›¸ã‹ã‚‰è¦ä»¶ã‚’è‡ªå‹•æŠ½å‡ºãƒ»åˆ†æã—ã€
è¦ä»¶å®šç¾©æ›¸ä½œæˆã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ãƒ„ãƒ¼ãƒ«
"""

import json
import re
import subprocess
from collections import Counter, defaultdict
from dataclasses import dataclass
from pathlib import Path
from typing import Any

import yaml


@dataclass
class Requirement:
    """æŠ½å‡ºã•ã‚ŒãŸè¦ä»¶"""
    id: str
    title: str
    description: str
    priority: str
    category: str
    specifications: list[str]
    keywords: list[str]
    complexity: int  # 1-5
    dependencies: list[str]


@dataclass
class SpecificationAnalysis:
    """ä»•æ§˜æ›¸åˆ†æçµæœ"""
    file_path: str
    title: str
    overview: str
    purpose: str
    requirements_mentioned: list[str]
    keywords: list[str]
    complexity_score: float
    size_lines: int
    last_modified: str


class SpecificationsAnalyzer:
    """ä»•æ§˜æ›¸åˆ†æã‚¨ãƒ³ã‚¸ãƒ³"""

    def __init__(self, specs_directory: Path):
        self.specs_dir = specs_directory
        self.analyses: list[SpecificationAnalysis] = []
        self.requirements: list[Requirement] = []

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†é¡è¾æ›¸
        self.category_keywords = {
            "ãƒ—ãƒ­ãƒƒãƒˆç®¡ç†": ["plot", "ãƒ—ãƒ­ãƒƒãƒˆ", "æ§‹é€ ", "hierarchy", "éšå±¤"],
            "AIå”å‰µåŸ·ç­†": ["ai", "claude", "write", "generation", "åŸ·ç­†", "å”å‰µ", "mcp"],
            "å“è³ªç®¡ç†": ["quality", "å“è³ª", "check", "ãƒã‚§ãƒƒã‚¯", "a31", "è©•ä¾¡"],
            "ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼": ["workflow", "ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼", "flow", "process", "é€²æ—"],
            "CLIãƒ»UI": ["cli", "command", "interface", "ui", "ã‚³ãƒãƒ³ãƒ‰"],
            "ãƒ‡ãƒ¼ã‚¿ç®¡ç†": ["data", "yaml", "repository", "ãƒ‡ãƒ¼ã‚¿", "ç®¡ç†"],
            "çµ±åˆãƒ»é€£æº": ["integration", "adapter", "çµ±åˆ", "é€£æº", "api"],
            "ã‚·ã‚¹ãƒ†ãƒ ç®¡ç†": ["system", "monitor", "log", "ã‚·ã‚¹ãƒ†ãƒ ", "ç›£è¦–"],
            "è¨­å®šç®¡ç†": ["config", "setting", "è¨­å®š", "configuration"],
            "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£": ["security", "auth", "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£", "èªè¨¼"]
        }

        # å„ªå…ˆåº¦åˆ¤å®šã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰
        self.priority_keywords = {
            "Critical": ["critical", "é‡è¦", "å¿…é ˆ", "required", "essential"],
            "High": ["high", "é«˜", "important", "é‡è¦åº¦é«˜"],
            "Medium": ["medium", "ä¸­", "normal", "é€šå¸¸"],
            "Low": ["low", "ä½", "optional", "ã‚ªãƒ—ã‚·ãƒ§ãƒ³"]
        }

    def analyze_all_specifications(self) -> dict[str, Any]:
        """å…¨ä»•æ§˜æ›¸ã®åˆ†æå®Ÿè¡Œ"""
        print("ğŸ” ä»•æ§˜æ›¸åˆ†æé–‹å§‹...")

        spec_files = list(self.specs_dir.rglob("*.md"))
        spec_files = [f for f in spec_files if "archive" not in str(f)]

        print(f"ğŸ“ å¯¾è±¡ä»•æ§˜æ›¸: {len(spec_files)}ä»¶")

        for spec_file in spec_files:
            try:
                analysis = self._analyze_single_specification(spec_file)
                self.analyses.append(analysis)
                print(f"  âœ… {spec_file.name}")
            except Exception as e:
                print(f"  âŒ {spec_file.name}: {e}")

        print("ğŸ”§ è¦ä»¶æŠ½å‡ºãƒ»åˆ†æä¸­...")
        self._extract_requirements()

        print("ğŸ“Š åˆ†æçµæœç”Ÿæˆä¸­...")
        result = self._generate_analysis_report()

        print("âœ… ä»•æ§˜æ›¸åˆ†æå®Œäº†")
        return result

    def _analyze_single_specification(self, spec_file: Path) -> SpecificationAnalysis:
        """å˜ä¸€ä»•æ§˜æ›¸ã®åˆ†æ"""
        content = spec_file.read_text(encoding="utf-8", errors="ignore")

        # åŸºæœ¬æƒ…å ±æŠ½å‡º
        title = self._extract_title(content)
        overview = self._extract_overview(content)
        purpose = self._extract_purpose(content)

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        keywords = self._extract_keywords(content)

        # è¦ä»¶è¨€åŠã®æŠ½å‡º
        requirements_mentioned = self._find_requirement_mentions(content)

        # è¤‡é›‘åº¦ã‚¹ã‚³ã‚¢ç®—å‡º
        complexity_score = self._calculate_complexity_score(content)

        # ãƒ•ã‚¡ã‚¤ãƒ«æƒ…å ±
        size_lines = len(content.split("\n"))

        try:
            # Git last modified (å¯èƒ½ã§ã‚ã‚Œã°)
            result = subprocess.run(
                ["git", "log", "-1", "--format=%ai", str(spec_file)],
                check=False, capture_output=True, text=True, cwd=self.specs_dir.parent
            )
            last_modified = result.stdout.strip() if result.returncode == 0 else "Unknown"
        except:
            last_modified = "Unknown"

        return SpecificationAnalysis(
            file_path=str(spec_file),
            title=title,
            overview=overview,
            purpose=purpose,
            requirements_mentioned=requirements_mentioned,
            keywords=keywords,
            complexity_score=complexity_score,
            size_lines=size_lines,
            last_modified=last_modified
        )

    def _extract_title(self, content: str) -> str:
        """ã‚¿ã‚¤ãƒˆãƒ«æŠ½å‡º"""
        lines = content.split("\n")
        for line in lines:
            if line.startswith("# "):
                return line[2:].strip()
        return "No Title"

    def _extract_overview(self, content: str) -> str:
        """æ¦‚è¦ã‚»ã‚¯ã‚·ãƒ§ãƒ³æŠ½å‡º"""
        patterns = [
            r"## æ¦‚è¦\n(.*?)(?=\n##|\n#|$)",
            r"## Overview\n(.*?)(?=\n##|\n#|$)",
            r"æ¦‚è¦[ï¼š:](.*?)(?=\n|$)",
        ]

        for pattern in patterns:
            match = re.search(pattern, content, re.DOTALL | re.MULTILINE)
            if match:
                return match.group(1).strip()[:500]  # æœ€åˆã®500æ–‡å­—

        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: æœ€åˆã®æ®µè½
        lines = [l.strip() for l in content.split("\n") if l.strip()]
        if len(lines) > 1:
            return lines[1][:200]
        return "No overview"

    def _extract_purpose(self, content: str) -> str:
        """ç›®çš„ãƒ»èƒŒæ™¯æŠ½å‡º"""
        patterns = [
            r"## ç›®çš„\n(.*?)(?=\n##|\n#|$)",
            r"## Purpose\n(.*?)(?=\n##|\n#|$)",
            r"## èƒŒæ™¯\n(.*?)(?=\n##|\n#|$)",
            r"ç›®çš„[ï¼š:](.*?)(?=\n|$)",
        ]

        for pattern in patterns:
            match = re.search(pattern, content, re.DOTALL | re.MULTILINE)
            if match:
                return match.group(1).strip()[:300]
        return "No purpose defined"

    def _extract_keywords(self, content: str) -> list[str]:
        """ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º"""
        keywords = []
        content_lower = content.lower()

        # ã‚«ãƒ†ã‚´ãƒªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‹ã‚‰æŠ½å‡º
        for category, category_keywords in self.category_keywords.items():
            for keyword in category_keywords:
                if keyword.lower() in content_lower:
                    keywords.append(keyword)

        # æŠ€è¡“ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡º
        tech_keywords = [
            "python", "yaml", "json", "api", "cli", "git", "docker",
            "test", "ãƒ†ã‚¹ãƒˆ", "unit", "integration", "çµ±åˆ",
            "ddd", "clean architecture", "solid",
            "claude", "openai", "ai", "äººå·¥çŸ¥èƒ½"
        ]

        for keyword in tech_keywords:
            if keyword.lower() in content_lower:
                keywords.append(keyword)

        return list(set(keywords))  # é‡è¤‡é™¤å»

    def _find_requirement_mentions(self, content: str) -> list[str]:
        """è¦ä»¶è¨€åŠã®ç™ºè¦‹"""
        req_patterns = [
            r"REQ-[A-Z]+-\d+",
            r"è¦ä»¶[ï¼š:]([^ã€‚\n]+)",
            r"requirement[ï¼š:]([^ã€‚\n]+)",
            r"æ©Ÿèƒ½[ï¼š:]([^ã€‚\n]+)",
        ]

        mentioned = []
        for pattern in req_patterns:
            matches = re.findall(pattern, content, re.IGNORECASE)
            mentioned.extend(matches)

        return mentioned

    def _calculate_complexity_score(self, content: str) -> float:
        """è¤‡é›‘åº¦ã‚¹ã‚³ã‚¢ç®—å‡º"""
        score = 0.0

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºãƒ™ãƒ¼ã‚¹
        lines = len(content.split("\n"))
        score += min(lines / 100, 5.0)  # æœ€å¤§5ç‚¹

        # ã‚»ã‚¯ã‚·ãƒ§ãƒ³æ•°ãƒ™ãƒ¼ã‚¹
        sections = len(re.findall(r"^##", content, re.MULTILINE))
        score += min(sections / 10, 3.0)  # æœ€å¤§3ç‚¹

        # æŠ€è¡“ç”¨èªå¯†åº¦ãƒ™ãƒ¼ã‚¹
        tech_terms = len(re.findall(r"\b(class|function|method|API|JSON|YAML)\b", content, re.IGNORECASE))
        score += min(tech_terms / 20, 2.0)  # æœ€å¤§2ç‚¹

        return round(score, 2)

    def _extract_requirements(self) -> None:
        """è¦ä»¶æŠ½å‡ºãƒ»ç”Ÿæˆ"""
        print("  ğŸ” è¦ä»¶ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æä¸­...")

        # ã‚«ãƒ†ã‚´ãƒªåˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
        category_groups = defaultdict(list)
        for analysis in self.analyses:
            category = self._categorize_specification(analysis)
            category_groups[category].append(analysis)

        req_id_counter = 1

        for category, specs in category_groups.items():
            print(f"    ğŸ“‹ {category}: {len(specs)}ä»¶")

            # ã‚«ãƒ†ã‚´ãƒªå†…ã§è¦ä»¶ã‚’çµ±åˆãƒ»æŠ½å‡º
            category_requirements = self._extract_category_requirements(category, specs)

            for req in category_requirements:
                req.id = f"REQ-{category[:4].upper()}-{req_id_counter:03d}"
                req_id_counter += 1
                self.requirements.append(req)

    def _categorize_specification(self, analysis: SpecificationAnalysis) -> str:
        """ä»•æ§˜æ›¸ã®ã‚«ãƒ†ã‚´ãƒªåˆ†é¡"""
        content = analysis.file_path + " " + analysis.title + " " + analysis.overview
        content_lower = content.lower()

        scores = {}
        for category, keywords in self.category_keywords.items():
            score = sum(1 for keyword in keywords if keyword.lower() in content_lower)
            if score > 0:
                scores[category] = score

        if scores:
            return max(scores.items(), key=lambda x: x[1])[0]
        return "ãã®ä»–"

    def _extract_category_requirements(self, category: str, specs: list[SpecificationAnalysis]) -> list[Requirement]:
        """ã‚«ãƒ†ã‚´ãƒªåˆ¥è¦ä»¶æŠ½å‡º"""
        requirements = []

        # ä»•æ§˜æ›¸ã‚’åˆ†æã—ã¦è¦ä»¶ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
        common_patterns = self._find_common_patterns(specs)

        for pattern in common_patterns:
            req = Requirement(
                id="",  # å¾Œã§è¨­å®š
                title=pattern["title"],
                description=pattern["description"],
                priority=self._determine_priority(pattern),
                category=category,
                specifications=[s.file_path for s in specs if pattern["title"].lower() in s.overview.lower()],
                keywords=pattern["keywords"],
                complexity=pattern["complexity"],
                dependencies=[]
            )
            requirements.append(req)

        return requirements

    def _find_common_patterns(self, specs: list[SpecificationAnalysis]) -> list[dict[str, Any]]:
        """å…±é€šãƒ‘ã‚¿ãƒ¼ãƒ³ç™ºè¦‹"""
        patterns = []

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é »åº¦åˆ†æ
        all_keywords = []
        for spec in specs:
            all_keywords.extend(spec.keywords)

        keyword_freq = Counter(all_keywords)
        common_keywords = [k for k, v in keyword_freq.most_common(10) if v >= 2]

        # å…±é€šæ©Ÿèƒ½ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æŠ½å‡º
        if common_keywords:
            pattern = {
                "title": f"{common_keywords[0]}æ©Ÿèƒ½",
                "description": f"{common_keywords[0]}ã«é–¢ã™ã‚‹æ©Ÿèƒ½è¦ä»¶",
                "keywords": common_keywords[:5],
                "complexity": min(5, max(1, len(specs))),
                "frequency": len(specs)
            }
            patterns.append(pattern)

        # å€‹åˆ¥ãƒ‘ã‚¿ãƒ¼ãƒ³ã®æŠ½å‡º
        for spec in specs:
            if spec.complexity_score > 3.0:  # è¤‡é›‘ãªä»•æ§˜æ›¸
                pattern = {
                    "title": spec.title,
                    "description": spec.overview,
                    "keywords": spec.keywords,
                    "complexity": min(5, int(spec.complexity_score)),
                    "frequency": 1
                }
                patterns.append(pattern)

        return patterns

    def _determine_priority(self, pattern: dict[str, Any]) -> str:
        """å„ªå…ˆåº¦åˆ¤å®š"""
        keywords_text = " ".join(pattern.get("keywords", [])).lower()

        for priority, priority_keywords in self.priority_keywords.items():
            if any(kw in keywords_text for kw in priority_keywords):
                return priority

        # é »åº¦ãƒ™ãƒ¼ã‚¹ã®å„ªå…ˆåº¦åˆ¤å®š
        frequency = pattern.get("frequency", 1)
        if frequency >= 5:
            return "Critical"
        if frequency >= 3:
            return "High"
        if frequency >= 2:
            return "Medium"
        return "Low"

    def _generate_analysis_report(self) -> dict[str, Any]:
        """åˆ†æãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        # ã‚«ãƒ†ã‚´ãƒªåˆ¥çµ±è¨ˆ
        category_stats = defaultdict(int)
        for analysis in self.analyses:
            category = self._categorize_specification(analysis)
            category_stats[category] += 1

        # è¦ä»¶çµ±è¨ˆ
        requirement_stats = {
            "total": len(self.requirements),
            "by_category": defaultdict(int),
            "by_priority": defaultdict(int),
        }

        for req in self.requirements:
            requirement_stats["by_category"][req.category] += 1
            requirement_stats["by_priority"][req.priority] += 1

        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰åˆ†æ
        all_keywords = []
        for analysis in self.analyses:
            all_keywords.extend(analysis.keywords)
        keyword_frequency = dict(Counter(all_keywords).most_common(20))

        return {
            "summary": {
                "total_specifications": len(self.analyses),
                "total_requirements": len(self.requirements),
                "analysis_date": "2025-09-03",
                "average_complexity": sum(a.complexity_score for a in self.analyses) / len(self.analyses) if self.analyses else 0
            },
            "specifications": [
                {
                    "file_path": a.file_path,
                    "title": a.title,
                    "category": self._categorize_specification(a),
                    "complexity_score": a.complexity_score,
                    "size_lines": a.size_lines,
                    "keywords_count": len(a.keywords)
                }
                for a in self.analyses
            ],
            "requirements": [
                {
                    "id": r.id,
                    "title": r.title,
                    "category": r.category,
                    "priority": r.priority,
                    "complexity": r.complexity,
                    "specifications_count": len(r.specifications)
                }
                for r in self.requirements
            ],
            "statistics": {
                "category_distribution": dict(category_stats),
                "requirement_stats": dict(requirement_stats),
                "keyword_frequency": keyword_frequency,
                "complexity_distribution": {
                    "low": sum(1 for a in self.analyses if a.complexity_score < 2.0),
                    "medium": sum(1 for a in self.analyses if 2.0 <= a.complexity_score < 4.0),
                    "high": sum(1 for a in self.analyses if a.complexity_score >= 4.0)
                }
            }
        }

    def export_results(self, output_dir: Path) -> None:
        """çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        output_dir.mkdir(parents=True, exist_ok=True)

        # åˆ†æãƒ¬ãƒãƒ¼ãƒˆJSONå‡ºåŠ›
        report = self._generate_analysis_report()
        with open(output_dir / "specification_analysis_report.json", "w", encoding="utf-8") as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        # è¦ä»¶ãƒªã‚¹ãƒˆYAMLå‡ºåŠ›
        requirements_data = {
            "requirements": [
                {
                    "id": r.id,
                    "title": r.title,
                    "description": r.description,
                    "priority": r.priority,
                    "category": r.category,
                    "specifications": r.specifications,
                    "keywords": r.keywords,
                    "complexity": r.complexity,
                    "dependencies": r.dependencies
                }
                for r in self.requirements
            ]
        }

        with open(output_dir / "extracted_requirements.yaml", "w", encoding="utf-8") as f:
            yaml.dump(requirements_data, f, allow_unicode=True, sort_keys=False, indent=2)

        # ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
        self._generate_summary_report(output_dir / "analysis_summary.md", report)

        print(f"ğŸ“„ çµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {output_dir}")

    def _generate_summary_report(self, output_path: Path, report: dict[str, Any]) -> None:
        """ã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ"""
        content = f"""# ä»•æ§˜æ›¸åˆ†æã‚µãƒãƒªãƒ¼ãƒ¬ãƒãƒ¼ãƒˆ

**åˆ†ææ—¥æ™‚**: {report['summary']['analysis_date']}
**å¯¾è±¡ä»•æ§˜æ›¸æ•°**: {report['summary']['total_specifications']}ä»¶
**æŠ½å‡ºè¦ä»¶æ•°**: {report['summary']['total_requirements']}ä»¶
**å¹³å‡è¤‡é›‘åº¦**: {report['summary']['average_complexity']:.2f}

## ã‚«ãƒ†ã‚´ãƒªåˆ¥åˆ†å¸ƒ

| ã‚«ãƒ†ã‚´ãƒª | ä»•æ§˜æ›¸æ•° | å‰²åˆ |
|---------|---------|------|
"""

        total_specs = report["summary"]["total_specifications"]
        for category, count in report["statistics"]["category_distribution"].items():
            percentage = (count / total_specs * 100) if total_specs > 0 else 0
            content += f"| {category} | {count} | {percentage:.1f}% |\n"

        content += """
## è¦ä»¶å„ªå…ˆåº¦åˆ†å¸ƒ

| å„ªå…ˆåº¦ | è¦ä»¶æ•° |
|-------|-------|
"""

        for priority, count in report["statistics"]["requirement_stats"]["by_priority"].items():
            content += f"| {priority} | {count} |\n"

        content += """
## ãƒˆãƒƒãƒ—ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰

| ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ | å‡ºç¾é »åº¦ |
|-----------|----------|
"""

        for keyword, freq in list(report["statistics"]["keyword_frequency"].items())[:10]:
            content += f"| {keyword} | {freq} |\n"

        content += f"""
## è¤‡é›‘åº¦åˆ†å¸ƒ

- **ä½è¤‡é›‘åº¦** (< 2.0): {report['statistics']['complexity_distribution']['low']}ä»¶
- **ä¸­è¤‡é›‘åº¦** (2.0-4.0): {report['statistics']['complexity_distribution']['medium']}ä»¶
- **é«˜è¤‡é›‘åº¦** (â‰¥ 4.0): {report['statistics']['complexity_distribution']['high']}ä»¶

## æ¬¡ã®ã‚¢ã‚¯ã‚·ãƒ§ãƒ³

1. é«˜å„ªå…ˆåº¦è¦ä»¶ã®è©³ç´°åˆ†æ
2. è¦ä»¶é–“ã®ä¾å­˜é–¢ä¿‚åˆ†æ
3. å®Ÿè£…è¨ˆç”»ã®ç­–å®š
4. ãƒ†ã‚¹ãƒˆæˆ¦ç•¥ã®ç¢ºå®š

---
*ã“ã®ãƒ¬ãƒãƒ¼ãƒˆã¯è‡ªå‹•ç”Ÿæˆã•ã‚Œã¾ã—ãŸ*
"""

        output_path.write_text(content, encoding="utf-8")


def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    specs_dir = Path("specs")
    if not specs_dir.exists():
        specs_dir = Path()  # ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‹ã‚‰æ¤œç´¢

    analyzer = SpecificationsAnalyzer(specs_dir)

    print("ğŸš€ ä»•æ§˜æ›¸è‡ªå‹•åˆ†æãƒ„ãƒ¼ãƒ«é–‹å§‹")
    print("=" * 50)

    # åˆ†æå®Ÿè¡Œ
    report = analyzer.analyze_all_specifications()

    # çµæœå‡ºåŠ›
    output_dir = Path("analysis_results")
    analyzer.export_results(output_dir)

    print("=" * 50)
    print("ğŸ“Š åˆ†æçµæœã‚µãƒãƒªãƒ¼:")
    print(f"  ä»•æ§˜æ›¸æ•°: {report['summary']['total_specifications']}ä»¶")
    print(f"  æŠ½å‡ºè¦ä»¶æ•°: {report['summary']['total_requirements']}ä»¶")
    print(f"  å¹³å‡è¤‡é›‘åº¦: {report['summary']['average_complexity']:.2f}")
    print(f"  å‡ºåŠ›å…ˆ: {output_dir}/")
    print("\nâœ… ä»•æ§˜æ›¸åˆ†æå®Œäº†")


if __name__ == "__main__":
    main()
