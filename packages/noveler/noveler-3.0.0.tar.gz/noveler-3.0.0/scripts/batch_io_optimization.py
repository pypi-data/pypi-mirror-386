#!/usr/bin/env python3
"""
FileIOOptimizer ä¸€æ‹¬é©ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
1,747å€‹ã®I/Oæ“ä½œã‚’åŠ¹ç‡çš„ã«FileIOOptimizerã§æœ€é©åŒ–

å®Ÿè¡Œæ–¹æ³•:
python batch_io_optimization.py
"""

import re
from pathlib import Path
from typing import Any


def apply_fileio_optimizer_batch() -> dict[str, Any]:
    """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã«FileIOOptimizerã‚’ä¸€æ‹¬é©ç”¨"""

    # æœ€é©åŒ–å¯¾è±¡ãƒ‘ã‚¿ãƒ¼ãƒ³
    optimization_patterns = [
        # åŸºæœ¬çš„ãªfile.open()ãƒ‘ã‚¿ãƒ¼ãƒ³
        {
            "pattern": r"with\s+([^.]+)\.open\(([^)]+)\)\s+as\s+([^:]+):\s*\n\s*([^=]+)\s*=\s*\3\.read\(\)",
            "replacement": r"# FileIOOptimizerä½¿ç”¨ã§æœ€é©åŒ–\n\1_content = _file_io_optimizer.optimized_read_text(\1, \2)\n\4 = \1_content",
        },
        # Path().open()ãƒ‘ã‚¿ãƒ¼ãƒ³
        {
            "pattern": r"with\s+([^.]+)\.Path\(\"r\"\)\.open\(encoding=\"utf-8\"\)\s+as\s+([^:]+):",
            "replacement": r"# FileIOOptimizerä½¿ç”¨ã§æœ€é©åŒ–\n\2_content = _file_io_optimizer.optimized_read_text(\1, encoding=\"utf-8\")",
        },
        # æ›¸ãè¾¼ã¿ãƒ‘ã‚¿ãƒ¼ãƒ³
        {
            "pattern": r"with\s+([^.]+)\.open\(\"w\",\s*encoding=\"utf-8\"\)\s+as\s+([^:]+):\s*\n\s*\2\.write\(([^)]+)\)",
            "replacement": r"# ãƒãƒƒãƒæ›¸ãè¾¼ã¿ã‚’ä½¿ç”¨\n_file_io_optimizer.batch_write_text(\1, \3, encoding=\"utf-8\")",
        }
    ]

    # é«˜é »åº¦I/Oãƒ•ã‚¡ã‚¤ãƒ«ä¸€è¦§ï¼ˆTOP 20ï¼‰
    high_priority_files = [
        "src/noveler/tools/type_annotation_fixer.py",  # 317ã‚¹ã‚³ã‚¢ã€11 I/O
        "src/noveler/tools/dependency_analyzer.py",   # 313ã‚¹ã‚³ã‚¢ã€25 I/O
        "src/noveler/infrastructure/config/project_detector.py",  # 297ã‚¹ã‚³ã‚¢ã€8 I/O
        "src/noveler/infrastructure/repositories/yaml_a31_checklist_repository.py",  # 273ã‚¹ã‚³ã‚¢ã€17 I/O
        "src/noveler/infrastructure/repositories/yaml_episode_repository.py",
        "src/noveler/infrastructure/repositories/yaml_plot_data_repository.py",
        "src/noveler/infrastructure/utils/yaml_utils.py",
        "src/noveler/application/use_cases/generate_episode_plot_use_case.py",
        "src/noveler/application/use_cases/integrated_writing_use_case.py",
        "src/noveler/application/use_cases/plot_quality_assurance_use_case.py",
        "src/noveler/application/use_cases/prompt_generation_use_case.py",
        "src/noveler/application/use_cases/text_analysis_use_case.py",
        "src/noveler/infrastructure/batch/batch_processor.py",
        "src/noveler/infrastructure/performance/async_file_processor.py"
    ]

    results = {
        "files_processed": 0,
        "optimizations_applied": 0,
        "files_optimized": [],
        "errors": []
    }

    project_root = Path()

    # Phase 1: é«˜å„ªå…ˆåº¦ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€é©åŒ–
    for file_path_str in high_priority_files:
        file_path = project_root / file_path_str
        if file_path.exists():
            try:
                result = optimize_single_file(file_path, optimization_patterns)
                results["files_processed"] += 1
                if result["optimized"]:
                    results["optimizations_applied"] += result["optimizations_count"]
                    results["files_optimized"].append(str(file_path))
            except Exception as e:
                results["errors"].append(f"{file_path}: {e!s}")

    # Phase 2: æ®‹ã‚Šã®Pythonãƒ•ã‚¡ã‚¤ãƒ«è‡ªå‹•æ¤œå‡ºãƒ»æœ€é©åŒ–
    remaining_files = list(project_root.rglob("src/**/*.py"))
    for file_path in remaining_files:
        if str(file_path) not in [project_root / f for f in high_priority_files]:
            if should_optimize_file(file_path):
                try:
                    result = optimize_single_file(file_path, optimization_patterns)
                    results["files_processed"] += 1
                    if result["optimized"]:
                        results["optimizations_applied"] += result["optimizations_count"]
                        results["files_optimized"].append(str(file_path))
                except Exception as e:
                    results["errors"].append(f"{file_path}: {e!s}")

    return results

def optimize_single_file(file_path: Path, patterns: list[dict]) -> dict[str, Any]:
    """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«ã®æœ€é©åŒ–"""

    try:
        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        with file_path.open(encoding="utf-8") as f:
            original_content = f.read()

        modified_content = original_content
        optimizations_count = 0

        # FileIOOptimizer importã®è¿½åŠ 
        if "from noveler.infrastructure.performance.comprehensive_performance_optimizer import FileIOOptimizer" not in modified_content:
            import_section = re.search(r"(from noveler\..*?\n)+", modified_content)
            if import_section:
                insert_pos = import_section.end()
                modified_content = (
                    modified_content[:insert_pos] +
                    "from noveler.infrastructure.performance.comprehensive_performance_optimizer import FileIOOptimizer\n" +
                    "_file_io_optimizer = FileIOOptimizer()\n\n" +
                    modified_content[insert_pos:]
                )
                optimizations_count += 1

        # å„ãƒ‘ã‚¿ãƒ¼ãƒ³ã®é©ç”¨
        for pattern_info in patterns:
            pattern = pattern_info["pattern"]
            replacement = pattern_info["replacement"]

            matches = list(re.finditer(pattern, modified_content, re.MULTILINE | re.DOTALL))
            if matches:
                for match in reversed(matches):  # å¾Œã‚ã‹ã‚‰ç½®æ›ã—ã¦ä½ç½®ã‚ºãƒ¬ã‚’é˜²ã
                    start, end = match.span()
                    modified_content = modified_content[:start] + re.sub(pattern, replacement, match.group()) + modified_content[end:]
                    optimizations_count += 1

        # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
        if optimizations_count > 0:
            backup_path = file_path.with_suffix(f"{file_path.suffix}.backup")
            with backup_path.open("w", encoding="utf-8") as f:
                f.write(original_content)

            # æœ€é©åŒ–ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«æ›¸ãè¾¼ã¿
            with file_path.open("w", encoding="utf-8") as f:
                f.write(modified_content)

        return {
            "optimized": optimizations_count > 0,
            "optimizations_count": optimizations_count,
            "backup_created": optimizations_count > 0
        }

    except Exception as e:
        return {
            "optimized": False,
            "optimizations_count": 0,
            "error": str(e)
        }

def should_optimize_file(file_path: Path) -> bool:
    """ãƒ•ã‚¡ã‚¤ãƒ«ãŒæœ€é©åŒ–å¯¾è±¡ã‹ã©ã†ã‹åˆ¤å®š"""

    # é™¤å¤–ãƒ‘ã‚¿ãƒ¼ãƒ³
    exclude_patterns = [
        "__pycache__",
        ".git",
        "test_",
        "_test.py",
        "migrations/",
        ".backup",
        "temp/",
        "archive/"
    ]

    file_str = str(file_path)
    if any(pattern in file_str for pattern in exclude_patterns):
        return False

    # I/Oæ“ä½œãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
    try:
        with file_path.open(encoding="utf-8") as f:
            content = f.read()

        # I/Oæ“ä½œãƒ‘ã‚¿ãƒ¼ãƒ³ã®å­˜åœ¨ãƒã‚§ãƒƒã‚¯
        io_patterns = [
            r"\.open\(",
            r"\.read_text\(",
            r"\.write_text\(",
            r"with\s+.*\.open\(",
            r"yaml\.load",
            r"yaml\.dump",
            r"json\.load",
            r"json\.dump"
        ]

        for pattern in io_patterns:
            if re.search(pattern, content):
                return True

        return False

    except Exception:
        return False

if __name__ == "__main__":
    print("ğŸš€ FileIOOptimizerä¸€æ‹¬æœ€é©åŒ–é–‹å§‹...")

    results = apply_fileio_optimizer_batch()

    print("\nâœ… æœ€é©åŒ–å®Œäº†!")
    print(f"ğŸ“Š å‡¦ç†ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {results['files_processed']}")
    print(f"ğŸ”§ æœ€é©åŒ–é©ç”¨æ•°: {results['optimizations_applied']}")
    print(f"ğŸ“ æœ€é©åŒ–ãƒ•ã‚¡ã‚¤ãƒ«æ•°: {len(results['files_optimized'])}")

    if results["errors"]:
        print(f"\nâš ï¸  ã‚¨ãƒ©ãƒ¼æ•°: {len(results['errors'])}")
        for error in results["errors"][:5]:  # æœ€åˆã®5ä»¶ã®ã¿è¡¨ç¤º
            print(f"   {error}")

    print("\nğŸ¯ ç›®æ¨™é”æˆçŠ¶æ³:")
    print(f"   I/Oæ“ä½œæœ€é©åŒ–æ•°: {results['optimizations_applied']}/1,747")
    completion_rate = (results["optimizations_applied"] / 1747) * 100
    print(f"   å®Œäº†ç‡: {completion_rate:.1f}%")

    if len(results["files_optimized"]) > 0:
        print("\nğŸ“ æœ€é©åŒ–ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ä¾‹:")
        for file_path in results["files_optimized"][:10]:  # æœ€åˆã®10ä»¶ã®ã¿è¡¨ç¤º
            print(f"   âœ“ {file_path}")
