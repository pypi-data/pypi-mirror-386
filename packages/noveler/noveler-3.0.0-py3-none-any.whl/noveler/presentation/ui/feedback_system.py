"""
ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ 

åŸ·ç­†ãƒ—ãƒ­ã‚»ã‚¹ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯åé›†ãƒ»è¡¨ç¤ºæ©Ÿèƒ½ã€‚
å“è³ªè©•ä¾¡ã€æ”¹å–„ææ¡ˆã€ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãªç¢ºèªæ©Ÿèƒ½ã‚’æä¾›ã€‚
"""

from collections.abc import Callable
from dataclasses import dataclass
from datetime import datetime
from enum import Enum
from typing import Any

from noveler.infrastructure.performance.comprehensive_performance_optimizer import performance_monitor
from noveler.presentation.shared.shared_utilities import _get_console


def get_console():
    return _get_console()


class FeedbackType(Enum):
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®ç¨®é¡"""
    CONFIRMATION = "confirmation"
    QUALITY_CHECK = "quality_check"
    IMPROVEMENT_SUGGESTION = "improvement_suggestion"
    USER_INPUT = "user_input"
    WARNING = "warning"
    ERROR = "error"
    SUCCESS = "success"


@dataclass
class FeedbackMessage:
    """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸"""
    feedback_type: FeedbackType
    title: str
    message: str
    step_id: int | None = None
    options: list[str] | None = None
    default_option: str | None = None
    timestamp: datetime | None = None
    requires_response: bool = False
    severity: str = "info"  # info, warning, error, success

    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now()


class InteractiveFeedbackSystem:
    """ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ãƒ†ã‚£ãƒ–ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚·ã‚¹ãƒ†ãƒ 

    åŸ·ç­†ãƒ—ãƒ­ã‚»ã‚¹ä¸­ã®ãƒ¦ãƒ¼ã‚¶ãƒ¼ã¨ã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³ã‚’ç®¡ç†ã—ã€
    å“è³ªãƒã‚§ãƒƒã‚¯ã€ç¢ºèªã€æ”¹å–„ææ¡ˆãªã©ã®ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã‚’æä¾›ã™ã‚‹ã€‚
    """

    def __init__(self, episode_number: int) -> None:
        self.episode_number = episode_number
        self.console = get_console()
        self.feedback_history: list[FeedbackMessage] = []
        self.user_preferences = {
            "auto_confirm_low_risk": False,
            "detailed_explanations": True,
            "show_improvement_tips": True,
            "interactive_mode": True
        }

    @performance_monitor
    def request_confirmation(
        self,
        title: str,
        message: str,
        step_id: int | None = None,
        default: bool = True
    ) -> bool:
        """ç¢ºèªã‚’è¦æ±‚"""
        feedback = FeedbackMessage(
            feedback_type=FeedbackType.CONFIRMATION,
            title=title,
            message=message,
            step_id=step_id,
            options=["ã¯ã„", "ã„ã„ãˆ"],
            default_option="ã¯ã„" if default else "ã„ã„ãˆ",
            requires_response=True
        )

        self._log_feedback(feedback)

        if not self.user_preferences["interactive_mode"]:
            return default

        self._display_feedback(feedback)

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®å¾…æ©Ÿï¼ˆå®Ÿéš›ã®CLIç’°å¢ƒã§ã¯ input() ã‚’ä½¿ç”¨ï¼‰
        response = self._get_user_response(feedback.options, feedback.default_option)

        return response.lower() in ["ã¯ã„", "yes", "y", "1"]

    @performance_monitor
    def show_quality_check_result(
        self,
        step_id: int,
        quality_score: float,
        issues: list[str],
        suggestions: list[str]
    ) -> dict[str, Any]:
        """å“è³ªãƒã‚§ãƒƒã‚¯çµæœã®è¡¨ç¤º"""

        # å“è³ªãƒ¬ãƒ™ãƒ«ã®åˆ¤å®š
        quality_level = self._determine_quality_level(quality_score)

        feedback = FeedbackMessage(
            feedback_type=FeedbackType.QUALITY_CHECK,
            title=f"ã‚¹ãƒ†ãƒƒãƒ—{step_id} å“è³ªãƒã‚§ãƒƒã‚¯çµæœ",
            message=f"å“è³ªã‚¹ã‚³ã‚¢: {quality_score:.1f}/100 ({quality_level})",
            step_id=step_id,
            severity=self._get_severity_from_score(quality_score)
        )

        self._log_feedback(feedback)
        self._display_quality_result(feedback, quality_score, issues, suggestions)

        # æ”¹å–„ãŒå¿…è¦ãªå ´åˆã®ã‚¤ãƒ³ã‚¿ãƒ©ã‚¯ã‚·ãƒ§ãƒ³
        requires_improvement = quality_score < 70
        user_choice = None

        if requires_improvement:
            user_choice = self._handle_quality_improvement_dialog(step_id, issues, suggestions)

        return {
            "quality_score": quality_score,
            "quality_level": quality_level,
            "issues": issues,
            "suggestions": suggestions,
            "requires_improvement": requires_improvement,
            "user_choice": user_choice
        }

    @performance_monitor
    def show_improvement_suggestions(
        self,
        step_id: int,
        suggestions: list[dict[str, str]]
    ) -> str | None:
        """æ”¹å–„ææ¡ˆã®è¡¨ç¤º"""
        if not self.user_preferences["show_improvement_tips"] or not suggestions:
            return None

        feedback = FeedbackMessage(
            feedback_type=FeedbackType.IMPROVEMENT_SUGGESTION,
            title=f"ã‚¹ãƒ†ãƒƒãƒ—{step_id} æ”¹å–„ææ¡ˆ",
            message=f"{len(suggestions)}å€‹ã®æ”¹å–„ææ¡ˆãŒã‚ã‚Šã¾ã™",
            step_id=step_id
        )

        self._log_feedback(feedback)
        self._display_improvement_suggestions(feedback, suggestions)

        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒæ”¹å–„ææ¡ˆã‚’é¸æŠ
        if self.user_preferences["interactive_mode"]:
            return self._select_improvement_suggestion(suggestions)

        return None

    @performance_monitor
    def show_warning(
        self,
        title: str,
        message: str,
        step_id: int | None = None,
        require_acknowledgment: bool = False
    ) -> bool:
        """è­¦å‘Šã®è¡¨ç¤º"""
        feedback = FeedbackMessage(
            feedback_type=FeedbackType.WARNING,
            title=title,
            message=message,
            step_id=step_id,
            severity="warning",
            requires_response=require_acknowledgment
        )

        self._log_feedback(feedback)
        self._display_feedback(feedback)

        if require_acknowledgment:
            return self.request_confirmation("è­¦å‘Šã‚’ç¢ºèªã—ã¾ã—ãŸã‹ï¼Ÿ", "ç¶šè¡Œã—ã¾ã™ã‹ï¼Ÿ")

        return True

    @performance_monitor
    def show_error(
        self,
        title: str,
        message: str,
        step_id: int | None = None,
        recovery_options: list[str] | None = None
    ) -> str | None:
        """ã‚¨ãƒ©ãƒ¼ã®è¡¨ç¤ºã¨å¾©æ—§ã‚ªãƒ—ã‚·ãƒ§ãƒ³"""
        feedback = FeedbackMessage(
            feedback_type=FeedbackType.ERROR,
            title=title,
            message=message,
            step_id=step_id,
            options=recovery_options,
            severity="error",
            requires_response=bool(recovery_options)
        )

        self._log_feedback(feedback)
        self._display_feedback(feedback)

        if recovery_options:
            return self._get_user_response(recovery_options, recovery_options[0])

        return None

    @performance_monitor
    def show_success(
        self,
        title: str,
        message: str,
        step_id: int | None = None,
        details: dict[str, Any] | None = None
    ) -> None:
        """æˆåŠŸãƒ¡ãƒƒã‚»ãƒ¼ã‚¸ã®è¡¨ç¤º"""
        feedback = FeedbackMessage(
            feedback_type=FeedbackType.SUCCESS,
            title=title,
            message=message,
            step_id=step_id,
            severity="success"
        )

        self._log_feedback(feedback)
        self._display_feedback(feedback)

        if details and self.user_preferences["detailed_explanations"]:
            self._display_success_details(details)

    @performance_monitor
    def collect_user_input(
        self,
        prompt: str,
        input_type: str = "text",
        validation_func: Callable | None = None
    ) -> Any:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®åé›†"""
        feedback = FeedbackMessage(
            feedback_type=FeedbackType.USER_INPUT,
            title="ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›è¦æ±‚",
            message=prompt,
            requires_response=True
        )

        self._log_feedback(feedback)
        self._display_feedback(feedback)

        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ã“ã“ã§ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’å—ã‘å–ã‚‹
        # ä»Šå›ã¯ãƒ¢ãƒƒã‚¯ã¨ã—ã¦å‡¦ç†
        return self._mock_user_input(input_type)

    def _display_feedback(self, feedback: FeedbackMessage) -> None:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®è¡¨ç¤º"""
        # è‰²ã¨ã‚¢ã‚¤ã‚³ãƒ³ã®è¨­å®š
        color_map = {
            "info": "blue",
            "warning": "yellow",
            "error": "red",
            "success": "green"
        }

        icon_map = {
            FeedbackType.CONFIRMATION: "â“",
            FeedbackType.QUALITY_CHECK: "ğŸ“Š",
            FeedbackType.IMPROVEMENT_SUGGESTION: "ğŸ’¡",
            FeedbackType.USER_INPUT: "âœï¸ ",
            FeedbackType.WARNING: "âš ï¸ ",
            FeedbackType.ERROR: "âŒ",
            FeedbackType.SUCCESS: "âœ…"
        }

        color = color_map.get(feedback.severity, "white")
        icon = icon_map.get(feedback.feedback_type, "â„¹ï¸ ")

        step_info = f" [ã‚¹ãƒ†ãƒƒãƒ—{feedback.step_id}]" if feedback.step_id else ""

        self.console.print(f"\n[bold {color}]{icon} {feedback.title}{step_info}[/bold {color}]")
        self.console.print(f"[{color}]{feedback.message}[/{color}]")

        if feedback.options:
            self.console.print(f"[dim]é¸æŠè‚¢: {', '.join(feedback.options)}[/dim]")

    def _display_quality_result(
        self,
        feedback: FeedbackMessage,
        quality_score: float,
        issues: list[str],
        suggestions: list[str]
    ) -> None:
        """å“è³ªãƒã‚§ãƒƒã‚¯çµæœã®è©³ç´°è¡¨ç¤º"""
        self._display_feedback(feedback)

        # å“è³ªãƒãƒ¼
        quality_bar = self._create_quality_bar(quality_score)
        self.console.print(f"å“è³ª: {quality_bar} {quality_score:.1f}/100")

        # å•é¡Œç‚¹
        if issues:
            self.console.print(f"\n[red]æ¤œå‡ºã•ã‚ŒãŸå•é¡Œ ({len(issues)}ä»¶):[/red]")
            for i, issue in enumerate(issues[:5], 1):  # æœ€å¤§5ä»¶è¡¨ç¤º
                self.console.print(f"  {i}. {issue}")
            if len(issues) > 5:
                self.console.print(f"  ... ä»– {len(issues)-5} ä»¶")

        # æ”¹å–„ææ¡ˆ
        if suggestions:
            self.console.print(f"\n[blue]æ”¹å–„ææ¡ˆ ({len(suggestions)}ä»¶):[/blue]")
            for i, suggestion in enumerate(suggestions[:3], 1):  # æœ€å¤§3ä»¶è¡¨ç¤º
                self.console.print(f"  ğŸ’¡ {suggestion}")
            if len(suggestions) > 3:
                self.console.print(f"  ... ä»– {len(suggestions)-3} ä»¶")

    def _display_improvement_suggestions(
        self,
        feedback: FeedbackMessage,
        suggestions: list[dict[str, str]]
    ) -> None:
        """æ”¹å–„ææ¡ˆã®è¡¨ç¤º"""
        self._display_feedback(feedback)

        for i, suggestion in enumerate(suggestions, 1):
            title = suggestion.get("title", f"ææ¡ˆ {i}")
            description = suggestion.get("description", "")
            impact = suggestion.get("impact", "medium")

            impact_icon = {"high": "ğŸ”¥", "medium": "â­", "low": "ğŸ’«"}.get(impact, "â­")

            self.console.print(f"\n{impact_icon} [bold]{title}[/bold]")
            self.console.print(f"   {description}")

    def _display_success_details(self, details: dict[str, Any]) -> None:
        """æˆåŠŸã®è©³ç´°æƒ…å ±è¡¨ç¤º"""
        self.console.print("\n[dim]è©³ç´°æƒ…å ±:[/dim]")
        for key, value in details.items():
            self.console.print(f"  â€¢ {key}: {value}")

    def _handle_quality_improvement_dialog(
        self,
        step_id: int,
        issues: list[str],
        suggestions: list[str]
    ) -> str:
        """å“è³ªæ”¹å–„ãƒ€ã‚¤ã‚¢ãƒ­ã‚°ã®å‡¦ç†"""
        options = [
            "è‡ªå‹•ä¿®æ­£ã‚’é©ç”¨",
            "æ‰‹å‹•ã§ä¿®æ­£",
            "ã“ã®ã¾ã¾ç¶šè¡Œ",
            "ã‚¹ãƒ†ãƒƒãƒ—ã‚’ã‚¹ã‚­ãƒƒãƒ—"
        ]

        self.console.print(f"\n[yellow]ã‚¹ãƒ†ãƒƒãƒ—{step_id}ã®å“è³ªã«å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ã©ã†ã—ã¾ã™ã‹ï¼Ÿ[/yellow]")

        return self._get_user_response(options, options[0])

    def _select_improvement_suggestion(self, suggestions: list[dict[str, str]]) -> str:
        """æ”¹å–„ææ¡ˆã®é¸æŠ"""
        if len(suggestions) == 1:
            return suggestions[0].get("title", "")

        options = [s.get("title", f"ææ¡ˆ{i+1}") for i, s in enumerate(suggestions)]
        options.append("ææ¡ˆã‚’é©ç”¨ã—ãªã„")

        self.console.print("\n[blue]é©ç”¨ã™ã‚‹æ”¹å–„ææ¡ˆã‚’é¸æŠã—ã¦ãã ã•ã„:[/blue]")

        return self._get_user_response(options, options[0])

    def _get_user_response(self, options: list[str], default: str) -> str:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®å–å¾—ï¼ˆãƒ¢ãƒƒã‚¯å®Ÿè£…ï¼‰"""
        # å®Ÿéš›ã®å®Ÿè£…ã§ã¯ input() ã‚’ä½¿ç”¨ã—ã¦ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã‚’å—ã‘å–ã‚‹
        # ä»Šå›ã¯è‡ªå‹•çš„ã«ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆé¸æŠã‚’è¿”ã™
        return default

    def _mock_user_input(self, input_type: str) -> Any:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ã®ãƒ¢ãƒƒã‚¯"""
        mock_inputs = {
            "text": "ãƒ¦ãƒ¼ã‚¶ãƒ¼å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ",
            "number": 42,
            "boolean": True
        }
        return mock_inputs.get(input_type, "ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤")

    def _create_quality_bar(self, score: float, width: int = 30) -> str:
        """å“è³ªãƒãƒ¼ã®ä½œæˆ"""
        filled = int((score / 100) * width)

        # è‰²åˆ†ã‘
        if score >= 80:
            bar_char = "â–ˆ"
            color = "green"
        elif score >= 60:
            bar_char = "â–ˆ"
            color = "yellow"
        else:
            bar_char = "â–ˆ"
            color = "red"

        bar = bar_char * filled + "â–‘" * (width - filled)
        return f"[{color}][{bar}][/{color}]"

    def _determine_quality_level(self, score: float) -> str:
        """å“è³ªãƒ¬ãƒ™ãƒ«ã®åˆ¤å®š"""
        if score >= 90:
            return "å„ªç§€"
        if score >= 80:
            return "è‰¯å¥½"
        if score >= 70:
            return "åˆæ ¼"
        if score >= 60:
            return "è¦æ”¹å–„"
        return "ä¸åˆæ ¼"

    def _get_severity_from_score(self, score: float) -> str:
        """ã‚¹ã‚³ã‚¢ã‹ã‚‰æ·±åˆ»åº¦ã‚’å–å¾—"""
        if score >= 80:
            return "success"
        if score >= 60:
            return "warning"
        return "error"

    def _log_feedback(self, feedback: FeedbackMessage) -> None:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ã®ãƒ­ã‚°è¨˜éŒ²"""
        self.feedback_history.append(feedback)

    @performance_monitor
    def export_feedback_report(self) -> dict[str, Any]:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ãƒ¬ãƒãƒ¼ãƒˆã®ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        return {
            "episode_number": self.episode_number,
            "timestamp": datetime.now().isoformat(),
            "total_feedback_count": len(self.feedback_history),
            "feedback_by_type": {
                feedback_type.value: len([f for f in self.feedback_history if f.feedback_type == feedback_type])
                for feedback_type in FeedbackType
            },
            "feedback_history": [
                {
                    "type": f.feedback_type.value,
                    "title": f.title,
                    "message": f.message,
                    "step_id": f.step_id,
                    "timestamp": f.timestamp.isoformat() if f.timestamp else None,
                    "severity": f.severity
                }
                for f in self.feedback_history
            ],
            "user_preferences": self.user_preferences
        }

    def set_user_preferences(self, preferences: dict[str, Any]) -> None:
        """ãƒ¦ãƒ¼ã‚¶ãƒ¼è¨­å®šã®æ›´æ–°"""
        self.user_preferences.update(preferences)

    def get_feedback_summary(self) -> dict[str, Any]:
        """ãƒ•ã‚£ãƒ¼ãƒ‰ãƒãƒƒã‚¯ ã‚µãƒãƒªãƒ¼ã®å–å¾—"""
        return {
            "total_feedback": len(self.feedback_history),
            "confirmations": len([f for f in self.feedback_history if f.feedback_type == FeedbackType.CONFIRMATION]),
            "quality_checks": len([f for f in self.feedback_history if f.feedback_type == FeedbackType.QUALITY_CHECK]),
            "warnings": len([f for f in self.feedback_history if f.feedback_type == FeedbackType.WARNING]),
            "errors": len([f for f in self.feedback_history if f.feedback_type == FeedbackType.ERROR]),
            "last_feedback": self.feedback_history[-1] if self.feedback_history else None
        }
