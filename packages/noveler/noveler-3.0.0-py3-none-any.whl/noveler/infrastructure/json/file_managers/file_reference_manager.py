#!/usr/bin/env python3
"""ãƒ•ã‚¡ã‚¤ãƒ«å‚ç…§ç®¡ç†ã‚¯ãƒ©ã‚¹ï¼ˆSPEC-MCP-HASH-001æº–æ‹ ï¼‰

Concurrency & durability notes:
- Updates to the in-memory hash index and its persistence file are protected by
  an `RLock` to avoid race conditions when `save_content()` is called from
  multiple threads (as exercised by concurrent tests).
- The index file is now written atomically: content is flushed to a temporary
  file in the same directory and then swapped into place with `os.replace()` to
  prevent partial writes being observed by other threads or file watchers.
"""

import json
import os
import threading
import uuid
from datetime import datetime, timedelta, timezone
from pathlib import Path

from noveler.infrastructure.json.models.file_reference_models import FileReferenceModel
from noveler.infrastructure.json.utils.hash_utils import calculate_sha256, verify_hash_format

# B20æº–æ‹ : å…±æœ‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆ©ç”¨ï¼ˆå¿…é ˆï¼‰
from noveler.presentation.shared.shared_utilities import console, get_logger


class FileReferenceManager:
    """ãƒ•ã‚¡ã‚¤ãƒ«å‚ç…§ç®¡ç†ã‚¯ãƒ©ã‚¹ï¼ˆSPEC-MCP-HASH-001æº–æ‹ ï¼‰

    B20æº–æ‹ :
    - å…±æœ‰ã‚³ãƒ³ãƒãƒ¼ãƒãƒ³ãƒˆåˆ©ç”¨ï¼ˆconsole, logger, path_serviceï¼‰
    - é‡è¤‡å®Ÿè£…å›é¿ï¼ˆæ—¢å­˜å®Ÿè£…æ‹¡å¼µï¼‰
    - DDD + Clean Architectureæº–æ‹ 
    """

    def __init__(self, base_output_dir: Path) -> None:
        # B20æº–æ‹ : çµ±ä¸€Loggerä½¿ç”¨
        self.logger = get_logger(__name__)

        self.base_output_dir = Path(base_output_dir)
        self._ensure_base_directory()

        # SPEC-MCP-HASH-001: ãƒãƒƒã‚·ãƒ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç®¡ç†
        self._hash_index: dict[str, list[Path]] = {}
        self._hash_cache: dict[str, FileReferenceModel] = {}
        self._hash_index_file = self.base_output_dir / ".hash_index.json"
        # Thread-safety: protect index mutations and saves
        self._index_lock = threading.RLock()
        self._load_hash_index()
        self._populate_hash_cache()

        self.logger.debug("FileReferenceManager initialized: %s", self.base_output_dir)

    def _ensure_base_directory(self) -> None:
        """åŸºåº•ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºä¿"""
        self.base_output_dir.mkdir(parents=True, exist_ok=True)

    def save_content(
        self, content: str, content_type: str, filename_prefix: str = "output", custom_filename: str | None = None
    ) -> FileReferenceModel:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ä¿å­˜ãƒ»ãƒ•ã‚¡ã‚¤ãƒ«å‚ç…§ç”Ÿæˆï¼ˆSPEC-MCP-HASH-001æ‹¡å¼µï¼‰"""
        # content_typeã‚’è¨±å¯ã•ã‚ŒãŸå€¤ã«åˆ¶é™
        allowed_types = {"text/markdown", "text/yaml", "application/json", "text/plain"}
        if content_type not in allowed_types:
            content_type = "text/plain"  # æœªçŸ¥ã®ã‚¿ã‚¤ãƒ—ã¯text/plainã¨ã—ã¦æ‰±ã†

        # ãƒ•ã‚¡ã‚¤ãƒ«åç”Ÿæˆ
        if custom_filename:
            filename = custom_filename
        else:
            timestamp = datetime.now(timezone.utc).strftime("%Y%m%d_%H%M%S")
            unique_id = str(uuid.uuid4())[:8]
            extension = self._get_extension_from_content_type(content_type)
            filename = f"{filename_prefix}_{timestamp}_{unique_id}{extension}"

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ä½œæˆ
        file_path = self.base_output_dir / filename

        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„æ›¸ãè¾¼ã¿
        file_path.write_text(content, encoding="utf-8")

        # SHA256è¨ˆç®—
        sha256_hash = calculate_sha256(file_path)

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºå–å¾—
        size_bytes = file_path.stat().st_size

        # SPEC-MCP-HASH-001: ãƒãƒƒã‚·ãƒ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°ï¼ˆæ’ä»–åˆ¶å¾¡ï¼‰
        self._update_hash_index(sha256_hash, file_path)

        # B20æº–æ‹ : çµ±ä¸€Consoleä½¿ç”¨ï¼ˆpytestå®Ÿè¡Œæ™‚ã¯é¨’ãŒã—ãã—ãªã„ï¼‰
        if not os.getenv("PYTEST_CURRENT_TEST"):
            console.print(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº†: {filename} ({size_bytes} bytes)", style="green")
        self.logger.info("File saved: %s, hash: %s...", filename, sha256_hash[:16])

        # FileReferenceModelä½œæˆï¼ˆãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã®ç›¸å¯¾ãƒ‘ã‚¹ï¼‰
        file_reference = FileReferenceModel(
            path=filename,  # ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿
            sha256=sha256_hash,
            size_bytes=size_bytes,
            content_type=content_type,
            created_at=datetime.now(timezone.utc),
        )
        self._hash_cache[sha256_hash] = file_reference
        return file_reference

    def verify_file_integrity(self, file_reference: FileReferenceModel) -> bool:
        """ãƒ•ã‚¡ã‚¤ãƒ«å®Œå…¨æ€§æ¤œè¨¼"""
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è§£æ±º
        file_path = self.base_output_dir / file_reference.path

        if not file_path.exists():
            return False

        # SHA256å†è¨ˆç®—ãƒ»æ¯”è¼ƒ
        current_hash = calculate_sha256(file_path)
        return current_hash == file_reference.sha256

    def load_file_content(self, file_reference: FileReferenceModel) -> str:
        """ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹èª­ã¿è¾¼ã¿ï¼ˆå®Œå…¨æ€§æ¤œè¨¼ä»˜ãï¼‰"""

        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹è§£æ±º
        file_path = self.base_output_dir / file_reference.path

        # å®Œå…¨æ€§ãƒã‚§ãƒƒã‚¯
        if not self.verify_file_integrity(file_reference):
            msg = f"ãƒ•ã‚¡ã‚¤ãƒ«å®Œå…¨æ€§ã‚¨ãƒ©ãƒ¼: {file_reference.path}"
            raise ValueError(msg)

        # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        return file_path.read_text(encoding=file_reference.encoding)

    def _get_extension_from_content_type(self, content_type: str) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—ã‹ã‚‰æ‹¡å¼µå­å–å¾—"""
        extension_map = {
            "text/markdown": ".md",
            "text/yaml": ".yaml",
            "application/json": ".json",
            "text/plain": ".txt",
        }
        return extension_map.get(content_type, ".txt")

    def cleanup_old_files(self, max_age_days: int = 30) -> list[str]:
        """å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ï¼ˆSPEC-MCP-HASH-001æ‹¡å¼µï¼‰"""
        cutoff_date = datetime.now(timezone.utc) - timedelta(days=max_age_days)
        deleted_files = []

        for file_path in self.base_output_dir.rglob("*"):
            if file_path.is_file() and file_path.name != ".hash_index.json":
                file_mtime = datetime.fromtimestamp(file_path.stat().st_mtime, tz=timezone.utc)
                if file_mtime < cutoff_date:
                    # SPEC-MCP-HASH-001: ãƒãƒƒã‚·ãƒ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰ã‚‚å‰Šé™¤
                    self._remove_from_hash_index(file_path)
                    file_path.unlink()
                    deleted_files.append(str(file_path))

        if deleted_files:
            self._save_hash_index()
            console.print(f"ğŸ—‘ï¸ å¤ã„ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤: {len(deleted_files)}å€‹", style="yellow")

        return deleted_files

    # SPEC-MCP-HASH-001: æ–°è¦ãƒãƒƒã‚·ãƒ¥ãƒ™ãƒ¼ã‚¹æ©Ÿèƒ½

    def find_file_by_hash(self, sha256: str) -> FileReferenceModel | None:
        """FR-001: SHA256ãƒãƒƒã‚·ãƒ¥ã«ã‚ˆã‚‹ãƒ•ã‚¡ã‚¤ãƒ«æ¤œç´¢"""
        # ãƒãƒƒã‚·ãƒ¥å½¢å¼æ¤œè¨¼
        if not verify_hash_format(sha256):
            error_msg = f"Invalid hash format: {sha256}"
            raise ValueError(error_msg)

        self.logger.debug("Searching for file with hash: %s...", sha256[:16])

        cached_ref = self._hash_cache.get(sha256)
        if cached_ref:
            file_path = self.base_output_dir / cached_ref.path
            if file_path.exists():
                return cached_ref
            del self._hash_cache[sha256]

        # ãƒãƒƒã‚·ãƒ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰æ¤œç´¢ï¼ˆO(1)æ€§èƒ½ï¼‰
        file_paths = self._hash_index.get(sha256, [])

        for file_path in file_paths:
            if file_path.exists():
                file_reference = self._create_file_reference_from_path(file_path, sha256)
                self._hash_cache[sha256] = file_reference
                return file_reference

        self.logger.debug("File not found for hash: %s...", sha256[:16])
        return None

    def get_file_by_hash(self, sha256: str) -> tuple[FileReferenceModel, str] | None:
        """FR-002: ãƒãƒƒã‚·ãƒ¥æŒ‡å®šã§ã®ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹å–å¾—"""
        file_ref = self.find_file_by_hash(sha256)
        if not file_ref:
            return None

        try:
            content = self.load_file_content(file_ref)
            self.logger.debug("File content loaded: %d chars", len(content))
            return (file_ref, content)
        except Exception:
            self.logger.exception("Failed to load file content")
            raise

    def has_file_changed(self, file_path: Path, previous_hash: str) -> bool:
        """FR-003: ãƒ•ã‚¡ã‚¤ãƒ«å¤‰æ›´æ¤œçŸ¥"""
        if not file_path.exists():
            error_msg = f"File not found: {file_path}"
            raise FileNotFoundError(error_msg)

        current_hash = calculate_sha256(file_path)
        changed = current_hash != previous_hash

        if changed:
            self.logger.debug("File changed detected: %s", file_path.name)

        return changed

    def track_changes(self) -> dict[str, bool]:
        """FR-003: è¤‡æ•°ãƒ•ã‚¡ã‚¤ãƒ«ã®å¤‰æ›´è¿½è·¡"""
        changes = {}

        for sha256, file_paths in self._hash_index.items():
            for file_path in file_paths:
                if file_path.exists():
                    try:
                        changed = self.has_file_changed(file_path, sha256)
                        changes[str(file_path)] = changed
                    except Exception:
                        self.logger.exception("Error tracking changes for %s", file_path)
                        changes[str(file_path)] = True  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯å¤‰æ›´ã‚ã‚Šã¨ã—ã¦æ‰±ã†

        return changes

    def list_files_with_hashes(self) -> dict[str, list[str]]:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ»ãƒãƒƒã‚·ãƒ¥ä¸€è¦§å–å¾—

        Returns a mapping of file path -> [sha256] for compatibility with tests.
        """
        result: dict[str, list[str]] = {}

        for sha256, file_paths in self._hash_index.items():
            for p in file_paths:
                if p.exists():
                    result[str(p)] = [sha256]

        self.logger.debug("Listed %d files with hashes", len(result))
        return result

    # ãƒãƒƒã‚·ãƒ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ç®¡ç†ãƒ¡ã‚½ãƒƒãƒ‰

    def _build_hash_index(self) -> dict[str, list[Path]]:
        """ãƒãƒƒã‚·ãƒ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰"""
        console.print("ğŸ” ãƒãƒƒã‚·ãƒ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ä¸­...", style="blue")
        index = {}

        total_files = 0
        for file_path in self.base_output_dir.rglob("*"):
            if file_path.is_file() and file_path.name != ".hash_index.json":
                try:
                    sha256 = calculate_sha256(file_path)
                    if sha256 not in index:
                        index[sha256] = []
                    index[sha256].append(file_path)
                    total_files += 1
                except Exception as e:
                    self.logger.warning("Failed to hash file %s: %s", file_path, e)

        console.print(f"âœ… ãƒãƒƒã‚·ãƒ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰å®Œäº†: {total_files}ãƒ•ã‚¡ã‚¤ãƒ«", style="green")
        return index

    def _update_hash_index(self, sha256: str, file_path: Path) -> None:
        """ãƒãƒƒã‚·ãƒ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ›´æ–°ï¼ˆæ’ä»–åˆ¶å¾¡ï¼‰"""
        with self._index_lock:
            if sha256 not in self._hash_index:
                self._hash_index[sha256] = []

            if file_path not in self._hash_index[sha256]:
                self._hash_index[sha256].append(file_path)

            self._hash_cache[sha256] = self._create_file_reference_from_path(file_path, sha256)

            # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ°¸ç¶šåŒ–
            self._save_hash_index()

    def _remove_from_hash_index(self, file_path: Path) -> None:
        """ãƒãƒƒã‚·ãƒ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰ãƒ•ã‚¡ã‚¤ãƒ«å‰Šé™¤ï¼ˆæ’ä»–åˆ¶å¾¡ï¼‰"""
        with self._index_lock:
            for sha256, file_paths in list(self._hash_index.items()):
                if file_path in file_paths:
                    file_paths.remove(file_path)
                    if not file_paths:  # ç©ºã«ãªã£ãŸã‚‰å‰Šé™¤
                        del self._hash_index[sha256]
                        self._hash_cache.pop(sha256, None)
                    else:
                        self._hash_cache[sha256] = self._create_file_reference_from_path(file_paths[0], sha256)
                    break

    def _save_hash_index(self) -> None:
        """ãƒãƒƒã‚·ãƒ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ°¸ç¶šåŒ–ï¼ˆæ’ä»–åˆ¶å¾¡ï¼‹ã‚¢ãƒˆãƒŸãƒƒã‚¯ä¿å­˜ï¼‰"""
        try:
            with self._index_lock:
                # Path â†’ str å¤‰æ›
                serializable_index: dict[str, list[str]] = {}
                for sha256, file_paths in self._hash_index.items():
                    serializable_index[sha256] = [str(p) for p in file_paths]

                index_data = {
                    "version": "1.0.0",
                    "created_at": datetime.now(timezone.utc).isoformat(),
                    "index": serializable_index,
                }

                tmp_path = self._hash_index_file.with_name(
                    f"{self._hash_index_file.name}.tmp.{uuid.uuid4().hex}"
                )
                tmp_path.write_text(
                    json.dumps(index_data, indent=2, ensure_ascii=False), encoding="utf-8"
                )
                # Atomic replace even on Windows
                os.replace(tmp_path, self._hash_index_file)

                self.logger.debug("Hash index saved: %d entries", len(self._hash_index))

        except Exception:
            self.logger.exception("Failed to save hash index")

    def _load_hash_index(self) -> None:
        """ãƒãƒƒã‚·ãƒ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹èª­ã¿è¾¼ã¿ï¼ˆæ’ä»–åˆ¶å¾¡ï¼‰"""
        with self._index_lock:
            if not self._hash_index_file.exists():
                self.logger.debug("Hash index file not found, building from scratch")
                self._hash_index = self._build_hash_index()
                self._save_hash_index()
                return

            try:
                index_data = json.loads(self._hash_index_file.read_text(encoding="utf-8"))

                # str â†’ Path å¤‰æ›
                self._hash_index = {}
                for sha256, file_paths in index_data.get("index", {}).items():
                    self._hash_index[sha256] = [Path(p) for p in file_paths]

                self.logger.debug("Hash index loaded: %d entries", len(self._hash_index))

                # å­˜åœ¨ã—ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                self._cleanup_hash_index()

            except Exception as e:
                self.logger.warning("Failed to load hash index, rebuilding: %s", e)
                self._hash_index = self._build_hash_index()
                self._save_hash_index()

    def _populate_hash_cache(self) -> None:
        """Populate in-memory cache for fast hash lookups (guarded)."""
        with self._index_lock:
            self._hash_cache.clear()
            for sha256, file_paths in self._hash_index.items():
                for file_path in file_paths:
                    if file_path.exists():
                        self._hash_cache[sha256] = self._create_file_reference_from_path(file_path, sha256)
                        break

    def _cleanup_hash_index(self) -> None:
        """å­˜åœ¨ã—ãªã„ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒãƒƒã‚·ãƒ¥ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‹ã‚‰å‰Šé™¤ï¼ˆæ’ä»–åˆ¶å¾¡ï¼‰"""
        with self._index_lock:
            cleaned = False

            for sha256, file_paths in list(self._hash_index.items()):
                existing_paths = [p for p in file_paths if p.exists()]

                if len(existing_paths) != len(file_paths):
                    if existing_paths:
                        self._hash_index[sha256] = existing_paths
                    else:
                        del self._hash_index[sha256]
                    cleaned = True

            if cleaned:
                self._save_hash_index()
                self.logger.debug("Hash index cleaned up")
                self._populate_hash_cache()

    def _create_file_reference_from_path(self, file_path: Path, sha256: str) -> FileReferenceModel:
        """ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰FileReferenceModelä½œæˆ"""
        stat = file_path.stat()

        # ãƒ•ã‚¡ã‚¤ãƒ«åã®ã¿ã®ç›¸å¯¾ãƒ‘ã‚¹
        relative_path = file_path.relative_to(self.base_output_dir)

        # ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã‚¿ã‚¤ãƒ—æ¨å®š
        extension = file_path.suffix.lower()
        content_type_map = {
            ".txt": "text/plain",
            ".md": "text/markdown",
            ".json": "application/json",
            ".yaml": "text/yaml",
            ".yml": "text/yaml",
        }
        content_type = content_type_map.get(extension, "text/plain")

        return FileReferenceModel(
            path=str(relative_path),
            sha256=sha256,
            size_bytes=stat.st_size,
            content_type=content_type,
            created_at=datetime.fromtimestamp(stat.st_ctime, tz=timezone.utc),
        )
