#!/usr/bin/env python3
"""
ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æãƒ„ãƒ¼ãƒ«
ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œã‚’æ¤œå‡ºã—ã€æœ€é©åŒ–ææ¡ˆã‚’ç”Ÿæˆ
"""

import ast
import asyncio
import json
import re
import time
from collections import defaultdict
from pathlib import Path
from typing import Any

from noveler.presentation.shared.shared_utilities import _get_console

console = _get_console()


class CodeAnalyzer:
    """ã‚³ãƒ¼ãƒ‰é™çš„è§£æã«ã‚ˆã‚‹ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹å•é¡Œæ¤œå‡º"""

    def __init__(self, _logger_service: Any = None, _console_service: Any = None) -> None:
        self.performance_issues: list[dict[str, Any]] = []
        self.file_io_patterns = [
            r"\.read_text\(",
            r"\.write_text\(",
            r"yaml\.load\(",
            r"yaml\.dump\(",
            r"json\.load\(",
            r"json\.dump\(",
            r"open\(",
            r"Path\(.*\)\.rglob\("
        ]

    def analyze_file(self, file_path: Path) -> dict[str, Any]:
        """å˜ä¸€ãƒ•ã‚¡ã‚¤ãƒ«åˆ†æ"""
        try:
            # æœ€é©åŒ–èª­ã¿è¾¼ã¿ãŒåˆ©ç”¨å¯èƒ½ãªã‚‰ä½¿ç”¨ã€ãªã‘ã‚Œã°é€šå¸¸èª­ã¿è¾¼ã¿
            content: str
            try:
                from noveler.infrastructure.performance.comprehensive_performance_optimizer import (  # noqa: PLC0415
                    performance_optimizer as _perf_opt,
                )

                content = _perf_opt.file_io_optimizer.optimized_read_text(file_path, encoding="utf-8")
            except Exception:
                content = file_path.read_text(encoding="utf-8")
            tree = ast.parse(content, filename=str(file_path))
            issues = {
                "file_path": str(file_path),
                "performance_issues": [],
                "optimization_opportunities": [],
                "complexity_metrics": self._calculate_complexity(tree),
                "io_operations": self._detect_io_operations(content),
                "potential_bottlenecks": []
            }
            self._analyze_ast_node(tree, issues)
            return issues
        except Exception as e:
            return {
                "file_path": str(file_path),
                "error": f"åˆ†æã‚¨ãƒ©ãƒ¼: {e}",
                "performance_issues": [],
                "optimization_opportunities": [],
                "complexity_metrics": {},
                "io_operations": [],
                "potential_bottlenecks": []
            }

    def _analyze_ast_node(self, node: ast.AST, issues: dict[str, Any]) -> None:
        """AST ãƒãƒ¼ãƒ‰åˆ†æ"""
        for child in ast.walk(node):
            if isinstance(child, (ast.For, ast.While)):
                if self._is_complex_loop(child):
                    issues["potential_bottlenecks"].append({
                        "type": "complex_loop",
                        "line": child.lineno,
                        "description": "è¤‡é›‘ãªãƒ«ãƒ¼ãƒ—å‡¦ç† - ä¸¦åˆ—åŒ–ã‚„æœ€é©åŒ–ã‚’æ¤œè¨"
                    })
            if isinstance(child, ast.For):
                nested_loops = self._count_nested_loops(child)
                if nested_loops >= 2:
                    issues["potential_bottlenecks"].append({
                        "type": "nested_loops",
                        "line": child.lineno,
                        "nesting_level": nested_loops,
                        "description": f"{nested_loops}é‡ãƒã‚¹ãƒˆãƒ«ãƒ¼ãƒ— - O(n^{nested_loops})è¨ˆç®—é‡"
                    })
            if isinstance(child, ast.Call) and hasattr(child.func, "attr"):
                if child.func.attr in ["read_text", "write_text", "open"]:
                    issues["io_operations"].append({
                        "type": "file_io",
                        "line": child.lineno,
                        "operation": child.func.attr
                    })

    def _is_complex_loop(self, loop_node: ast.AST) -> bool:
        """è¤‡é›‘ãªãƒ«ãƒ¼ãƒ—ã‹ã©ã†ã‹ã®åˆ¤å®š"""
        body_lines = 0
        for stmt in getattr(loop_node, "body", []):
            if hasattr(stmt, "lineno"):
                body_lines += 1
        return body_lines > 10

    def _count_nested_loops(self, node: ast.For) -> int:
        """ãƒã‚¹ãƒˆã—ãŸãƒ«ãƒ¼ãƒ—ã®æ·±ã•ã‚«ã‚¦ãƒ³ãƒˆ"""
        max_depth = 0
        current_depth = 0

        def visit_node(n: ast.AST) -> None:
            nonlocal max_depth, current_depth
            if isinstance(n, (ast.For, ast.While)):
                current_depth += 1
                max_depth = max(max_depth, current_depth)
            for child in ast.iter_child_nodes(n):
                visit_node(child)
            if isinstance(n, (ast.For, ast.While)):
                current_depth -= 1

        visit_node(node)
        return max_depth

    def _calculate_complexity(self, tree: ast.AST) -> dict[str, int]:
        """ã‚³ãƒ¼ãƒ‰ã®è¤‡é›‘åº¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹è¨ˆç®—"""
        metrics = {"functions": 0, "classes": 0, "lines_of_code": 0, "cyclomatic_complexity": 0}
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                metrics["functions"] += 1
                metrics["cyclomatic_complexity"] += self._calculate_cyclomatic_complexity(node)
            elif isinstance(node, ast.ClassDef):
                metrics["classes"] += 1
        return metrics

    def _calculate_cyclomatic_complexity(self, func_node: ast.FunctionDef) -> int:
        """å¾ªç’°çš„è¤‡é›‘åº¦è¨ˆç®—"""
        complexity = 1
        for node in ast.walk(func_node):
            if isinstance(node, (ast.If, ast.While, ast.For, ast.ExceptHandler)):
                complexity += 1
            elif isinstance(node, ast.BoolOp):
                complexity += len(node.values) - 1
        return complexity

    def _detect_io_operations(self, content: str) -> list[dict[str, Any]]:
        """I/Oæ“ä½œãƒ‘ã‚¿ãƒ¼ãƒ³æ¤œå‡º"""
        io_ops = []
        for pattern in self.file_io_patterns:
            matches = re.finditer(pattern, content)
            for match in matches:
                line_num = content[:match.start()].count("\n") + 1
                io_ops.append({
                    "type": "file_io",
                    "pattern": pattern,
                    "line": line_num,
                    "match": match.group()
                })
        return io_ops


class PerformanceBottleneckAnalyzer:
    """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åŒ…æ‹¬åˆ†æã‚·ã‚¹ãƒ†ãƒ """

    def __init__(self, _logger_service: Any = None, _console_service: Any = None) -> None:
        self.code_analyzer = CodeAnalyzer()
        self.analysis_results: dict[str, Any] = {}

    async def analyze_project_performance(self, project_root: Path) -> dict[str, Any]:
        """ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆå…¨ä½“ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æ"""
        console.print("ğŸ” ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æé–‹å§‹...", style="bold blue")
        python_files = list(project_root.rglob("*.py"))
        console.print(f"ğŸ“ åˆ†æå¯¾è±¡ãƒ•ã‚¡ã‚¤ãƒ«: {len(python_files)}ä»¶")

        static_analysis_results = await self._run_static_analysis(python_files)
        critical_files = self._identify_critical_files(static_analysis_results)
        bottleneck_analysis = self._analyze_bottlenecks(static_analysis_results)
        optimization_recommendations = self._generate_comprehensive_recommendations(bottleneck_analysis)

        self.analysis_results = {
            "timestamp": time.time(),
            "project_root": str(project_root),
            "total_files_analyzed": len(python_files),
            "static_analysis": static_analysis_results,
            "critical_files": critical_files,
            "bottleneck_analysis": bottleneck_analysis,
            "optimization_recommendations": optimization_recommendations,
            "performance_summary": self._generate_performance_summary(),
        }

        console.print("âœ… ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹åˆ†æå®Œäº†", style="bold green")
        return self.analysis_results

    async def _run_static_analysis(self, python_files: list[Path]) -> list[dict[str, Any]]:
        """é™çš„è§£æå®Ÿè¡Œ"""
        results: list[dict[str, Any]] = []
        semaphore = asyncio.Semaphore(10)

        async def _analyze_file(file_path: Path) -> Any:
            async with semaphore:
                return await asyncio.to_thread(self.code_analyzer.analyze_file, file_path)

        console.print("ğŸ”¬ é™çš„è§£æå®Ÿè¡Œä¸­...")
        tasks = [_analyze_file(file_path) for file_path in python_files]
        results_any = await asyncio.gather(*tasks, return_exceptions=True)

        valid_results = []
        for i, result in enumerate(results_any):
            if isinstance(result, Exception):
                console.print(f"âš ï¸ åˆ†æã‚¨ãƒ©ãƒ¼: {python_files[i]} - {result}", style="yellow")
            else:
                valid_results.append(result)

        return valid_results

    def _identify_critical_files(self, analysis_results: list[dict[str, Any]]) -> list[dict[str, Any]]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ç‰¹å®š"""
        critical_files = []
        for result in analysis_results:
            if "error" in result:
                continue

            score = 0
            io_count = len(result["io_operations"])
            score += io_count * 2

            bottleneck_count = len(result["potential_bottlenecks"])
            score += bottleneck_count * 5

            complexity = result["complexity_metrics"]
            score += complexity.get("cyclomatic_complexity", 0)

            for bottleneck in result["potential_bottlenecks"]:
                if bottleneck["type"] == "nested_loops":
                    score += bottleneck["nesting_level"] * 10

            if score > 20:
                critical_files.append({
                    "file_path": result["file_path"],
                    "critical_score": score,
                    "io_operations": io_count,
                    "bottlenecks": bottleneck_count,
                    "complexity": complexity,
                    "recommendations": self._generate_file_recommendations(result)
                })

        critical_files.sort(key=lambda x: x["critical_score"], reverse=True)
        return critical_files

    def _analyze_bottlenecks(self, analysis_results: list[dict[str, Any]]) -> dict[str, Any]:
        """ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æ"""
        bottleneck_summary = {
            "total_io_operations": 0,
            "total_bottlenecks": 0,
            "bottleneck_types": defaultdict(int),
            "io_operation_types": defaultdict(int),
            "high_complexity_files": []
        }

        for result in analysis_results:
            if "error" in result:
                continue

            bottleneck_summary["total_io_operations"] += len(result["io_operations"])
            for io_op in result["io_operations"]:
                bottleneck_summary["io_operation_types"][io_op["type"]] += 1

            bottleneck_summary["total_bottlenecks"] += len(result["potential_bottlenecks"])
            for bottleneck in result["potential_bottlenecks"]:
                bottleneck_summary["bottleneck_types"][bottleneck["type"]] += 1

            complexity = result["complexity_metrics"]
            if complexity.get("cyclomatic_complexity", 0) > 50:
                bottleneck_summary["high_complexity_files"].append({
                    "file_path": result["file_path"],
                    "complexity": complexity["cyclomatic_complexity"]
                })

        return dict(bottleneck_summary)

    def _generate_file_recommendations(self, file_analysis: dict[str, Any]) -> list[str]:
        """ãƒ•ã‚¡ã‚¤ãƒ«å›ºæœ‰ã®æœ€é©åŒ–æ¨å¥¨äº‹é …"""
        recommendations = []

        io_count = len(file_analysis["io_operations"])
        if io_count > 10:
            recommendations.append(
                f"ğŸ”§ ãƒ•ã‚¡ã‚¤ãƒ«I/Oæœ€é©åŒ–: {io_count}å€‹ã®I/Oæ“ä½œ - ãƒãƒƒãƒå‡¦ç†ã‚„ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’æ¤œè¨"
            )

        nested_loops = [b for b in file_analysis["potential_bottlenecks"] if b["type"] == "nested_loops"]
        if nested_loops:
            max_nesting = max(loop["nesting_level"] for loop in nested_loops)
            recommendations.append(
                f"âš¡ ãƒã‚¹ãƒˆãƒ«ãƒ¼ãƒ—æœ€é©åŒ–: æœ€å¤§{max_nesting}é‡ãƒã‚¹ãƒˆ - ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ è¦‹ç›´ã—ã‚’æ¨å¥¨"
            )

        complexity = file_analysis["complexity_metrics"].get("cyclomatic_complexity", 0)
        if complexity > 30:
            recommendations.append(
                f"ğŸ“Š è¤‡é›‘åº¦å‰Šæ¸›: å¾ªç’°çš„è¤‡é›‘åº¦{complexity} - é–¢æ•°åˆ†å‰²ã‚’æ¤œè¨"
            )

        return recommendations

    def _generate_comprehensive_recommendations(self, bottleneck_analysis: dict[str, Any]) -> list[str]:
        """åŒ…æ‹¬çš„æœ€é©åŒ–æ¨å¥¨äº‹é …ç”Ÿæˆ"""
        recommendations = []

        total_io = bottleneck_analysis["total_io_operations"]
        if total_io > 100:
            recommendations.append(
                f"ğŸ—„ï¸ I/Oæœ€é©åŒ–ï¼ˆå„ªå…ˆåº¦: é«˜ï¼‰: {total_io}å€‹ã®ãƒ•ã‚¡ã‚¤ãƒ«æ“ä½œæ¤œå‡º - "
            )

        yaml_ops = bottleneck_analysis["io_operation_types"].get("file_io", 0)
        if yaml_ops > 50:
            recommendations.append(
                f"ğŸ“„ YAML/JSONæœ€é©åŒ–ï¼ˆå„ªå…ˆåº¦: é«˜ï¼‰: {yaml_ops}å€‹ã®å‡¦ç†æ“ä½œ - "
                "YAMLOptimizerã«ã‚ˆã‚‹ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨åŠ¹ç‡çš„ãªå‡¦ç†ã‚’å®Ÿè£…"
            )

        complex_loops = bottleneck_analysis["bottleneck_types"].get("complex_loop", 0)
        nested_loops = bottleneck_analysis["bottleneck_types"].get("nested_loops", 0)
        if complex_loops + nested_loops > 10:
            recommendations.append(
                f"âš¡ ä¸¦åˆ—å‡¦ç†å°å…¥ï¼ˆå„ªå…ˆåº¦: ä¸­ï¼‰: {complex_loops + nested_loops}å€‹ã®é‡ã„ãƒ«ãƒ¼ãƒ— - "
                "AsyncOperationOptimizerã«ã‚ˆã‚‹éåŒæœŸå‡¦ç†ã‚’æ¤œè¨"
            )

        high_complexity_files = len(bottleneck_analysis["high_complexity_files"])
        if high_complexity_files > 5:
            recommendations.append(
                f"ğŸ’¾ ãƒ¡ãƒ¢ãƒªæœ€é©åŒ–ï¼ˆå„ªå…ˆåº¦: ä¸­ï¼‰: {high_complexity_files}å€‹ã®é«˜è¤‡é›‘åº¦ãƒ•ã‚¡ã‚¤ãƒ« - "
                "MemoryOptimizerã«ã‚ˆã‚‹åŠ¹ç‡çš„ãªãƒ‡ãƒ¼ã‚¿å‡¦ç†ã‚’å®Ÿè£…"
            )

        if bottleneck_analysis["total_bottlenecks"] > 20:
            recommendations.append(
                "ğŸ¯ å…·ä½“çš„æ”¹å–„ç®‡æ‰€: ä»¥ä¸‹ã®ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å„ªå…ˆçš„ã«æœ€é©åŒ–ã‚’é–‹å§‹ã—ã¦ãã ã•ã„"
            )

        return recommendations

    def _generate_performance_summary(self) -> dict[str, Any]:
        """ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚µãƒãƒªãƒ¼ç”Ÿæˆ"""
        return {
            "analysis_timestamp": time.time(),
            "optimization_impact_estimate": {
                "potential_response_time_improvement": "30-50%",
                "potential_memory_savings": "25-40%",
                "implementation_effort": "medium",
                "priority_level": "high"
            }
        }

    def print_analysis_report(self) -> None:
        """åˆ†æçµæœãƒ¬ãƒãƒ¼ãƒˆè¡¨ç¤º"""
        if not self.analysis_results:
            console.print("âš ï¸ åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“", style="yellow")
            return

        results = self.analysis_results

        console.print("\n" + "=" * 80, style="bold")
        console.print("ğŸš€ ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ãƒœãƒˆãƒ«ãƒãƒƒã‚¯åˆ†æãƒ¬ãƒãƒ¼ãƒˆ", style="bold blue")
        console.print("=" * 80, style="bold")

        console.print("\nğŸ“Š åˆ†æã‚µãƒãƒªãƒ¼:", style="bold")
        console.print(f"  â€¢ åˆ†æãƒ•ã‚¡ã‚¤ãƒ«æ•°: {results['total_files_analyzed']}ä»¶")
        console.print(f"  â€¢ ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«: {len(results['critical_files'])}ä»¶")
        console.print(f"  â€¢ ç·I/Oæ“ä½œ: {results['bottleneck_analysis']['total_io_operations']}å€‹")
        console.print(f"  â€¢ ç·ãƒœãƒˆãƒ«ãƒãƒƒã‚¯: {results['bottleneck_analysis']['total_bottlenecks']}å€‹")

        if results["critical_files"]:
            console.print("\nğŸš¨ ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«ãƒ•ã‚¡ã‚¤ãƒ« TOP 10:", style="bold red")
            for i, file_info in enumerate(results["critical_files"][:10], 1):
                console.print(f"  {i}. {Path(file_info['file_path']).name}")
                console.print(
                    f"     ã‚¹ã‚³ã‚¢: {file_info['critical_score']}, "
                    f"I/Oæ“ä½œ: {file_info['io_operations']}, "
                    f"ãƒœãƒˆãƒ«ãƒãƒƒã‚¯: {file_info['bottlenecks']}"
                )

        console.print("\nğŸ’¡ æœ€é©åŒ–æ¨å¥¨äº‹é …:", style="bold green")
        for i, rec in enumerate(results["optimization_recommendations"], 1):
            console.print(f"  {i}. {rec}")

        summary = results["performance_summary"]
        impact = summary["optimization_impact_estimate"]
        console.print("\nğŸ“ˆ æ”¹å–„äºˆæ¸¬:", style="bold cyan")
        console.print(f"  â€¢ ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“æ”¹å–„: {impact['potential_response_time_improvement']}")
        console.print(f"  â€¢ ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡å‰Šæ¸›: {impact['potential_memory_savings']}")
        console.print(f"  â€¢ å®Ÿè£…å·¥æ•°: {impact['implementation_effort']}")
        console.print(f"  â€¢ å„ªå…ˆåº¦: {impact['priority_level']}")

        console.print("=" * 80, style="bold")

    def export_analysis_results(self, output_path: Path) -> None:
        """åˆ†æçµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        if not self.analysis_results:
            console.print("âš ï¸ ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå¯¾è±¡ã®åˆ†æçµæœãŒã‚ã‚Šã¾ã›ã‚“", style="yellow")
            return

        output_path.parent.mkdir(parents=True, exist_ok=True)
        with output_path.open("w", encoding="utf-8") as f:
            json.dump(self.analysis_results, f, ensure_ascii=False, indent=2, default=str)

        console.print(f"ğŸ“„ åˆ†æçµæœã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {output_path}", style="green")


async def main() -> None:
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    analyzer = PerformanceBottleneckAnalyzer()
    project_root = Path("src/noveler")

    if not project_root.exists():
        console.print(f"âŒ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {project_root}", style="red")
        return

    results = await analyzer.analyze_project_performance(project_root)
    analyzer.print_analysis_report()

    # çµæœã‚’ä¿å­˜
    output_path = Path("reports") / "performance_analysis.json"
    analyzer.export_analysis_results(output_path)


if __name__ == "__main__":
    asyncio.run(main())
