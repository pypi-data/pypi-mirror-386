from typing import Any, Dict, List, Literal, Optional, Union

from pydantic import BaseModel, Field

# ----------------------------------------------------------------------------
# Schemas for manifest.yaml (Embedded in Test Containers)
# ----------------------------------------------------------------------------


class SystemInput(BaseModel):
    """Defines a system input that the container requires."""

    name: str = Field(
        ...,
        description="The system input name, e.g., 'system_under_test', 'simulator_system', 'evaluator_system'.",
    )
    type: str = Field(
        ..., description="The system type, e.g., 'llm_api' or 'rest_api'."
    )
    required: bool = Field(True, description="Whether this system input is required.")
    description: Optional[str] = Field(
        None, description="Description of the system's role in the test."
    )


class InputParameter(BaseModel):
    """Defines a parameter that can be passed to the test container."""

    name: str
    type: Literal["string", "integer", "float", "boolean", "list", "object"]
    required: bool = False
    description: Optional[str] = None


class OutputMetric(BaseModel):
    """Defines a metric that will be present in the test container's output."""

    name: str
    type: Literal["string", "integer", "float", "boolean", "list", "object"]
    description: Optional[str] = None


class OutputArtifact(BaseModel):
    """Defines a file artifact generated by the test container."""

    name: str
    path: str
    description: Optional[str] = None


class Manifest(BaseModel):
    """Schema for the manifest.yaml file inside a test container."""

    name: str = Field(..., description="The canonical name for the test framework.")
    version: str
    description: Optional[str] = None
    host_access: bool = Field(
        False,
        description="Whether the container requires host access (e.g., for Docker-in-Docker).",
    )
    input_systems: List[SystemInput] = Field(
        ...,
        description="Systems required as input. Should minimally include a system_under_test",
    )
    input_schema: List[InputParameter] = Field(
        [], description="Defines the schema for the user-provided 'params' object."
    )
    output_metrics: Union[List[str], List[OutputMetric]] = Field(
        [],
        description="Defines expected high-level metrics in the final JSON output. Can be a simple list of strings or detailed metric definitions.",
    )
    output_artifacts: Optional[List[OutputArtifact]] = None


# ----------------------------------------------------------------------------
# Schema for systems.yaml (User-provided)
# ----------------------------------------------------------------------------


class SystemDefinition(BaseModel):
    """Base system definition."""

    description: Optional[str] = Field(
        None,
        description="Description of the system being evaluated.",
    )

    provider: Optional[str] = Field(
        None,
        description="Name of the provider of the system, either 'custom' for internal systems or 'openai, aws-bedrock...' for external systems.",
    )


# LLM API system


class LLMAPIParams(BaseModel):
    """Parameters for the LLM API systems."""

    base_url: str = Field(
        ...,
        description="Base URL for the OpenAI-compatible API (e.g., 'http://localhost:4000/v1', 'https://api.openai.com/v1')",
    )
    model: str = Field(
        ...,
        description="Model name to use with the API",
    )
    env_file: Optional[str] = Field(
        None,
        description="Path to .env file containing environment variables for authentication",
    )
    api_key: Optional[str] = Field(
        None,
        description="Direct API key for authentication (alternative to env_file)",
    )


class LLMAPIConfig(SystemDefinition):
    """Configuration for LLM API systems."""

    type: Literal["llm_api"] = Field(
        ...,
        description="LLM API system: llm_api",
    )
    params: LLMAPIParams = Field(
        ...,
        description="Parameters specific to the LLM API system (e.g., base url, model name, API key and env file).",
    )


# Generic system


class GenericSystemConfig(SystemDefinition):
    """Generic system configuration for system types without specific validation.

    This allows backward compatibility and support for system types that don't have dedicated config classes yet.
    """

    type: str = Field(
        ...,
        description="System type, e.g., 'rest_api', 'custom_api', etc.",
    )
    params: Dict[str, Any] = Field(
        ...,
        description="Parameters specific to the system type.",
    )


SystemConfig = Union[LLMAPIConfig, GenericSystemConfig]


class SystemsConfig(BaseModel):
    """Schema for the top-level systems configuration file.

    Extension Guide:
        1. Create a new XXXConfig class inheriting from SystemDefinition. e.g, RESTAPIConfig
        2. Create a new XXXParam class for the parameters of the system. e.g. RESTAPIParams
        2. Add the new system definition (XXXConfig) to the SystemConfig union type
            e.g. SystemConfig = Union[LLMAPIConfig, XXXConfig, ..., GenericSystemConfig]

    """

    systems: Dict[str, SystemConfig] = Field(
        ..., description="Dictionary of system definitions."
    )


# ----------------------------------------------------------------------------
# Schema for test_suite.yaml (User-provided)
# ----------------------------------------------------------------------------


class TestDefinitionBase(BaseModel):
    """Base class for test configuration fields shared between TestDefinition and TestSuiteDefault."""

    systems_under_test: Optional[List[str]] = Field(
        None,
        description="A list of system names (from systems.yaml) to run this test against. Can be inherited from test_suite_default.",
    )
    systems: Optional[Dict[str, str]] = Field(
        None,
        description="Optional additional systems for the test (e.g., simulator_system, evaluator_system).",
    )
    tags: Optional[List[str]] = Field(
        None, description="Optional tags for filtering and reporting."
    )
    params: Optional[Dict[str, Any]] = Field(
        None, description="Parameters to be passed to the test container's entrypoint."
    )
    volumes: Optional[Dict[str, Any]] = Field(
        None, description="Optional input/output mounts."
    )


class TestDefinition(TestDefinitionBase):
    """A single test to be executed."""

    name: str = Field(
        ..., description="A unique, human-readable name for this test instance."
    )
    description: Optional[str] = Field(
        None,
        description="A short summary of the purpose of the test and what it aims to validate.",
    )
    image: str = Field(
        ...,
        description="The Docker image to run for this test, e.g., 'my-registry/garak:latest'.",
    )


class TestSuiteDefault(TestDefinitionBase):
    """Default values that apply to all tests in the suite unless overridden."""

    pass


class SuiteConfig(BaseModel):
    """Schema for the top-level Test Suite configuration file."""

    suite_name: str = Field(..., description="Name of this test suite.")
    test_suite_default: Optional[TestSuiteDefault] = Field(
        None,
        description="Default values that apply to all tests in the suite unless overridden",
    )
    description: Optional[str] = Field(
        None,
        description="A short summary of the test suite and what it aims to evaluate.",
    )
    test_suite: List[TestDefinition] = Field(
        ..., description="List of individual focused tests."
    )


# ----------------------------------------------------------------------------
# Schema for Score Card (User-provided)
# ----------------------------------------------------------------------------


class ScoreCardFilter(BaseModel):
    """Defines which test results an indicator applies to."""

    test_name: str = Field(
        ...,
        description="Test name to filter by, e.g., 'run_mock_on_compatible_system_my_llm_service'",
    )


class AssessmentRule(BaseModel):
    """Individual assessment outcome with condition."""

    outcome: str = Field(
        ..., description="Assessment outcome, e.g., 'PASS', 'FAIL', 'A', 'F'"
    )
    condition: Literal[
        "equal_to",
        "greater_than",
        "less_than",
        "greater_equal",
        "less_equal",
        "all_true",
        "any_false",
        "count_equals",
    ] = Field(..., description="Condition to evaluate against the metric value")
    threshold: Optional[Union[int, float, bool]] = Field(
        None, description="Threshold value for comparison conditions"
    )
    description: Optional[str] = Field(
        None, description="Human-readable description for this assessment outcome"
    )


class ScoreCardIndicator(BaseModel):
    """Individual score card indicator with filtering and assessment."""

    name: str = Field(
        ..., description="Human-readable name for this score card indicator"
    )
    apply_to: ScoreCardFilter = Field(
        ...,
        description="Filter criteria for which test results this indicator applies to",
    )
    metric: str = Field(
        ...,
        description="Path to metric within test_results object. Supports dot notation for nested objects ('vulnerability_stats.Toxicity.overall_pass_rate'), and bracket notation for keys with dots ('probe_results[\"encoding.InjectHex\"][\"encoding.DecodeMatch\"].passed')",
    )
    assessment: List[AssessmentRule] = Field(
        ..., description="List of assessment rules to evaluate against the metric"
    )


class ScoreCard(BaseModel):
    """Complete grading score card configuration."""

    score_card_name: str = Field(..., description="Name of the grading score card")
    indicators: List[ScoreCardIndicator] = Field(
        ..., description="List of score card indicators to evaluate"
    )
