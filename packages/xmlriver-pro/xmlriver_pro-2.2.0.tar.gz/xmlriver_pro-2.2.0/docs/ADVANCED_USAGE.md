# –ü—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ

[‚Üê –ù–∞–∑–∞–¥ –∫ README](../README.md) ‚Ä¢ [–î–æ–∫—É–º–µ–Ω—Ç–∞—Ü–∏—è](README.md) ‚Ä¢ [API Reference](API_REFERENCE.md)

–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ —Å—Ü–µ–Ω–∞—Ä–∏–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è XMLRiver Pro –¥–ª—è —Å–ª–æ–∂–Ω—ã—Ö –∑–∞–¥–∞—á.

## –°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ

- [–ú–∞—Å—Å–æ–≤—ã–π –ø–æ–∏—Å–∫](#–º–∞—Å—Å–æ–≤—ã–π-–ø–æ–∏—Å–∫)
- [–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–æ–∑–∏—Ü–∏–π](#–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥-–ø–æ–∑–∏—Ü–∏–π)
- [–ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤](#–∞–Ω–∞–ª–∏–∑-–∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤)
- [–ê–Ω–∞–ª–∏–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —è–¥—Ä–∞ —Å Wordstat](#–∞–Ω–∞–ª–∏–∑-—Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ-—è–¥—Ä–∞-—Å-wordstat)
- [–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤](#—ç–∫—Å–ø–æ—Ä—Ç-—Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤)
- [–ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤](#–∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ-—Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤)
- [–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞](#–ø–∞–∫–µ—Ç–Ω–∞—è-–æ–±—Ä–∞–±–æ—Ç–∫–∞)
- [–ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç—å—é](#–∞–¥–∞–ø—Ç–∏–≤–Ω–æ–µ-—É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ-—Å–∫–æ—Ä–æ—Å—Ç—å—é)
- [–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–æ—Ç–æ–∫–æ–≤](#–º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥-–ø–æ—Ç–æ–∫–æ–≤)

## –ú–∞—Å—Å–æ–≤—ã–π –ø–æ–∏—Å–∫

### –ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–∞—Å—Å–æ–≤—ã–π –ø–æ–∏—Å–∫

```python
import asyncio
from xmlriver_pro import AsyncGoogleClient, AsyncYandexClient

async def mass_search(queries, search_engines=["google", "yandex"]):
    """–ê—Å–∏–Ω—Ö—Ä–æ–Ω–Ω—ã–π –º–∞—Å—Å–æ–≤—ã–π –ø–æ–∏—Å–∫ –ø–æ —Å–ø–∏—Å–∫—É –∑–∞–ø—Ä–æ—Å–æ–≤"""
    results = {}

    if "google" in search_engines:
        async with AsyncGoogleClient(user_id=123, api_key="your_google_key") as google:
            results["google"] = {}

            for query in queries:
                try:
                    search_results = await google.search(query)
                    results["google"][query] = {
                        "total": search_results.total_results,
                        "returned": len(search_results.results),
                        "results": search_results.results
                    }
                    await asyncio.sleep(1)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                except Exception as e:
                    results["google"][query] = {"error": str(e)}

    if "yandex" in search_engines:
        async with AsyncYandexClient(user_id=123, api_key="your_yandex_key") as yandex:
            results["yandex"] = {}

            for query in queries:
                try:
                    search_results = await yandex.search(query)
                    results["yandex"][query] = {
                        "total": search_results.total_results,
                        "returned": len(search_results.results),
                        "results": search_results.results
                    }
                    await asyncio.sleep(1)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
                except Exception as e:
                    results["yandex"][query] = {"error": str(e)}

    return results

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
async def main():
    queries = ["python programming", "machine learning", "data science"]
    results = await mass_search(queries)
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(queries)} –∑–∞–ø—Ä–æ—Å–æ–≤")

asyncio.run(main())
```

### –ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –º–∞—Å—Å–æ–≤—ã–π –ø–æ–∏—Å–∫

```python
import asyncio
from xmlriver_pro import AsyncGoogleClient, AsyncYandexClient

async def parallel_mass_search(queries, search_engines=["google", "yandex"]):
    """–ü–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –º–∞—Å—Å–æ–≤—ã–π –ø–æ–∏—Å–∫ —Å —Å–µ–º–∞—Ñ–æ—Ä–æ–º"""
    results = {}
    semaphore = asyncio.Semaphore(5)  # –ú–∞–∫—Å–∏–º—É–º 5 –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤

    async def search_query(client, query, engine):
        async with semaphore:
            try:
                search_results = await client.search(query)
                return {
                    "engine": engine,
                    "query": query,
                    "total": search_results.total_results,
                    "returned": len(search_results.results),
                    "results": search_results.results
                }
            except Exception as e:
                return {
                    "engine": engine,
                    "query": query,
                    "error": str(e)
                }

    tasks = []

    if "google" in search_engines:
        async with AsyncGoogleClient(user_id=123, api_key="your_google_key") as google:
            for query in queries:
                task = search_query(google, query, "google")
                tasks.append(task)

    if "yandex" in search_engines:
        async with AsyncYandexClient(user_id=123, api_key="your_yandex_key") as yandex:
            for query in queries:
                task = search_query(yandex, query, "yandex")
                tasks.append(task)

    # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
    results = await asyncio.gather(*tasks)
    return results

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
async def main():
    queries = ["python", "javascript", "java", "c++", "go"]
    results = await parallel_mass_search(queries)

    for result in results:
        if "error" in result:
            print(f"–û—à–∏–±–∫–∞ {result['engine']} –¥–ª—è '{result['query']}': {result['error']}")
        else:
            print(f"{result['engine']} - '{result['query']}': {result['total']} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

asyncio.run(main())
```

## –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–æ–∑–∏—Ü–∏–π

### –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –ø–æ–∑–∏—Ü–∏–π —Å–∞–π—Ç–∞

```python
import asyncio
from xmlriver_pro import AsyncGoogleClient, AsyncYandexClient
from datetime import datetime

async def monitor_positions(domain, keywords, days=30):
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–æ–∑–∏—Ü–∏–π —Å–∞–π—Ç–∞ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
    positions = {}

    async with AsyncGoogleClient(user_id=123, api_key="your_google_key") as google:
        for keyword in keywords:
            try:
                results = await google.search(keyword, num_results=100)

                # –ò—â–µ–º –¥–æ–º–µ–Ω –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö
                position = None
                for i, result in enumerate(results.results, 1):
                    if domain in result.url:
                        position = i
                        break

                positions[keyword] = {
                    "position": position,
                    "total_results": results.total_results,
                    "date": datetime.now().isoformat(),
                    "url": result.url if position else None
                }

                await asyncio.sleep(2)  # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏

            except Exception as e:
                positions[keyword] = {"error": str(e)}

    return positions

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
async def main():
    domain = "python.org"
    keywords = ["python programming", "python tutorial", "python documentation"]

    positions = await monitor_positions(domain, keywords)

    for keyword, data in positions.items():
        if "error" in data:
            print(f"–û—à–∏–±–∫–∞ –¥–ª—è '{keyword}': {data['error']}")
        else:
            pos = data["position"]
            if pos:
                print(f"'{keyword}': –ø–æ–∑–∏—Ü–∏—è {pos} –∏–∑ {data['total_results']}")
            else:
                print(f"'{keyword}': –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —Ç–æ–ø-100")

asyncio.run(main())
```

## –ê–Ω–∞–ª–∏–∑ –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤

### –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–∞–º–∏

```python
import asyncio
from xmlriver_pro import AsyncGoogleClient
from collections import defaultdict

async def analyze_competitors(keywords, competitors):
    """–ê–Ω–∞–ª–∏–∑ –ø–æ–∑–∏—Ü–∏–π –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –ø–æ –∫–ª—é—á–µ–≤—ã–º —Å–ª–æ–≤–∞–º"""
    analysis = {}

    async with AsyncGoogleClient(user_id=123, api_key="your_google_key") as google:
        for keyword in keywords:
            try:
                results = await google.search(keyword, num_results=50)

                competitor_positions = {}
                for competitor in competitors:
                    positions = []
                    for i, result in enumerate(results.results, 1):
                        if competitor in result.url:
                            positions.append(i)

                    competitor_positions[competitor] = positions

                analysis[keyword] = {
                    "total_results": results.total_results,
                    "competitor_positions": competitor_positions,
                    "top_10_domains": [result.url for result in results.results[:10]]
                }

                await asyncio.sleep(2)

            except Exception as e:
                analysis[keyword] = {"error": str(e)}

    return analysis

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
async def main():
    keywords = ["python web framework", "python api", "python testing"]
    competitors = ["django.com", "flask.palletsprojects.com", "fastapi.tiangolo.com"]

    analysis = await analyze_competitors(keywords, competitors)

    for keyword, data in analysis.items():
        if "error" in data:
            print(f"–û—à–∏–±–∫–∞ –¥–ª—è '{keyword}': {data['error']}")
        else:
            print(f"\n'{keyword}':")
            for competitor, positions in data["competitor_positions"].items():
                if positions:
                    print(f"  {competitor}: –ø–æ–∑–∏—Ü–∏–∏ {positions}")
                else:
                    print(f"  {competitor}: –Ω–µ –Ω–∞–π–¥–µ–Ω–æ –≤ —Ç–æ–ø-50")

asyncio.run(main())
```

## –ê–Ω–∞–ª–∏–∑ —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —è–¥—Ä–∞ —Å Wordstat

### –°–±–æ—Ä —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —è–¥—Ä–∞

```python
import asyncio
from xmlriver_pro import AsyncWordstatClient
from typing import Set, List
import json

async def collect_semantic_core(
    seed_keywords: List[str],
    region: int = 213,  # –ú–æ—Å–∫–≤–∞
    device: str = "desktop",
    min_frequency: int = 100,
    depth: int = 2
) -> dict:
    """
    –°–±–æ—Ä —Å–µ–º–∞–Ω—Ç–∏—á–µ—Å–∫–æ–≥–æ —è–¥—Ä–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ seed keywords
    
    Args:
        seed_keywords: –ù–∞—á–∞–ª—å–Ω—ã–µ –∫–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞
        region: ID —Ä–µ–≥–∏–æ–Ω–∞ –Ø–Ω–¥–µ–∫—Å–∞
        device: –¢–∏–ø —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞
        min_frequency: –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å –∑–∞–ø—Ä–æ—Å–∞
        depth: –ì–ª—É–±–∏–Ω–∞ —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è (—Å–∫–æ–ª—å–∫–æ —É—Ä–æ–≤–Ω–µ–π –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–π)
    """
    all_keywords = {}
    processed = set()
    to_process = set(seed_keywords)
    
    async with AsyncWordstatClient(user_id=123, api_key="key") as client:
        for level in range(depth):
            print(f"\nüìä –£—Ä–æ–≤–µ–Ω—å {level + 1}/{depth}")
            print(f"–û–±—Ä–∞–±–æ—Ç–∫–∞ {len(to_process)} –∑–∞–ø—Ä–æ—Å–æ–≤...")
            
            next_level = set()
            
            for keyword in to_process:
                if keyword in processed:
                    continue
                    
                try:
                    # –ü–æ–ª—É—á–∞–µ–º —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å
                    frequency = await client.get_frequency(
                        keyword,
                        regions=region,
                        device=device
                    )
                    
                    # –§–∏–ª—å—Ç—Ä—É–µ–º –ø–æ –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç–∏
                    if frequency < min_frequency:
                        processed.add(keyword)
                        continue
                    
                    # –ü–æ–ª—É—á–∞–µ–º –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–∏
                    result = await client.get_words(
                        keyword,
                        regions=region,
                        device=device
                    )
                    
                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
                    all_keywords[keyword] = {
                        "frequency": frequency,
                        "level": level + 1,
                        "associations": [
                            {
                                "text": kw.text,
                                "value": kw.value
                            }
                            for kw in result.associations
                            if kw.value >= min_frequency
                        ][:10]  # –¢–æ–ø-10
                    }
                    
                    # –î–æ–±–∞–≤–ª—è–µ–º –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–∏ –¥–ª—è —Å–ª–µ–¥—É—é—â–µ–≥–æ —É—Ä–æ–≤–Ω—è
                    for kw in result.associations[:5]:  # –¢–æ–ø-5 –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è
                        if kw.value >= min_frequency and kw.text not in processed:
                            next_level.add(kw.text)
                    
                    processed.add(keyword)
                    print(f"‚úÖ {keyword}: {frequency:,} –∑–∞–ø—Ä–æ—Å–æ–≤")
                    
                    await asyncio.sleep(0.5)  # Rate limiting
                    
                except Exception as e:
                    print(f"‚ùå {keyword}: {e}")
                    processed.add(keyword)
            
            to_process = next_level
            
            if not to_process:
                print("–ë–æ–ª—å—à–µ –Ω–µ—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —Ä–∞—Å—à–∏—Ä–µ–Ω–∏—è")
                break
    
    return all_keywords

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
async def main():
    seed_keywords = ["–∫—É–ø–∏—Ç—å —Ç–µ–ª–µ—Ñ–æ–Ω", "—Å–º–∞—Ä—Ç—Ñ–æ–Ω"]
    
    semantic_core = await collect_semantic_core(
        seed_keywords=seed_keywords,
        region=213,  # –ú–æ—Å–∫–≤–∞
        min_frequency=1000,
        depth=2
    )
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    with open("semantic_core.json", "w", encoding="utf-8") as f:
        json.dump(semantic_core, f, ensure_ascii=False, indent=2)
    
    print(f"\nüìà –°–æ–±—Ä–∞–Ω–æ {len(semantic_core)} –∑–∞–ø—Ä–æ—Å–æ–≤")
    
    # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ —É—Ä–æ–≤–Ω—è–º
    by_level = {}
    total_freq = 0
    for kw, data in semantic_core.items():
        level = data["level"]
        by_level[level] = by_level.get(level, 0) + 1
        total_freq += data["frequency"]
    
    print("\nüìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    for level, count in sorted(by_level.items()):
        print(f"  –£—Ä–æ–≤–µ–Ω—å {level}: {count} –∑–∞–ø—Ä–æ—Å–æ–≤")
    print(f"  –°—É–º–º–∞—Ä–Ω–∞—è —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å: {total_freq:,}")

asyncio.run(main())
```

### –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤

```python
import asyncio
from xmlriver_pro import AsyncWordstatClient
from typing import List, Dict
import json

async def cluster_keywords(keywords: List[str], region: int = 213) -> Dict:
    """
    –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–ø—Ä–æ—Å–æ–≤ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏—è –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–π
    """
    keyword_data = {}
    
    async with AsyncWordstatClient(user_id=123, api_key="key") as client:
        print("üìä –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –ø–æ –∑–∞–ø—Ä–æ—Å–∞–º...")
        
        for keyword in keywords:
            try:
                result = await client.get_words(keyword, regions=region)
                frequency = await client.get_frequency(keyword, regions=region)
                
                keyword_data[keyword] = {
                    "frequency": frequency,
                    "associations": set(kw.text for kw in result.associations[:20])
                }
                
                print(f"‚úÖ {keyword}: {frequency:,} –∑–∞–ø—Ä–æ—Å–æ–≤")
                await asyncio.sleep(0.5)
                
            except Exception as e:
                print(f"‚ùå {keyword}: {e}")
    
    # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è
    clusters = []
    processed = set()
    
    for kw1 in keywords:
        if kw1 in processed or kw1 not in keyword_data:
            continue
        
        cluster = {
            "core_keyword": kw1,
            "frequency": keyword_data[kw1]["frequency"],
            "related": []
        }
        
        # –ò—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –∑–∞–ø—Ä–æ—Å—ã
        for kw2 in keywords:
            if kw2 == kw1 or kw2 in processed or kw2 not in keyword_data:
                continue
            
            # –í—ã—á–∏—Å–ª—è–µ–º –ø–µ—Ä–µ—Å–µ—á–µ–Ω–∏–µ –∞—Å—Å–æ—Ü–∏–∞—Ü–∏–π
            intersection = keyword_data[kw1]["associations"] & keyword_data[kw2]["associations"]
            similarity = len(intersection) / min(
                len(keyword_data[kw1]["associations"]),
                len(keyword_data[kw2]["associations"])
            ) if keyword_data[kw1]["associations"] and keyword_data[kw2]["associations"] else 0
            
            # –ï—Å–ª–∏ —Å—Ö–æ–¥—Å—Ç–≤–æ > 30%, –¥–æ–±–∞–≤–ª—è–µ–º –≤ –∫–ª–∞—Å—Ç–µ—Ä
            if similarity > 0.3:
                cluster["related"].append({
                    "keyword": kw2,
                    "frequency": keyword_data[kw2]["frequency"],
                    "similarity": round(similarity * 100, 1)
                })
                processed.add(kw2)
        
        clusters.append(cluster)
        processed.add(kw1)
    
    return {
        "clusters": sorted(clusters, key=lambda x: x["frequency"], reverse=True),
        "total_keywords": len(keywords),
        "clusters_count": len(clusters)
    }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
async def main():
    keywords = [
        "–∫—É–ø–∏—Ç—å —Ç–µ–ª–µ—Ñ–æ–Ω",
        "—Å–º–∞—Ä—Ç—Ñ–æ–Ω —Ü–µ–Ω–∞",
        "—Ç–µ–ª–µ—Ñ–æ–Ω –Ω–µ–¥–æ—Ä–æ–≥–æ",
        "–º–æ–±–∏–ª—å–Ω—ã–π —Ç–µ–ª–µ—Ñ–æ–Ω",
        "–∫—É–ø–∏—Ç—å —Å–º–∞—Ä—Ç—Ñ–æ–Ω",
        "–∞–π—Ñ–æ–Ω —Ü–µ–Ω–∞",
        "samsung —Ç–µ–ª–µ—Ñ–æ–Ω",
    ]
    
    result = await cluster_keywords(keywords, region=213)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    with open("keyword_clusters.json", "w", encoding="utf-8") as f:
        json.dump(result, f, ensure_ascii=False, indent=2)
    
    print(f"\nüìä –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞:")
    print(f"  –í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {result['total_keywords']}")
    print(f"  –°–æ–∑–¥–∞–Ω–æ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤: {result['clusters_count']}")
    
    print("\nüîç –¢–æ–ø-3 –∫–ª–∞—Å—Ç–µ—Ä–∞:")
    for i, cluster in enumerate(result["clusters"][:3], 1):
        print(f"\n{i}. {cluster['core_keyword']} ({cluster['frequency']:,} –∑–∞–ø—Ä–æ—Å–æ–≤)")
        for rel in cluster["related"][:3]:
            print(f"   ‚îî‚îÄ {rel['keyword']} ({rel['similarity']}% —Å—Ö–æ–∂–µ—Å—Ç—å)")

asyncio.run(main())
```

### –ê–Ω–∞–ª–∏–∑ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏

```python
import asyncio
from xmlriver_pro import AsyncWordstatClient
from datetime import datetime, timedelta
import json

async def analyze_seasonality(
    keywords: List[str],
    region: int = 213,
    months: int = 12
) -> Dict:
    """
    –ê–Ω–∞–ª–∏–∑ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏ –∑–∞–ø—Ä–æ—Å–æ–≤ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ N –º–µ—Å—è—Ü–µ–≤
    """
    # –í—ã—á–∏—Å–ª—è–µ–º –¥–∏–∞–ø–∞–∑–æ–Ω –¥–∞—Ç
    end_date = datetime.now()
    start_date = end_date - timedelta(days=30 * months)
    
    results = {}
    
    async with AsyncWordstatClient(user_id=123, api_key="key") as client:
        print(f"üìä –ê–Ω–∞–ª–∏–∑ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç–∏ –∑–∞ {months} –º–µ—Å—è—Ü–µ–≤")
        print(f"–ü–µ—Ä–∏–æ–¥: {start_date.strftime('%d.%m.%Y')} - {end_date.strftime('%d.%m.%Y')}\n")
        
        for keyword in keywords:
            try:
                # –ü–æ–ª—É—á–∞–µ–º –¥–∏–Ω–∞–º–∏–∫—É –ø–æ –º–µ—Å—è—Ü–∞–º
                history = await client.get_history(
                    keyword,
                    regions=region,
                    period="month",
                    start=start_date.strftime("%d.%m.%Y"),
                    end=end_date.strftime("%d.%m.%Y")
                )
                
                # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –¥–∞–Ω–Ω—ã–µ
                values = [point.absolute_value for point in history.history]
                avg_value = sum(values) / len(values) if values else 0
                max_value = max(values) if values else 0
                min_value = min(values) if values else 0
                
                # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –ø–∏–∫–æ–≤—ã–π –º–µ—Å—è—Ü
                peak_month = max(history.history, key=lambda x: x.absolute_value) if history.history else None
                
                results[keyword] = {
                    "total_frequency": history.total_value,
                    "average_monthly": int(avg_value),
                    "max_monthly": max_value,
                    "min_monthly": min_value,
                    "peak_month": peak_month.date if peak_month else None,
                    "volatility": round((max_value - min_value) / avg_value * 100, 1) if avg_value else 0,
                    "history": [
                        {"date": point.date, "value": point.absolute_value}
                        for point in history.history
                    ]
                }
                
                print(f"‚úÖ {keyword}:")
                print(f"   –°—Ä–µ–¥–Ω—è—è —á–∞—Å—Ç–æ—Ç–Ω–æ—Å—Ç—å: {int(avg_value):,}/–º–µ—Å")
                print(f"   –í–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å: {results[keyword]['volatility']}%")
                if peak_month:
                    print(f"   –ü–∏–∫: {peak_month.date} ({peak_month.absolute_value:,})")
                
                await asyncio.sleep(0.5)
                
            except Exception as e:
                print(f"‚ùå {keyword}: {e}")
                results[keyword] = {"error": str(e)}
    
    return results

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
async def main():
    keywords = ["–∫—É–ø–∏—Ç—å –µ–ª–∫—É", "–∫–æ–Ω–¥–∏—Ü–∏–æ–Ω–µ—Ä", "–∑–∏–º–Ω—è—è —Ä–µ–∑–∏–Ω–∞"]
    
    seasonality = await analyze_seasonality(keywords, months=12)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
    with open("seasonality_analysis.json", "w", encoding="utf-8") as f:
        json.dump(seasonality, f, ensure_ascii=False, indent=2)
    
    print("\nüìà –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω!")
    print("\nüî• –°–∞–º—ã–µ –≤–æ–ª–∞—Ç–∏–ª—å–Ω—ã–µ –∑–∞–ø—Ä–æ—Å—ã:")
    volatile = sorted(
        [(k, v["volatility"]) for k, v in seasonality.items() if "volatility" in v],
        key=lambda x: x[1],
        reverse=True
    )
    for keyword, vol in volatile[:5]:
        print(f"  {keyword}: {vol}%")

asyncio.run(main())
```

**–°–º. —Ç–∞–∫–∂–µ:** [WORDSTAT_GUIDE.md](WORDSTAT_GUIDE.md) - –ø–æ–ª–Ω–æ–µ —Ä—É–∫–æ–≤–æ–¥—Å—Ç–≤–æ –ø–æ Wordstat API

## –≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

### –≠–∫—Å–ø–æ—Ä—Ç –≤ CSV

```python
import asyncio
import csv
from xmlriver_pro import AsyncGoogleClient

async def export_to_csv(queries, filename="search_results.csv"):
    """–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –≤ CSV"""
    results = []

    async with AsyncGoogleClient(user_id=123, api_key="your_google_key") as google:
        for query in queries:
            try:
                search_results = await google.search(query, num_results=10)

                for result in search_results.results:
                    results.append({
                        "query": query,
                        "position": result.rank,
                        "title": result.title,
                        "url": result.url,
                        "snippet": result.snippet,
                        "total_results": search_results.total_results
                    })

                await asyncio.sleep(1)

            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}': {e}")

    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ CSV
    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
        fieldnames = ['query', 'position', 'title', 'url', 'snippet', 'total_results']
        writer = csv.DictWriter(csvfile, fieldnames=fieldnames)

        writer.writeheader()
        writer.writerows(results)

    print(f"–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(results)} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ {filename}")

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
async def main():
    queries = ["python programming", "machine learning", "data science"]
    await export_to_csv(queries)

asyncio.run(main())
```

### –≠–∫—Å–ø–æ—Ä—Ç –≤ JSON

```python
import asyncio
import json
from xmlriver_pro import AsyncGoogleClient

async def export_to_json(queries, filename="search_results.json"):
    """–≠–∫—Å–ø–æ—Ä—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞ –≤ JSON"""
    results = []

    async with AsyncGoogleClient(user_id=123, api_key="your_google_key") as google:
        for query in queries:
            try:
                search_results = await google.search(query, num_results=10)

                query_results = {
                    "query": query,
                    "total_results": search_results.total_results,
                    "results": [
                        {
                            "position": result.rank,
                            "title": result.title,
                            "url": result.url,
                            "snippet": result.snippet
                        }
                        for result in search_results.results
                    ]
                }
                results.append(query_results)

                await asyncio.sleep(1)

            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}': {e}")

    # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –≤ JSON
    with open(filename, 'w', encoding='utf-8') as jsonfile:
        json.dump(results, jsonfile, ensure_ascii=False, indent=2)

    print(f"–≠–∫—Å–ø–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–æ {len(results)} –∑–∞–ø—Ä–æ—Å–æ–≤ –≤ {filename}")

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
async def main():
    queries = ["python programming", "machine learning", "data science"]
    await export_to_json(queries)

asyncio.run(main())
```

## –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

### –ü—Ä–æ—Å—Ç–æ–µ –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ

```python
import asyncio
import json
import hashlib
from pathlib import Path
from xmlriver_pro import AsyncGoogleClient

class SearchCache:
    def __init__(self, cache_dir="cache"):
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

    def _get_cache_key(self, query, **kwargs):
        """–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –∫–ª—é—á –∫—ç—à–∞ –Ω–∞ –æ—Å–Ω–æ–≤–µ –∑–∞–ø—Ä–æ—Å–∞ –∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        cache_data = {"query": query, **kwargs}
        cache_string = json.dumps(cache_data, sort_keys=True)
        return hashlib.md5(cache_string.encode()).hexdigest()

    def get(self, query, **kwargs):
        """–ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∏–∑ –∫—ç—à–∞"""
        cache_key = self._get_cache_key(query, **kwargs)
        cache_file = self.cache_dir / f"{cache_key}.json"

        if cache_file.exists():
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return None

    def set(self, query, result, **kwargs):
        """–°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –≤ –∫—ç—à"""
        cache_key = self._get_cache_key(query, **kwargs)
        cache_file = self.cache_dir / f"{cache_key}.json"

        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)

async def cached_search(queries, cache_ttl_hours=24):
    """–ü–æ–∏—Å–∫ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
    cache = SearchCache()
    results = []

    async with AsyncGoogleClient(user_id=123, api_key="your_google_key") as google:
        for query in queries:
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
            cached_result = cache.get(query)
            if cached_result:
                print(f"–†–µ–∑—É–ª—å—Ç–∞—Ç –¥–ª—è '{query}' –≤–∑—è—Ç –∏–∑ –∫—ç—à–∞")
                results.append(cached_result)
                continue

            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–∏—Å–∫
            try:
                search_results = await google.search(query, num_results=10)

                result = {
                    "query": query,
                    "total_results": search_results.total_results,
                    "results": [
                        {
                            "position": result.rank,
                            "title": result.title,
                            "url": result.url,
                            "snippet": result.snippet
                        }
                        for result in search_results.results
                    ]
                }

                # –°–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –∫—ç—à
                cache.set(query, result)
                results.append(result)

                await asyncio.sleep(1)

            except Exception as e:
                print(f"–û—à–∏–±–∫–∞ –¥–ª—è –∑–∞–ø—Ä–æ—Å–∞ '{query}': {e}")

    return results

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
async def main():
    queries = ["python programming", "machine learning", "data science"]
    results = await cached_search(queries)
    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {len(results)} –∑–∞–ø—Ä–æ—Å–æ–≤")

asyncio.run(main())
```

## –ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞

### –û–±—Ä–∞–±–æ—Ç–∫–∞ –±–æ–ª—å—à–∏—Ö –æ–±—ä–µ–º–æ–≤ –¥–∞–Ω–Ω—ã—Ö

```python
import asyncio
from xmlriver_pro import AsyncGoogleClient
from typing import List, Dict, Any

async def batch_process(queries: List[str], batch_size: int = 10):
    """–ü–∞–∫–µ—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∑–∞–ø—Ä–æ—Å–æ–≤"""
    results = []

    # –†–∞–∑–±–∏–≤–∞–µ–º –∑–∞–ø—Ä–æ—Å—ã –Ω–∞ –±–∞—Ç—á–∏
    for i in range(0, len(queries), batch_size):
        batch = queries[i:i + batch_size]
        print(f"–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –±–∞—Ç—á {i//batch_size + 1}: {len(batch)} –∑–∞–ø—Ä–æ—Å–æ–≤")

        batch_results = await process_batch(batch)
        results.extend(batch_results)

        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –±–∞—Ç—á–∞–º–∏
        if i + batch_size < len(queries):
            await asyncio.sleep(5)

    return results

async def process_batch(queries: List[str]) -> List[Dict[str, Any]]:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –±–∞—Ç—á–∞ –∑–∞–ø—Ä–æ—Å–æ–≤"""
    batch_results = []

    async with AsyncGoogleClient(user_id=123, api_key="your_google_key") as google:
        # –°–æ–∑–¥–∞–µ–º –∑–∞–¥–∞—á–∏ –¥–ª—è –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏
        tasks = []
        for query in queries:
            task = process_single_query(google, query)
            tasks.append(task)

        # –í—ã–ø–æ–ª–Ω—è–µ–º –≤—Å–µ –∑–∞–¥–∞—á–∏ –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
        batch_results = await asyncio.gather(*tasks, return_exceptions=True)

        # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        processed_results = []
        for i, result in enumerate(batch_results):
            if isinstance(result, Exception):
                processed_results.append({
                    "query": queries[i],
                    "error": str(result)
                })
            else:
                processed_results.append(result)

    return processed_results

async def process_single_query(google, query: str) -> Dict[str, Any]:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ–¥–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
    try:
        search_results = await google.search(query, num_results=10)

        return {
            "query": query,
            "total_results": search_results.total_results,
            "results": [
                {
                    "position": result.rank,
                    "title": result.title,
                    "url": result.url,
                    "snippet": result.snippet
                }
                for result in search_results.results
            ]
        }
    except Exception as e:
        return {
            "query": query,
            "error": str(e)
        }

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
async def main():
    # –ë–æ–ª—å—à–æ–π —Å–ø–∏—Å–æ–∫ –∑–∞–ø—Ä–æ—Å–æ–≤
    queries = [f"python topic {i}" for i in range(100)]

    results = await batch_process(queries, batch_size=20)

    successful = len([r for r in results if "error" not in r])
    failed = len([r for r in results if "error" in r])

    print(f"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {successful} —É—Å–ø–µ—à–Ω–æ, {failed} —Å –æ—à–∏–±–∫–∞–º–∏")

asyncio.run(main())
```

## –ê–¥–∞–ø—Ç–∏–≤–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ —Å–∫–æ—Ä–æ—Å—Ç—å—é

### –î–∏–Ω–∞–º–∏—á–µ—Å–∫–∞—è —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∫–∞ —Å–∫–æ—Ä–æ—Å—Ç–∏

```python
import asyncio
import time
from xmlriver_pro import AsyncGoogleClient
from xmlriver_pro.core import RateLimitError

class AdaptiveRateLimiter:
    def __init__(self, initial_delay=1.0, max_delay=60.0, backoff_factor=2.0):
        self.delay = initial_delay
        self.max_delay = max_delay
        self.backoff_factor = backoff_factor
        self.last_request_time = 0

    async def wait_if_needed(self):
        """–û–∂–∏–¥–∞–Ω–∏–µ –µ—Å–ª–∏ –Ω—É–∂–Ω–æ —Å–æ–±–ª—é–¥–∞—Ç—å rate limit"""
        current_time = time.time()
        time_since_last = current_time - self.last_request_time

        if time_since_last < self.delay:
            await asyncio.sleep(self.delay - time_since_last)

        self.last_request_time = time.time()

    def increase_delay(self):
        """–£–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –ø—Ä–∏ –æ—à–∏–±–∫–∞—Ö rate limit"""
        self.delay = min(self.delay * self.backoff_factor, self.max_delay)
        print(f"–£–≤–µ–ª–∏—á–µ–Ω–∞ –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–æ {self.delay:.2f} —Å–µ–∫—É–Ω–¥")

    def decrease_delay(self):
        """–£–º–µ–Ω—å—à–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –ø—Ä–∏ —É—Å–ø–µ—à–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–∞—Ö"""
        self.delay = max(self.delay / self.backoff_factor, 0.1)
        print(f"–£–º–µ–Ω—å—à–µ–Ω–∞ –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–æ {self.delay:.2f} —Å–µ–∫—É–Ω–¥")

async def adaptive_search(queries, max_concurrent=5):
    """–ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ–∏—Å–∫ —Å –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—É–ª–∏—Ä–æ–≤–∫–æ–π —Å–∫–æ—Ä–æ—Å—Ç–∏"""
    rate_limiter = AdaptiveRateLimiter()
    semaphore = asyncio.Semaphore(max_concurrent)
    results = []

    async def search_with_retry(google, query):
        async with semaphore:
            for attempt in range(3):  # –ú–∞–∫—Å–∏–º—É–º 3 –ø–æ–ø—ã—Ç–∫–∏
                try:
                    await rate_limiter.wait_if_needed()
                    search_results = await google.search(query)

                    # –£–º–µ–Ω—å—à–∞–µ–º –∑–∞–¥–µ—Ä–∂–∫—É –ø—Ä–∏ —É—Å–ø–µ—Ö–µ
                    rate_limiter.decrease_delay()

                    return {
                        "query": query,
                        "total_results": search_results.total_results,
                        "results": search_results.results
                    }

                except RateLimitError as e:
                    print(f"Rate limit –¥–ª—è '{query}': {e}")
                    rate_limiter.increase_delay()

                    if attempt < 2:  # –ù–µ –ø–æ—Å–ª–µ–¥–Ω—è—è –ø–æ–ø—ã—Ç–∫–∞
                        await asyncio.sleep(rate_limiter.delay)
                        continue
                    else:
                        return {"query": query, "error": str(e)}

                except Exception as e:
                    return {"query": query, "error": str(e)}

    async with AsyncGoogleClient(user_id=123, api_key="your_google_key") as google:
        tasks = [search_with_retry(google, query) for query in queries]
        results = await asyncio.gather(*tasks)

    return results

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
async def main():
    queries = ["python programming", "machine learning", "data science", "web development"]
    results = await adaptive_search(queries)

    for result in results:
        if "error" in result:
            print(f"–û—à–∏–±–∫–∞ –¥–ª—è '{result['query']}': {result['error']}")
        else:
            print(f"'{result['query']}': {result['total_results']} —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤")

asyncio.run(main())
```

## –ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –ø–æ—Ç–æ–∫–æ–≤

### –û—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤

```python
import asyncio
import time
from xmlriver_pro import AsyncGoogleClient

async def monitor_concurrent_usage(queries):
    """–ú–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –ø–æ—Ç–æ–∫–æ–≤"""
    start_time = time.time()
    active_requests = 0
    max_concurrent = 0

    async def search_with_monitoring(google, query, request_id):
        nonlocal active_requests, max_concurrent

        active_requests += 1
        max_concurrent = max(max_concurrent, active_requests)

        print(f"–ó–∞–ø—Ä–æ—Å {request_id}: '{query}' (–∞–∫—Ç–∏–≤–Ω—ã—Ö: {active_requests})")

        try:
            result = await google.search(query)
            return {"request_id": request_id, "query": query, "result": result}
        except Exception as e:
            return {"request_id": request_id, "query": query, "error": str(e)}
        finally:
            active_requests -= 1
            print(f"–ó–∞–ø—Ä–æ—Å {request_id} –∑–∞–≤–µ—Ä—à–µ–Ω (–∞–∫—Ç–∏–≤–Ω—ã—Ö: {active_requests})")

    async with AsyncGoogleClient(user_id=123, api_key="your_google_key", max_concurrent=5) as google:
        tasks = []
        for i, query in enumerate(queries):
            task = search_with_monitoring(google, query, i + 1)
            tasks.append(task)

        results = await asyncio.gather(*tasks)

    end_time = time.time()
    duration = end_time - start_time

    print(f"\n–°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞:")
    print(f"–í—Å–µ–≥–æ –∑–∞–ø—Ä–æ—Å–æ–≤: {len(queries)}")
    print(f"–ú–∞–∫—Å–∏–º—É–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö: {max_concurrent}")
    print(f"–í—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {duration:.2f} —Å–µ–∫—É–Ω–¥")
    print(f"–°—Ä–µ–¥–Ω—è—è —Å–∫–æ—Ä–æ—Å—Ç—å: {len(queries)/duration:.2f} –∑–∞–ø—Ä–æ—Å–æ–≤/—Å–µ–∫")

    return results

# –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ
async def main():
    queries = [f"python topic {i}" for i in range(20)]
    results = await monitor_concurrent_usage(queries)

    successful = len([r for r in results if "error" not in r])
    print(f"–£—Å–ø–µ—à–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞–Ω–æ: {successful}/{len(queries)}")

asyncio.run(main())
```

---

**–ü–æ–¥—Ä–æ–±–Ω–µ–µ:**
- [Examples](examples.md) - –±–∞–∑–æ–≤—ã–µ –ø—Ä–∏–º–µ—Ä—ã –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
- [API Reference](API_REFERENCE.md) - –ø–æ–ª–Ω—ã–π —Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ API
- [Troubleshooting](TROUBLESHOOTING.md) - —Ä–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º
