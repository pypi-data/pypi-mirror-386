{
  "timestamp": "2025-10-17T22:00:39.426853",
  "ollama_config": {
    "base_url": "http://localhost:11434",
    "model": "gemma3:4b"
  },
  "summary": {
    "total_tests": 5,
    "successful_tests": 0,
    "failed_tests": 5,
    "success_rate": 0.0
  },
  "results": [
    {
      "test": "local_mcp_with_ollama",
      "status": "failed",
      "error": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model={'model': 'ollama/gemma3:4b', 'api_base': 'http://localhost:11434', 'temperature': 0.7, 'max_tokens': 2000}\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers"
    },
    {
      "test": "remote_mcp_with_ollama",
      "status": "failed",
      "error": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model={'model': 'ollama/gemma3:4b', 'api_base': 'http://localhost:11434', 'temperature': 0.7, 'max_tokens': 2000}\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers"
    },
    {
      "test": "multiple_mcp_servers_with_ollama",
      "status": "failed",
      "error": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model={'model': 'ollama/gemma3:4b', 'api_base': 'http://localhost:11434', 'temperature': 0.7, 'max_tokens': 2000}\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers"
    },
    {
      "test": "streamable_http_managed",
      "status": "failed",
      "error": "litellm.BadRequestError: LLM Provider NOT provided. Pass in the LLM provider you are trying to call. You passed model={'model': 'ollama/gemma3:4b', 'api_base': 'http://localhost:11434', 'temperature': 0.7, 'max_tokens': 2000}\n Pass model as E.g. For 'Huggingface' inference endpoints pass in `completion(model='huggingface/starcoder',..)` Learn more: https://docs.litellm.ai/docs/providers"
    },
    {
      "test": "streamable_http_manual",
      "status": "failed",
      "error": "'MCPServerAdapter' object has no attribute 'is_connected'"
    }
  ]
}