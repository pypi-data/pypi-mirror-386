# Optional, if present will automatically create a litellm client
# Model name should be in litellm format of (provider)/(model_name)
# ex: anthropic/claude-sonnet-4-20250514
ALTK_MODEL_NAME=

# Optional, if present will create a specific LLM provider
#   - Provider values include openai.sync, watsonx, and others
#   - Particularly useful for components that require validating clients
#       - i.e. SPARC will need litellm.output_val or similar
#   - If present, ALTK_MODEL_NAME will need to be in the provider's format
#   - (see toolkit-core/llm/README for more details)
ALTK_LLM_PROVIDER=

# For Anthropic
ANTHROPIC_API_KEY=

# For OpenAI
OPENAI_API_KEY=

# For Ollama
# OLLAMA_API_URL= # optional, default is the ollama default localhost:11434
# OLLAMA_API_KEY= # optional

# For WatsonX
WX_API_KEY=
WX_PROJECT_ID=
WX_URL="https://us-south.ml.cloud.ibm.com"

# For Azure OpenAI
AZURE_OPENAI_API_KEY=
AZURE_API_BASE=
AZURE_API_VERSION=

# Used by Code Generation to run generated code in sandbox container
#. - (see post-tool-reflection/code_generation/README for more details)
DOCKER_HOST=
