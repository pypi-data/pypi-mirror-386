Metadata-Version: 2.4
Name: nexgen-sdk
Version: 0.1.2
Summary: A custom Python SDK for interacting with AI models, compatible with OpenAI's API format
Author-email: Dhruv Karnwal <kdhruv.fsd@gmail.com>
License-Expression: MIT
Project-URL: Homepage, https://github.com/Dhruv1969Karnwal/ai-sdk-python
Project-URL: Documentation, https://ai-sdk.readthedocs.io
Project-URL: Repository, https://github.com/Dhruv1969Karnwal/ai-sdk-python.git
Project-URL: Issues, https://github.com/Dhruv1969Karnwal/ai-sdk-python/issues
Keywords: ai,openai,sdk,machine-learning,nlp,llm,chatbot
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Developers
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Description-Content-Type: text/markdown
License-File: LICENSE
Requires-Dist: httpx<1.0.0,>=0.23.0
Requires-Dist: typing_extensions>=3.7.4; python_version < "3.8"
Provides-Extra: dev
Requires-Dist: pytest>=7.0.0; extra == "dev"
Requires-Dist: pytest-asyncio>=0.20.0; extra == "dev"
Requires-Dist: black>=22.0.0; extra == "dev"
Requires-Dist: flake8>=5.0.0; extra == "dev"
Requires-Dist: mypy>=0.950; extra == "dev"
Requires-Dist: tox>=3.25.0; extra == "dev"
Provides-Extra: docs
Requires-Dist: sphinx>=5.0.0; extra == "docs"
Requires-Dist: sphinx-rtd-theme>=1.0.0; extra == "docs"
Dynamic: license-file

# AI SDK Python

A comprehensive Python SDK for interacting with AI models, compatible with OpenAI's API format and supporting multiple providers.

## Features

- ü§ñ **Multiple Provider Support**: OpenAI, LiteLLM, and VLLM providers
- üîÑ **Streaming Support**: Real-time streaming responses with event handling
- üõ°Ô∏è **Error Handling**: Comprehensive error handling with custom exceptions
- üîß **Flexible Configuration**: Environment variables, config files, and programmatic setup
- üì¶ **Easy Integration**: Simple, intuitive API design
- üéØ **Type Safety**: Full type hints support

## Installation

```bash
pip install nexgen-sdk
```

For development:
```bash
pip install nexgen-sdk[dev]
```

## Quick Start

### Basic Usage

```python
from ai_sdk import AISDKClient

# Initialize client
client = AISDKClient(
    api_key="your-api-key",
    provider="openai"  # or "vllm", "litellm"
)

# Create chat completion
response = client.chat().create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "user", "content": "Hello, world!"}
    ]
)

print(response['choices'][0]['message']['content'])
```

### Streaming

```python
# Stream responses in real-time
response = client.chat().create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "Tell me a story"}],
    stream=True
)

for chunk in response:
    if chunk['choices'][0]['delta'].get('content'):
        print(chunk['choices'][0]['delta']['content'], end='', flush=True)
```

### Provider-Specific Examples

#### VLLM Provider (requires base_url)
```python
# Local vLLM deployment
client = AISDKClient(
    provider="vllm",
    base_url="http://localhost:8000",  # Required for vllm
    api_key="your-api-key"
)

# Custom vLLM endpoint
client = AISDKClient(
    provider="vllm",
    base_url="https://your-vllm-endpoint.com",  # Required
    api_key="your-api-key"
)

# New format supports complex user content objects
response = client.chat().create(
    system_prompt="You are a helpful AI assistant",
    messages=[
        {
            "role": "user",
            "content": {
                "query": "What is Python?",
                "detail_level": "intermediate",
                "include_examples": True
            }
        }
    ],
    model="your-model-name"
)
```

## Configuration

### Environment Variables
```bash
export AI_SDK_API_KEY="your-api-key"
export AI_SDK_BASE_URL="https://api.openai.com/v1"
export AI_SDK_TIMEOUT="30.0"
```

### Builder Pattern
```python
client = (AISDKClient.builder()
           .with_provider("vllm")
           .with_api_key("your-key")
           .with_base_url("https://your-vllm-endpoint")
           .with_timeout(60.0)
           .build())
```

## Providers

### OpenAI Provider
- **Base URL**: `https://api.openai.com/v1`
- **Models**: GPT-3.5, GPT-4, and all OpenAI models
- **Features**: Chat, completions, embeddings

### VLLM Provider
- **Required Configuration**: `base_url` parameter is mandatory
- **Custom Endpoints**: Supports any VLLM deployment (local, cloud, custom)
- **New Format**: Enhanced message format with complex user content
- **Streaming**: Optimized real-time streaming

**Note**: When using `provider="vllm"`, you must specify `base_url` pointing to your vLLM deployment.

### LiteLLM Provider
- **Unified Interface**: Access 100+ models through LiteLLM
- **Model Support**: OpenAI, Anthropic, Cohere, and more
- **Fallback**: Automatic fallback between providers

## API Reference

### Client
- `AISDKClient()` - Main client class
- `chat()` - Access chat completions
- `completions()` - Access text completions  
- `embeddings()` - Access embeddings

### Chat Completions
- `create()` - Create chat completion or stream
- Parameters: `messages`, `model`, `temperature`, `stream`, etc.

### Error Handling
```python
from ai_sdk.exceptions import (
    AISDKException,
    APIException,
    AuthenticationException,
    RateLimitException
)

try:
    response = client.chat().create(...)
except AuthenticationException:
    print("Invalid API key")
except RateLimitException:
    print("Rate limit exceeded")
except AISDKException as e:
    print(f"SDK Error: {e}")
```

## Development

### Setup
```bash
git clone https://github.com/Dhruv1969Karnwal/ai-sdk-python
cd ai-sdk-python
pip install -e ".[dev]"
```

### Running Tests
```bash
pytest
```

### Code Formatting
```bash
black ai_sdk
flake8 ai_sdk
mypy ai_sdk
```

## Examples

See the `examples/` directory for comprehensive usage examples:

- `vllm_examples.py` - Complete VLLM provider examples with different deployments
- `new_format_example.py` - VLLM new format examples
- `debug_streaming.py` - Streaming debug examples
- `test_clean_streaming.py` - Clean streaming tests

### VLLM Quick Examples

```python
# Local vLLM deployment
client = AISDKClient(
    provider="vllm",
    base_url="http://localhost:8000",
    api_key="your-key"
)

# Custom endpoint
client = AISDKClient(
    provider="vllm",
    base_url="https://your-vllm-endpoint.com",
    api_key="your-key"
)

# Missing base_url will show helpful error message
# client = AISDKClient(provider="vllm", api_key="your-key")  # Error!

## Requirements

- Python 3.7+
- httpx>=0.23.0
- typing_extensions>=3.7.4 (Python < 3.8)

### Optional Dependencies
- litellm>=1.0.0 (for LiteLLM provider)

## License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## Support

- üìñ [Documentation](https://ai-sdk.readthedocs.io)
- üêõ [Issues](https://github.com/Dhruv1969Karnwal/ai-sdk-python/issues)
- üí¨ [Discussions](https://github.com/Dhruv1969Karnwal/ai-sdk-python/discussions)

## Contributing

Contributions are welcome! Please see our [Contributing Guide](CONTRIBUTING.md) for details.

## Changelog

See [CHANGELOG.md](CHANGELOG.md) for version history.
