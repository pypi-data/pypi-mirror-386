"""ä¸»åº”ç”¨ç±»"""

import asyncio
from datetime import datetime
import os
import logging
from textual.app import App, ComposeResult
from textual.containers import Container, Vertical
from textual.widgets import Header, Footer, Static, Button
from textual.worker import Worker
from textual import on

from .data_models import ChatSession, SessionManager
from .widgets import (
    ModelSelectorWidget, ChatHistoryWidget, ChatInputWidget, 
    CustomTextArea, ToolsListModal, SessionHistoryModal, ContextWindowModal,
    ModelConfigManagerWidget
)
from .widgets.config_widgets import ModelConfigModal
from .styles import CSS
from .context_manager import ContextManager, SessionContextManager
from .token_calculator import calculate_token_stats
from .utils.ai_helpers import (
    assess_planning_readiness,
    plan_task_steps,
    get_enabled_tools_openai_format,
    requires_user_confirmation,
    process_tool_calls
)
from .utils import needs_tool_call, execute_tool_call
from .utils.chat_flow import execute_task_steps, process_ai_response
from .utils.chat_stream import stream_chat_with_tools_async, stream_chat_async, stream_and_process_response, augment_system_prompt, sanitize_tool_messages, enforce_openai_tool_sequence, process_tool_sequence
from ketacli.sdk.ai.client import AIClient
from ketacli.sdk.ai.function_call import function_registry, function_executor
from ketacli.sdk.ai.tool_output_compressor import compress_if_large
from textual.widget import Widget

# è½»é‡æ—¥å¿—ï¼šå†™å…¥åˆ°ä»“åº“æ ¹ç›®å½•çš„ log/textual_debug.log
logger = logging.getLogger("ketacli.textual")
if not logger.handlers:
    logger.setLevel(logging.DEBUG)
    try:
        base_dir = os.path.abspath(os.path.join(os.path.dirname(__file__), "../../../"))
        log_dir = os.path.join(base_dir, "log")
        os.makedirs(log_dir, exist_ok=True)
        # åˆå§‹åŒ–æ—¶æ¸…ç©ºæ—¥å¿—æ–‡ä»¶
        log_path = os.path.join(log_dir, "textual_debug.log")
        with open(log_path, "w", encoding="utf-8"):
            pass
        fh = logging.FileHandler(log_path)
        fh.setFormatter(logging.Formatter("%(asctime)s [%(levelname)s] %(message)s"))
        logger.addHandler(fh)
    except Exception:
        # è‹¥æ–‡ä»¶æ—¥å¿—åˆå§‹åŒ–å¤±è´¥ï¼Œä¸å½±å“è¿è¡Œ
        pass


class InteractiveChatApp(App):
    """äº¤äº’å¼èŠå¤©åº”ç”¨"""
    
    CSS = CSS
    
    BINDINGS = [
        ("q", "quit", "é€€å‡º"),
        ("c", "clear_chat", "æ¸…ç©ºå¯¹è¯"),
        ("n", "clear_chat", "æ–°ä¼šè¯"),
        ("t", "show_tools", "æ˜¾ç¤ºå·¥å…·"),
        ("i", "focus_input", "èšç„¦è¾“å…¥æ¡†"),
        ("h", "show_session_history", "å†å²ä¼šè¯"),
        ("m", "show_model_config", "æ¨¡å‹é…ç½®"),
        ("k", "show_context", "ä¸Šä¸‹æ–‡"),
    ]
    
    def __init__(self, **kwargs):
        """åˆå§‹åŒ–äº¤äº’å¼èŠå¤©åº”ç”¨
        
        åˆå§‹åŒ–åº”ç”¨çŠ¶æ€ï¼ŒåŒ…æ‹¬AIå®¢æˆ·ç«¯ã€ä¼šè¯ç®¡ç†ã€ä¸Šä¸‹æ–‡ç®¡ç†å’Œå·¥å…·é…ç½®ç­‰
        
        Args:
            **kwargs: ä¼ é€’ç»™çˆ¶ç±»Appçš„å‚æ•°
        """
        super().__init__(**kwargs)
        self.ai_client = AIClient()
        self.conversation_history = []
        self.user_raw_inputs = [] # å­˜å‚¨ç”¨æˆ·åŸå§‹è¾“å…¥
        self._chat_in_progress = False
        self._current_ai_task = None  # å½“å‰AIå“åº”ä»»åŠ¡
        # è§„åˆ’çŠ¶æ€é”ï¼šä¸€æ¬¡ä¼šè¯ä»…å…è®¸ä¸€æ¬¡ä»»åŠ¡è§„åˆ’
        self.plan_execution_in_progress = False
        self.planning_locked = False
        
        # æ˜¯å¦å¯ç”¨æµå¼è¾“å‡ºï¼ˆå¯é€šè¿‡æ¨¡å‹é…ç½®æˆ–å¤–éƒ¨å‚æ•°è¦†ç›–ï¼‰
        self.enable_streaming = True
        
        # ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        self.context_manager = ContextManager()
        self.session_context_manager = SessionContextManager()
        self.session_manager = SessionManager()
        self.current_session = None

        # å·¥å…·å¯ç”¨çŠ¶æ€ï¼šé»˜è®¤å¼€å¯èµ„æºåˆ—å‡ºã€æ—¥å¿—/æŒ‡æ ‡æœç´¢ã€è·å–æ–‡æ¡£
        # å¯¹åº”å‡½æ•°åï¼šlist_assets, list_queryable_assets, search_data_for_log, search_data_for_metric, get_docs
        self.enabled_tools = {
            "list_assets",
            "list_queryable",
            "search_data",
            "get_repo_fields",
            "get_docs",
        }
        
        # é€šçŸ¥è¿‡æ»¤é…ç½®ï¼šä»…å±•ç¤ºé‡è¦ä¿¡æ¯ï¼ˆerror/warning/successï¼‰
        self._important_severities = {"error", "warning", "success"}
        # æ˜æ˜¾çš„è°ƒè¯•/å™ªéŸ³æ ‡è®°ï¼Œç»Ÿä¸€å±è”½
        self._debug_markers = ("DEBUG", "ğŸ§ª", "ğŸ”§", "â¡ï¸", "ğŸ”—", "âš™ï¸", "ğŸ“©", "ğŸ› ï¸", "ğŸ§¹", "ğŸ”")

    def notify(self, message, **kwargs):
        """ç»Ÿä¸€è¿‡æ»¤é€šçŸ¥ï¼ˆå§”æ‰˜å…¬å…±è¿‡æ»¤é€»è¾‘ï¼‰ï¼Œä»…ä¿ç•™é‡è¦æç¤ºã€‚"""
        try:
            from .utils import filter_notification
        except Exception:
            # å›é€€ï¼šè‹¥å¯¼å…¥å¤±è´¥ï¼Œä»ç›´æ¥è°ƒç”¨çˆ¶ç±»
            if "markup" not in kwargs:
                kwargs['markup'] = False
            return super().notify(message, **kwargs)
        should_send, prepared = filter_notification(message, kwargs, getattr(self, "_debug_markers", ()))
        if not should_send:
            return
        return super().notify(message, **prepared)
        
    def compose(self) -> ComposeResult:
        """æ„å»ºåº”ç”¨UIå¸ƒå±€
        
        å®šä¹‰åº”ç”¨çš„ç•Œé¢ç»“æ„ï¼ŒåŒ…æ‹¬å¤´éƒ¨ã€èŠå¤©å®¹å™¨å’Œåº•éƒ¨ç»„ä»¶
        
        Returns:
            ComposeResult: åŒ…å«UIç»„ä»¶çš„ç”Ÿæˆå™¨ç»“æœ
        """
        yield Header()
        
        with Container(classes="chat-container"):
            yield Static("ğŸ¤– AIæ™ºèƒ½å¯¹è¯åŠ©æ‰‹", classes="chat-header")
            
            with Vertical(classes="chat-main"):
                yield ModelSelectorWidget(id="model-selector")
                yield ChatHistoryWidget(id="chat-history", classes="chat-history")
                yield ChatInputWidget(id="chat-input", classes="chat-input-container")
                
        yield Footer()
        
    def on_mount(self) -> None:
        """åº”ç”¨æŒ‚è½½æ—¶çš„åˆå§‹åŒ–æ“ä½œ
        
        åœ¨åº”ç”¨UIå®ŒæˆæŒ‚è½½åæ‰§è¡Œåˆå§‹åŒ–æ“ä½œï¼ŒåŒ…æ‹¬åˆå§‹åŒ–AIå®¢æˆ·ç«¯å’Œæ·»åŠ æ¬¢è¿æ¶ˆæ¯
        """
        self._initialize_ai_client()
        self._add_welcome_message()
        
    def _initialize_ai_client(self):
        """åˆå§‹åŒ–AIå®¢æˆ·ç«¯
        
        ä»ç³»ç»Ÿæç¤ºè¯æ–‡ä»¶åŠ è½½æç¤ºè¯å¹¶åˆ›å»ºAIå®¢æˆ·ç«¯å®ä¾‹ã€‚
        å¦‚æœåˆå§‹åŒ–å¤±è´¥ï¼Œä¼šæ˜¾ç¤ºé”™è¯¯é€šçŸ¥ã€‚
        """
        try:
            # ä¿®æ­£ç³»ç»Ÿæç¤ºè¯æ–‡ä»¶è·¯å¾„ï¼ŒæŒ‡å‘ sdk/ai/prompts/system.md
            prompt_path = os.path.abspath(
                os.path.join(os.path.dirname(__file__), "../ai/prompts/system.md")
            )
            with open(prompt_path, "r", encoding="utf-8") as f:
                system_prompt = f.read()
            self.ai_client = AIClient(system_prompt=system_prompt)
            # è¯»å–æ¨¡å‹é…ç½®ä¸­çš„ streaming å¼€å…³ï¼ˆè‹¥å­˜åœ¨ï¼‰
            try:
                # 1) ç›´æ¥å­—æ®µ
                self.enable_streaming = bool(getattr(self.ai_client.model_config, "streaming", self.enable_streaming))
                # 2) extra_params ä¸­çš„ streaming / enable_streaming / stream
                extra = getattr(self.ai_client.model_config, "extra_params", {}) or {}
                for key in ("streaming", "enable_streaming", "stream"):
                    if key in extra:
                        self.enable_streaming = bool(extra.get(key))
                        break
            except Exception:
                pass
        except Exception as e:
            self.notify(f"AIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}", severity="error")
            
    def _add_welcome_message(self):
        """æ·»åŠ æ¬¢è¿æ¶ˆæ¯
        
        åœ¨èŠå¤©å†å²ä¸­æ·»åŠ æ¬¢è¿æ¶ˆæ¯ï¼ŒåŒ…æ‹¬åŠŸèƒ½ä»‹ç»ã€ç¤ºä¾‹å’Œå½“å‰å¯ç”¨çš„å·¥å…·åˆ—è¡¨ã€‚
        """
        chat_history = self.query_one("#chat-history", ChatHistoryWidget)
        welcome_msg = f"""ğŸ‘‹ æ¬¢è¿ä½¿ç”¨KetaOps AIæ™ºèƒ½å¯¹è¯åŠ©æ‰‹ï¼

æˆ‘å¯ä»¥å¸®åŠ©æ‚¨ï¼š
- ğŸ“Š æ•°æ®æŸ¥è¯¢å’Œåˆ†æ
- ğŸ” æ™ºèƒ½æœç´¢
- ğŸ“ˆ èµ„æºç›‘æ§
- ğŸ› ï¸ ç³»ç»Ÿç®¡ç†

æ¨èç¤ºä¾‹ï¼š
- å¯¹å½“å‰ç³»ç»Ÿè¿›è¡Œå·¡æ£€ï¼ŒåŒ…æ‹¬é›†ç¾¤çŠ¶æ€ã€èŠ‚ç‚¹çŠ¶æ€ã€åˆ†ç‰‡çŠ¶æ€ç­‰
- æŸ¥è¯¢traceidä¸ºxxxçš„è®°å½•ï¼Œåˆ†æå…¶æ½œåœ¨çš„æ€§èƒ½æˆ–å¼‚å¸¸
- åˆ›å»ºä¸€ä¸ªrepoã€soucetypeç­‰èµ„æºï¼ˆéœ€æ‰‹åŠ¨å¼€å¯å·¥å…·ï¼ŒæŒ‰`T`æŸ¥çœ‹å¯ç”¨å·¥å…·ï¼‰
- æŸ¥è¯¢logs_ketaä»“åº“ä¸­çš„å¼‚å¸¸æ—¥å¿—å¹¶åˆ†æ

è¯·è¾“å…¥æ‚¨çš„é—®é¢˜ï¼Œæˆ‘ä¼šå°½åŠ›ä¸ºæ‚¨æä¾›å¸®åŠ©ã€‚

å½“å‰å·²å¯ç”¨å·¥å…·ï¼š{', '.join([f'`{x}`' for x in self.enabled_tools]) or 'æ— '}ã€‚æ‚¨ä¹Ÿå¯ä»¥æŒ‰ `T` æŸ¥çœ‹å¯ç”¨çš„å·¥å…·åˆ—è¡¨æ¥å¯ç”¨æˆ–å…³é—­å·¥å…·å¼€å…³ã€‚"""
        
        chat_history.add_message("assistant", welcome_msg)

    def _get_enabled_tools_openai_format(self):
        """è·å–å·²å¯ç”¨å·¥å…·çš„OpenAIæ ¼å¼å®šä¹‰åˆ—è¡¨
        
        ä»å…¨å±€å·¥å…·æ³¨å†Œè¡¨ä¸­ç­›é€‰å‡ºå·²å¯ç”¨çš„å·¥å…·ï¼Œå¹¶è¿”å›å…¶OpenAIæ ¼å¼å®šä¹‰ã€‚
        
        Returns:
            list: å·²å¯ç”¨å·¥å…·çš„OpenAIæ ¼å¼å®šä¹‰åˆ—è¡¨ï¼Œå¦‚æœå‡ºé”™åˆ™è¿”å›ç©ºåˆ—è¡¨
        """
        return get_enabled_tools_openai_format(self.enabled_tools)
        
    def on_chat_input_widget_stop_requested(self, message: ChatInputWidget.StopRequested) -> None:
        """å¤„ç†èŠå¤©è¾“å…¥æ§ä»¶çš„åœæ­¢è¯·æ±‚äº‹ä»¶
        
        å½“ç”¨æˆ·è¯·æ±‚åœæ­¢å½“å‰AIä»»åŠ¡æ—¶è§¦å‘ï¼Œå–æ¶ˆæ­£åœ¨è¿›è¡Œçš„AIä»»åŠ¡å¹¶é‡ç½®ç›¸å…³çŠ¶æ€ã€‚
        
        Args:
            message: åœæ­¢è¯·æ±‚äº‹ä»¶å¯¹è±¡
        """
        """å¤„ç†åœæ­¢è¯·æ±‚"""
        if self._current_ai_task:
            self._current_ai_task.cancel()
            self._current_ai_task = None
            self._chat_in_progress = False
            # ä¿æŒè®¡åˆ’æš‚åœçŠ¶æ€ä¸é”ï¼Œé˜²æ­¢åç»­æ¶ˆæ¯è§¦å‘é‡æ–°è§„åˆ’
            try:
                setattr(self, "plan_status", "paused")
                setattr(self, "plan_execution_in_progress", True)
                setattr(self, "planning_locked", True)
                logger.debug("[stop] æ‰‹åŠ¨åœæ­¢ï¼šæ ‡è®°è®¡åˆ’ä¸º pausedï¼Œé”å®šè§„åˆ’=Trueï¼Œæ‰§è¡Œä¸­=True")
            except Exception:
                pass
            
            # é‡ç½®æŒ‰é’®çŠ¶æ€
            chat_input = self.query_one("#chat-input", ChatInputWidget)
            chat_input.set_processing(False)
            
            # æ˜¾ç¤ºåœæ­¢æ¶ˆæ¯
            chat_history = self.query_one("#chat-history", ChatHistoryWidget)
            if chat_history._current_streaming_widget:
                chat_history.finish_streaming_message("**[å·²åœæ­¢å“åº”]**")
            else:
                try:
                    # éæµå¼æ¨¡å¼ä¸‹ä¹Ÿç»™å‡ºåœæ­¢æç¤º
                    chat_history.add_message("assistant", "**[å·²åœæ­¢å“åº”]**")
                except Exception:
                    pass
            
            self.notify("å·²åœæ­¢AIå“åº”", severity="success")

    def on_chat_input_widget_message_sent(self, message: ChatInputWidget.MessageSent) -> None:
        """å¤„ç†èŠå¤©è¾“å…¥æ§ä»¶çš„æ¶ˆæ¯å‘é€äº‹ä»¶
        
        å½“ç”¨æˆ·åœ¨èŠå¤©è¾“å…¥æ§ä»¶ä¸­å‘é€æ¶ˆæ¯æ—¶è§¦å‘ï¼Œå°†ç”¨æˆ·æ¶ˆæ¯æ·»åŠ åˆ°èŠå¤©å†å²å¹¶å¯åŠ¨AIå“åº”å¤„ç†ã€‚
        
        Args:
            message: æ¶ˆæ¯å‘é€äº‹ä»¶å¯¹è±¡
        """
        # è°ƒè¯•ï¼šè®°å½•æ”¶åˆ°çš„ç”¨æˆ·æ¶ˆæ¯
        try:
            msg_preview = (message.message or "").strip().replace("\n", " ")[:120]
            logger.debug(f"[input] æ”¶åˆ°ç”¨æˆ·æ¶ˆæ¯ï¼Œé•¿åº¦={len(message.message or '')}ï¼Œé¢„è§ˆ='{msg_preview}'ï¼Œè¿›è¡Œä¸­={self._chat_in_progress}")
        except Exception:
            pass
        """å¤„ç†ç”¨æˆ·å‘é€çš„æ¶ˆæ¯"""
        if self._chat_in_progress:
            chat_input = self.query_one("#chat-input", ChatInputWidget)
            chat_input.set_loading(False)
            # è¿›åº¦æç¤ºå¼±åŒ–ï¼Œå‡å°‘å™ªéŸ³
            self.notify("å¯¹è¯æ­£åœ¨è¿›è¡Œä¸­ï¼Œè¯·ç¨å€™...", severity="info")
            return
            
        user_message = message.message
        # è®°å½•ç”¨æˆ·åŸå§‹è¾“å…¥
        try:
            self.user_raw_inputs.append(user_message)
        except Exception:
            pass
        chat_history = self.query_one("#chat-history", ChatHistoryWidget)
        
        # å¦‚æœæ˜¯æ–°ä¼šè¯ï¼Œåˆ›å»ºä¼šè¯
        if not self.current_session:
            self.current_session = ChatSession.create_new()
        
        # æ·»åŠ ç”¨æˆ·æ¶ˆæ¯åˆ°å†å²ï¼Œè®¡ç®—tokenç»Ÿè®¡
        model_selector = self.query_one("#model-selector", ModelSelectorWidget)
        selected_model = model_selector.get_selected_model() or "gpt-3.5-turbo"
        
        # è®¡ç®—ç”¨æˆ·æ¶ˆæ¯çš„tokenç»Ÿè®¡
        user_msg_dict = {"role": "user", "content": user_message}
        context_messages = [{"role": msg["role"], "content": msg["content"]} for msg in self.conversation_history]
        user_token_stats = calculate_token_stats(
            current_message=user_msg_dict,
            context_messages=context_messages
        )
        
        chat_history.add_message("user", user_message, token_stats=user_token_stats)
        
        # è°ƒè¯•ï¼šè®°å½•tokenç»Ÿè®¡
        try:
            user_tokens = user_token_stats.get("current_tokens") if isinstance(user_token_stats, dict) else None
            ctx_tokens = user_token_stats.get("context_tokens") if isinstance(user_token_stats, dict) else None
            logger.debug(f"[input] tokenç»Ÿè®¡ï¼šå½“å‰={user_tokens}ï¼Œä¸Šä¸‹æ–‡={ctx_tokens}ï¼Œå†å²æ¡æ•°={len(self.conversation_history)}")
        except Exception:
            pass
        
        # è®¾ç½®å¤„ç†çŠ¶æ€
        chat_input = self.query_one("#chat-input", ChatInputWidget)
        chat_input.set_processing(True)
        try:
            logger.debug("[flow] å¯åŠ¨AIå“åº”å¤„ç†ä»»åŠ¡ï¼ˆworkerï¼‰")
        except Exception:
            pass
        
        # å¼‚æ­¥å¤„ç†AIå“åº”
        self._current_ai_task = self.run_worker(self._process_ai_response(user_message))
        
    async def _assess_planning_readiness(self, user_text: str) -> dict:
        """è¯„ä¼°æ˜¯å¦åº”ç«‹å³ç”Ÿæˆè®¡åˆ’ï¼Œæˆ–å…ˆè¿›è¡Œä¿¡æ¯å‘ç°ã€‚
        
        åˆ†æç”¨æˆ·è¾“å…¥æ–‡æœ¬ï¼Œåˆ¤æ–­æ˜¯å¦æœ‰è¶³å¤Ÿä¿¡æ¯ç›´æ¥ç”Ÿæˆæ‰§è¡Œè®¡åˆ’ï¼Œæˆ–éœ€è¦å…ˆè¿›è¡Œä¿¡æ¯æ”¶é›†ã€‚
        ä½¿ç”¨AIæ¨¡å‹è¿›è¡Œåˆ¤æ–­ï¼Œå¤±è´¥æ—¶å›é€€åˆ°åŸºäºå…³é”®è¯çš„å¯å‘å¼åˆ¤æ–­ã€‚
        
        Args:
            user_text: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
            
        Returns:
            dict: åŒ…å«plan_now(æ˜¯å¦ç«‹å³è§„åˆ’)å’Œreason(åŸå› )çš„å­—å…¸
        """
        return await assess_planning_readiness(self.ai_client, user_text)

    async def _plan_task_steps(self, user_text: str) -> dict:
        """ä½¿ç”¨AIåˆ¤æ–­ç±»å‹ä¸å¤æ‚åº¦ï¼Œå¹¶è¿”å›è§„åˆ’ç»“æœå­—å…¸ã€‚
        
        æ ¹æ®ç”¨æˆ·è¾“å…¥ï¼ŒAIå…ˆåˆ¤æ–­ç±»å‹ï¼ˆé—®é¢˜/ä»»åŠ¡ï¼‰ï¼›å½“ä¸ºä»»åŠ¡æ—¶æŒ‰å¤æ‚åº¦æ‹†åˆ†ä¸ºå¯æ‰§è¡Œæ­¥éª¤ï¼›é—®é¢˜ç±»å‹ä¸æ‹†åˆ†ï¼Œç”±ä¸Šæ¸¸ç›´æ¥å›ç­”ã€‚
        
        Args:
            user_text: ç”¨æˆ·è¾“å…¥çš„æ–‡æœ¬
            
        Returns:
            dict: {"type": "question|task", "complexity": "low|high", "steps": [str]}
        """
        return await plan_task_steps(
            self.ai_client,
            user_text,
            enabled_tools=self.enabled_tools,
            conversation_history=self.conversation_history,
            user_raw_inputs=getattr(self, "user_raw_inputs", []),
        )
    
    async def _execute_task_steps(self, steps: list, original_user_text: str) -> None:
        """é¡ºåºæ‰§è¡Œä»»åŠ¡æ­¥éª¤ï¼šå§”æ‰˜é€šç”¨æ‰§è¡Œæ¨¡å—
        
        ä½¿ç”¨é€šç”¨æ¨¡å—æ‰§è¡Œæ­¥éª¤ï¼ŒåŒ…å«å·¥å…·è°ƒç”¨ã€æ€»ç»“ä¸å®Œæˆåˆ¤å®šã€‚
        """
        return await execute_task_steps(self, steps, original_user_text)
    
    async def _process_ai_response(self, user_message: str):
        """å¤„ç†AIå“åº”ï¼Œå§”æ‰˜åˆ°é€šç”¨æµç¨‹"""
        try:
            logger.debug(f"[flow] è¿›å…¥ _process_ai_responseï¼Œæ¶ˆæ¯é•¿åº¦={len(user_message or '')}")
        except Exception:
            pass
        await process_ai_response(self, user_message)

    async def _stream_chat_with_tools_async(self, messages, tools):
        """å§”æ‰˜åˆ°å…¬å…±æµå¼å®ç°ï¼ˆæ”¯æŒå·¥å…·è°ƒç”¨ï¼‰"""
        return stream_chat_with_tools_async(self, messages, tools)
    
    async def _stream_chat_async(self, messages):
        """å§”æ‰˜åˆ°å…¬å…±æµå¼å®ç°"""
        return stream_chat_async(self, messages)
            
    def _augment_system_prompt(self, base: str) -> str:
        return augment_system_prompt(base)

    def _sanitize_tool_messages(self, messages: list, provider: str) -> list:
        return sanitize_tool_messages(messages, provider)

    def _enforce_openai_tool_sequence(self, msgs: list) -> tuple[list, int, int]:
        return enforce_openai_tool_sequence(msgs)

    def _should_force_tool_call(self, msgs: list, user_text: str) -> bool:
        try:
            text = (user_text or "").strip().lower()
        except Exception:
            text = ""
        continue_words = {"ç»§ç»­", "ç»§ç»­æ‰§è¡Œ", "ç»§ç»­æŸ¥è¯¢", "ç»§ç»­æ£€ç´¢", "ç»§ç»­åˆ†æ", "go on", "continue", "carry on"}
        force_by_text = text in continue_words or (len(text) <= 6 and any(w in text for w in {"ç»§ç»­", "go", "cont"}))
        has_prev_tool_calls = any(
            (m.get("role") == "assistant" and (m.get("tool_calls") or []))
            for m in self.conversation_history
        )
        return bool(force_by_text and has_prev_tool_calls)

    def _prepare_messages(self, user_message: str) -> list:
        try:
            logger.debug(f"[context] å‡†å¤‡æ¶ˆæ¯ï¼šå†å²æ¡æ•°={len(self.conversation_history)}ï¼Œç”¨æˆ·æ–‡æœ¬é•¿åº¦={len(user_message or '')}")
        except Exception:
            pass
        current_message = {"role": "user", "content": user_message}
        if len(self.conversation_history) > 20:
            self.context_manager.update_config(max_messages=15)
            original_messages = self.conversation_history
            compressed_messages = self.context_manager.process_messages(
                original_messages, force_compress=True
            )
            provider = getattr(self.ai_client.model_config, "provider", "")
            sanitized_messages = self._sanitize_tool_messages(compressed_messages, provider)
            try:
                logger.debug(f"[context] ä¸Šä¸‹æ–‡å‹ç¼©ï¼šåŸå§‹={len(original_messages)}ï¼Œå‹ç¼©å={len(compressed_messages)}ï¼Œæä¾›æ–¹={provider}")
            except Exception:
                pass
            if len(compressed_messages) < len(original_messages):
                try:
                    stats = self.context_manager.compressor.get_compression_stats(
                        original_messages, compressed_messages
                    )
                    tokens_saved = max(
                        0,
                        stats.get("estimated_original_tokens", 0)
                        - stats.get("estimated_compressed_tokens", 0)
                    )
                    self.notify(
                        f"ğŸ—œï¸ ä¸Šä¸‹æ–‡å·²å‹ç¼©: {len(original_messages)}â†’{len(compressed_messages)}æ¡æ¶ˆæ¯, èŠ‚çœ{tokens_saved}ä¸ªtoken",
                        timeout=3
                    )
                except Exception:
                    pass
            removed_count = len(compressed_messages) - len(sanitized_messages)
            if removed_count > 0:
                try:
                    logger.debug(f"[context] å·¥å…·æ¶ˆæ¯è§„èŒƒåŒ–ï¼šç§»é™¤ä¸åˆè§„å·¥å…·æ¶ˆæ¯ {removed_count} æ¡")
                except Exception:
                    pass
                self.notify(f"å·²ç§»é™¤ {removed_count} æ¡ä¸åˆè§„çš„å·¥å…·æ¶ˆæ¯ï¼Œé¿å…è¯·æ±‚é”™è¯¯", severity="warning")
            try:
                logger.debug(f"[context] è¿”å›æ¶ˆæ¯æ•°={len(sanitized_messages) + 1}")
            except Exception:
                pass
            return sanitized_messages + [current_message]
        else:
            try:
                logger.debug(f"[context] è¿”å›æ¶ˆæ¯æ•°={len(self.conversation_history) + 1}")
            except Exception:
                pass
            return self.conversation_history + [current_message]

    async def _process_tool_sequence(self, messages: list) -> tuple:
        """å¤„ç†å·¥å…·è°ƒç”¨åºåˆ—ï¼ˆå§”æ‰˜åˆ°å…¬å…±å®ç°ï¼‰"""
        return await process_tool_sequence(self, messages)
            
    async def _stream_and_process_response(self, messages: list, tools: list, streaming_widget) -> tuple:
        """æµå¼è·å–å¹¶å¤„ç†AIå“åº”ï¼ˆå§”æ‰˜å…¬å…±å®ç°ï¼‰"""
        return await stream_and_process_response(self, messages, tools, streaming_widget)
        
    async def _filter_and_deduplicate_tool_calls(self, tool_calls: list, executed_tool_signatures: set) -> tuple:
        """å§”æ‰˜åˆ°å…¬å…±å®ç°ï¼Œè¿‡æ»¤å¹¶å»é‡å·¥å…·è°ƒç”¨"""
        return await filter_and_deduplicate_tool_calls(self, tool_calls, executed_tool_signatures)

    async def _execute_tool_call(self, tool_call, tools, chat_history, messages):
        """æ‰§è¡Œå•ä¸ªå·¥å…·è°ƒç”¨ï¼ˆå§”æ‰˜åˆ°å…¬å…±å®ç°ï¼‰"""
        tool_executor = getattr(self, "tool_executor", None)
        notifier = getattr(self, "notify", None)
        return await execute_tool_call(
            tool_call,
            tools,
            chat_history,
            messages,
            tool_executor=tool_executor,
            notifier=notifier,
        )
        

    def _needs_tool_call(self, content: str) -> bool:
        """å§”æ‰˜åˆ° utils.needs_tool_callï¼Œé›†ä¸­å·¥å…·è§¦å‘é€»è¾‘"""
        return needs_tool_call(content)

    # å…œåº•æ–¹æ¡ˆæš‚ä¸å¯ç”¨ï¼šæ”¹ä¸ºå¼ºåŒ–è°ƒè¯•ä¿¡æ¯ï¼Œå®šä½æ¨¡å‹æœªè¿”å› tool_calls çš„åŸå› 

    def _requires_user_confirmation(self, text: str) -> bool:
        return requires_user_confirmation(text)

    def on_button_pressed(self, event: Button.Pressed) -> None:
        """å¤„ç†æŒ‰é’®ç‚¹å‡»äº‹ä»¶"""
        if event.button.id == "tools-button":
            # ä¿æŒä¸å¿«æ·é”® Ctrl+T ä¸€è‡´ï¼šä¼ å…¥å½“å‰å·²å¯ç”¨å·¥å…·ä»¥é¢„é€‰å¤é€‰æ¡†
            try:
                self.push_screen(ToolsListModal(enabled_tools=self.enabled_tools))
            except Exception:
                # å›é€€ï¼šè‹¥å¼‚å¸¸åˆ™ä»å°è¯•æ‰“å¼€ï¼Œä½†ä¸é¢„é€‰
                self.push_screen(ToolsListModal())
        elif event.button.id == "new-session-button":
            self.action_clear_chat()
            
    def action_clear_chat(self) -> None:
        """æ¸…ç©ºå¯¹è¯å†å²"""
        # ä¿å­˜å½“å‰ä¼šè¯
        self._save_current_session()
        
        # æ¸…ç©ºå¯¹è¯
        chat_history = self.query_one("#chat-history", ChatHistoryWidget)
        chat_history.clear_history()
        self.conversation_history.clear()
        try:
            self.user_raw_inputs.clear()
        except Exception:
            pass
        self.current_session = None
        self._chat_in_progress = False
        # é‡ç½®è§„åˆ’æ‰§è¡ŒçŠ¶æ€ï¼Œé¿å…æ®‹ç•™é”å½±å“åç»­ä¼šè¯
        self.plan_execution_in_progress = False
        self.planning_locked = False
        self._add_welcome_message()
        self.notify("å¯¹è¯å†å²å·²æ¸…ç©º", severity="success")
        
    def action_show_tools(self) -> None:
        """æ˜¾ç¤ºå·¥å…·åˆ—è¡¨"""
        try:
            self.push_screen(ToolsListModal(enabled_tools=self.enabled_tools))
        except Exception as e:
            self.notify(f"æ‰“å¼€å·¥å…·åˆ—è¡¨å¤±è´¥: {e}", severity="error")

    @on(ToolsListModal.ToolsSaved)
    def on_tools_list_modal_tools_saved(self, message: ToolsListModal.ToolsSaved) -> None:
        """å¤„ç†å·¥å…·é€‰æ‹©ä¿å­˜äº‹ä»¶"""
        try:
            selected = set(message.selected_tools or [])
            self.enabled_tools = selected
            # ç®€å•æç¤ºå½“å‰å¯ç”¨å·¥å…·
            names_preview = ", ".join(list(selected)[:6]) if selected else "(æ— )"
            self.notify(f"âœ… å·²æ›´æ–°å¯ç”¨å·¥å…·ï¼š{names_preview}", timeout=4, severity="success")
        except Exception as e:
            self.notify(f"æ›´æ–°å¯ç”¨å·¥å…·å¤±è´¥: {e}", severity="error")
        
    def action_show_session_history(self) -> None:
        """æ˜¾ç¤ºå†å²ä¼šè¯"""
        modal = SessionHistoryModal(self.session_manager)
        self.push_screen(modal)
        
    def action_show_model_config(self) -> None:
        """æ˜¾ç¤ºæ¨¡å‹é…ç½®ç®¡ç†"""
        from .model_config_app import ModelConfigScreen
        self.push_screen(ModelConfigScreen())
    
    def action_show_context(self) -> None:
        """æ˜¾ç¤ºä¸Šä¸‹æ–‡çª—å£"""
        try:
            self.push_screen(ContextWindowModal())
        except Exception as e:
            self.notify(f"æ‰“å¼€ä¸Šä¸‹æ–‡çª—å£å¤±è´¥: {e}", severity="error")
    
    @on(ModelConfigModal.ConfigSaved)
    def on_model_config_saved(self, event: ModelConfigModal.ConfigSaved) -> None:
        """å¤„ç†æ¨¡å‹é…ç½®ä¿å­˜äº‹ä»¶ï¼Œè½¬å‘ç»™å½“å‰çš„ ModelConfigScreen"""
        # æ·»åŠ è°ƒè¯•ä¿¡æ¯
        self.notify("DEBUG: InteractiveChatApp.on_model_config_saved è¢«è°ƒç”¨ï¼Œå‡†å¤‡è½¬å‘ç»™ ModelConfigScreen", severity="info")
        
        # è·å–å½“å‰å±å¹•æ ˆä¸­çš„ ModelConfigScreen
        from .model_config_app import ModelConfigScreen
        for screen in reversed(self.screen_stack):
            if isinstance(screen, ModelConfigScreen):
                # æ‰¾åˆ°äº† ModelConfigScreenï¼Œè½¬å‘æ¶ˆæ¯
                self.notify("DEBUG: æ‰¾åˆ° ModelConfigScreenï¼Œè½¬å‘ ConfigSaved æ¶ˆæ¯", severity="info")
                screen.on_config_saved(event)
                break
        else:
            self.notify("DEBUG: æœªæ‰¾åˆ° ModelConfigScreen", severity="warning")
        
        # åˆ·æ–°ä¸»ç•Œé¢çš„æ¨¡å‹é€‰æ‹©å™¨
        try:
            model_selector = self.query_one(ModelSelectorWidget)
            model_selector.refresh_model_list()
            self.notify("DEBUG: ä¸»ç•Œé¢æ¨¡å‹é€‰æ‹©å™¨å·²åˆ·æ–°", severity="info")
        except Exception as e:
            self.notify(f"DEBUG: åˆ·æ–°æ¨¡å‹é€‰æ‹©å™¨å¤±è´¥: {e}", severity="warning")
        
    def action_focus_input(self) -> None:
        """èšç„¦åˆ°è¾“å…¥æ¡†"""
        input_widget = self.query_one("#message-input", CustomTextArea)
        input_widget.focus()
    
    def on_session_history_modal_session_selected(self, message) -> None:
        """å¤„ç†å†å²ä¼šè¯é€‰æ‹©äº‹ä»¶"""
        self._load_session(message.session)
    
    def _load_session(self, session: ChatSession):
        """åŠ è½½æŒ‡å®šä¼šè¯"""
        # ä¿å­˜å½“å‰ä¼šè¯
        if self.current_session and self.conversation_history:
            self.current_session.messages = self.conversation_history.copy()
            self.session_manager.save_session(self.current_session)
        
        # åŠ è½½æ–°ä¼šè¯
        self.current_session = session
        self.conversation_history = session.messages.copy()
        
        # æ›´æ–°UI
        chat_history = self.query_one("#chat-history", ChatHistoryWidget)
        chat_history.clear_history()
        
        # é‡æ–°æ˜¾ç¤ºå†å²æ¶ˆæ¯
        for message in self.conversation_history:
            if message["role"] == "user":
                chat_history.add_message("user", message["content"])
            elif message["role"] == "assistant":
                chat_history.add_message("assistant", message["content"])
        
        # åŠ è½½å®Œæˆåæç¤ºä¸€æ¬¡
        self.notify(f"å·²åŠ è½½ä¼šè¯: {session.get_display_title()}", severity="success")
    
    def _save_current_session(self):
        """ä¿å­˜å½“å‰ä¼šè¯
        
        å°†å½“å‰ä¼šè¯çš„æ¶ˆæ¯å†å²å’Œä¸Šä¸‹æ–‡ä¿å­˜åˆ°ä¼šè¯ç®¡ç†å™¨ä¸­ï¼Œ
        ç¡®ä¿ä¼šè¯çŠ¶æ€åœ¨åº”ç”¨é‡å¯åèƒ½å¤Ÿæ¢å¤ã€‚
        """
        """ä¿å­˜å½“å‰ä¼šè¯"""
        if not self.conversation_history:
            return
        
        if not self.current_session:
            # åˆ›å»ºæ–°ä¼šè¯
            self.current_session = ChatSession.create_new()
        
        # æ›´æ–°ä¼šè¯æ¶ˆæ¯
        self.current_session.messages = self.conversation_history.copy()
        self.current_session.updated_at = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        # ä¿å­˜ä¼šè¯
        self.session_manager.save_session(self.current_session)


def run_interactive_chat():
    """è¿è¡Œäº¤äº’å¼å¯¹è¯åº”ç”¨"""
    import signal
    import sys
    import threading
    import concurrent.futures
    
    # æ·»åŠ ä¿¡å·å¤„ç†ï¼Œç¡®ä¿ç¨‹åºå¯ä»¥æ­£å¸¸é€€å‡º
    def signal_handler(sig, frame):
        print("\næ­£åœ¨å®‰å…¨é€€å‡ºç¨‹åº...")
        # å…³é—­æ‰€æœ‰çº¿ç¨‹æ± 
        for executor in concurrent.futures._thread._global_shutdown_thread_pools:
            if hasattr(executor, '_threads'):
                for thread in executor._threads:
                    if thread is not None:
                        thread._tstate_lock = None
        # é€€å‡ºç¨‹åº
        sys.exit(0)
    
    # æ³¨å†Œä¿¡å·å¤„ç†å™¨
    signal.signal(signal.SIGINT, signal_handler)
    
    try:
        app = InteractiveChatApp()
        app.run()
    except KeyboardInterrupt:
        print("\næ£€æµ‹åˆ°é”®ç›˜ä¸­æ–­ï¼Œæ­£åœ¨å®‰å…¨é€€å‡º...")
        sys.exit(0)
    except Exception as e:
        print(f"äº¤äº’å¼èŠå¤©åº”ç”¨å¯åŠ¨å¤±è´¥: {e}")
        import traceback
        with open("interactive_chat_error.log", "w") as f:
            traceback.print_exc(file=f)
