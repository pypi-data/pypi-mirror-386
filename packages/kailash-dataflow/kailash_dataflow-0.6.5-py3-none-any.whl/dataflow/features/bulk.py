"""
DataFlow Bulk Operations

High-performance bulk database operations.
"""

from typing import Any, Dict, List


class BulkOperations:
    """High-performance bulk operations for DataFlow."""

    def __init__(self, dataflow_instance):
        self.dataflow = dataflow_instance

    async def bulk_create(
        self,
        model_name: str,
        data: List[Dict[str, Any]],
        batch_size: int = 1000,
        **kwargs,
    ) -> Dict[str, Any]:
        """Perform bulk create operation."""
        import logging

        logger = logging.getLogger(__name__)

        logger.warning(
            f"BULK_CREATE ENTRY: model={model_name}, data_count={len(data) if data else 0}, kwargs={kwargs}"
        )

        # Handle None data
        if data is None:
            return {"success": False, "error": "Data cannot be None"}

        # Handle empty data list (valid - insert 0 records)
        if len(data) == 0:
            return {
                "records_processed": 0,
                "success_count": 0,
                "failure_count": 0,
                "batches": 0,
                "batch_size": batch_size,
                "success": True,
            }

        # Apply tenant context if multi-tenant
        if self.dataflow.config.security.multi_tenant and self.dataflow._tenant_context:
            tenant_id = self.dataflow._tenant_context.get("tenant_id")
            for record in data:
                record["tenant_id"] = tenant_id

        # Auto-convert ISO datetime strings to datetime objects for each record
        from ..core.nodes import convert_datetime_fields

        model_fields = self.dataflow.get_model_fields(model_name)
        for record in data:
            convert_datetime_fields(record, model_fields, logger)

        # Perform actual database insertion
        try:
            connection_string = self.dataflow.config.database.get_connection_url(
                self.dataflow.config.environment
            )
            database_type = self.dataflow._detect_database_type()
            table_name = self.dataflow._class_name_to_table_name(model_name)

            logger.warning(
                f"BULK_CREATE: conn={connection_string[:50]}..., db_type={database_type}, table={table_name}"
            )

            # Build INSERT query from data
            if not data:
                return {
                    "records_processed": 0,
                    "success_count": 0,
                    "failure_count": 0,
                    "batches": 0,
                    "batch_size": batch_size,
                    "success": True,
                }

            # Get column names from first record
            columns = list(data[0].keys())
            column_names = ", ".join(columns)

            # Build VALUES clause with placeholders
            total_inserted = 0
            batches_processed = 0

            # Process in batches
            for i in range(0, len(data), batch_size):
                batch = data[i : i + batch_size]
                values_placeholders = []
                params = []

                for record in batch:
                    if database_type.lower() == "postgresql":
                        placeholders = ", ".join(
                            [
                                f"${j + 1}"
                                for j in range(len(params), len(params) + len(columns))
                            ]
                        )
                    elif database_type.lower() == "mysql":
                        placeholders = ", ".join(["%s"] * len(columns))
                    else:  # sqlite
                        placeholders = ", ".join(["?"] * len(columns))

                    values_placeholders.append(f"({placeholders})")
                    params.extend([record.get(col) for col in columns])

                values_clause = ", ".join(values_placeholders)
                query = (
                    f"INSERT INTO {table_name} ({column_names}) VALUES {values_clause}"
                )

                logger.warning(
                    f"BULK_CREATE: Executing batch {batches_processed + 1}, query='{query[:100]}...', param_count={len(params)}"
                )

                # Execute using AsyncSQLDatabaseNode
                from kailash.nodes.data.async_sql import AsyncSQLDatabaseNode

                sql_node = AsyncSQLDatabaseNode(
                    connection_string=connection_string,
                    database_type=database_type,
                    query=query,
                    params=params,
                    fetch_mode="all",
                    validate_queries=False,
                    transaction_mode="auto",
                )
                result = await sql_node.async_run()

                logger.warning(f"BULK_CREATE: SQL result={result}")

                # Extract rows_affected from result
                # For INSERT: AsyncSQLDatabaseNode returns row_count at result level
                # For DELETE: returns rows_affected in data[0]
                rows_affected = 0
                if result and "result" in result:
                    result_data = result["result"]
                    # Try row_count first (INSERT operations)
                    if "row_count" in result_data:
                        rows_affected = result_data.get("row_count", 0)
                    # Fall back to rows_affected in data (DELETE operations)
                    elif "data" in result_data and len(result_data["data"]) > 0:
                        rows_affected = result_data["data"][0].get("rows_affected", 0)

                total_inserted += rows_affected
                batches_processed += 1

            success_result = {
                "records_processed": total_inserted,
                "success_count": total_inserted,
                "failure_count": 0,
                "batches": batches_processed,
                "batch_size": batch_size,
                "success": True,
            }
            logger.warning(f"BULK_CREATE SUCCESS: {success_result}")
            return success_result

        except Exception as e:
            error_result = {
                "success": False,
                "error": f"Bulk create operation failed: {str(e)}",
                "records_processed": 0,
            }
            logger.error(f"BULK_CREATE EXCEPTION: {e}", exc_info=True)
            logger.error(f"BULK_CREATE ERROR RESULT: {error_result}")
            return error_result

    async def bulk_update(
        self,
        model_name: str,
        data: List[Dict[str, Any]] = None,
        filter_criteria: Dict[str, Any] = None,
        update_values: Dict[str, Any] = None,
        batch_size: int = 1000,
        **kwargs,
    ) -> Dict[str, Any]:
        """Perform bulk update operation."""
        import logging

        logger = logging.getLogger(__name__)

        logger.warning(
            f"BULK_UPDATE ENTRY: model={model_name}, data={data}, filter={filter_criteria}, update={update_values}, kwargs={kwargs}"
        )

        # Extract safe_mode and confirmed parameters
        safe_mode = kwargs.get("safe_mode", True)
        confirmed = kwargs.get("confirmed", False)

        # Determine operation mode based on which parameters have actual content
        # Filter-based: has non-empty update_values (filter can be empty = update all)
        # Data-based: has data parameter (can be empty list)
        has_update_values = update_values is not None and bool(update_values)
        has_data = data is not None

        is_filter_based = has_update_values
        is_data_based = has_data and not has_update_values

        # Validation: Empty filter requires confirmation (only for filter-based updates)
        if is_filter_based and not filter_criteria:
            # Empty dict {} means update ALL records - require confirmation
            logger.warning("BULK_UPDATE: Empty filter detected, checking confirmation")
            if safe_mode and not confirmed:
                error_result = {
                    "success": False,
                    "error": "Bulk update with empty filter requires confirmed=True. "
                    "Empty filter will update ALL records in the table. "
                    "Set confirmed=True to proceed or provide a specific filter.",
                    "records_processed": 0,
                }
                logger.error(f"BULK_UPDATE VALIDATION FAILED: {error_result}")
                return error_result

        if is_filter_based:
            # Filter-based bulk update - perform actual database operation
            logger.warning("BULK_UPDATE: Processing filter-based update")
            try:
                # Auto-convert ISO datetime strings to datetime objects in update_values
                from ..core.nodes import convert_datetime_fields

                model_fields = self.dataflow.get_model_fields(model_name)
                update_values = convert_datetime_fields(
                    update_values, model_fields, logger
                )

                # Get database connection and execute UPDATE
                connection_string = self.dataflow.config.database.get_connection_url(
                    self.dataflow.config.environment
                )
                database_type = self.dataflow._detect_database_type()
                table_name = self.dataflow._class_name_to_table_name(model_name)

                logger.warning(
                    f"BULK_UPDATE: conn={connection_string[:50]}..., db_type={database_type}, table={table_name}"
                )

                # Build SET clause from update_values
                set_parts = []
                params = []
                for field, value in update_values.items():
                    if database_type.lower() == "postgresql":
                        set_parts.append(f"{field} = ${len(params) + 1}")
                    elif database_type.lower() == "mysql":
                        set_parts.append(f"{field} = %s")
                    else:  # sqlite
                        set_parts.append(f"{field} = ?")
                    params.append(value)

                set_clause = "SET " + ", ".join(set_parts)

                # Build WHERE clause from filter
                if filter_criteria:
                    # Has specific filter criteria
                    where_parts = []
                    for field, value in filter_criteria.items():
                        if database_type.lower() == "postgresql":
                            where_parts.append(f"{field} = ${len(params) + 1}")
                        elif database_type.lower() == "mysql":
                            where_parts.append(f"{field} = %s")
                        else:  # sqlite
                            where_parts.append(f"{field} = ?")
                        params.append(value)
                    where_clause = "WHERE " + " AND ".join(where_parts)
                else:
                    # Empty filter = update all (requires confirmation, already checked above)
                    where_clause = ""

                query = f"UPDATE {table_name} {set_clause} {where_clause}"
                logger.warning(
                    f"BULK_UPDATE: Executing query='{query}' with params={params}"
                )

                # Execute using AsyncSQLDatabaseNode
                from kailash.nodes.data.async_sql import AsyncSQLDatabaseNode

                sql_node = AsyncSQLDatabaseNode(
                    connection_string=connection_string,
                    database_type=database_type,
                    query=query,
                    params=params,
                    fetch_mode="all",
                    validate_queries=False,
                    transaction_mode="auto",
                )
                result = await sql_node.async_run()

                logger.warning(f"BULK_UPDATE: SQL result={result}")

                # Extract rows_affected from result
                # AsyncSQLDatabaseNode returns: {'result': {'data': [{'rows_affected': N}], ...}}
                rows_affected = 0
                if result and "result" in result:
                    result_data = result["result"]
                    if "data" in result_data and len(result_data["data"]) > 0:
                        rows_affected = result_data["data"][0].get("rows_affected", 0)

                success_result = {
                    "filter": filter_criteria,
                    "update": update_values,
                    "records_processed": rows_affected,
                    "success_count": rows_affected,
                    "failure_count": 0,
                    "success": True,
                }
                logger.warning(f"BULK_UPDATE SUCCESS: {success_result}")
                return success_result
            except Exception as e:
                error_result = {
                    "success": False,
                    "error": f"Bulk update operation failed: {str(e)}",
                    "records_processed": 0,
                }
                logger.error(f"BULK_UPDATE EXCEPTION: {e}", exc_info=True)
                logger.error(f"BULK_UPDATE ERROR RESULT: {error_result}")
                return error_result
        elif data is not None:
            # Data-based bulk update - update records by id
            logger.warning("BULK_UPDATE: Processing data-based update")

            # Handle empty data list (valid - update 0 records)
            if len(data) == 0:
                return {
                    "records_processed": 0,
                    "success_count": 0,
                    "failure_count": 0,
                    "batches": 0,
                    "batch_size": batch_size,
                    "success": True,
                }

            # Auto-convert ISO datetime strings to datetime objects for each record
            from ..core.nodes import convert_datetime_fields

            model_fields = self.dataflow.get_model_fields(model_name)
            for record in data:
                convert_datetime_fields(record, model_fields, logger)

            try:
                connection_string = self.dataflow.config.database.get_connection_url(
                    self.dataflow.config.environment
                )
                database_type = self.dataflow._detect_database_type()
                table_name = self.dataflow._class_name_to_table_name(model_name)

                logger.warning(
                    f"BULK_UPDATE: conn={connection_string[:50]}..., db_type={database_type}, table={table_name}"
                )

                total_updated = 0
                batches_processed = 0

                # Process in batches
                for i in range(0, len(data), batch_size):
                    batch = data[i : i + batch_size]

                    # Execute individual UPDATEs for each record
                    for record in batch:
                        if "id" not in record:
                            logger.warning(
                                f"BULK_UPDATE: Skipping record without id: {record}"
                            )
                            continue

                        # Build SET clause from record (exclude id)
                        set_parts = []
                        params = []
                        for field, value in record.items():
                            if field == "id":
                                continue
                            if database_type.lower() == "postgresql":
                                set_parts.append(f"{field} = ${len(params) + 1}")
                            elif database_type.lower() == "mysql":
                                set_parts.append(f"{field} = %s")
                            else:  # sqlite
                                set_parts.append(f"{field} = ?")
                            params.append(value)

                        if not set_parts:
                            logger.warning(
                                f"BULK_UPDATE: No fields to update for record: {record}"
                            )
                            continue

                        set_clause = "SET " + ", ".join(set_parts)

                        # Build WHERE clause for id
                        if database_type.lower() == "postgresql":
                            where_clause = f"WHERE id = ${len(params) + 1}"
                        elif database_type.lower() == "mysql":
                            where_clause = "WHERE id = %s"
                        else:  # sqlite
                            where_clause = "WHERE id = ?"
                        params.append(record["id"])

                        query = f"UPDATE {table_name} {set_clause} {where_clause}"

                        # Execute using AsyncSQLDatabaseNode
                        from kailash.nodes.data.async_sql import AsyncSQLDatabaseNode

                        sql_node = AsyncSQLDatabaseNode(
                            connection_string=connection_string,
                            database_type=database_type,
                            query=query,
                            params=params,
                            fetch_mode="all",
                            validate_queries=False,
                            transaction_mode="auto",
                        )
                        result = await sql_node.async_run()

                        # Extract rows_affected
                        rows_affected = 0
                        if result and "result" in result:
                            result_data = result["result"]
                            if "data" in result_data and len(result_data["data"]) > 0:
                                rows_affected = result_data["data"][0].get(
                                    "rows_affected", 0
                                )

                        total_updated += rows_affected

                    batches_processed += 1

                success_result = {
                    "records_processed": total_updated,
                    "success_count": total_updated,
                    "failure_count": 0,
                    "batches": batches_processed,
                    "batch_size": batch_size,
                    "success": True,
                }
                logger.warning(f"BULK_UPDATE SUCCESS: {success_result}")
                return success_result
            except Exception as e:
                error_result = {
                    "success": False,
                    "error": f"Bulk update operation failed: {str(e)}",
                    "records_processed": 0,
                }
                logger.error(f"BULK_UPDATE EXCEPTION: {e}", exc_info=True)
                logger.error(f"BULK_UPDATE ERROR RESULT: {error_result}")
                return error_result

        return {"success": False, "error": "Either data or filter+update required"}

    async def bulk_delete(
        self,
        model_name: str,
        data: List[Dict[str, Any]] = None,
        filter_criteria: Dict[str, Any] = None,
        batch_size: int = 1000,
        **kwargs,
    ) -> Dict[str, Any]:
        """Perform bulk delete operation."""
        import logging

        logger = logging.getLogger(__name__)

        logger.warning(
            f"BULK_DELETE ENTRY: model={model_name}, data={data}, filter={filter_criteria}, kwargs={kwargs}"
        )

        # Extract safe_mode and confirmed parameters
        safe_mode = kwargs.get("safe_mode", True)
        confirmed = kwargs.get("confirmed", False)

        logger.warning(
            f"BULK_DELETE VALIDATION: safe_mode={safe_mode}, confirmed={confirmed}"
        )

        # Validation: Empty filter requires confirmation
        if filter_criteria is not None and not filter_criteria:
            # Empty dict {} means delete ALL records - require confirmation
            logger.warning("BULK_DELETE: Empty filter detected, checking confirmation")
            if safe_mode and not confirmed:
                error_result = {
                    "success": False,
                    "error": "Bulk delete with empty filter requires confirmed=True. "
                    "Empty filter will delete ALL records in the table. "
                    "Set confirmed=True to proceed or provide a specific filter.",
                    "records_processed": 0,
                }
                logger.error(f"BULK_DELETE VALIDATION FAILED: {error_result}")
                return error_result

        if filter_criteria is not None:
            # Filter-based bulk delete - perform actual database operation
            logger.warning("BULK_DELETE: Processing filter-based delete")
            try:
                # Get database connection and execute DELETE
                connection_string = self.dataflow.config.database.get_connection_url(
                    self.dataflow.config.environment
                )
                database_type = self.dataflow._detect_database_type()
                table_name = self.dataflow._class_name_to_table_name(model_name)

                logger.warning(
                    f"BULK_DELETE: conn={connection_string[:50]}..., db_type={database_type}, table={table_name}"
                )

                # Build WHERE clause from filter
                if filter_criteria:
                    # Has specific filter criteria
                    where_parts = []
                    params = []
                    for field, value in filter_criteria.items():
                        if database_type.lower() == "postgresql":
                            where_parts.append(f"{field} = ${len(params) + 1}")
                        elif database_type.lower() == "mysql":
                            where_parts.append(f"{field} = %s")
                        else:  # sqlite
                            where_parts.append(f"{field} = ?")
                        params.append(value)
                    where_clause = "WHERE " + " AND ".join(where_parts)
                else:
                    # Empty filter = delete all (requires confirmation, already checked above)
                    where_clause = ""
                    params = []

                query = f"DELETE FROM {table_name} {where_clause}"
                logger.warning(
                    f"BULK_DELETE: Executing query='{query}' with params={params}"
                )

                # Execute using AsyncSQLDatabaseNode
                from kailash.nodes.data.async_sql import AsyncSQLDatabaseNode

                sql_node = AsyncSQLDatabaseNode(
                    connection_string=connection_string,
                    database_type=database_type,
                    query=query,
                    params=params,
                    fetch_mode="all",
                    validate_queries=False,
                    transaction_mode="auto",
                )
                result = await sql_node.async_run()

                logger.warning(f"BULK_DELETE: SQL result={result}")

                # Extract rows_affected from result
                # AsyncSQLDatabaseNode returns: {'result': {'data': [{'rows_affected': N}], ...}}
                rows_affected = 0
                if result and "result" in result:
                    result_data = result["result"]
                    if "data" in result_data and len(result_data["data"]) > 0:
                        rows_affected = result_data["data"][0].get("rows_affected", 0)

                success_result = {
                    "filter": filter_criteria,
                    "records_processed": rows_affected,
                    "success_count": rows_affected,
                    "failure_count": 0,
                    "success": True,
                }
                logger.warning(f"BULK_DELETE SUCCESS: {success_result}")
                return success_result
            except Exception as e:
                error_result = {
                    "success": False,
                    "error": f"Bulk delete operation failed: {str(e)}",
                    "records_processed": 0,
                }
                logger.error(f"BULK_DELETE EXCEPTION: {e}", exc_info=True)
                logger.error(f"BULK_DELETE ERROR RESULT: {error_result}")
                return error_result
        elif data is not None:
            # Data-based bulk delete (empty list [] is valid)
            return {
                "records_processed": len(data),
                "success_count": len(data),
                "failure_count": 0,
                "batch_size": batch_size,
                "success": True,
            }

        return {"success": False, "error": "Either data or filter required"}

    async def bulk_upsert(
        self,
        model_name: str,
        data: List[Dict[str, Any]],
        conflict_resolution: str = "skip",
        batch_size: int = 1000,
        **kwargs,
    ) -> Dict[str, Any]:
        """Perform bulk upsert (insert or update) operation.

        WARNING: This is currently a STUB implementation that returns simulated data.
        Real database upsert operations are NOT yet implemented.
        Data is NOT being inserted or updated in the database.
        """
        import logging

        logger = logging.getLogger(__name__)
        logger.warning(
            f"BULK_UPSERT WARNING: This is a STUB implementation! "
            f"No actual database operations are performed. "
            f"Data will NOT be inserted or updated. "
            f"Model: {model_name}, Records: {len(data)}"
        )

        # Apply tenant context if multi-tenant
        if self.dataflow.config.security.multi_tenant and self.dataflow._tenant_context:
            tenant_id = self.dataflow._tenant_context.get("tenant_id")
            for record in data:
                record["tenant_id"] = tenant_id

        # Auto-convert ISO datetime strings to datetime objects for each record
        from ..core.nodes import convert_datetime_fields

        model_fields = self.dataflow.get_model_fields(model_name)
        for record in data:
            convert_datetime_fields(record, model_fields, logger)

        # Simulate upsert with conflict resolution
        total_records = len(data)
        inserted = int(total_records * 0.7)  # 70% new records
        updated = total_records - inserted  # 30% existing records

        return {
            "records_processed": total_records,
            "inserted": inserted,
            "updated": updated,
            "skipped": 0,
            "conflict_resolution": conflict_resolution,
            "batch_size": batch_size,
            "success": True,
        }
