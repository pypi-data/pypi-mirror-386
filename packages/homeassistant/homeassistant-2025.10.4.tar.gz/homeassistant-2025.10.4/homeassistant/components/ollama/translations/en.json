{
    "config": {
        "abort": {
            "already_configured": "Service is already configured"
        },
        "error": {
            "cannot_connect": "Failed to connect",
            "invalid_url": "Invalid hostname or IP address",
            "unknown": "Unexpected error"
        },
        "step": {
            "user": {
                "data": {
                    "url": "URL"
                }
            }
        }
    },
    "config_subentries": {
        "ai_task_data": {
            "abort": {
                "cannot_connect": "Failed to connect",
                "download_failed": "Model downloading failed",
                "entry_not_loaded": "Failed to add agent. The configuration is disabled.",
                "reconfigure_successful": "Re-configuration was successful"
            },
            "entry_type": "AI task",
            "initiate_flow": {
                "reconfigure": "Reconfigure AI task",
                "user": "Add AI task"
            },
            "progress": {
                "download": "Please wait while the model is downloaded, which may take a very long time. Check your Ollama server logs for more details."
            },
            "step": {
                "download": {
                    "title": "Downloading model"
                },
                "set_options": {
                    "data": {
                        "keep_alive": "Keep alive",
                        "max_history": "Max history messages",
                        "model": "Model",
                        "name": "Name",
                        "num_ctx": "Context window size",
                        "prompt": "Instructions",
                        "think": "Think before responding"
                    },
                    "data_description": {
                        "keep_alive": "Duration in seconds for Ollama to keep model in memory. -1 = indefinite, 0 = never.",
                        "num_ctx": "Maximum number of text tokens the model can process. Lower to reduce Ollama RAM, or increase for a large number of exposed entities.",
                        "prompt": "Instruct how the LLM should respond. This can be a template.",
                        "think": "If enabled, the LLM will think before responding. This can improve response quality but may increase latency."
                    }
                }
            }
        },
        "conversation": {
            "abort": {
                "cannot_connect": "Failed to connect",
                "download_failed": "Model downloading failed",
                "entry_not_loaded": "Failed to add agent. The configuration is disabled.",
                "reconfigure_successful": "Re-configuration was successful"
            },
            "entry_type": "Conversation agent",
            "initiate_flow": {
                "reconfigure": "Reconfigure conversation agent",
                "user": "Add conversation agent"
            },
            "progress": {
                "download": "Please wait while the model is downloaded, which may take a very long time. Check your Ollama server logs for more details."
            },
            "step": {
                "download": {
                    "title": "Downloading model"
                },
                "set_options": {
                    "data": {
                        "keep_alive": "Keep alive",
                        "llm_hass_api": "Control Home Assistant",
                        "max_history": "Max history messages",
                        "model": "Model",
                        "name": "Name",
                        "num_ctx": "Context window size",
                        "prompt": "Instructions",
                        "think": "Think before responding"
                    },
                    "data_description": {
                        "keep_alive": "Duration in seconds for Ollama to keep model in memory. -1 = indefinite, 0 = never.",
                        "num_ctx": "Maximum number of text tokens the model can process. Lower to reduce Ollama RAM, or increase for a large number of exposed entities.",
                        "prompt": "Instruct how the LLM should respond. This can be a template.",
                        "think": "If enabled, the LLM will think before responding. This can improve response quality but may increase latency."
                    }
                }
            }
        }
    },
    "exceptions": {
        "unsupported_attachment_type": {
            "message": "Ollama only supports image attachments in user content, but received non-image attachment."
        }
    }
}