{
    "config": {
        "abort": {
            "already_configured": "Dienst ist bereits konfiguriert"
        },
        "error": {
            "authentication_error": "Ung\u00fcltige Authentifizierung",
            "cannot_connect": "Verbindung fehlgeschlagen",
            "timeout_connect": "Zeit\u00fcberschreitung beim Verbindungsaufbau",
            "unknown": "Unerwarteter Fehler"
        },
        "step": {
            "user": {
                "data": {
                    "api_key": "API-Schl\u00fcssel"
                }
            }
        }
    },
    "config_subentries": {
        "conversation": {
            "abort": {
                "entry_not_loaded": "Es k\u00f6nnen keine Dinge hinzugef\u00fcgt werden, wenn die Konfiguration deaktiviert ist.",
                "reconfigure_successful": "Die Neukonfiguration war erfolgreich"
            },
            "entry_type": "Konversationsagent",
            "error": {
                "thinking_budget_too_large": "Die maximale Anzahl an Token muss gr\u00f6\u00dfer sein als das Denkbudget."
            },
            "initiate_flow": {
                "reconfigure": "Konversationsagent neu konfigurieren",
                "user": "Konversationsagent hinzuf\u00fcgen"
            },
            "step": {
                "set_options": {
                    "data": {
                        "chat_model": "Modell",
                        "llm_hass_api": "Home Assistant steuern",
                        "max_tokens": "Maximale Anzahl an Token, die als Antwort zur\u00fcckgegeben werden",
                        "name": "Name",
                        "prompt": "Anweisungen",
                        "recommended": "Empfohlene Modelleinstellungen",
                        "temperature": "Temperatur",
                        "thinking_budget_tokens": "Denkbudget"
                    },
                    "data_description": {
                        "prompt": "Gib an, wie das LLM antworten soll. Dies kann ein Template sein.",
                        "thinking_budget_tokens": "Die Anzahl der Token, die das Modell verwenden kann, um \u00fcber die Antwort nachzudenken (aus der maximalen Gesamtzahl der Token). Setze den Wert auf 1024 oder h\u00f6her, um erweitertes Denken zu erm\u00f6glichen."
                    }
                }
            }
        }
    }
}