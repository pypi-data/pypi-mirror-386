name: benchmark

on:
  workflow_dispatch:
    inputs:
      # Run Parameters
      n_jobs:
        description: "Number of parallel jobs"
        required: true
        type: number
        default: 3
      tries_per_task:
        description: "Number of tries per task"
        required: true
        type: number
        default: 3

      # Falco Configuration
      use_vision:
        description: "Use vision capabilities"
        required: true
        type: boolean
        default: true
      headless:
        description: "Run in headless mode"
        required: true
        type: boolean
        default: true
      model:
        description: "Model to use"
        required: true
        type: string
        default: "vertex_ai/gemini-2.0-flash"
      max_steps:
        description: "Maximum steps per task"
        required: true
        type: number
        default: 20
      proxies:
        description: "Use proxies"
        required: true
        type: boolean
        default: false
      user_agent:
        description: "Custom user agent (optional)"
        required: false
        type: string
        default: ""

concurrency:
  group: >-
    ${{ github.workflow }}-${{ github.ref }}-
    ${{ github.event.inputs.task_set_name }}-${{ github.event.inputs.model }}
  cancel-in-progress: true

env:
  PYTHON_VERSION: "3.11"
  CACHE_TYPE: "pip"

jobs:
  run-benchmark:
    runs-on: ubuntu-latest
    timeout-minutes: 180
    steps:
      - uses: actions/checkout@v4

      - name: Set environment variables
        run: |
          echo "DISABLE_TELEMETRY=true" >> $GITHUB_ENV
          echo "CEREBRAS_API_KEY=${{ secrets.CEREBRAS_API_KEY_CICD }}" >> $GITHUB_ENV
          echo "OPENAI_API_KEY=${{ secrets.OPENAI_API_KEY }}" >> $GITHUB_ENV

      - id: 'auth'
        uses: 'google-github-actions/auth@v2'
        with:
          credentials_json: ${{ secrets.VERTEX_AI }}

      - name: Install uv
        uses: astral-sh/setup-uv@v5
        with:
            enable-cache: true
            cache-dependency-glob: "uv.lock"

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: ${{ env.PYTHON_VERSION }}
          cache: ${{ env.CACHE_TYPE }}

      - name: Cache patchright
        id: cache-patchright
        uses: actions/cache@v4
        with:
          path: |
            ~/.cache/ms-playwright
            ~/.cache/patchright
            ~/.local/share/patchright
            ${{ github.workspace }}/.patchright
          key: ${{ runner.os }}-patchright-${{ hashFiles('**/pyproject.toml') }}-playwright1155-ffmpeg1011-v1
          restore-keys: |
            ${{ runner.os }}-patchright-${{ hashFiles('**/pyproject.toml') }}-playwright1155-ffmpeg1011-
            ${{ runner.os }}-patchright-

      - name: Install dependencies
        run: uv sync --dev --all-extras

      - name: Install patchright
        if: steps.cache-patchright.outputs.cache-hit != 'true'
        run: |
          echo "Cache miss - installing patchright"
          uv run patchright install --with-deps chromium  --no-shell

      - name: Generate TOML config
        run: |
          cat > benchmark.toml << EOF
          [RunParameters]
          n_jobs = ${{ github.event.inputs.n_jobs }}
          tries_per_task = ${{ github.event.inputs.tries_per_task }}
          evaluator = "None"
          capture_logging = true

          [RunParameters.task_set]
          name = "WebVoyagerSimple"

          [Falco]
          use_vision = ${{ github.event.inputs.use_vision }}
          headless = ${{ github.event.inputs.headless }}
          model = "${{ github.event.inputs.model }}"
          max_steps = ${{ github.event.inputs.max_steps }}
          proxies = ${{ github.event.inputs.proxies }}

          EOF

      - name: Debug config.toml content
        run: |
          cat benchmark.toml

      - name: Run benchmark unit tests
        run: uv run pytest tests/integration/test_e2e.py --capture=no -p no:asyncio --config benchmark.toml

      - name: Upload md results as step summary
        if: always()
        run: cat dist/results.html >> $GITHUB_STEP_SUMMARY

      - name: Upload Logs / Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: benchmark-results
          path: |
            dist/*
