Metadata-Version: 2.4
Name: acryl-datahub-actions
Version: 1.3.0.1rc5
Summary: An action framework to work with DataHub real time changes.
Home-page: https://docs.datahub.com/
License: Apache-2.0
Project-URL: Documentation, https://docs.datahub.com/docs/actions
Project-URL: Source, https://github.com/acryldata/datahub-actions
Project-URL: Changelog, https://github.com/acryldata/datahub-actions/releases
Classifier: Development Status :: 5 - Production/Stable
Classifier: Programming Language :: Python
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3 :: Only
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Information Technology
Classifier: Intended Audience :: System Administrators
Classifier: Operating System :: Unix
Classifier: Operating System :: POSIX :: Linux
Classifier: Environment :: Console
Classifier: Environment :: MacOS X
Classifier: Topic :: Software Development
Requires-Python: >=3.9
Description-Content-Type: text/markdown
Requires-Dist: h11>=0.16
Requires-Dist: ratelimit
Requires-Dist: PyYAML
Requires-Dist: click-default-group
Requires-Dist: toml>=0.10.0
Requires-Dist: python-dateutil>=2.8.0
Requires-Dist: prometheus-client
Requires-Dist: progressbar2
Requires-Dist: tenacity
Requires-Dist: typing-inspect
Requires-Dist: pydantic<3.0.0,>=2.0.0
Requires-Dist: stackprinter
Requires-Dist: httpcore>=1.0.9
Requires-Dist: aws-msk-iam-sasl-signer-python==1.0.2
Requires-Dist: click>=6.0.0
Requires-Dist: entrypoints
Requires-Dist: acryl-datahub[datahub-kafka]==1.3.0.1rc5
Requires-Dist: azure-identity==1.21.0
Provides-Extra: base
Requires-Dist: PyYAML; extra == "base"
Requires-Dist: click-default-group; extra == "base"
Requires-Dist: toml>=0.10.0; extra == "base"
Requires-Dist: python-dateutil>=2.8.0; extra == "base"
Requires-Dist: prometheus-client; extra == "base"
Requires-Dist: progressbar2; extra == "base"
Requires-Dist: stackprinter; extra == "base"
Requires-Dist: click>=6.0.0; extra == "base"
Requires-Dist: entrypoints; extra == "base"
Requires-Dist: tenacity; extra == "base"
Provides-Extra: kafka
Requires-Dist: PyYAML; extra == "kafka"
Requires-Dist: click-default-group; extra == "kafka"
Requires-Dist: toml>=0.10.0; extra == "kafka"
Requires-Dist: python-dateutil>=2.8.0; extra == "kafka"
Requires-Dist: prometheus-client; extra == "kafka"
Requires-Dist: progressbar2; extra == "kafka"
Requires-Dist: stackprinter; extra == "kafka"
Requires-Dist: confluent-kafka[schemaregistry]; extra == "kafka"
Requires-Dist: click>=6.0.0; extra == "kafka"
Requires-Dist: entrypoints; extra == "kafka"
Requires-Dist: tenacity; extra == "kafka"
Provides-Extra: executor
Requires-Dist: PyYAML; extra == "executor"
Requires-Dist: click-default-group; extra == "executor"
Requires-Dist: toml>=0.10.0; extra == "executor"
Requires-Dist: python-dateutil>=2.8.0; extra == "executor"
Requires-Dist: prometheus-client; extra == "executor"
Requires-Dist: progressbar2; extra == "executor"
Requires-Dist: acryl-executor==0.2.6; extra == "executor"
Requires-Dist: stackprinter; extra == "executor"
Requires-Dist: click>=6.0.0; extra == "executor"
Requires-Dist: entrypoints; extra == "executor"
Requires-Dist: tenacity; extra == "executor"
Provides-Extra: slack
Requires-Dist: PyYAML; extra == "slack"
Requires-Dist: click-default-group; extra == "slack"
Requires-Dist: toml>=0.10.0; extra == "slack"
Requires-Dist: python-dateutil>=2.8.0; extra == "slack"
Requires-Dist: prometheus-client; extra == "slack"
Requires-Dist: progressbar2; extra == "slack"
Requires-Dist: slack-bolt>=1.15.5; extra == "slack"
Requires-Dist: stackprinter; extra == "slack"
Requires-Dist: click>=6.0.0; extra == "slack"
Requires-Dist: entrypoints; extra == "slack"
Requires-Dist: tenacity; extra == "slack"
Provides-Extra: teams
Requires-Dist: PyYAML; extra == "teams"
Requires-Dist: click-default-group; extra == "teams"
Requires-Dist: toml>=0.10.0; extra == "teams"
Requires-Dist: python-dateutil>=2.8.0; extra == "teams"
Requires-Dist: prometheus-client; extra == "teams"
Requires-Dist: progressbar2; extra == "teams"
Requires-Dist: stackprinter; extra == "teams"
Requires-Dist: click>=6.0.0; extra == "teams"
Requires-Dist: entrypoints; extra == "teams"
Requires-Dist: pymsteams>=0.2.2; extra == "teams"
Requires-Dist: tenacity; extra == "teams"
Provides-Extra: tag-propagation
Requires-Dist: PyYAML; extra == "tag-propagation"
Requires-Dist: click-default-group; extra == "tag-propagation"
Requires-Dist: toml>=0.10.0; extra == "tag-propagation"
Requires-Dist: python-dateutil>=2.8.0; extra == "tag-propagation"
Requires-Dist: prometheus-client; extra == "tag-propagation"
Requires-Dist: progressbar2; extra == "tag-propagation"
Requires-Dist: stackprinter; extra == "tag-propagation"
Requires-Dist: click>=6.0.0; extra == "tag-propagation"
Requires-Dist: entrypoints; extra == "tag-propagation"
Requires-Dist: tenacity; extra == "tag-propagation"
Provides-Extra: term-propagation
Requires-Dist: PyYAML; extra == "term-propagation"
Requires-Dist: click-default-group; extra == "term-propagation"
Requires-Dist: toml>=0.10.0; extra == "term-propagation"
Requires-Dist: python-dateutil>=2.8.0; extra == "term-propagation"
Requires-Dist: prometheus-client; extra == "term-propagation"
Requires-Dist: progressbar2; extra == "term-propagation"
Requires-Dist: stackprinter; extra == "term-propagation"
Requires-Dist: click>=6.0.0; extra == "term-propagation"
Requires-Dist: entrypoints; extra == "term-propagation"
Requires-Dist: tenacity; extra == "term-propagation"
Provides-Extra: snowflake-tag-propagation
Requires-Dist: PyYAML; extra == "snowflake-tag-propagation"
Requires-Dist: acryl-datahub[snowflake-slim]==1.3.0.1rc5; extra == "snowflake-tag-propagation"
Requires-Dist: click-default-group; extra == "snowflake-tag-propagation"
Requires-Dist: toml>=0.10.0; extra == "snowflake-tag-propagation"
Requires-Dist: python-dateutil>=2.8.0; extra == "snowflake-tag-propagation"
Requires-Dist: prometheus-client; extra == "snowflake-tag-propagation"
Requires-Dist: progressbar2; extra == "snowflake-tag-propagation"
Requires-Dist: stackprinter; extra == "snowflake-tag-propagation"
Requires-Dist: click>=6.0.0; extra == "snowflake-tag-propagation"
Requires-Dist: entrypoints; extra == "snowflake-tag-propagation"
Requires-Dist: tenacity; extra == "snowflake-tag-propagation"
Provides-Extra: doc-propagation
Requires-Dist: PyYAML; extra == "doc-propagation"
Requires-Dist: click-default-group; extra == "doc-propagation"
Requires-Dist: toml>=0.10.0; extra == "doc-propagation"
Requires-Dist: python-dateutil>=2.8.0; extra == "doc-propagation"
Requires-Dist: prometheus-client; extra == "doc-propagation"
Requires-Dist: progressbar2; extra == "doc-propagation"
Requires-Dist: stackprinter; extra == "doc-propagation"
Requires-Dist: click>=6.0.0; extra == "doc-propagation"
Requires-Dist: entrypoints; extra == "doc-propagation"
Requires-Dist: tenacity; extra == "doc-propagation"
Provides-Extra: all
Requires-Dist: PyYAML; extra == "all"
Requires-Dist: acryl-datahub[snowflake-slim]==1.3.0.1rc5; extra == "all"
Requires-Dist: click-default-group; extra == "all"
Requires-Dist: toml>=0.10.0; extra == "all"
Requires-Dist: python-dateutil>=2.8.0; extra == "all"
Requires-Dist: prometheus-client; extra == "all"
Requires-Dist: progressbar2; extra == "all"
Requires-Dist: acryl-executor==0.2.6; extra == "all"
Requires-Dist: slack-bolt>=1.15.5; extra == "all"
Requires-Dist: stackprinter; extra == "all"
Requires-Dist: confluent-kafka[schemaregistry]; extra == "all"
Requires-Dist: click>=6.0.0; extra == "all"
Requires-Dist: entrypoints; extra == "all"
Requires-Dist: pymsteams>=0.2.2; extra == "all"
Requires-Dist: tenacity; extra == "all"
Provides-Extra: dev
Requires-Dist: toml>=0.10.0; extra == "dev"
Requires-Dist: python-dateutil>=2.8.0; extra == "dev"
Requires-Dist: prometheus-client; extra == "dev"
Requires-Dist: types-freezegun; extra == "dev"
Requires-Dist: acryl-executor==0.2.6; extra == "dev"
Requires-Dist: pytest-cov>=2.8.1; extra == "dev"
Requires-Dist: stackprinter; extra == "dev"
Requires-Dist: aws-msk-iam-sasl-signer-python==1.0.2; extra == "dev"
Requires-Dist: entrypoints; extra == "dev"
Requires-Dist: acryl-datahub[datahub-kafka]==1.3.0.1rc5; extra == "dev"
Requires-Dist: azure-identity==1.21.0; extra == "dev"
Requires-Dist: twine; extra == "dev"
Requires-Dist: types-six; extra == "dev"
Requires-Dist: requests-mock; extra == "dev"
Requires-Dist: types-dataclasses; extra == "dev"
Requires-Dist: acryl-datahub[snowflake-slim]==1.3.0.1rc5; extra == "dev"
Requires-Dist: progressbar2; extra == "dev"
Requires-Dist: pytest-docker>=0.10.3; extra == "dev"
Requires-Dist: tox; extra == "dev"
Requires-Dist: jsonpickle; extra == "dev"
Requires-Dist: typing-inspect; extra == "dev"
Requires-Dist: ruff==0.11.7; extra == "dev"
Requires-Dist: coverage>=5.1; extra == "dev"
Requires-Dist: httpcore>=1.0.9; extra == "dev"
Requires-Dist: types-toml; extra == "dev"
Requires-Dist: types-cachetools; extra == "dev"
Requires-Dist: types-pytz; extra == "dev"
Requires-Dist: freezegun; extra == "dev"
Requires-Dist: h11>=0.16; extra == "dev"
Requires-Dist: PyYAML; extra == "dev"
Requires-Dist: types-PyYAML; extra == "dev"
Requires-Dist: pytest-dependency>=0.5.1; extra == "dev"
Requires-Dist: confluent-kafka[schemaregistry]; extra == "dev"
Requires-Dist: click>=6.0.0; extra == "dev"
Requires-Dist: types-PyMySQL; extra == "dev"
Requires-Dist: types-python-dateutil; extra == "dev"
Requires-Dist: types-click==0.1.12; extra == "dev"
Requires-Dist: types-setuptools; extra == "dev"
Requires-Dist: tenacity; extra == "dev"
Requires-Dist: deepdiff; extra == "dev"
Requires-Dist: build; extra == "dev"
Requires-Dist: ratelimit; extra == "dev"
Requires-Dist: sqlalchemy-stubs; extra == "dev"
Requires-Dist: click-default-group; extra == "dev"
Requires-Dist: types-requests; extra == "dev"
Requires-Dist: pydantic<3.0.0,>=2.0.0; extra == "dev"
Requires-Dist: pytest>=6.2.2; extra == "dev"
Requires-Dist: mypy==1.17.1; extra == "dev"
Requires-Dist: pymsteams>=0.2.2; extra == "dev"
Requires-Dist: slack-bolt>=1.15.5; extra == "dev"
Provides-Extra: integration-tests
Requires-Dist: acryl-datahub[snowflake-slim]==1.3.0.1rc5; extra == "integration-tests"
Requires-Dist: acryl-executor==0.2.6; extra == "integration-tests"
Requires-Dist: confluent-kafka[schemaregistry]; extra == "integration-tests"
Requires-Dist: pymsteams>=0.2.2; extra == "integration-tests"
Requires-Dist: slack-bolt>=1.15.5; extra == "integration-tests"
Dynamic: classifier
Dynamic: description
Dynamic: description-content-type
Dynamic: home-page
Dynamic: license
Dynamic: project-url
Dynamic: provides-extra
Dynamic: requires-dist
Dynamic: requires-python
Dynamic: summary

# ⚡ DataHub Actions Framework

Welcome to DataHub Actions! The Actions framework makes responding to realtime changes in your Metadata Graph easy, enabling you to seamlessly integrate [DataHub](https://github.com/datahub-project/datahub) into a broader events-based architecture.

For a detailed introduction, check out the [original announcement](https://www.youtube.com/watch?v=7iwNxHgqxtg&t=2189s) of the DataHub Actions Framework at the DataHub April 2022 Town Hall. For a more in-depth look at use cases and concepts, check out [DataHub Actions Concepts](../docs/actions/concepts.md).

## Quickstart

To get started right away, check out the [DataHub Actions Quickstart](../docs/actions/quickstart.md) Guide.

## Prerequisites

The DataHub Actions CLI commands are an extension of the base `datahub` CLI commands. We recommend
first installing the `datahub` CLI:

```shell
python3 -m pip install --upgrade pip wheel setuptools
python3 -m pip install --upgrade acryl-datahub
datahub --version
```

> Note that the Actions Framework requires a version of `acryl-datahub` >= v0.8.34

## Installation

Next, simply install the `acryl-datahub-actions` package from PyPi:

```shell
python3 -m pip install --upgrade pip wheel setuptools
python3 -m pip install --upgrade acryl-datahub-actions
datahub actions version
```

## Configuring an Action

Actions are configured using a YAML file, much in the same way DataHub ingestion sources are. An action configuration file consists of the following

1. Action Pipeline Name (Should be unique and static)
2. Source Configurations
3. Transform + Filter Configurations
4. Action Configuration
5. Pipeline Options (Optional)
6. DataHub API configs (Optional - required for select actions)

With each component being independently pluggable and configurable.

```yml
# 1. Required: Action Pipeline Name
name: <action-pipeline-name>

# 2. Required: Event Source - Where to source event from.
source:
  type: <source-type>
  config:
    # Event Source specific configs (map)

# 3a. Optional: Filter to run on events (map)
filter:
  event_type: <filtered-event-type>
  event:
    # Filter event fields by exact-match
    <filtered-event-fields>

# 3b. Optional: Custom Transformers to run on events (array)
transform:
  - type: <transformer-type>
    config:
      # Transformer-specific configs (map)

# 4. Required: Action - What action to take on events.
action:
  type: <action-type>
  config:
    # Action-specific configs (map)

# 5. Optional: Additional pipeline options (error handling, etc)
options:
  retry_count: 0 # The number of times to retry an Action with the same event. (If an exception is thrown). 0 by default.
  failure_mode: "CONTINUE" # What to do when an event fails to be processed. Either 'CONTINUE' to make progress or 'THROW' to stop the pipeline. Either way, the failed event will be logged to a failed_events.log file.
  failed_events_dir: "/tmp/datahub/actions" # The directory in which to write a failed_events.log file that tracks events which fail to be processed. Defaults to "/tmp/logs/datahub/actions".

# 6. Optional: DataHub API configuration
datahub:
  server: "http://localhost:8080" # Location of DataHub API
  # token: <your-access-token> # Required if Metadata Service Auth enabled
```

### Example: Hello World

An simple configuration file for a "Hello World" action, which simply prints all events it receives, is

```yml
# 1. Action Pipeline Name
name: "hello_world"
# 2. Event Source: Where to source event from.
source:
  type: "kafka"
  config:
    connection:
      bootstrap: ${KAFKA_BOOTSTRAP_SERVER:-localhost:9092}
      schema_registry_url: ${SCHEMA_REGISTRY_URL:-http://localhost:8081}
# 3. Action: What action to take on events.
action:
  type: "hello_world"
```

We can modify this configuration further to filter for specific events, by adding a "filter" block.

```yml
# 1. Action Pipeline Name
name: "hello_world"

# 2. Event Source - Where to source event from.
source:
  type: "kafka"
  config:
    connection:
      bootstrap: ${KAFKA_BOOTSTRAP_SERVER:-localhost:9092}
      schema_registry_url: ${SCHEMA_REGISTRY_URL:-http://localhost:8081}

# 3. Filter - Filter events that reach the Action
filter:
  event_type: "EntityChangeEvent_v1"
  event:
    category: "TAG"
    operation: "ADD"
    modifier: "urn:li:tag:pii"

# 4. Action - What action to take on events.
action:
  type: "hello_world"
```

## Running an Action

To run a new Action, just use the `actions` CLI command

```
datahub actions -c <config.yml>
```

Once the Action is running, you will see

```
Action Pipeline with name '<action-pipeline-name>' is now running.
```

### Running multiple Actions

You can run multiple actions pipeline within the same command. Simply provide multiple
config files by restating the "-c" command line argument.

For example,

```
datahub actions -c <config-1.yaml> -c <config-2.yaml>
```

### Running in debug mode

Simply append the `--debug` flag to the CLI to run your action in debug mode.

```
datahub actions -c <config.yaml> --debug
```

### Stopping an Action

Just issue a Control-C as usual. You should see the Actions Pipeline shut down gracefully, with a small
summary of processing results.

```
Actions Pipeline with name '<action-pipeline-name' has been stopped.
```

## Supported Events

Two event types are currently supported. Read more about them below.

- [Entity Change Event V1](../docs/actions/events/entity-change-event.md)
- [Metadata Change Log V1](../docs/actions/events/metadata-change-log-event.md)

## Supported Event Sources

Currently, the only event source that is officially supported is `kafka`, which polls for events
via a Kafka Consumer.

- [Kafka Event Source](../docs/actions/sources/kafka-event-source.md)

## Supported Actions

By default, DataHub supports a set of standard actions plugins. These can be found inside the folder
`src/datahub-actions/plugins`.

Some pre-included Actions include

- [Hello World](../docs/actions/actions/hello_world.md)
- [Executor](../docs/actions/actions/executor.md)

## Development

### Build and Test

Notice that we support all actions command using a separate `datahub-actions` CLI entry point. Feel free
to use this during development.

```
# Build datahub-actions module
./gradlew datahub-actions:build

# Drop into virtual env
cd datahub-actions && source venv/bin/activate

# Start hello world action
datahub-actions actions -c ../examples/hello_world.yaml

# Start ingestion executor action
datahub-actions actions -c ../examples/executor.yaml

# Start multiple actions
datahub-actions actions -c ../examples/executor.yaml -c ../examples/hello_world.yaml
```

### Developing a Transformer

To develop a new Transformer, check out the [Developing a Transformer](../docs/actions/guides/developing-a-transformer.md) guide.

### Developing an Action

To develop a new Action, check out the [Developing an Action](../docs/actions/guides/developing-an-action.md) guide.

## Contributing

Contributing guidelines follow those of the [main DataHub project](../docs/CONTRIBUTING.md). We are accepting contributions for Actions, Transformers, and general framework improvements (tests, error handling, etc).

## Resources

Check out the [original announcement](https://www.youtube.com/watch?v=7iwNxHgqxtg&t=2189s) of the DataHub Actions Framework at the DataHub April 2022 Town Hall.

## License

[Apache 2.0](./LICENSE)
