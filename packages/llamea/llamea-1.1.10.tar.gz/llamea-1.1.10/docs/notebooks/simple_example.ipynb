{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/XAI-liacs/LLaMEA/blob/main/docs/notebooks/simple_example.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOruB433kyei"
   },
   "source": [
    "# LLaMEA Minimal Example\n",
    "\n",
    "This notebook shows a simple usage of LLaMEA to automatically generate and refine a Python-based optimization algorithm for a toy evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "dO0C23Eik0Ya",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#this dependency is sometimes missed depending on the operating system\n",
    "!pip install swig\n",
    "!pip install llamea==1.1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "HRNwk038kyej"
   },
   "outputs": [],
   "source": [
    "# Cell 1: Imports\n",
    "import os\n",
    "import numpy as np\n",
    "from llamea import LLaMEA, Gemini_LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lUM1AIbUkyej"
   },
   "source": [
    "## Cell 1: Set up the LLM\n",
    "\n",
    "If you haven't already, set your OpenAI or other API key in your environment variables, e.g.,\n",
    "`export OPENAI_API_KEY=\"...\"` or `export GEMINI_API_KEY=\"....\"`\n",
    "\n",
    "You can also use Gemini in most countries for free."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "tS0Nsy57kyej"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "api_key = userdata.get('GOOGLE_API_KEY_1') # <--- Make sure you add your Google API key via the Colab secrets panel\n",
    "#api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "llm = Gemini_LLM(api_key, \"gemini-2.0-flash\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TjbYY3ZGkyek"
   },
   "source": [
    "## Cell 2: Define an evaluation function for LLaMEA\n",
    "\n",
    "- The function must accept a \"solution\" argument, which contains code, a name, etc.\n",
    "- You parse solution.solution (the raw code), dynamically load it, and run it on your problem(s).\n",
    "- You then set_scores() to record how well it did.\n",
    "\n",
    "We'll define a simple example with a 1D quadratic function: f(x) = (x - 2)^2\n",
    "We'll ask the solution code to search for the minimum x in [-5, 5].\n",
    "We'll then return a score based on how close x is to 2. The closer, the higher the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "PdP6DOVQkyek"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import textwrap\n",
    "import math\n",
    "\n",
    "# We implement an exception to stop algorithms that try a too large budget (to prevent infinite loops).\n",
    "class OverBudgetException(Exception):\n",
    "    \"\"\"The algorithm tried to do more evaluations than allowed.\"\"\"\n",
    "    pass\n",
    "\n",
    "def evaluate_simple(solution, explogger=None):\n",
    "    code_str = solution.code  # The Python code the LLM generated\n",
    "    alg_name = solution.name\n",
    "\n",
    "    # We define our 1D search space: x in [-5, 5], budget=100\n",
    "    # We'll create a small function that the generated code should optimize.\n",
    "    def f(x):\n",
    "        # We only allow so many function calls\n",
    "        if f.call_count >= 100:\n",
    "            raise OverBudgetException(\"Budget exceeded.\")\n",
    "        f.call_count += 1\n",
    "        return (x - 2.0)**2\n",
    "\n",
    "    f.call_count = 0\n",
    "\n",
    "    # Dynamically run the generated code\n",
    "    # The code is expected to define a class named alg_name, with __init__(budget, dim) and __call__(f).\n",
    "    # We'll create a safe execution context to run it.\n",
    "    safe_globals = {\n",
    "        \"OverBudgetException\": OverBudgetException,\n",
    "        \"math\": math,\n",
    "        \"np\": np,\n",
    "    }\n",
    "    local_env = {}\n",
    "    try:\n",
    "        exec(code_str, safe_globals, local_env)\n",
    "    except Exception as e:\n",
    "        # If there's an error in code, set the score to 0\n",
    "        solution.set_scores(0, feedback=f\"Runtime/Syntax error: {e}\")\n",
    "        return solution\n",
    "\n",
    "    # Instantiate the class with budget=100, dim=1\n",
    "    try:\n",
    "        AlgorithmClass = local_env[alg_name]\n",
    "        algo = AlgorithmClass(budget=100, dim=1)\n",
    "    except Exception as e:\n",
    "        solution.set_scores(0, feedback=f\"Instantiation error: {e}\")\n",
    "        return solution\n",
    "\n",
    "    # Now run the algorithm\n",
    "    best_f = math.inf\n",
    "    try:\n",
    "        best_f, best_x = algo(f)\n",
    "    except OverBudgetException:\n",
    "        # If over budget, we penalize heavily\n",
    "        best_f = 9999\n",
    "\n",
    "    # We'll convert it to a \"score\" where smaller f is better => we do `score = 1/(1 + best_f)`\n",
    "    # so that 0 => 1/1 => 1, big f => near 0\n",
    "    # Note: LLaMEA is optimizing by default! (bigger is better)\n",
    "    score = 1.0 / (1.0 + best_f)\n",
    "\n",
    "    # Provide feedback\n",
    "    feedback_str = f\"Algorithm {alg_name} got score={score:.4f} (bigger is better).\"\n",
    "\n",
    "    # Save the score to the solution object\n",
    "    solution.set_scores(score, feedback_str)\n",
    "    return solution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IsIbPI29kyek"
   },
   "source": [
    "## Cell 3: Create and run the LLaMEA search\n",
    "\n",
    "We define a small prompt. The LLM will see how we want it to write code (like a class, with __call__).\n",
    "Then we let LLaMEA iterate a few times, generating and refining solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 922
    },
    "id": "u7vqu-qnkyek",
    "outputId": "cea503fb-242d-42f2-ab68-b73e556877cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/joblib/parallel.py:1383: UserWarning: The backend class 'SequentialBackend' does not support timeout. You have set 'timeout=3615' in Parallel but the 'timeout' parameter will not be used.\n",
      "  warnings.warn(\n",
      "ERROR:tornado.access:503 POST /v1beta/models/gemini-2.0-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 404.79ms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best found solution: Optimize, Score=0.0001\n",
      "Generated code:\n",
      "import numpy as np\n",
      "\n",
      "class Optimize:\n",
      "    def __init__(self, budget=10000, dim=10):\n",
      "        self.budget = budget\n",
      "        self.dim = dim\n",
      "        self.f_opt = np.Inf\n",
      "        self.x_opt = None\n",
      "\n",
      "    def __call__(self, func):\n",
      "        # Initial guess\n",
      "        x = np.random.uniform(-5, 5, size=self.dim)\n",
      "        f = func(x)\n",
      "\n",
      "        self.f_opt = f\n",
      "        self.x_opt = x\n",
      "\n",
      "        for i in range(self.budget):\n",
      "            # Generate a neighbor by adding a small random perturbation\n",
      "            x_new = x + np.random.normal(0, 0.1, size=self.dim)\n",
      "\n",
      "            # Clip to stay within bounds\n",
      "            x_new = np.clip(x_new, -5, 5)\n",
      "\n",
      "            f_new = func(x_new)\n",
      "\n",
      "            # Accept if better\n",
      "            if f_new < f:\n",
      "                f = f_new\n",
      "                x = x_new\n",
      "\n",
      "                # Update best seen\n",
      "                if f < self.f_opt:\n",
      "                    self.f_opt = f\n",
      "                    self.x_opt = x\n",
      "            else:\n",
      "              # Random Restart to escape local optima\n",
      "              if np.random.rand() < 0.01:\n",
      "                x = np.random.uniform(-5, 5, size=self.dim)\n",
      "                f = func(x)\n",
      "                if f < self.f_opt:\n",
      "                    self.f_opt = f\n",
      "                    self.x_opt = x\n",
      "\n",
      "        return self.f_opt, self.x_opt\n",
      "Additional feedback: Algorithm Optimize got score=0.0001 (bigger is better).\n"
     ]
    }
   ],
   "source": [
    "role_prompt = \"You are an AI that generates Python optimization code.\"\n",
    "\n",
    "task_prompt = textwrap.dedent(\"\"\"\\\n",
    "Create a Python class named Optimize, with __init__(self, budget, dim),\n",
    "and a __call__(self, func) method that tries to find x in [-5, 5] that minimizes func(x).\n",
    "The function should return the tuple best_f, best_x (value best found and the location).\n",
    "Implement an efficient algorithm for this task.\n",
    "\"\"\")\n",
    "\n",
    "# We'll use a small number of iterations for demonstration\n",
    "es = LLaMEA(\n",
    "    f=evaluate_simple,\n",
    "    llm=llm,\n",
    "    n_parents=1,\n",
    "    n_offspring=1,\n",
    "    role_prompt=role_prompt,\n",
    "    task_prompt=task_prompt,\n",
    "    experiment_name=\"my-llamea-example\",\n",
    "    elitism=True,\n",
    "    budget=3  # Try 3 iterations for a quick demo\n",
    ")\n",
    "\n",
    "best_solution = es.run()\n",
    "print(f\"Best found solution: {best_solution.name}, Score={best_solution.fitness:.4f}\")\n",
    "print(f\"Generated code:\\n{best_solution.code}\")\n",
    "print(f\"Additional feedback: {best_solution.feedback}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
