"""Smart caching system for NLSQ optimization.

This module provides intelligent caching for expensive computations,
particularly Jacobian evaluations and function calls.
"""

import hashlib
import json
import os
import time
import warnings
from collections.abc import Callable
from functools import wraps
from typing import Any

import numpy as np

from nlsq.config import JAXConfig

_jax_config = JAXConfig()

import builtins
import contextlib

import jax.numpy as jnp


class SmartCache:
    """Intelligent caching system for optimization computations.

    This class provides:
    - Memory and disk caching with LRU eviction
    - Automatic cache key generation from function arguments
    - Cache persistence across sessions
    - Cache invalidation and warming strategies

    Attributes
    ----------
    cache_dir : str
        Directory for disk cache storage
    memory_cache : dict
        In-memory cache storage
    disk_cache_enabled : bool
        Whether disk caching is enabled
    max_memory_items : int
        Maximum items in memory cache
    cache_stats : dict
        Cache hit/miss statistics
    """

    def __init__(
        self,
        cache_dir: str = ".nlsq_cache",
        max_memory_items: int = 1000,
        disk_cache_enabled: bool = True,
        enable_stats: bool = True,
    ):
        """Initialize smart cache.

        Parameters
        ----------
        cache_dir : str
            Directory for disk cache
        max_memory_items : int
            Maximum items in memory cache
        disk_cache_enabled : bool
            Enable disk caching
        enable_stats : bool
            Track cache statistics
        """
        self.cache_dir = cache_dir
        self.memory_cache: dict[str, tuple[Any, float]] = {}  # value, timestamp
        self.access_count: dict[str, int] = {}  # Track access frequency
        self.disk_cache_enabled = disk_cache_enabled
        self.max_memory_items = max_memory_items
        self.enable_stats = enable_stats

        # Statistics
        self.cache_stats = {
            "hits": 0,
            "misses": 0,
            "memory_hits": 0,
            "disk_hits": 0,
            "evictions": 0,
        }

        # Create cache directory if needed
        if disk_cache_enabled and not os.path.exists(cache_dir):
            try:
                os.makedirs(cache_dir)
            except OSError:
                warnings.warn(f"Could not create cache directory {cache_dir}")
                self.disk_cache_enabled = False

    def cache_key(self, *args, **kwargs) -> str:
        """Generate cache key from arguments.

        Parameters
        ----------
        *args : tuple
            Positional arguments
        **kwargs : dict
            Keyword arguments

        Returns
        -------
        key : str
            MD5 hash of arguments
        """
        key_parts = []

        for arg in args:
            if isinstance(arg, (np.ndarray, jnp.ndarray)):
                # For arrays, use shape, dtype, and hash of values
                arr_flat = np.asarray(arg).flatten()
                if len(arr_flat) > 100:
                    # For large arrays, combine sampling with full array hash to prevent collisions
                    sample_indices = np.linspace(0, len(arr_flat) - 1, 100, dtype=int)
                    sample = arr_flat[sample_indices]
                    # Include hash of full array data for uniqueness
                    full_hash = hashlib.sha256(arr_flat.tobytes()).hexdigest()[:16]
                    key_parts.append(
                        f"array_{arg.shape}_{arg.dtype}_{hash(sample.tobytes())}_{full_hash}"
                    )
                else:
                    # For small arrays, use full array
                    key_parts.append(
                        f"array_{arg.shape}_{arg.dtype}_{hash(arr_flat.tobytes())}"
                    )
            elif callable(arg):
                # For functions, use their name and module
                key_parts.append(f"func_{arg.__module__}_{arg.__name__}")
            else:
                key_parts.append(str(arg))

        # Add kwargs
        for k, v in sorted(kwargs.items()):
            key_parts.append(f"{k}={v}")

        key_str = "|".join(key_parts)
        return hashlib.md5(key_str.encode(), usedforsecurity=False).hexdigest()

    def get(self, key: str) -> Any | None:
        """Get value from cache.

        Parameters
        ----------
        key : str
            Cache key

        Returns
        -------
        value : Any or None
            Cached value or None if not found
        """
        # Check memory cache first
        if key in self.memory_cache:
            value, timestamp = self.memory_cache[key]
            self.access_count[key] = self.access_count.get(key, 0) + 1

            if self.enable_stats:
                self.cache_stats["hits"] += 1
                self.cache_stats["memory_hits"] += 1

            # Move to end (LRU)
            del self.memory_cache[key]
            self.memory_cache[key] = (value, timestamp)

            return value

        # Check disk cache
        if self.disk_cache_enabled:
            cache_file = os.path.join(self.cache_dir, f"{key}.npz")
            if os.path.exists(cache_file):
                try:
                    value = self._load_from_disk(cache_file)

                    if self.enable_stats:
                        self.cache_stats["hits"] += 1
                        self.cache_stats["disk_hits"] += 1

                    # Add to memory cache
                    self._add_to_memory_cache(key, value)
                    return value

                except Exception as e:
                    warnings.warn(f"Could not load from disk cache: {e}")
                    # Remove corrupted cache file
                    with contextlib.suppress(builtins.BaseException):
                        os.remove(cache_file)

        if self.enable_stats:
            self.cache_stats["misses"] += 1

        return None

    def set(self, key: str, value: Any):
        """Set value in cache.

        Parameters
        ----------
        key : str
            Cache key
        value : Any
            Value to cache
        """
        # Add to memory cache
        self._add_to_memory_cache(key, value)

        # Save to disk cache
        if self.disk_cache_enabled:
            cache_file = os.path.join(self.cache_dir, f"{key}.npz")
            try:
                self._save_to_disk(cache_file, value)
            except Exception as e:
                warnings.warn(f"Could not save to disk cache: {e}")

    def _add_to_memory_cache(self, key: str, value: Any):
        """Add item to memory cache with LRU eviction.

        Parameters
        ----------
        key : str
            Cache key
        value : Any
            Value to cache
        """
        # Check if we need to evict
        if len(self.memory_cache) >= self.max_memory_items:
            # Evict least recently used item
            if self.memory_cache:
                oldest_key = next(iter(self.memory_cache))
                del self.memory_cache[oldest_key]
                if oldest_key in self.access_count:
                    del self.access_count[oldest_key]

                if self.enable_stats:
                    self.cache_stats["evictions"] += 1

        self.memory_cache[key] = (value, time.time())
        self.access_count[key] = 1

    def invalidate(self, key: str | None = None):
        """Invalidate cache entries.

        Parameters
        ----------
        key : str, optional
            Specific key to invalidate, or None to clear all
        """
        if key is None:
            # Clear all caches
            self.memory_cache.clear()
            self.access_count.clear()

            if self.disk_cache_enabled:
                try:
                    for file in os.listdir(self.cache_dir):
                        if file.endswith(".npz"):
                            os.remove(os.path.join(self.cache_dir, file))
                except OSError as e:
                    warnings.warn(f"Could not clear disk cache: {e}")
        else:
            # Clear specific key
            if key in self.memory_cache:
                del self.memory_cache[key]
            if key in self.access_count:
                del self.access_count[key]

            if self.disk_cache_enabled:
                cache_file = os.path.join(self.cache_dir, f"{key}.npz")
                if os.path.exists(cache_file):
                    with contextlib.suppress(builtins.BaseException):
                        os.remove(cache_file)

    def get_stats(self) -> dict:
        """Get cache statistics.

        Returns
        -------
        stats : dict
            Cache statistics including hit rate
        """
        total_accesses = self.cache_stats["hits"] + self.cache_stats["misses"]

        if total_accesses > 0:
            hit_rate = self.cache_stats["hits"] / total_accesses
        else:
            hit_rate = 0.0

        return {
            **self.cache_stats,
            "hit_rate": hit_rate,
            "memory_size": len(self.memory_cache),
            "total_accesses": total_accesses,
        }

    def optimize_cache(self):
        """Optimize cache by removing rarely accessed items."""
        if not self.access_count:
            return

        # Calculate average access count
        avg_access = np.mean(list(self.access_count.values()))

        # Remove items with below-average access
        keys_to_remove = [
            key for key, count in self.access_count.items() if count < avg_access * 0.5
        ]

        for key in keys_to_remove:
            self.invalidate(key)

    def _save_to_disk(self, cache_file: str, value: Any):
        """Save value to disk using safe serialization.

        Uses numpy.savez for arrays and JSON for other data types.
        This is much safer than pickle as it doesn't execute arbitrary code.

        Parameters
        ----------
        cache_file : str
            Path to cache file
        value : Any
            Value to save
        """
        # Check if value is array-like (numpy or JAX array)
        if isinstance(value, (np.ndarray, jnp.ndarray)):
            # Convert JAX array to numpy for saving
            if isinstance(value, jnp.ndarray):
                value = np.asarray(value)
            np.savez_compressed(cache_file, data=value)
        elif isinstance(value, (dict, list, str, int, float, bool, type(None))):
            # Use JSON for simple data types
            json_file = cache_file.replace(".npz", ".json")
            with open(json_file, "w") as f:
                json.dump(value, f)
        elif isinstance(value, tuple) and all(
            isinstance(v, (np.ndarray, jnp.ndarray)) for v in value
        ):
            # Handle tuple of arrays (common for multi-output functions)
            arrays_dict: dict[str, Any] = {
                f"arr_{i}": np.asarray(v) for i, v in enumerate(value)
            }
            arrays_dict["_is_tuple"] = np.array([True])
            arrays_dict["_length"] = np.array([len(value)])
            np.savez_compressed(cache_file, **arrays_dict)
        else:
            # For other types, convert to numpy array if possible
            try:
                arr = np.asarray(value)
                np.savez_compressed(cache_file, data=arr)
            except (ValueError, TypeError):
                warnings.warn(
                    f"Cannot safely cache type {type(value).__name__}, skipping disk cache"
                )

    def _load_from_disk(self, cache_file: str) -> Any:
        """Load value from disk using safe deserialization.

        Uses numpy.load for arrays and JSON for other data types.

        Parameters
        ----------
        cache_file : str
            Path to cache file

        Returns
        -------
        value : Any
            Loaded value
        """
        # Check if JSON file exists
        json_file = cache_file.replace(".npz", ".json")
        if os.path.exists(json_file):
            with open(json_file) as f:
                return json.load(f)

        # Load from numpy file
        with np.load(cache_file, allow_pickle=False) as data:
            # Check if it's a tuple of arrays
            if "_is_tuple" in data.files:
                length = int(data["_length"])
                return tuple(data[f"arr_{i}"] for i in range(length))
            # Single array
            elif "data" in data.files:
                return data["data"]
            else:
                # Legacy format or unknown structure
                raise ValueError(f"Unknown cache file structure: {data.files}")


def cached_function(cache: SmartCache | None = None, ttl: float | None = None):
    """Decorator for caching function results.

    Parameters
    ----------
    cache : SmartCache, optional
        Cache instance to use (creates new if None)
    ttl : float, optional
        Time-to-live in seconds for cached values

    Returns
    -------
    decorator : function
        Decorator function
    """
    if cache is None:
        cache = SmartCache()

    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            # Generate cache key
            cache_key = cache.cache_key(func, *args, **kwargs)

            # Check cache
            cached_result = cache.get(cache_key)

            if cached_result is not None:
                # Check TTL if specified
                if ttl is not None:
                    _value, timestamp = cache.memory_cache.get(cache_key, (None, 0))
                    if time.time() - timestamp > ttl:
                        # Expired, recompute
                        cached_result = None

            if cached_result is None:
                # Compute and cache
                result = func(*args, **kwargs)
                cache.set(cache_key, result)
                return result

            return cached_result

        # Add cache management methods
        wrapper.cache = cache  # type: ignore[attr-defined]
        wrapper.invalidate = cache.invalidate  # type: ignore[attr-defined]
        wrapper.get_stats = cache.get_stats  # type: ignore[attr-defined]

        return wrapper

    return decorator


def cached_jacobian(cache: SmartCache | None = None):
    """Decorator specifically for caching Jacobian evaluations.

    Parameters
    ----------
    cache : SmartCache, optional
        Cache instance to use

    Returns
    -------
    decorator : function
        Decorator function
    """
    if cache is None:
        cache = SmartCache(max_memory_items=100)  # Jacobians can be large

    def decorator(func):
        @wraps(func)
        def wrapper(x, *params):
            # Create cache key from x and params
            cache_key = cache.cache_key(x, *params)

            # Check cache
            cached_result = cache.get(cache_key)
            if cached_result is not None:
                return cached_result

            # Compute and cache
            result = func(x, *params)
            cache.set(cache_key, result)
            return result

        wrapper.cache = cache  # type: ignore[attr-defined]
        wrapper.invalidate = cache.invalidate  # type: ignore[attr-defined]

        return wrapper

    return decorator


class JITCompilationCache:
    """Cache for JAX JIT-compiled functions.

    This cache stores compiled functions to avoid recompilation
    when function signatures match.
    """

    def __init__(self):
        """Initialize JIT compilation cache."""
        self.compiled_functions = {}
        self.compilation_times = {}

    def get_or_compile(self, func: Callable, static_argnums: tuple = ()) -> Callable:
        """Get cached compilation or compile and cache.

        Parameters
        ----------
        func : callable
            Function to compile
        static_argnums : tuple
            Static argument numbers for JIT

        Returns
        -------
        compiled_func : callable
            JIT-compiled function
        """
        from jax import jit

        # Create key from function and static args
        key = (func.__module__, func.__name__, static_argnums)

        if key in self.compiled_functions:
            return self.compiled_functions[key]

        # Compile and cache
        start_time = time.time()
        compiled_func = jit(func, static_argnums=static_argnums)
        compilation_time = time.time() - start_time

        self.compiled_functions[key] = compiled_func
        self.compilation_times[key] = compilation_time

        return compiled_func

    def clear(self):
        """Clear compilation cache."""
        self.compiled_functions.clear()
        self.compilation_times.clear()

    def get_stats(self) -> dict:
        """Get compilation statistics.

        Returns
        -------
        stats : dict
            Compilation statistics
        """
        return {
            "cached_functions": len(self.compiled_functions),
            "total_compilation_time": sum(self.compilation_times.values()),
            "functions": list(self.compiled_functions.keys()),
        }


# Global cache instances
_global_cache = SmartCache()
_jit_cache = JITCompilationCache()


def get_global_cache() -> SmartCache:
    """Get global cache instance.

    Returns
    -------
    cache : SmartCache
        Global cache instance
    """
    return _global_cache


def get_jit_cache() -> JITCompilationCache:
    """Get JIT compilation cache.

    Returns
    -------
    cache : JITCompilationCache
        JIT compilation cache
    """
    return _jit_cache


def clear_all_caches():
    """Clear all global caches."""
    _global_cache.invalidate()
    _jit_cache.clear()
