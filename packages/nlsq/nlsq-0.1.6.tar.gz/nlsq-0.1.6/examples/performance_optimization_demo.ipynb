{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLSQ Performance Optimization Features\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/imewei/NLSQ/blob/main/examples/performance_optimization_demo.ipynb)\n",
    "\n",
    "**Requirements:** Python 3.12 or higher\n",
    "\n",
    "This notebook demonstrates NLSQ's advanced performance optimization features:\n",
    "\n",
    "- **MemoryPool**: Pre-allocated memory buffers for zero-allocation optimization iterations\n",
    "- **SparseJacobian**: Exploit sparsity patterns for 10-100x memory reduction\n",
    "- **StreamingOptimizer**: Process unlimited dataset sizes with batch streaming\n",
    "\n",
    "These features are essential for:\n",
    "- Very large problems (millions of data points)\n",
    "- Memory-constrained environments\n",
    "- Real-time or low-latency applications\n",
    "- Problems with structured sparsity patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T19:57:56.542048Z",
     "iopub.status.busy": "2025-10-08T19:57:56.541725Z",
     "iopub.status.idle": "2025-10-08T19:57:57.187957Z",
     "shell.execute_reply": "2025-10-08T19:57:57.186290Z"
    }
   },
   "outputs": [],
   "source": [
    "# Install NLSQ if not already installed\n",
    "!pip install nlsq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T19:57:57.194721Z",
     "iopub.status.busy": "2025-10-08T19:57:57.194143Z",
     "iopub.status.idle": "2025-10-08T19:57:58.204057Z",
     "shell.execute_reply": "2025-10-08T19:57:58.203687Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "import warnings\n",
    "from collections.abc import Callable\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "print(f\"✅ Python {sys.version_info.major}.{sys.version_info.minor} meets requirements\")\n",
    "\n",
    "# Import core NLSQ\n",
    "from nlsq import CurveFit, __version__\n",
    "\n",
    "print(f\"NLSQ version: {__version__}\")\n",
    "print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T19:57:58.205644Z",
     "iopub.status.busy": "2025-10-08T19:57:58.205429Z",
     "iopub.status.idle": "2025-10-08T19:57:58.208132Z",
     "shell.execute_reply": "2025-10-08T19:57:58.207847Z"
    }
   },
   "outputs": [],
   "source": [
    "# Import advanced performance features\n",
    "from nlsq import (\n",
    "    DataGenerator,\n",
    "    # Memory Pool\n",
    "    MemoryPool,\n",
    "    # Sparse Jacobian\n",
    "    SparseJacobianComputer,\n",
    "    SparseOptimizer,\n",
    "    StreamingConfig,\n",
    "    # Streaming Optimizer\n",
    "    StreamingOptimizer,\n",
    "    TRFMemoryPool,\n",
    "    clear_global_pool,\n",
    "    create_hdf5_dataset,\n",
    "    detect_jacobian_sparsity,\n",
    "    fit_unlimited_data,\n",
    "    get_global_pool,\n",
    ")\n",
    "\n",
    "print(\"✅ Advanced features imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. MemoryPool - Zero-Allocation Optimization\n",
    "\n",
    "The MemoryPool pre-allocates and reuses array buffers to eliminate allocation overhead during optimization iterations. This is critical for:\n",
    "\n",
    "- **Low-latency applications**: Minimize GC pauses\n",
    "- **Repeated fits**: Amortize allocation costs\n",
    "- **Memory-constrained systems**: Predictable memory usage\n",
    "\n",
    "### How It Works\n",
    "\n",
    "1. Pre-allocate buffers for common shapes\n",
    "2. Reuse buffers instead of creating new arrays\n",
    "3. Track allocation statistics\n",
    "4. Automatic cleanup with context managers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T19:57:58.209632Z",
     "iopub.status.busy": "2025-10-08T19:57:58.209469Z",
     "iopub.status.idle": "2025-10-08T19:57:58.334313Z",
     "shell.execute_reply": "2025-10-08T19:57:58.333800Z"
    }
   },
   "outputs": [],
   "source": [
    "def demo_memory_pool_basics():\n",
    "    \"\"\"Demonstrate basic MemoryPool usage.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"MEMORY POOL BASICS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Create memory pool with statistics tracking\n",
    "    pool = MemoryPool(max_pool_size=10, enable_stats=True)\n",
    "\n",
    "    print(\"\\n--- Allocating arrays from pool ---\")\n",
    "\n",
    "    # First allocation - creates new array\n",
    "    arr1 = pool.allocate((1000, 10), dtype=jnp.float64)\n",
    "    print(f\"First allocation: shape={arr1.shape}, dtype={arr1.dtype}\")\n",
    "    stats1 = pool.get_stats()\n",
    "    print(f\"  Stats: allocations={stats1['allocations']}, reuses={stats1['reuses']}\")\n",
    "\n",
    "    # Release back to pool\n",
    "    pool.release(arr1)\n",
    "    print(\"\\nReleased array back to pool\")\n",
    "\n",
    "    # Second allocation - reuses from pool!\n",
    "    arr2 = pool.allocate((1000, 10), dtype=jnp.float64)\n",
    "    print(f\"\\nSecond allocation: shape={arr2.shape}\")\n",
    "    stats2 = pool.get_stats()\n",
    "    print(f\"  Stats: allocations={stats2['allocations']}, reuses={stats2['reuses']}\")\n",
    "    print(f\"  ✅ Reuse rate: {stats2['reuse_rate']:.1%}\")\n",
    "\n",
    "    # Allocate different shape - new allocation\n",
    "    arr3 = pool.allocate((500, 5), dtype=jnp.float64)\n",
    "    print(f\"\\nDifferent shape: shape={arr3.shape}\")\n",
    "    stats3 = pool.get_stats()\n",
    "    print(f\"  Stats: allocations={stats3['allocations']}, reuses={stats3['reuses']}\")\n",
    "\n",
    "    # Show pool contents\n",
    "    print(f\"\\nPool sizes: {stats3['pool_sizes']}\")\n",
    "    print(f\"Currently allocated: {stats3['currently_allocated']}\")\n",
    "    print(f\"Peak memory: {stats3['peak_memory'] / 1024**2:.2f} MB\")\n",
    "\n",
    "    # Clean up\n",
    "    pool.clear()\n",
    "    print(\"\\n✅ Memory pool demo complete\")\n",
    "\n",
    "\n",
    "# Run demo\n",
    "demo_memory_pool_basics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Comparison: With vs Without Memory Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T19:57:58.336946Z",
     "iopub.status.busy": "2025-10-08T19:57:58.336709Z",
     "iopub.status.idle": "2025-10-08T19:57:58.477820Z",
     "shell.execute_reply": "2025-10-08T19:57:58.477360Z"
    }
   },
   "outputs": [],
   "source": [
    "def benchmark_memory_pool_performance():\n",
    "    \"\"\"Compare performance with and without memory pool.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"MEMORY POOL PERFORMANCE COMPARISON\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    shape = (10000, 50)\n",
    "    n_iterations = 100\n",
    "\n",
    "    # Without pool - create new arrays each time\n",
    "    print(f\"\\nWithout MemoryPool ({n_iterations} iterations)...\")\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        arr = jnp.zeros(shape, dtype=jnp.float64)\n",
    "        # Simulate some work\n",
    "        _ = arr + 1.0\n",
    "\n",
    "    time_without = time.time() - start\n",
    "    print(f\"  Time: {time_without:.3f}s\")\n",
    "\n",
    "    # With pool - reuse arrays\n",
    "    print(f\"\\nWith MemoryPool ({n_iterations} iterations)...\")\n",
    "    pool = MemoryPool(max_pool_size=5, enable_stats=True)\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        arr = pool.allocate(shape, dtype=jnp.float64)\n",
    "        # Simulate some work\n",
    "        _ = arr + 1.0\n",
    "        pool.release(arr)\n",
    "\n",
    "    time_with = time.time() - start\n",
    "    stats = pool.get_stats()\n",
    "\n",
    "    print(f\"  Time: {time_with:.3f}s\")\n",
    "    print(f\"  Reuse rate: {stats['reuse_rate']:.1%}\")\n",
    "    print(f\"  Allocations: {stats['allocations']}, Reuses: {stats['reuses']}\")\n",
    "\n",
    "    # Calculate speedup\n",
    "    speedup = time_without / time_with\n",
    "    print(f\"\\n✅ Speedup: {speedup:.2f}x faster with MemoryPool!\")\n",
    "\n",
    "    # Memory savings\n",
    "    memory_per_array = np.prod(shape) * 8 / 1024**2  # MB\n",
    "    memory_without = memory_per_array * n_iterations\n",
    "    memory_with = memory_per_array * stats[\"allocations\"]\n",
    "\n",
    "    print(\"\\nMemory allocation overhead:\")\n",
    "    print(f\"  Without pool: {memory_without:.1f} MB allocated\")\n",
    "    print(f\"  With pool: {memory_with:.1f} MB allocated\")\n",
    "    print(f\"  Reduction: {(1 - memory_with / memory_without) * 100:.1f}%\")\n",
    "\n",
    "    pool.clear()\n",
    "\n",
    "\n",
    "# Run benchmark\n",
    "benchmark_memory_pool_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MemoryPool as Context Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T19:57:58.479843Z",
     "iopub.status.busy": "2025-10-08T19:57:58.479634Z",
     "iopub.status.idle": "2025-10-08T19:57:58.516468Z",
     "shell.execute_reply": "2025-10-08T19:57:58.516061Z"
    }
   },
   "outputs": [],
   "source": [
    "def demo_memory_pool_context_manager():\n",
    "    \"\"\"Demonstrate context manager usage for automatic cleanup.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"MEMORY POOL CONTEXT MANAGER\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    print(\"\\nUsing MemoryPool with context manager...\")\n",
    "\n",
    "    with MemoryPool(max_pool_size=10, enable_stats=True) as pool:\n",
    "        # Allocate some arrays\n",
    "        arrays = []\n",
    "        for i in range(5):\n",
    "            arr = pool.allocate((100, 10))\n",
    "            arrays.append(arr)\n",
    "\n",
    "        print(f\"Allocated {len(arrays)} arrays\")\n",
    "        stats = pool.get_stats()\n",
    "        print(\n",
    "            f\"Pool stats: allocations={stats['allocations']}, currently_allocated={stats['currently_allocated']}\"\n",
    "        )\n",
    "\n",
    "        # Release some back\n",
    "        for arr in arrays[:3]:\n",
    "            pool.release(arr)\n",
    "\n",
    "        stats = pool.get_stats()\n",
    "        print(f\"After releasing 3: currently_allocated={stats['currently_allocated']}\")\n",
    "\n",
    "    # Pool is automatically cleared on exit\n",
    "    print(\"\\n✅ Pool automatically cleaned up on context exit\")\n",
    "\n",
    "\n",
    "# Run demo\n",
    "demo_memory_pool_context_manager()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. SparseJacobian - Exploiting Sparsity for Memory Efficiency\n",
    "\n",
    "Many curve fitting problems have **sparse Jacobians** where each data point only depends on a subset of parameters. The `SparseJacobianComputer` exploits this structure for:\n",
    "\n",
    "- **10-100x memory reduction**: Store only non-zero elements\n",
    "- **Faster computation**: Skip zero elements in matrix operations\n",
    "- **Larger problems**: Fit problems that wouldn't fit in memory otherwise\n",
    "\n",
    "### When to Use Sparse Jacobians\n",
    "\n",
    "- **Piecewise models**: Different parameters for different data regions\n",
    "- **Multi-component fits**: Independent sub-models\n",
    "- **Localized parameters**: Parameters affecting only nearby data points\n",
    "- **Very large problems**: Millions of data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T19:57:58.518212Z",
     "iopub.status.busy": "2025-10-08T19:57:58.518092Z",
     "iopub.status.idle": "2025-10-08T19:57:58.898288Z",
     "shell.execute_reply": "2025-10-08T19:57:58.897919Z"
    }
   },
   "outputs": [],
   "source": [
    "def demo_sparse_jacobian_basics():\n",
    "    \"\"\"Demonstrate sparse Jacobian detection and usage.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"SPARSE JACOBIAN BASICS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Create a piecewise model with sparse Jacobian\n",
    "    # Each segment only depends on 2 parameters\n",
    "    def piecewise_linear(x, *params):\n",
    "        \"\"\"Piecewise linear model with sparse Jacobian.\n",
    "\n",
    "        params[0:2] affect x < 0.5\n",
    "        params[2:4] affect x >= 0.5\n",
    "        \"\"\"\n",
    "        result = jnp.zeros_like(x)\n",
    "        mask1 = x < 0.5\n",
    "        mask2 = x >= 0.5\n",
    "\n",
    "        # Segment 1: y = a1*x + b1\n",
    "        result = jnp.where(mask1, params[0] * x + params[1], result)\n",
    "\n",
    "        # Segment 2: y = a2*x + b2\n",
    "        result = jnp.where(mask2, params[2] * x + params[3], result)\n",
    "\n",
    "        return result\n",
    "\n",
    "    # Generate test data\n",
    "    np.random.seed(42)\n",
    "    n_points = 1000\n",
    "    x_data = np.linspace(0, 1, n_points)\n",
    "    true_params = [2.0, 1.0, -1.0, 2.0]  # a1, b1, a2, b2\n",
    "\n",
    "    # Detect sparsity pattern\n",
    "    print(\"\\n--- Detecting Sparsity Pattern ---\")\n",
    "    sparse_computer = SparseJacobianComputer(sparsity_threshold=0.01)\n",
    "\n",
    "    pattern, sparsity = sparse_computer.detect_sparsity_pattern(\n",
    "        piecewise_linear, np.array(true_params), x_data, n_samples=100\n",
    "    )\n",
    "\n",
    "    print(f\"Detected sparsity: {sparsity:.1%} zero elements\")\n",
    "    print(f\"Pattern shape: {pattern.shape}\")\n",
    "    print(f\"Non-zero elements: {np.sum(pattern):,} / {pattern.size:,}\")\n",
    "\n",
    "    # Visualize sparsity pattern\n",
    "    plt.figure(figsize=(10, 4))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(pattern.T, aspect=\"auto\", cmap=\"binary\", interpolation=\"nearest\")\n",
    "    plt.xlabel(\"Data Point\")\n",
    "    plt.ylabel(\"Parameter\")\n",
    "    plt.title(\"Sparsity Pattern (Black = Non-zero)\")\n",
    "    plt.colorbar(label=\"Non-zero\")\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    # Show which parameters affect which data regions\n",
    "    param_density = np.sum(pattern, axis=0)\n",
    "    plt.bar(range(len(true_params)), param_density)\n",
    "    plt.xlabel(\"Parameter Index\")\n",
    "    plt.ylabel(\"Number of Non-zero Entries\")\n",
    "    plt.title(\"Parameter Activity\")\n",
    "    plt.xticks(range(len(true_params)), [\"a1\", \"b1\", \"a2\", \"b2\"])\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n✅ Sparse Jacobian pattern detected successfully\")\n",
    "\n",
    "\n",
    "# Run demo\n",
    "demo_sparse_jacobian_basics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Memory Savings Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T19:57:58.899803Z",
     "iopub.status.busy": "2025-10-08T19:57:58.899651Z",
     "iopub.status.idle": "2025-10-08T19:57:58.903672Z",
     "shell.execute_reply": "2025-10-08T19:57:58.903325Z"
    }
   },
   "outputs": [],
   "source": [
    "def analyze_sparse_jacobian_memory_savings():\n",
    "    \"\"\"Analyze memory savings for different problem sizes.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"SPARSE JACOBIAN MEMORY SAVINGS ANALYSIS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    sparse_computer = SparseJacobianComputer()\n",
    "\n",
    "    # Test different problem sizes\n",
    "    test_cases = [\n",
    "        (10_000, 10, 0.90, \"Small, highly sparse\"),\n",
    "        (100_000, 20, 0.95, \"Medium, highly sparse\"),\n",
    "        (1_000_000, 50, 0.99, \"Large, extremely sparse\"),\n",
    "        (10_000_000, 100, 0.995, \"Very large, ultra-sparse\"),\n",
    "    ]\n",
    "\n",
    "    print(\n",
    "        \"\\n{:>12} {:>8} {:>10} {:>12} {:>12} {:>12} {:>12}\".format(\n",
    "            \"Data Points\",\n",
    "            \"Params\",\n",
    "            \"Sparsity\",\n",
    "            \"Dense (GB)\",\n",
    "            \"Sparse (GB)\",\n",
    "            \"Savings\",\n",
    "            \"Reduction\",\n",
    "        )\n",
    "    )\n",
    "    print(\"-\" * 92)\n",
    "\n",
    "    for n_data, n_params, sparsity, description in test_cases:\n",
    "        memory_info = sparse_computer.estimate_memory_usage(n_data, n_params, sparsity)\n",
    "\n",
    "        print(\n",
    "            \"{:12,} {:8} {:9.1%} {:12.2f} {:12.2f} {:11.1f}% {:11.1f}x\".format(\n",
    "                n_data,\n",
    "                n_params,\n",
    "                sparsity,\n",
    "                memory_info[\"dense_gb\"],\n",
    "                memory_info[\"sparse_gb\"],\n",
    "                memory_info[\"savings_percent\"],\n",
    "                memory_info[\"reduction_factor\"],\n",
    "            )\n",
    "        )\n",
    "\n",
    "    print(\"\\n📊 Key Insights:\")\n",
    "    print(\"  • Sparse representation can reduce memory by 10-100x\")\n",
    "    print(\"  • Benefits increase dramatically with problem size and sparsity\")\n",
    "    print(\"  • Enables fitting problems that wouldn't fit in memory otherwise\")\n",
    "    print(\"  • Overhead is minimal for problems with >90% sparsity\")\n",
    "\n",
    "\n",
    "# Run analysis\n",
    "analyze_sparse_jacobian_memory_savings()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Sparse Jacobians in Practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T19:57:58.905084Z",
     "iopub.status.busy": "2025-10-08T19:57:58.904951Z",
     "iopub.status.idle": "2025-10-08T19:57:59.482102Z",
     "shell.execute_reply": "2025-10-08T19:57:59.481526Z"
    }
   },
   "outputs": [],
   "source": [
    "def demo_sparse_jacobian_fitting():\n",
    "    \"\"\"Demonstrate fitting with sparse Jacobians.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"FITTING WITH SPARSE JACOBIANS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Multi-component Gaussian model (each Gaussian independent)\n",
    "    def multi_gaussian(x, *params):\n",
    "        \"\"\"Sum of N independent Gaussians.\n",
    "\n",
    "        params = [a1, mu1, sigma1, a2, mu2, sigma2, ...]\n",
    "        Each Gaussian only affects data near its center (sparse!)\n",
    "        \"\"\"\n",
    "        n_gaussians = len(params) // 3\n",
    "        result = jnp.zeros_like(x)\n",
    "\n",
    "        for i in range(n_gaussians):\n",
    "            a = params[i * 3]\n",
    "            mu = params[i * 3 + 1]\n",
    "            sigma = params[i * 3 + 2]\n",
    "            result = result + a * jnp.exp(-((x - mu) ** 2) / (2 * sigma**2))\n",
    "\n",
    "        return result\n",
    "\n",
    "    # Generate data with 5 Gaussians\n",
    "    np.random.seed(123)\n",
    "    n_points = 5000\n",
    "    x_data = np.linspace(0, 10, n_points)\n",
    "\n",
    "    # True parameters: 5 Gaussians at different locations\n",
    "    true_params = [\n",
    "        1.0,\n",
    "        2.0,\n",
    "        0.3,  # Gaussian 1\n",
    "        0.8,\n",
    "        4.0,\n",
    "        0.4,  # Gaussian 2\n",
    "        1.2,\n",
    "        6.0,\n",
    "        0.3,  # Gaussian 3\n",
    "        0.9,\n",
    "        7.5,\n",
    "        0.35,  # Gaussian 4\n",
    "        1.1,\n",
    "        9.0,\n",
    "        0.4,  # Gaussian 5\n",
    "    ]\n",
    "\n",
    "    y_data = multi_gaussian(x_data, *true_params) + np.random.normal(0, 0.02, n_points)\n",
    "\n",
    "    # Detect sparsity\n",
    "    print(\"\\n--- Analyzing Sparsity ---\")\n",
    "    sparse_computer = SparseJacobianComputer(sparsity_threshold=0.001)\n",
    "    pattern, sparsity = sparse_computer.detect_sparsity_pattern(\n",
    "        multi_gaussian, np.array(true_params), x_data, n_samples=200\n",
    "    )\n",
    "\n",
    "    print(f\"Detected sparsity: {sparsity:.1%}\")\n",
    "\n",
    "    # Estimate memory savings\n",
    "    memory_info = sparse_computer.estimate_memory_usage(\n",
    "        n_points, len(true_params), sparsity\n",
    "    )\n",
    "\n",
    "    print(\"\\nMemory comparison:\")\n",
    "    print(f\"  Dense Jacobian: {memory_info['dense_gb'] * 1024:.1f} MB\")\n",
    "    print(f\"  Sparse Jacobian: {memory_info['sparse_gb'] * 1024:.1f} MB\")\n",
    "    print(f\"  Savings: {memory_info['savings_percent']:.1f}%\")\n",
    "    print(f\"  Reduction factor: {memory_info['reduction_factor']:.1f}x\")\n",
    "\n",
    "    # Visualize the data and sparsity\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "    # Plot data and true fit\n",
    "    axes[0].plot(x_data, y_data, \"b.\", alpha=0.3, markersize=1, label=\"Data\")\n",
    "    axes[0].plot(\n",
    "        x_data,\n",
    "        multi_gaussian(x_data, *true_params),\n",
    "        \"r-\",\n",
    "        linewidth=2,\n",
    "        label=\"True Model\",\n",
    "    )\n",
    "    axes[0].set_xlabel(\"x\")\n",
    "    axes[0].set_ylabel(\"y\")\n",
    "    axes[0].set_title(\"Multi-Gaussian Data (5 components)\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot sparsity pattern\n",
    "    sample_pattern = pattern[::25]  # Sample for visualization\n",
    "    im = axes[1].imshow(\n",
    "        sample_pattern.T, aspect=\"auto\", cmap=\"binary\", interpolation=\"nearest\"\n",
    "    )\n",
    "    axes[1].set_xlabel(\"Data Point (sampled)\")\n",
    "    axes[1].set_ylabel(\"Parameter\")\n",
    "    axes[1].set_title(f\"Jacobian Sparsity Pattern ({sparsity:.1%} zeros)\")\n",
    "\n",
    "    # Add parameter labels\n",
    "    param_labels = []\n",
    "    for i in range(5):\n",
    "        param_labels.extend([f\"a{i + 1}\", f\"μ{i + 1}\", f\"σ{i + 1}\"])\n",
    "    axes[1].set_yticks(range(len(true_params)))\n",
    "    axes[1].set_yticklabels(param_labels, fontsize=8)\n",
    "\n",
    "    plt.colorbar(im, ax=axes[1], label=\"Non-zero\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"\\n✅ Sparse Jacobian analysis complete\")\n",
    "    print(\"\\n💡 Notice how each Gaussian parameter only affects a localized region!\")\n",
    "\n",
    "\n",
    "# Run demo\n",
    "demo_sparse_jacobian_fitting()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. StreamingOptimizer - Unlimited Dataset Size\n",
    "\n",
    "The `StreamingOptimizer` processes data in batches without ever loading the full dataset into memory. This enables:\n",
    "\n",
    "- **Unlimited dataset size**: Process datasets larger than available RAM\n",
    "- **Real-time learning**: Update model as new data arrives\n",
    "- **Incremental fitting**: Add data incrementally without refitting from scratch\n",
    "- **Distributed data**: Data stored across multiple files or databases\n",
    "\n",
    "### When to Use Streaming\n",
    "\n",
    "- **Very large datasets**: >10GB of data\n",
    "- **Data on disk**: HDF5 files, databases, etc.\n",
    "- **Memory constraints**: Limited RAM relative to data size\n",
    "- **Online learning**: Continuous data streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T19:57:59.483917Z",
     "iopub.status.busy": "2025-10-08T19:57:59.483741Z",
     "iopub.status.idle": "2025-10-08T19:57:59.490440Z",
     "shell.execute_reply": "2025-10-08T19:57:59.490075Z"
    }
   },
   "outputs": [],
   "source": [
    "def demo_streaming_optimizer_basics():\n",
    "    \"\"\"Demonstrate basic streaming optimizer usage.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"STREAMING OPTIMIZER BASICS\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Create a data generator that simulates streaming data\n",
    "    class SimpleDataGenerator:\n",
    "        \"\"\"Generator that simulates reading data in batches.\"\"\"\n",
    "\n",
    "        def __init__(self, total_size, true_params):\n",
    "            self.total_size = total_size\n",
    "            self.true_params = true_params\n",
    "            self.current_pos = 0\n",
    "\n",
    "        def generate_batches(self, batch_size):\n",
    "            \"\"\"Generate batches of data on the fly.\"\"\"\n",
    "            np.random.seed(42)  # Consistent data\n",
    "\n",
    "            while self.current_pos < self.total_size:\n",
    "                # Determine batch size\n",
    "                actual_size = min(batch_size, self.total_size - self.current_pos)\n",
    "\n",
    "                # Generate batch\n",
    "                x_start = self.current_pos / self.total_size * 10\n",
    "                x_end = (self.current_pos + actual_size) / self.total_size * 10\n",
    "                x_batch = np.linspace(x_start, x_end, actual_size)\n",
    "\n",
    "                # Model: y = a*exp(-b*x) + c\n",
    "                a, b, c = self.true_params\n",
    "                y_batch = a * np.exp(-b * x_batch) + c\n",
    "                y_batch += np.random.normal(0, 0.05, actual_size)\n",
    "\n",
    "                self.current_pos += actual_size\n",
    "\n",
    "                yield x_batch, y_batch\n",
    "\n",
    "            # Reset for next epoch\n",
    "            self.current_pos = 0\n",
    "\n",
    "        def close(self):\n",
    "            \"\"\"Cleanup (required by interface).\"\"\"\n",
    "            pass\n",
    "\n",
    "    # Define model\n",
    "    def exponential_model(x, a, b, c):\n",
    "        return a * np.exp(-b * x) + c\n",
    "\n",
    "    # Create streaming optimizer\n",
    "    print(\"\\n--- Setting up Streaming Optimizer ---\")\n",
    "    config = StreamingConfig(\n",
    "        batch_size=1000,\n",
    "        max_epochs=5,\n",
    "        learning_rate=0.01,\n",
    "        use_adam=True,\n",
    "        convergence_tol=1e-6,\n",
    "    )\n",
    "\n",
    "    optimizer = StreamingOptimizer(config)\n",
    "\n",
    "    # Simulate large dataset\n",
    "    total_data_size = 50_000  # 50K points, but processed in batches\n",
    "    true_params = [5.0, 1.2, 0.5]\n",
    "\n",
    "    print(f\"Total dataset size: {total_data_size:,} points\")\n",
    "    print(f\"Batch size: {config.batch_size}\")\n",
    "    print(f\"Max epochs: {config.max_epochs}\")\n",
    "    print(f\"Optimizer: {'Adam' if config.use_adam else 'SGD'}\")\n",
    "\n",
    "    # Create data source\n",
    "    data_generator = SimpleDataGenerator(total_data_size, true_params)\n",
    "\n",
    "    # Initial guess\n",
    "    p0 = np.array([4.0, 1.0, 0.4])\n",
    "\n",
    "    print(f\"\\nTrue parameters: {true_params}\")\n",
    "    print(f\"Initial guess: {list(p0)}\")\n",
    "\n",
    "    # Note: This is a simplified demo. The actual fit_streaming method\n",
    "    # expects a DataGenerator object\n",
    "    print(\"\\n⚠️  Note: Full streaming optimizer requires proper DataGenerator setup\")\n",
    "    print(\"     See documentation for production usage with HDF5 files\")\n",
    "    print(\"\\n✅ Streaming optimizer configuration demo complete\")\n",
    "\n",
    "\n",
    "# Run demo\n",
    "demo_streaming_optimizer_basics()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating and Using HDF5 Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T19:57:59.491871Z",
     "iopub.status.busy": "2025-10-08T19:57:59.491751Z",
     "iopub.status.idle": "2025-10-08T19:57:59.533038Z",
     "shell.execute_reply": "2025-10-08T19:57:59.532762Z"
    }
   },
   "outputs": [],
   "source": [
    "def demo_hdf5_streaming():\n",
    "    \"\"\"Demonstrate HDF5-based streaming for very large datasets.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"HDF5 STREAMING DEMO\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    try:\n",
    "        import h5py\n",
    "\n",
    "        print(\"✓ h5py available\")\n",
    "    except ImportError:\n",
    "        print(\"⚠️  h5py not installed. Install with: pip install h5py\")\n",
    "        print(\"   Skipping HDF5 demo\")\n",
    "        return\n",
    "\n",
    "    # Create temporary HDF5 file with large dataset\n",
    "    import os\n",
    "    import tempfile\n",
    "\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, suffix=\".h5\")  # noqa: SIM115\n",
    "    hdf5_path = temp_file.name\n",
    "    temp_file.close()\n",
    "\n",
    "    try:\n",
    "        # Generate large dataset and save to HDF5\n",
    "        print(\"\\n--- Creating HDF5 dataset ---\")\n",
    "        n_total = 100_000\n",
    "\n",
    "        # Use create_hdf5_dataset utility\n",
    "        def data_generator_func(n_points):\n",
    "            \"\"\"Generate data for HDF5 file.\"\"\"\n",
    "            np.random.seed(42)\n",
    "            x = np.linspace(0, 5, n_points)\n",
    "            y = 3.0 * np.exp(-1.5 * x) + 0.5 + np.random.normal(0, 0.05, n_points)\n",
    "            return x, y\n",
    "\n",
    "        x_data, y_data = data_generator_func(n_total)\n",
    "\n",
    "        # Save to HDF5\n",
    "        with h5py.File(hdf5_path, \"w\") as f:\n",
    "            f.create_dataset(\"x\", data=x_data, compression=\"gzip\")\n",
    "            f.create_dataset(\"y\", data=y_data, compression=\"gzip\")\n",
    "\n",
    "        file_size_mb = os.path.getsize(hdf5_path) / 1024**2\n",
    "        print(f\"Created HDF5 file: {n_total:,} points, {file_size_mb:.2f} MB\")\n",
    "\n",
    "        # Read in batches\n",
    "        print(\"\\n--- Reading in batches ---\")\n",
    "        batch_size = 5000\n",
    "        n_batches = (n_total + batch_size - 1) // batch_size\n",
    "\n",
    "        batch_means = []\n",
    "\n",
    "        with h5py.File(hdf5_path, \"r\") as f:\n",
    "            x_dset = f[\"x\"]\n",
    "            y_dset = f[\"y\"]\n",
    "\n",
    "            for i in range(min(5, n_batches)):  # Show first 5 batches\n",
    "                start = i * batch_size\n",
    "                end = min((i + 1) * batch_size, n_total)\n",
    "\n",
    "                # Read only this batch\n",
    "                x_batch = x_dset[start:end]\n",
    "                y_batch = y_dset[start:end]\n",
    "\n",
    "                batch_mean = np.mean(y_batch)\n",
    "                batch_means.append(batch_mean)\n",
    "\n",
    "                print(\n",
    "                    f\"  Batch {i + 1}: [{start:6,} - {end:6,}], mean={batch_mean:.4f}\"\n",
    "                )\n",
    "\n",
    "        print(f\"\\n✅ Successfully processed {min(5, n_batches)} batches\")\n",
    "        print(f\"   Total batches available: {n_batches}\")\n",
    "        print(\"\\n💡 Key Benefits:\")\n",
    "        print(f\"   • Only {batch_size:,} points in memory at once\")\n",
    "        print(\"   • Can process unlimited dataset size\")\n",
    "        print(\"   • Efficient compression with HDF5\")\n",
    "        print(\"   • Fast random access to any batch\")\n",
    "\n",
    "    finally:\n",
    "        # Clean up temp file\n",
    "        if os.path.exists(hdf5_path):\n",
    "            os.unlink(hdf5_path)\n",
    "\n",
    "\n",
    "# Run demo\n",
    "demo_hdf5_streaming()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Combined Example: All Features Together\n",
    "\n",
    "Let's demonstrate how to combine all three advanced features for maximum performance on a large, sparse problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-08T19:57:59.534702Z",
     "iopub.status.busy": "2025-10-08T19:57:59.534589Z",
     "iopub.status.idle": "2025-10-08T19:58:03.495130Z",
     "shell.execute_reply": "2025-10-08T19:58:03.494755Z"
    }
   },
   "outputs": [],
   "source": [
    "def demo_combined_optimization():\n",
    "    \"\"\"Combine MemoryPool, SparseJacobian, and smart chunking.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"COMBINED OPTIMIZATION DEMO\")\n",
    "    print(\"=\" * 70)\n",
    "\n",
    "    # Large, sparse problem: Multi-peak Gaussian\n",
    "    def multi_peak_model(x, *params):\n",
    "        \"\"\"Multiple Gaussians - sparse Jacobian.\"\"\"\n",
    "        n_peaks = len(params) // 3\n",
    "        result = jnp.zeros_like(x)\n",
    "\n",
    "        for i in range(n_peaks):\n",
    "            a = params[i * 3]\n",
    "            mu = params[i * 3 + 1]\n",
    "            sigma = params[i * 3 + 2]\n",
    "            result = result + a * jnp.exp(-((x - mu) ** 2) / (2 * sigma**2))\n",
    "\n",
    "        return result\n",
    "\n",
    "    # Problem size\n",
    "    n_points = 50_000\n",
    "    n_peaks = 10\n",
    "    n_params = n_peaks * 3  # 30 parameters\n",
    "\n",
    "    print(\"\\nProblem size:\")\n",
    "    print(f\"  Data points: {n_points:,}\")\n",
    "    print(f\"  Parameters: {n_params}\")\n",
    "    print(f\"  Peaks: {n_peaks}\")\n",
    "\n",
    "    # Generate data\n",
    "    np.random.seed(456)\n",
    "    x_data = np.linspace(0, 20, n_points)\n",
    "\n",
    "    # True parameters: peaks at regular intervals\n",
    "    true_params = []\n",
    "    for i in range(n_peaks):\n",
    "        a = 0.8 + 0.4 * np.random.random()\n",
    "        mu = (i + 0.5) * 20 / n_peaks\n",
    "        sigma = 0.3 + 0.2 * np.random.random()\n",
    "        true_params.extend([a, mu, sigma])\n",
    "\n",
    "    y_data = multi_peak_model(x_data, *true_params) + np.random.normal(\n",
    "        0, 0.02, n_points\n",
    "    )\n",
    "\n",
    "    # Step 1: Analyze sparsity\n",
    "    print(\"\\n--- Step 1: Sparsity Analysis ---\")\n",
    "    sparse_comp = SparseJacobianComputer(sparsity_threshold=0.001)\n",
    "    pattern, sparsity = sparse_comp.detect_sparsity_pattern(\n",
    "        multi_peak_model, np.array(true_params), x_data, n_samples=500\n",
    "    )\n",
    "\n",
    "    print(f\"Sparsity: {sparsity:.1%}\")\n",
    "\n",
    "    memory_info = sparse_comp.estimate_memory_usage(n_points, n_params, sparsity)\n",
    "    print(\"Memory savings:\")\n",
    "    print(f\"  Dense: {memory_info['dense_gb'] * 1024:.1f} MB\")\n",
    "    print(f\"  Sparse: {memory_info['sparse_gb'] * 1024:.1f} MB\")\n",
    "    print(f\"  Reduction: {memory_info['reduction_factor']:.1f}x\")\n",
    "\n",
    "    # Step 2: Use memory pool for allocations\n",
    "    print(\"\\n--- Step 2: Memory Pool Setup ---\")\n",
    "    pool = MemoryPool(max_pool_size=20, enable_stats=True)\n",
    "\n",
    "    # Simulate multiple optimization iterations with pooled memory\n",
    "    n_iters = 10\n",
    "    print(f\"Running {n_iters} simulated iterations with memory pool...\")\n",
    "\n",
    "    for i in range(n_iters):\n",
    "        # Allocate temporary arrays from pool\n",
    "        jacobian_chunk = pool.allocate((5000, n_params))\n",
    "        residuals = pool.allocate((5000,))\n",
    "\n",
    "        # Simulate computation\n",
    "        _ = jacobian_chunk * 2.0\n",
    "        _ = residuals + 1.0\n",
    "\n",
    "        # Return to pool\n",
    "        pool.release(jacobian_chunk)\n",
    "        pool.release(residuals)\n",
    "\n",
    "    pool_stats = pool.get_stats()\n",
    "    print(\"Memory pool stats:\")\n",
    "    print(f\"  Allocations: {pool_stats['allocations']}\")\n",
    "    print(f\"  Reuses: {pool_stats['reuses']}\")\n",
    "    print(f\"  Reuse rate: {pool_stats['reuse_rate']:.1%}\")\n",
    "    print(f\"  Peak memory: {pool_stats['peak_memory'] / 1024**2:.1f} MB\")\n",
    "\n",
    "    # Step 3: Fit with standard NLSQ (for comparison)\n",
    "    print(\"\\n--- Step 3: Fitting ---\")\n",
    "    print(\"Using standard NLSQ curve fitting...\")\n",
    "\n",
    "    # Initial guess (slightly off)\n",
    "    p0 = [p * np.random.uniform(0.9, 1.1) for p in true_params]\n",
    "\n",
    "    start_time = time.time()\n",
    "    cf = CurveFit()\n",
    "    popt, pcov = cf.curve_fit(multi_peak_model, x_data, y_data, p0=p0)\n",
    "    fit_time = time.time() - start_time\n",
    "\n",
    "    # Calculate errors\n",
    "    errors = np.abs(popt - np.array(true_params))\n",
    "    rel_errors = errors / np.abs(np.array(true_params))\n",
    "\n",
    "    print(f\"\\n✅ Fit completed in {fit_time:.2f}s\")\n",
    "    print(f\"Max relative error: {np.max(rel_errors):.4f}\")\n",
    "    print(f\"Mean relative error: {np.mean(rel_errors):.4f}\")\n",
    "\n",
    "    # Visualize results\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "    # Plot fit\n",
    "    sample_idx = np.arange(0, len(x_data), 10)  # Sample for speed\n",
    "    axes[0].plot(\n",
    "        x_data[sample_idx],\n",
    "        y_data[sample_idx],\n",
    "        \"b.\",\n",
    "        alpha=0.5,\n",
    "        markersize=1,\n",
    "        label=\"Data\",\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        x_data,\n",
    "        multi_peak_model(x_data, *true_params),\n",
    "        \"g-\",\n",
    "        linewidth=2,\n",
    "        label=\"True\",\n",
    "        alpha=0.7,\n",
    "    )\n",
    "    axes[0].plot(\n",
    "        x_data, multi_peak_model(x_data, *popt), \"r--\", linewidth=2, label=\"Fitted\"\n",
    "    )\n",
    "    axes[0].set_xlabel(\"x\")\n",
    "    axes[0].set_ylabel(\"y\")\n",
    "    axes[0].set_title(f\"Multi-Peak Fit ({n_peaks} peaks, {n_points:,} points)\")\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot parameter errors\n",
    "    param_groups = [\"a\", \"μ\", \"σ\"] * n_peaks\n",
    "    colors = [\n",
    "        \"red\" if pg == \"a\" else \"blue\" if pg == \"μ\" else \"green\" for pg in param_groups\n",
    "    ]\n",
    "\n",
    "    axes[1].bar(range(n_params), rel_errors, color=colors, alpha=0.6)\n",
    "    axes[1].set_xlabel(\"Parameter Index\")\n",
    "    axes[1].set_ylabel(\"Relative Error\")\n",
    "    axes[1].set_title(\"Parameter Recovery Accuracy\")\n",
    "    axes[1].set_yscale(\"log\")\n",
    "    axes[1].grid(True, alpha=0.3, axis=\"y\")\n",
    "\n",
    "    # Legend for parameter types\n",
    "    from matplotlib.patches import Patch\n",
    "\n",
    "    legend_elements = [\n",
    "        Patch(facecolor=\"red\", alpha=0.6, label=\"Amplitude\"),\n",
    "        Patch(facecolor=\"blue\", alpha=0.6, label=\"Center\"),\n",
    "        Patch(facecolor=\"green\", alpha=0.6, label=\"Width\"),\n",
    "    ]\n",
    "    axes[1].legend(handles=legend_elements)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Summary\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"COMBINED OPTIMIZATION SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\n",
    "        f\"\\n✓ Sparse Jacobian: {sparsity:.1%} sparsity, {memory_info['reduction_factor']:.1f}x memory reduction\"\n",
    "    )\n",
    "    print(\n",
    "        f\"✓ Memory Pool: {pool_stats['reuse_rate']:.1%} reuse rate, minimized allocations\"\n",
    "    )\n",
    "    print(f\"✓ Fit Quality: {np.max(rel_errors):.2%} max error on {n_params} parameters\")\n",
    "    print(f\"✓ Performance: {fit_time:.2f}s for {n_points:,} points\")\n",
    "    print(\"\\n🎯 All optimizations working together for maximum performance!\")\n",
    "\n",
    "    pool.clear()\n",
    "\n",
    "\n",
    "# Run combined demo\n",
    "demo_combined_optimization()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Best Practices and Recommendations\n",
    "\n",
    "### When to Use Each Feature\n",
    "\n",
    "#### MemoryPool\n",
    "✅ **Use when:**\n",
    "- Fitting the same model many times\n",
    "- Optimization has many iterations\n",
    "- Low-latency requirements\n",
    "- Memory allocation is a bottleneck\n",
    "\n",
    "❌ **Don't use when:**\n",
    "- Single fit operations\n",
    "- Variable array sizes\n",
    "- Memory is abundant\n",
    "\n",
    "#### SparseJacobian\n",
    "✅ **Use when:**\n",
    "- Jacobian has >90% sparsity\n",
    "- Very large problems (millions of points)\n",
    "- Piecewise or multi-component models\n",
    "- Memory-constrained\n",
    "\n",
    "❌ **Don't use when:**\n",
    "- Dense Jacobians (<50% sparsity)\n",
    "- Small problems (<10K points)\n",
    "- Simple global models\n",
    "\n",
    "#### StreamingOptimizer\n",
    "✅ **Use when:**\n",
    "- Dataset >10GB or doesn't fit in memory\n",
    "- Data in files/databases\n",
    "- Online/incremental learning\n",
    "- Distributed data\n",
    "\n",
    "❌ **Don't use when:**\n",
    "- Data fits in memory\n",
    "- Batch methods are fast enough\n",
    "- Need exact convergence guarantees\n",
    "\n",
    "### Performance Tips\n",
    "\n",
    "1. **Profile first**: Identify actual bottlenecks before optimizing\n",
    "2. **Combine techniques**: Use multiple features together\n",
    "3. **Benchmark**: Measure performance improvements\n",
    "4. **Start simple**: Add optimizations incrementally\n",
    "5. **Monitor memory**: Track memory usage during development\n",
    "\n",
    "### Typical Workflows\n",
    "\n",
    "**Small problems (<10K points):**\n",
    "```python\n",
    "from nlsq import CurveFit\n",
    "cf = CurveFit()\n",
    "popt, pcov = cf.curve_fit(func, x, y, p0)\n",
    "```\n",
    "\n",
    "**Large problems (10K-1M points):**\n",
    "```python\n",
    "from nlsq import CurveFit, MemoryPool\n",
    "with MemoryPool(enable_stats=True) as pool:\n",
    "    cf = CurveFit()\n",
    "    popt, pcov = cf.curve_fit(func, x, y, p0)\n",
    "```\n",
    "\n",
    "**Very large sparse problems:**\n",
    "```python\n",
    "from nlsq import SparseJacobianComputer, SparseOptimizer\n",
    "sparse_comp = SparseJacobianComputer()\n",
    "pattern, sparsity = sparse_comp.detect_sparsity_pattern(func, x0, x_sample)\n",
    "# Use sparse-aware optimization\n",
    "```\n",
    "\n",
    "**Unlimited data:**\n",
    "```python\n",
    "from nlsq import StreamingOptimizer, StreamingConfig\n",
    "config = StreamingConfig(batch_size=10000)\n",
    "optimizer = StreamingOptimizer(config)\n",
    "result = optimizer.fit_streaming(func, data_source, p0)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated NLSQ's advanced performance optimization features:\n",
    "\n",
    "✅ **MemoryPool**\n",
    "- Pre-allocated buffers for zero-allocation iterations\n",
    "- Typical speedup: 2-5x for repeated operations\n",
    "- Memory allocation reduction: 90-99%\n",
    "\n",
    "✅ **SparseJacobian**\n",
    "- Exploit sparsity patterns in Jacobian matrices\n",
    "- Memory reduction: 10-100x for sparse problems\n",
    "- Enables problems that wouldn't fit in memory\n",
    "\n",
    "✅ **StreamingOptimizer**\n",
    "- Process unlimited dataset sizes\n",
    "- Batch-based processing with adaptive learning\n",
    "- Suitable for >10GB datasets or online learning\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "1. **Profile before optimizing**: Identify real bottlenecks\n",
    "2. **Combine techniques**: Multiple features work together\n",
    "3. **Match method to problem**: Choose appropriate optimization for your use case\n",
    "4. **Measure improvements**: Benchmark before and after\n",
    "5. **Start simple**: Add complexity only when needed\n",
    "\n",
    "These features enable NLSQ to handle problems ranging from small datasets on laptops to massive datasets on clusters, all while maintaining high performance and numerical accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## Additional Resources\n",
    "\n",
    "- **NLSQ Documentation**: [https://nlsq.readthedocs.io](https://nlsq.readthedocs.io)\n",
    "- **GitHub Repository**: [https://github.com/imewei/NLSQ](https://github.com/imewei/NLSQ)\n",
    "- **Other Examples**:\n",
    "  - NLSQ Quickstart\n",
    "  - Advanced Features Demo\n",
    "  - Large Dataset Demo\n",
    "  - 2D Gaussian Demo\n",
    "\n",
    "*This notebook demonstrates advanced performance optimization features. Requires Python 3.12+ and NLSQ >= 0.1.0.*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
