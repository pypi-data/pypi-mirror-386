{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5676e100-c255-4871-b167-01a788309112",
   "metadata": {},
   "source": [
    "## In this tutorial we create a CNN and dataloaders, and train / prune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cf8b22-eca4-48eb-8343-1dc144ee5737",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchvision\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27197caf-85a2-48b7-af76-a5ff943408ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\" # Needs to be set, some pruning layers as well as the quantizers are Keras\n",
    "import keras\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "keras.backend.set_image_data_format(\"channels_first\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea5a763-a029-495d-a03a-390048d749f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torchvision.models.resnet18()\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7c72d3-05c8-4c68-a87c-fb0ee8570ee8",
   "metadata": {},
   "source": [
    "## Add pruning and quantization\n",
    "![Layer replacement](images/replace_layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcd31d4a-5246-47d0-9426-32113265d787",
   "metadata": {},
   "source": [
    "To add pruning and quantization, we need a config file that defines how to do that. Let's load a config file from pquant/configs/configs_pdp.yaml. The training function we use later will add the pruning layers and quantized activations automatically using this config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec145f1-502c-4fd0-84ed-e87b84a27374",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pquant import get_default_config\n",
    "\n",
    "# pruning_methods: \"autosparse, cl, cs, dst, mdmm, pdp, wanda\"\n",
    "pruning_method = \"pdp\"\n",
    "config = get_default_config(pruning_method)\n",
    "# Set target sparsity to 80% (20% of weights are non-zero). This parameter exists only for some pruning methods\n",
    "config[\"pruning_parameters\"][\"sparsity\"] = 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3ef3115-2f3d-43e1-a199-4a19d667f796",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace layers with compressed layers\n",
    "from pquant import add_compression_layers\n",
    "input_shape = (256,3,32,32)\n",
    "model = add_compression_layers(model, config, input_shape)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a7241c-0ebd-492c-b4d4-3b269e5afc4d",
   "metadata": {},
   "source": [
    "## Pruning and quantization in the config\n",
    "From the config we see that we are using the PDP pruning method, unstructured version. We aim for 80% weights pruned (sparsity 0.8), and we quantize the model to 8 bits (1 bit goes to sign). \n",
    "By default, all convolutional and linear layers, as well as activations will be quantized using the default values ```default_integer_bits``` and ```default_fractional_bits```. Similarly, by default all convolutional and linear layers will be pruned.\n",
    "\n",
    "What is in a config? The config consists of 3 main parts: the `pruning_parameters`, the `quantization_parameters` and the `training_parameters`. The parameters outside of these at the end are values mostly related to optimizers and schedulers used in testing with a ResNet. They will be removed in the future, we don't have to worry about them. \n",
    "\n",
    "![Config example](images/config_example.png)\n",
    "\n",
    "The `pruning_parameters` are different for each pruning algorithm, and they are explained in more detail [here](https://github.com/nroope/PQuant/blob/dev/docs/pruning_methods.md). \n",
    "\n",
    "The `quantization_parameters` are explained [here](https://github.com/nroope/PQuant/blob/dev/docs/quantization_parameters.md) \n",
    "\n",
    "The `training_parameters`:\n",
    "\n",
    "Hyperparameters related to the training loop. The training loop has 3 possible stages: pretraining, training, fine-tuning. Generally speaking, during pretraining, no pruning is done, and for HGQ no loss is calculated. During training, pruning is done and HGQ loss calculated. During fine-tuning, the pruning mask is fixed and rounded to 0s and 1s. Quantization is done in all 3 stages.\n",
    "\n",
    "![Training stages](images/training_loop.png)\n",
    "\n",
    "If `pruning_first` is true, run pruning before quantization, else quantize first. `rewind`, `rounds` and `save_weights_epoch` are related to multi-round training (CS pruning method). `rounds` sets the number of training rounds of length `epochs`, `save_weights_epoch` sets the epoch at the first round at which to save the weights as a checkpoint. \n",
    "\n",
    "`rewind` defines when to reset the weights to the saved weights checkpoint, and it has 3 possible settings: \n",
    "  \n",
    "   - `rounds` resets the weights back to the checkpoint after each round, \n",
    "   - `post-ticket-search` resets the weights only after all the rounds are finished\n",
    "   - `never` never resets the weights\n",
    "\n",
    "\n",
    "We'll show later how to create a custom quantization / pruning config file from an existing config for a given model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bf6ec9-9fb3-4f56-bffc-8676341cde32",
   "metadata": {},
   "source": [
    "## About the different epochs\n",
    "\n",
    "The config above defines 20 ```pretraining_epochs```, 100 ```epochs``` and 20 ```fine_tuning_epochs```. What stages are used and what happens during each stage is pruning algorithm specific.\n",
    "\n",
    "![Pruning method stages](images/pruning_method_stages.png)\n",
    "\n",
    "In PDP, the pretraining phase consists of training without pruning, followed by calculation of layerwise pruning budgets. After pretraining is finished and the layerwise pruning budgets have been calculated, the training with pruning begins. The mask during this training is a soft mask, consisting of values ranging between (and including) 0 and 1. \n",
    "\n",
    "The fine-tuning step in PDP is optional (not mentioned in the original paper), and during it the mask is fixed and rounded to 0s and 1s."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251515c3-00ac-4110-b8d8-a8c9100f6e6b",
   "metadata": {},
   "source": [
    "## Create data set\n",
    "#### Let's create the data loader and the training and validation loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c24bff-9937-4670-8cff-022ddc7a0aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "\n",
    "def get_cifar10_data(batch_size):\n",
    "    normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "    train_transform = transforms.Compose([transforms.RandomHorizontalFlip(), transforms.RandomCrop(32, padding=4), \n",
    "                                          transforms.ToTensor(), normalize])\n",
    "    test_transform = transforms.Compose([transforms.ToTensor(), normalize])  \n",
    "    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                        download=True, transform=train_transform)\n",
    "    valset = torchvision.datasets.CIFAR10(root='./data', train=True,\n",
    "                                       download=True, transform=test_transform)\n",
    "    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n",
    "                                          shuffle=True, num_workers=4, pin_memory=True)\n",
    "\n",
    "    val_loader = torch.utils.data.DataLoader(valset, batch_size=batch_size,\n",
    "                                         shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader\n",
    "\n",
    "from quantizers.fixed_point.fixed_point_ops import get_fixed_quantizer\n",
    "# Set up input quantizer\n",
    "quantizer = get_fixed_quantizer(overflow_mode=\"SAT\")\n",
    "\n",
    "\n",
    "def train_resnet(model, trainloader, device, loss_func, epoch, optimizer, scheduler, *args, **kwargs):\n",
    "    for data in trainloader:\n",
    "        inputs, labels = data\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        inputs = quantizer(inputs, k=torch.tensor(1.), i=torch.tensor(0.), f=torch.tensor(7.)) # 8 bits input quantization (1 bit for sign)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_func(outputs, labels)\n",
    "        losses = get_model_losses(model, torch.tensor(0.).to(device))\n",
    "        loss += losses\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch += 1\n",
    "    if scheduler is not None:\n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "from pquant import get_layer_keep_ratio, get_model_losses\n",
    "\n",
    "def validate_resnet(model, testloader, device, loss_func, epoch, *args, **kwargs):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            inputs, labels = data\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            inputs = quantizer(inputs, k=torch.tensor(1.), i=torch.tensor(0.), f=torch.tensor(7.)) # 8 bits input quantization (1 bit for sign)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        ratio = get_layer_keep_ratio(model)\n",
    "        print(f'Accuracy: {100 * correct / total:.2f}%, remaining_weights: {ratio * 100:.2f}%')\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "train_loader, val_loader = get_cifar10_data(BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3794868-9103-47cd-9baf-4cbcd703115a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d28865b-afdb-4773-be30-717486d9786a",
   "metadata": {},
   "source": [
    "## Create loss function, scheduler and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f88af88-ef7a-4d30-8cff-f39225d5a96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=0.0001, momentum=0.9)\n",
    "scheduler = CosineAnnealingLR(optimizer, 200)\n",
    "loss_function = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9850c23a-2abc-4904-9c69-859492b450a8",
   "metadata": {},
   "source": [
    "## Train model\n",
    "Training time. We use the train_compressed_model function from pquant to train. We need to provide some parameters such as training and validation functions, their input parameters, the model and the config file. The function automatically adds pruning layers and replaces activations with a quantized variant, trains the model, and removes the pruning layers after training is done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f2414c-1f4d-4a30-a143-920497e60ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pquant import iterative_train\n",
    "\"\"\"\n",
    "Inputs to train_resnet we defined previously are:\n",
    "          model, trainloader, device, loss_func, epoch, optimizer, scheduler, **kwargs\n",
    "\"\"\"\n",
    "\n",
    "trained_model = iterative_train(model = model, \n",
    "                                config = config, \n",
    "                                train_func = train_resnet, \n",
    "                                valid_func = validate_resnet, \n",
    "                                trainloader = train_loader, \n",
    "                                testloader = val_loader, \n",
    "                                device = device, \n",
    "                                loss_func = loss_function,\n",
    "                                optimizer = optimizer, \n",
    "                                scheduler = scheduler\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a02f27-e2c7-4f0c-ac90-10b6b4ac09e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8994c123-92ad-4815-9217-c2dca2f80a6b",
   "metadata": {},
   "source": [
    "We see from that with PDP, the number of weights goes down during training, until it reaches the target sparsity (sparsity of 80%, or ~20% remaining weights). The function that calculates the remaining weights feeds the weights through the quantizer and pruning method, and calculates the ratio between non-zero weights and all weights. However, since PDP uses a soft mask during training, the percentage of remaining weights seems to go down rather noisily (can even drop from 80% to ~20% remaining weights). The algorithm actually increases the sparsity linearly.\n",
    "During fine-tuning the mask is fixed, and turned into a mask of 0s and 1s by a simple rounding operation, so the remaining weights stay the same during each epoch.\n",
    "\n",
    "In the original paper for PDP there was no fine-tuning after the creation of the final hard mask. We have added fine-tuning here as an option that can be turned off by simply setting ```fine_tuning_epochs``` to 0 in the config file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678209db-91ad-4090-9480-76f6061fdf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pquant import remove_pruning_from_model\n",
    "import matplotlib.pyplot as plt\n",
    "# Remove compression layers, leaves Quantized activations in place\n",
    "model = remove_pruning_from_model(trained_model, config)\n",
    "\n",
    "# Plot remaining weights\n",
    "names = []\n",
    "remaining = []\n",
    "total_w = []\n",
    "nonzeros = []\n",
    "for n, m in trained_model.named_modules():\n",
    "    if isinstance(m, (torch.nn.Conv1d, torch.nn.Conv2d, torch.nn.Linear)):\n",
    "        names.append(n)\n",
    "        nonzero = np.count_nonzero(m.weight.detach().cpu())\n",
    "        remaining_pct = nonzero / m.weight.numel()\n",
    "        remaining.append(remaining_pct)\n",
    "        total_w.append(m.weight.numel())\n",
    "        nonzeros.append(nonzero)\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 5))\n",
    "ax[0].bar(range(len(names)), remaining)\n",
    "ax[0].set_xticks(range(len(names)))\n",
    "ax[0].set_xticklabels(names)\n",
    "ax[0].tick_params(axis='x', labelrotation=270)\n",
    "new_ytick = []\n",
    "for i in ax[0].get_yticklabels():\n",
    "    ytick = f\"{float(i.get_text()) * 100}%\"\n",
    "    new_ytick.append(ytick)\n",
    "ax[0].set_yticklabels(new_ytick)\n",
    "ax[0].title.set_text(\"Remaining weights per layer\")\n",
    "\n",
    "ax[1].bar(range(len(nonzeros)), total_w, color=\"lightcoral\", label=\"total weights\")\n",
    "ax[1].bar(range(len(nonzeros)), nonzeros, color=\"steelblue\", label=\"nonzero weights\")\n",
    "ax[1].set_xticks(range(len(names)))\n",
    "ax[1].set_xticklabels(names)\n",
    "ax[1].tick_params(axis='x', labelrotation=270)\n",
    "ax[1].title.set_text(\"Weights per layer\")\n",
    "ax[1].legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98f0f07-4f01-40d8-a162-8778ec310b9a",
   "metadata": {},
   "source": [
    "## Custom config from existing config\n",
    "Using the ```pquant/configs/config_pdp.yaml``` as base, let's customize the quantization and pruning scheme. \n",
    "\n",
    "The function we use will go through the model's layers and do the following: \n",
    "\n",
    "Quantization:\n",
    "\n",
    "        1. Looks for the names of convolutional, linear and average pooling layers, as well as names of the activations (layer type activations and functional types)\n",
    "        2. Adds the name of the layer to the layer_specific list, along with a default quantization scheme of 0 and 7 for weight and bias (if bias is not None)\n",
    "\n",
    "Pruning: \n",
    "\n",
    "        1. Looks for convolutional and linear layers and adds their name to the disable_pruning_for_layers list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62cc068a-4e3b-4e61-8c23-2b8ba37adcea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Base config\n",
    "pruning_method = \"pdp\"\n",
    "config = get_default_config(pruning_method)\n",
    "model = torchvision.models.resnet18()\n",
    "\n",
    "\n",
    "from pquant import add_default_layer_quantization_pruning_to_config\n",
    "config = add_default_layer_quantization_pruning_to_config(model, config)\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdfe45b-1432-4cec-a3dd-b46f837beb7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save config\n",
    "from pquant.core.utils import write_config_to_yaml\n",
    "write_config_to_yaml(config, \"prune_quantize_example.yaml\", sort_keys=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdb7669-ea71-4e86-b128-e313e83fbc96",
   "metadata": {},
   "source": [
    "Now that we have the custom config, it is up to us to modify the quantization bits for each layer that will not use the default value. If a layer uses the default value it can be removed from the ```layer_specific``` list.\n",
    "\n",
    "For pruning, leave those layers to the ```disable_pruning_for_layers``` list that will not be pruned, others need to be removed from the list."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742e0b00-220d-47a7-9774-af694f0afe1f",
   "metadata": {},
   "source": [
    "## About replacing layers and activations\n",
    "Layers that can currently be compressed for PyTorch models: ```nn.Conv1d, nn.Conv2d, nn.Linear nn.AvgPool1d, nn.AvgPool2d, nn.AvgPool3d```.\n",
    "\n",
    "Activations that can currently be automatically be replaced with a quantized variant: ```nn.ReLU, nn.Tanh```. The activations are replaced by a quantized variant, found in ```pquant.core.activations_quantizer.py```.\n",
    "\n",
    "\n",
    "Layers that can currently be compressed for TensorFlow models: ```Conv1D, Conv2D, Dense, DepthwiseConv2D, SeparableConv2D, AveragePooling1D, AveragePooling2D, AveragePooling3D, ReLU, Activation(tanh/relu)```. In TensorFlow the activations can also be a part of the `Dense`, `Conv2D` etc. layer, in which case this activation is extracted and added as a separate quantized layer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baece7d0-e23a-4e31-ba1e-1cf61cb05356",
   "metadata": {},
   "source": [
    "## More about activations\n",
    "If using layer type activations, note that if you want to keep the fine-grained control over the quantization of the activation, reusing an activation layer can cause problems, as all activations will use the quantization bits set for that particular layer. To avoid this, use a separate ```nn.Tanh``` / ```nn.ReLU``` for each activation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cd524cd-0145-4001-9ede-1b85e56171ff",
   "metadata": {},
   "source": [
    "## HGQ\n",
    "To use HGQ, enable it in the config: `config[\"quantization_parameters\"][\"use_high_granularity_quantization\"] = True`. Other relevant parameters to tune are `config[\"quantization_parameters\"][\"default_integer_bits\"]`, `config[\"quantization_parameters\"][\"default_fractional_bits\"]`, `config[\"quantization_parameters\"][\"hgq_gamma\"]`. If the parameter `config[\"quantization_parameters\"][\"hgq_heterogeneous\"]` is true, HGQ learns quantization bits for each weight separately, and if false, it learns one integer and one fractional bit per layer.\n",
    "When using HGQ, we advice using Adam instead of SGD as an optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aadafd9-f0df-473c-84de-2c2a72148ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "473019d7-502c-44b5-bf06-612b857756c6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
