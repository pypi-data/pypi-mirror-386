pruning_parameters:
  alpha: 0.5
  alpha_reset_epoch: 90
  autotune_epochs: 10
  backward_sparsity: false
  disable_pruning_for_layers: # Disable pruning for these layers, even if enable_pruning is true
    -
  enable_pruning: true
  pruning_method: autosparse
  threshold_decay: 0
  threshold_init: -5.0
  threshold_type: channelwise
quantization_parameters:
  default_integer_bits: 0.
  default_fractional_bits: 7.
  enable_quantization: true
  hgq_gamma: 0.0003
  hgq_heterogeneous: True
  layer_specific: []
  use_high_granularity_quantization: false
  use_real_tanh: false
  use_relu_multiplier: true
  use_symmetric_quantization: false
fitcompress_parameters:
  enable_fitcompress : false
  optimize_quantization : true
  quantization_schedule : [7.,4.,3.,2.]
  pruning_schedule : {start : 0, end : -3, steps : 40}
  compression_goal : 0.10
  optimize_pruning : false
  greedy_astar : true
  approximate : true
  lambda : 1
training_parameters:
  epochs: 100
  fine_tuning_epochs: 0
  pretraining_epochs: 0
  pruning_first: false
  rewind: never
  rounds: 1
  save_weights_epoch: -1.0
batch_size: 64
cosine_tmax: 200
gamma: 0.1
l2_decay: 3.0517578125e-05
label_smoothing: 0.1
lr: 0.01
lr_schedule: cosine
milestones:
- -1
- -1
momentum: 0.875
optimizer: sgd
plot_frequency: 100
