"""Integrated demo showing how Embedded mode uses the Metadata Store.

This demo demonstrates the connection between:
1. High-level nexus.connect() API (user-facing)
2. Low-level SQLAlchemy Metadata Store (internal)

It shows how file operations through nexus.connect()
translate to metadata store operations.
"""

import tempfile
from datetime import UTC, datetime, timedelta
from pathlib import Path

import nexus
from nexus.storage.metadata_store import SQLAlchemyMetadataStore


def main() -> None:
    """Run the integrated demo."""
    print("=" * 70)
    print("Nexus Integrated Demo: Embedded Mode + Metadata Store")
    print("=" * 70)

    with tempfile.TemporaryDirectory() as tmpdir:
        data_dir = Path(tmpdir) / "nexus-data"
        db_path = data_dir / "metadata.db"

        print(f"\nüìÅ Data directory: {data_dir}")
        print(f"üíæ Database: {db_path}")

        # ============================================================
        # Part 1: Using High-Level User API
        # ============================================================
        print("\n" + "=" * 70)
        print("PART 1: High-Level User API (nexus.connect)")
        print("=" * 70)

        # Initialize using nexus.connect() - the recommended way
        print("\n1. Connecting to Nexus...")
        # Set agent_id so permission policies can auto-assign ownership
        nx = nexus.connect(config={"data_dir": str(data_dir), "agent_id": "demo-user"})
        print("   ‚úì Connected via nexus.connect()")
        print("   ‚úì Mode auto-detected: embedded")
        print(f"   ‚úì Using metadata store at: {db_path}")
        print("   ‚úì Agent context: demo-user (for permission tracking)")

        # Write files using high-level API
        print("\n2. Writing files via nexus API...")
        nx.write("/documents/report.pdf", b"PDF content here...")
        nx.write("/images/photo.jpg", b"JPEG data here...")
        nx.write("/data/config.json", b'{"setting": "enabled"}')
        print("   ‚úì Wrote 3 files")

        # Read a file
        print("\n3. Reading file...")
        content = nx.read("/documents/report.pdf")
        print(f"   Content: {content.decode()}")

        # List files
        print("\n4. Listing files...")
        files = nx.list()
        print(f"   Found {len(files)} files:")
        for f in files:
            print(f"   - {f}")

        # Close connection
        nx.close()
        print("\n   ‚úì Connection closed")

        # ============================================================
        # Part 2: Inspecting Low-Level Metadata Store
        # ============================================================
        print("\n" + "=" * 70)
        print("PART 2: Low-Level Metadata Store (Internal View)")
        print("=" * 70)

        # Open the same database directly
        print("\n5. Opening metadata store directly...")
        store = SQLAlchemyMetadataStore(db_path)
        print("   ‚úì Connected to same database")

        # Inspect stored metadata
        print("\n6. Inspecting file metadata...")
        all_files = store.list()
        print(f"   Total files in database: {len(all_files)}")

        for file_meta in all_files:
            print(f"\n   üìÑ {file_meta.path}")
            print(f"      Backend: {file_meta.backend_name}")
            print(f"      Physical path: {file_meta.physical_path}")
            print(f"      Size: {file_meta.size} bytes")
            print(f"      ETag: {file_meta.etag}")
            print(f"      Version: {file_meta.version}")
            print(f"      Created: {file_meta.created_at}")
            print(f"      Modified: {file_meta.modified_at}")

        # Add custom metadata (not available in high-level API yet)
        print("\n7. Adding custom metadata (low-level feature)...")
        store.set_file_metadata("/documents/report.pdf", "author", "John Doe")
        store.set_file_metadata("/documents/report.pdf", "department", "Engineering")
        store.set_file_metadata("/documents/report.pdf", "confidential", True)
        print("   ‚úì Added custom metadata")

        # Retrieve custom metadata
        author = store.get_file_metadata("/documents/report.pdf", "author")
        department = store.get_file_metadata("/documents/report.pdf", "department")
        confidential = store.get_file_metadata("/documents/report.pdf", "confidential")
        print(f"   Author: {author}")
        print(f"   Department: {department}")
        print(f"   Confidential: {confidential}")

        store.close()

        # ============================================================
        # Part 3: Re-open and Verify Persistence
        # ============================================================
        print("\n" + "=" * 70)
        print("PART 3: Persistence Test")
        print("=" * 70)

        print("\n8. Re-connecting to Nexus...")
        nx2 = nexus.connect(config={"data_dir": str(data_dir), "agent_id": "demo-user"})
        files2 = nx2.list()
        print(f"   ‚úì Files still present: {len(files2)}")
        for f in files2:
            print(f"   - {f}")

        # Verify content is still readable
        print("\n9. Verifying file content...")
        content2 = nx2.read("/data/config.json")
        print(f"   Content: {content2.decode()}")
        print("   ‚úì Data persisted correctly!")

        # Delete a file
        print("\n10. Deleting a file...")
        nx2.delete("/images/photo.jpg")
        remaining = nx2.list()
        print(f"   ‚úì Remaining files: {len(remaining)}")
        for f in remaining:
            print(f"   - {f}")

        nx2.close()

        # Verify deletion in metadata store
        print("\n11. Verifying deletion in metadata store...")
        store2 = SQLAlchemyMetadataStore(db_path)
        final_files = store2.list()
        print(f"   Files in metadata store: {len(final_files)}")
        for file_meta in final_files:
            print(f"   - {file_meta.path}")

        # Check custom metadata still exists
        author2 = store2.get_file_metadata("/documents/report.pdf", "author")
        print(f"\n   Custom metadata preserved: author={author2}")

        store2.close()

        # ============================================================
        # Part 4: Path Routing and Directory Operations
        # ============================================================
        print("\n" + "=" * 70)
        print("PART 4: Path Routing & Directory Operations (NEW)")
        print("=" * 70)

        print("\n12. Testing directory operations...")
        # Use simple paths without workspace prefix to avoid routing conflicts
        nx3 = nexus.connect(config={"data_dir": str(data_dir), "agent_id": "agent1"})

        # Create directory structure
        print("\n   Creating directory structure...")
        nx3.mkdir("/projects/data", parents=True, exist_ok=True)
        nx3.mkdir("/projects/models", parents=True, exist_ok=True)
        print("   ‚úì Created: /projects/data")
        print("   ‚úì Created: /projects/models")

        # Check if directories exist
        print("\n13. Checking directory existence...")
        is_dir1 = nx3.is_directory("/projects")
        is_dir2 = nx3.is_directory("/projects/data")
        is_dir3 = nx3.is_directory("/documents")  # Should be True (from earlier)
        print(f"   /projects is directory: {is_dir1}")
        print(f"   /projects/data is directory: {is_dir2}")
        print(f"   /documents is directory: {is_dir3}")

        # Write files into created directories
        print("\n14. Writing files into directory structure...")
        nx3.write("/projects/data/file1.txt", b"Agent 1 data file")
        nx3.write("/projects/data/file2.txt", b"Another file")
        nx3.write("/projects/models/model.pkl", b"model data")
        print("   ‚úì Wrote files to /projects/data/")
        print("   ‚úì Wrote files to /projects/models/")

        # Create nested directories with parents=True
        print("\n15. Creating deeply nested directories...")
        nx3.mkdir("/ml/experiments/run1", parents=True, exist_ok=True)
        nx3.write("/ml/experiments/run1/results.json", b'{"accuracy": 0.95}')
        print("   ‚úì Created: /projects/ml/experiments/run1 (with parents)")
        print("   ‚úì Wrote: /projects/ml/experiments/run1/results.json")

        # List all files
        print("\n16. Listing all files in workspace...")
        all_files = nx3.list()
        workspace_files = [
            f for f in all_files if f.startswith("/workspace") or f.startswith("/projects")
        ]
        print(f"   Found {len(workspace_files)} files in workspace:")
        for f in sorted(workspace_files):
            print(f"   - {f}")

        # Test path routing
        print("\n17. Demonstrating path routing...")
        print("   Router maps virtual paths to physical backend paths")
        print("   Virtual: /projects/data/file1.txt")
        print("   ‚Üí Backend: projects/data/file1.txt")
        print("   ‚Üí Physical: {data_dir}/files/<content_hash>")

        # Remove a directory (will fail - not empty)
        print("\n18. Testing rmdir (should fail - not empty)...")
        try:
            nx3.rmdir("/projects/data", recursive=False)
            print("   ‚úó Should have failed!")
        except OSError as e:
            print(f"   ‚úì Correctly failed: {e}")

        # Remove directory recursively
        print("\n19. Removing directory recursively...")
        nx3.rmdir("/projects/models", recursive=True)
        print("   ‚úì Removed /projects/models (recursive)")

        # Verify removal
        remaining_files = [f for f in nx3.list() if f.startswith("/projects")]
        print(f"   Remaining project files: {len(remaining_files)}")
        for f in sorted(remaining_files):
            print(f"   - {f}")

        nx3.close()

        # ============================================================
        # Part 5: Multi-Mount Configuration (INTERNAL APIs)
        # ============================================================
        print("\n" + "=" * 70)
        print("PART 5: Multi-Mount Configuration (Educational - Internal APIs)")
        print("=" * 70)
        print("NOTE: This section uses internal router APIs for educational purposes.")
        print("      In production code, use only user-facing APIs (read/write/delete/etc).")

        print("\n20. Testing multiple mount points...")
        # NOTE: This section demonstrates INTERNAL router APIs with is_admin=True
        # This is for educational purposes only - production code should use user-facing APIs
        nx4 = nexus.connect(
            config={"data_dir": str(data_dir), "agent_id": "demo-user", "is_admin": True}
        )

        # Create separate backend for special namespace (INTERNAL API)
        from nexus.backends.local import LocalBackend

        special_backend = LocalBackend(data_dir / "special-isolated")
        nx4.router.add_mount("/special", special_backend, priority=10)  # INTERNAL

        print("   ‚úì Added mount: /special ‚Üí isolated backend (INTERNAL API)")
        print("   ‚úì Default mount: / ‚Üí main backend")

        # Write to different mounts (USER-FACING API)
        nx4.write("/special/isolated.txt", b"in special backend")
        nx4.write("/other/regular.txt", b"in default backend")

        print("\n21. Verifying routing (INTERNAL API - for demonstration)...")
        route_special = nx4.router.route("/special/test.txt", is_admin=True)  # INTERNAL
        route_other = nx4.router.route("/other/test.txt", is_admin=True)  # INTERNAL

        print(f"   /special/test.txt ‚Üí mount: {route_special.mount_point}")
        print(f"   /other/test.txt ‚Üí mount: {route_other.mount_point}")

        # List files from both mounts
        all_files_multi = nx4.list()
        special_files = [f for f in all_files_multi if f.startswith("/special")]
        other_files = [f for f in all_files_multi if f.startswith("/other")]

        print(f"\n   Special mount files: {len(special_files)}")
        for f in sorted(special_files)[:3]:
            print(f"   - {f}")

        print(f"\n   Default mount files: {len(other_files)}")
        for f in sorted(other_files)[:3]:
            print(f"   - {f}")

        nx4.close()

        # ============================================================
        # Part 6: Namespace & Tenant Isolation (NEW in v0.1.0)
        # ============================================================
        print("\n" + "=" * 70)
        print("PART 6: Namespace & Tenant Isolation (Educational + User-facing)")
        print("=" * 70)
        print("NOTE: Sections 22-24 use internal APIs for education.")
        print("      Section 25-29 show recommended user-facing approach.")

        # Add small delay to ensure file handles are released (macOS tmpfs issue)
        import time

        time.sleep(0.5)

        print("\n22. Demonstrating path parsing and namespace extraction...")
        print("    (INTERNAL API - for educational purposes)")

        try:
            nx5 = nexus.connect(config={"data_dir": str(data_dir), "agent_id": "demo-user"})
        except Exception as e:
            print(f"\n[WARNING] Could not open connection (tmpfs locking issue): {e}")
            print("Skipping Parts 6-14 (namespace isolation, work detection, etc.)")
            print("Note: This is a macOS tmpfs limitation when reusing databases.")
            print("Jumping to Part 15 (sync operations)...")
            nx5 = None

        if nx5 is not None:
            # Parse different namespace paths

            test_paths = [
                "/workspace/acme/agent1/data/file.txt",
                "/shared/acme/datasets/model.pkl",
                "/archives/acme/2024/01/backup.tar",
                "/external/s3/bucket/file.txt",
                "/system/config/settings.json",
            ]

        print("   Parsing virtual paths to extract namespace info:")
        for path in test_paths:
            info = nx5.router.parse_path(path)
            print(f"\n   Path: {path}")
            print(f"   ‚Üí Namespace: {info.namespace}")
            print(f"   ‚Üí Tenant: {info.tenant_id or 'N/A'}")
            print(f"   ‚Üí Agent: {info.agent_id or 'N/A'}")
            print(f"   ‚Üí Relative path: {info.relative_path or '(root)'}")

        print("\n23. Testing path validation and security...")

        # Valid paths
        valid_paths = [
            "/workspace/tenant1/agent1/data.txt",
            "/shared/tenant1/file.txt",
            "/external/backend/file.txt",
        ]
        print("   Valid paths (should pass):")
        for path in valid_paths:
            try:
                normalized = nx5.router.validate_path(path)
                print(f"   ‚úì {path} ‚Üí {normalized}")
            except Exception as e:
                print(f"   ‚úó {path} ‚Üí ERROR: {e}")

        # Invalid paths (security issues)
        from nexus.core.router import InvalidPathError

        invalid_paths = [
            ("/workspace/../../etc/passwd", "path traversal"),
            ("/workspace/file\x00name.txt", "null byte"),
            ("workspace/relative", "relative path"),
        ]
        print("\n   Invalid paths (security checks):")
        for path, reason in invalid_paths:
            try:
                nx5.router.validate_path(path)
                print(f"   ‚úó {path} should have been rejected ({reason})!")
            except InvalidPathError:
                print(f"   ‚úì Rejected {reason}: {repr(path)[:50]}")

        print("\n24. Namespace configuration and access control...")

        # Show namespace configurations (INTERNAL API - for educational purposes only)
        print("   Default namespace configurations:")
        print("   (NOTE: Accessing ._namespaces is internal API, shown for education)")
        for ns_name in ["workspace", "shared", "external", "system", "archives"]:
            ns_config = nx5.router._namespaces[ns_name]
            print(f"\n   {ns_name}:")
            print(f"   - Read-only: {ns_config.readonly}")
            print(f"   - Admin-only: {ns_config.admin_only}")
            print(f"   - Requires tenant: {ns_config.requires_tenant}")

        # Close old instance and create with custom namespace (USER-FACING API)
        nx5.close()
        print("\n   Creating instance with custom namespace...")
        from nexus.core.router import NamespaceConfig

        custom_ns = NamespaceConfig(
            name="experiments", readonly=False, admin_only=False, requires_tenant=True
        )

        # USER-FACING: Pass custom_namespaces parameter
        nx5 = nexus.connect(
            config={
                "data_dir": str(data_dir),
                "custom_namespaces": [custom_ns],
                "agent_id": "demo-user",
            }
        )
        print("   ‚úì Registered custom namespace: 'experiments' (via config)")

        print("\n25. Testing tenant isolation (INTERNAL APIs - educational)...")

        # Mount workspace backend (INTERNAL API)
        from nexus.backends.local import LocalBackend

        workspace_backend = LocalBackend(data_dir / "workspace-tenant-test")
        nx5.router.add_mount("/workspace", workspace_backend, priority=10)  # INTERNAL
        nx5.router.add_mount("/shared", workspace_backend, priority=10)  # INTERNAL

        # Tenant "acme" accessing their own resources (INTERNAL API)
        print("   Tenant 'acme' accessing own resources:")
        try:
            route = nx5.router.route(  # INTERNAL
                "/workspace/acme/agent1/data.txt", tenant_id="acme", is_admin=False
            )
            print("   ‚úì Access granted to /workspace/acme/agent1/data.txt")
            print(f"     ‚Üí Mount: {route.mount_point}")
            print(f"     ‚Üí Backend path: {route.backend_path}")
        except Exception as e:
            print(f"   ‚úó Unexpected error: {e}")

        # Tenant "acme" trying to access "other-tenant" resources
        from nexus.core.router import AccessDeniedError

        print("\n   Tenant 'acme' accessing 'other-tenant' resources:")
        try:
            route = nx5.router.route(
                "/workspace/other-tenant/agent1/data.txt", tenant_id="acme", is_admin=False
            )
            print("   ‚úó Should have been denied!")
        except AccessDeniedError as e:
            print("   ‚úì Access denied (tenant isolation enforced)")
            print(f"     ‚Üí {e}")

        # Admin accessing any tenant's resources
        print("\n   Admin accessing 'other-tenant' resources:")
        try:
            route = nx5.router.route(
                "/workspace/other-tenant/agent1/data.txt", tenant_id="acme", is_admin=True
            )
            print("   ‚úì Admin access granted to any tenant")
            print(f"     ‚Üí Backend path: {route.backend_path}")
        except Exception as e:
            print(f"   ‚úó Unexpected error: {e}")

        print("\n26. Testing read-only namespaces (INTERNAL APIs - educational)...")

        # Mount archives backend (INTERNAL API)
        archives_backend = LocalBackend(data_dir / "archives-test")
        nx5.router.add_mount("/archives", archives_backend, priority=10)  # INTERNAL

        # Reading from archives (should succeed)
        print("   Reading from /archives (read-only namespace):")
        try:
            route = nx5.router.route(
                "/archives/acme/2024/backup.tar",
                tenant_id="acme",
                is_admin=False,
                check_write=False,
            )
            print("   ‚úì Read access granted")
            print(f"     ‚Üí Readonly: {route.readonly}")
        except Exception as e:
            print(f"   ‚úó Unexpected error: {e}")

        # Writing to archives (should fail)
        print("\n   Writing to /archives (should fail):")
        try:
            route = nx5.router.route(
                "/archives/acme/2024/backup.tar",
                tenant_id="acme",
                is_admin=False,
                check_write=True,
            )
            print("   ‚úó Write should have been denied!")
        except AccessDeniedError as e:
            print("   ‚úì Write denied (read-only namespace)")
            print(f"     ‚Üí {e}")

        print("\n27. Testing admin-only namespaces (INTERNAL APIs - educational)...")

        # Mount system backend (INTERNAL API)
        system_backend = LocalBackend(data_dir / "system-test")
        nx5.router.add_mount("/system", system_backend, priority=10)  # INTERNAL

        # Non-admin accessing system (should fail)
        print("   Non-admin accessing /system namespace:")
        try:
            route = nx5.router.route(
                "/system/config/settings.json", is_admin=False, check_write=False
            )
            print("   ‚úó Non-admin access should have been denied!")
        except AccessDeniedError as e:
            print("   ‚úì Access denied (admin-only namespace)")
            print(f"     ‚Üí {e}")

        # Admin accessing system (should succeed)
        print("\n   Admin accessing /system namespace:")
        try:
            route = nx5.router.route(
                "/system/config/settings.json", is_admin=True, check_write=False
            )
            print("   ‚úì Admin access granted")
            print(f"     ‚Üí Backend path: {route.backend_path}")
            print(f"     ‚Üí Readonly: {route.readonly}")
        except Exception as e:
            print(f"   ‚úó Unexpected error: {e}")

        print("\n28. Practical example: Multi-tenant workspace isolation...")

        # Create workspace structure for multiple tenants
        print("   Creating multi-tenant workspace:")

        # Tenant 1: ACME Corp
        acme_files = [
            "/workspace/acme/agent1/tasks/task1.json",
            "/workspace/acme/agent1/data/results.csv",
            "/workspace/acme/agent2/tasks/task2.json",
            "/shared/acme/models/classifier.pkl",
        ]

        # Tenant 2: Tech Inc
        tech_files = [
            "/workspace/techincCorp/agent1/tasks/analysis.json",
            "/workspace/techincCorp/agent1/data/metrics.csv",
            "/shared/techincCorp/datasets/training_data.csv",
        ]

        print("   Tenant: ACME Corp")
        for path in acme_files:
            info = nx5.router.parse_path(path)
            print(f"   - {path}")
            print(f"     Tenant: {info.tenant_id}, Agent: {info.agent_id or 'shared'}")

        print("\n   Tenant: Tech Inc")
        for path in tech_files:
            info = nx5.router.parse_path(path)
            print(f"   - {path}")
            print(f"     Tenant: {info.tenant_id}, Agent: {info.agent_id or 'shared'}")

            print("\n   Enforcing isolation:")
            print("   ‚úì ACME's agent1 can only access /workspace/acme/agent1/")
            print("   ‚úì ACME's agents can share via /shared/acme/")
            print("   ‚úì Tech Inc cannot access ACME's resources")
            print("   ‚úì Admins can access all tenants for maintenance")

            print("\n29. Summary of namespace features:")
            print("   Namespaces defined:")
            print("   - workspace/  : Agent-specific scratch space (tenant+agent required)")
            print("   - shared/     : Tenant-wide shared data (tenant required)")
            print("   - external/   : Pass-through to external backends (no tenant)")
            print("   - system/     : System metadata (admin-only, read-only)")
            print("   - archives/   : Cold storage (tenant required, read-only)")
            print()
            print("   Security features:")
            print("   ‚úì Path validation (null bytes, control chars, path traversal)")
            print("   ‚úì Tenant isolation (enforced by namespace)")
            print("   ‚úì Admin override (full access when needed)")
            print("   ‚úì Read-only namespaces (archives, system)")
            print("   ‚úì Custom namespace registration")

            nx5.close()
        else:
            # If nx5 failed to connect, skip Parts 6-14 and jump to Part 15
            print("\n[INFO] Skipping Parts 6-14 due to database locking issue")
            print("[INFO] Continuing to Part 15: rclone-style CLI Operations...")

        # End of nx5 conditional block - Parts 6-14 run only if nx5 succeeds

        # ============================================================
        # Part 7: End-to-End Tenant Isolation (USER-FACING APIs ONLY!)
        # ============================================================
        print("\n" + "=" * 70)
        print("PART 7: End-to-End Tenant Isolation - USER-FACING APIs ONLY!")
        print("=" * 70)
        print("NOTE: This section demonstrates the RECOMMENDED approach.")
        print("      Uses only public APIs: read(), write(), delete(), mkdir(), etc.")

        print("\n30. Creating multi-tenant Embedded instances...")

        # Create separate instances for each tenant
        nx_acme = nexus.connect(
            config={
                "data_dir": str(data_dir / "multi-tenant"),
                "tenant_id": "acme",
                "agent_id": "agent1",
            }
        )
        nx_tech = nexus.connect(
            config={
                "data_dir": str(data_dir / "multi-tenant"),
                "tenant_id": "techinc",
                "agent_id": "agent1",
            }
        )
        nx_admin = nexus.connect(
            config={"data_dir": str(data_dir / "multi-tenant"), "is_admin": True}
        )

        print("   ‚úì Created ACME tenant instance (tenant_id='acme')")
        print("   ‚úì Created TechInc tenant instance (tenant_id='techinc')")
        print("   ‚úì Created Admin instance (is_admin=True)")

        print("\n31. Testing write isolation...")
        # ACME writes to their workspace
        nx_acme.write("/workspace/acme/agent1/secret.txt", b"ACME confidential data")
        print("   ‚úì ACME wrote: /workspace/acme/agent1/secret.txt")

        # TechInc writes to their workspace
        nx_tech.write("/workspace/techinc/agent1/data.json", b'{"project": "tech-project"}')
        print("   ‚úì TechInc wrote: /workspace/techinc/agent1/data.json")

        # ACME writes to shared
        nx_acme.write("/shared/acme/models/v1.pkl", b"ACME ML model")
        print("   ‚úì ACME wrote: /shared/acme/models/v1.pkl")

        print("\n32. Testing read isolation...")
        # ACME can read their own files
        acme_secret = nx_acme.read("/workspace/acme/agent1/secret.txt")
        print(f"   ‚úì ACME read their own file: {acme_secret.decode()}")

        # TechInc cannot read ACME's files
        print("\n   TechInc attempting to read ACME's file...")
        try:
            nx_tech.read("/workspace/acme/agent1/secret.txt")
            print("   ‚úó Should have been blocked!")
        except Exception as e:
            print(f"   ‚úì Access denied: {type(e).__name__}")
            print(f"     ‚Üí {e}")

        print("\n33. Testing write isolation to other tenant...")
        try:
            nx_tech.write("/workspace/acme/agent1/hacked.txt", b"malicious data")
            print("   ‚úó Should have been blocked!")
        except Exception as e:
            print(f"   ‚úì Write blocked: {type(e).__name__}")
            print(f"     ‚Üí {e}")

        print("\n34. Testing delete isolation...")
        try:
            nx_tech.delete("/workspace/acme/agent1/secret.txt")
            print("   ‚úó Should have been blocked!")
        except Exception as e:
            print(f"   ‚úì Delete blocked: {type(e).__name__}")
            print(f"     ‚Üí {e}")

        print("\n35. Testing admin override...")
        # Admin can read any tenant's files
        admin_read = nx_admin.read("/workspace/acme/agent1/secret.txt")
        print(f"   ‚úì Admin read ACME's file: {admin_read.decode()}")

        admin_read2 = nx_admin.read("/workspace/techinc/agent1/data.json")
        print(f"   ‚úì Admin read TechInc's file: {admin_read2.decode()}")

        # Admin can write to any tenant's workspace
        nx_admin.write("/workspace/acme/agent1/admin-note.txt", b"Admin audit log")
        print("   ‚úì Admin wrote to ACME's workspace")

        print("\n36. Testing read-only namespace enforcement...")
        # Try to write to archives (read-only)
        try:
            nx_acme.write("/archives/acme/2024/backup.tar", b"backup data")
            print("   ‚úó Should have been blocked (read-only)!")
        except Exception as e:
            print(f"   ‚úì Write to archives blocked: {type(e).__name__}")
            print(f"     ‚Üí {e}")

        print("\n37. Testing admin-only namespace enforcement...")
        # Non-admin cannot access /system
        try:
            nx_acme.write("/system/config.json", b'{"setting": "value"}')
            print("   ‚úó Should have been blocked (admin-only)!")
        except Exception as e:
            print(f"   ‚úì Access to /system blocked: {type(e).__name__}")
            print(f"     ‚Üí {e}")

        print("\n38. Testing directory isolation...")
        # Admin creates a directory in agent2's workspace (agents can't cross-access)
        nx_admin.mkdir("/workspace/acme/agent2/experiments", parents=True)
        print("   ‚úì Admin created directory: /workspace/acme/agent2/experiments")

        # TechInc cannot delete ACME's directory
        try:
            nx_tech.rmdir("/workspace/acme/agent2/experiments", recursive=True)
            print("   ‚úó Should have been blocked!")
        except Exception as e:
            print(f"   ‚úì Directory deletion blocked: {type(e).__name__}")

        print("\n39. Testing agent-level isolation...")
        # Create agent-specific instances
        nx_agent1 = nexus.connect(
            config={
                "data_dir": str(data_dir / "multi-tenant"),
                "tenant_id": "acme",
                "agent_id": "agent1",
            }
        )
        nx_agent2 = nexus.connect(
            config={
                "data_dir": str(data_dir / "multi-tenant"),
                "tenant_id": "acme",
                "agent_id": "agent2",
            }
        )

        print("   ‚úì Created agent1 instance (tenant='acme', agent='agent1')")
        print("   ‚úì Created agent2 instance (tenant='acme', agent='agent2')")

        # Agent1 writes to their workspace
        nx_agent1.write("/workspace/acme/agent1/task.json", b'{"status": "in_progress"}')
        print("\n   Agent1 wrote to /workspace/acme/agent1/task.json")

        # Agent1 can read their own file
        agent1_data = nx_agent1.read("/workspace/acme/agent1/task.json")
        print(f"   Agent1 read their own file: {agent1_data.decode()}")

        # Agent2 cannot read Agent1's workspace
        print("\n   Agent2 attempting to read Agent1's file...")
        try:
            nx_agent2.read("/workspace/acme/agent1/task.json")
            print("   ‚úó Should have been blocked!")
        except Exception as e:
            print(f"   ‚úì Agent isolation enforced: {type(e).__name__}")
            print(f"     ‚Üí {e}")

        # Agents can collaborate via /shared
        print("\n   Testing agent collaboration via /shared namespace...")
        nx_agent1.write("/shared/acme/team-data.json", b'{"project": "collaboration"}')
        print("   Agent1 wrote to /shared/acme/team-data.json")

        shared_data = nx_agent2.read("/shared/acme/team-data.json")
        print(f"   Agent2 read from shared: {shared_data.decode()}")
        print("   ‚úì Agents can collaborate via /shared namespace!")

        nx_agent1.close()
        nx_agent2.close()

        print("\n40. Summary of tenant and agent isolation:")
        print("   Tenant isolation:")
        print("   ‚úì Tenant 'acme' cannot access tenant 'techinc' resources")
        print("   ‚úì Tenant 'techinc' cannot access tenant 'acme' resources")
        print()
        print("   Agent isolation (workspace only):")
        print("   ‚úì Agent 'agent1' cannot access agent 'agent2' workspace")
        print("   ‚úì Agent 'agent2' cannot access agent 'agent1' workspace")
        print("   ‚úì Agents can collaborate via /shared namespace")
        print()
        print("   Admin privileges:")
        print("   ‚úì Admin can access all tenant and agent resources")
        print()
        print("   Namespace enforcement:")
        print("   ‚úì Read-only namespaces (/archives, /system) enforced")
        print("   ‚úì Admin-only namespaces (/system) enforced")
        print("   ‚úì All file and directory operations respect isolation")

        nx_acme.close()
        nx_tech.close()
        nx_admin.close()

        # ============================================================
        # Part 8: Content-Addressable Storage (CAS) with Embedded
        # ============================================================
        print("\n" + "=" * 70)
        print("PART 8: Content-Addressable Storage (CAS) - NEW!")
        print("=" * 70)

        print("\n40. Using Nexus with automatic CAS deduplication...")
        # CAS is now always enabled - no special flag needed!
        nx_cas = nexus.connect(config={"data_dir": str(data_dir / "cas-mode")})
        print("   ‚úì Connected (CAS automatic)")
        print(f"   ‚úì Storage location: {data_dir / 'cas-mode'}")

        # Write content
        print("\n41. Writing content via CAS-enabled Nexus...")
        content1 = b"This is important data that will be content-addressed"
        nx_cas.write("/documents/data.txt", content1)

        # Get metadata to see content hash
        meta1 = nx_cas.metadata.get("/documents/data.txt")
        hash1 = meta1.etag  # etag is SHA-256 hash
        print(f"   Content hash (etag): {hash1[:16]}...{hash1[-8:]}")
        print(f"   Ref count: {nx_cas.backend.get_ref_count(hash1)}")
        print(f"   Size: {meta1.size} bytes")
        print(f"   Backend: {meta1.backend_name}")

        # Verify directory structure
        print("\n   Physical storage path:")
        print(f"   cas/{hash1[:2]}/{hash1[2:4]}/{hash1}")
        print(f"   Structure: {hash1[:2]}/{hash1[2:4]}/{hash1}")

        # Write identical content (deduplication)
        print("\n42. Testing automatic content deduplication...")
        content2 = b"This is important data that will be content-addressed"  # Same content!
        nx_cas.write("/reports/summary.txt", content2)  # Different path, same content

        meta2 = nx_cas.metadata.get("/reports/summary.txt")
        hash2 = meta2.etag
        print(f"   Second file hash: {hash2[:16]}...{hash2[-8:]}")
        print(f"   Hashes match: {hash1 == hash2}")
        print(f"   Ref count (auto-incremented): {nx_cas.backend.get_ref_count(hash1)}")
        print(f"   Physical paths match: {meta1.physical_path == meta2.physical_path}")
        print("   ‚úì Content deduplicated - only stored once!")

        # Write different content
        print("\n43. Writing different content...")
        content3 = b"Different content with different hash"
        nx_cas.write("/logs/access.log", content3)

        meta3 = nx_cas.metadata.get("/logs/access.log")
        hash3 = meta3.etag
        print(f"   New content hash: {hash3[:16]}...{hash3[-8:]}")
        print(f"   Different from first: {hash1 != hash3}")
        print(f"   Ref count: {nx_cas.backend.get_ref_count(hash3)}")

        # Read content back
        print("\n44. Reading content transparently...")
        retrieved = nx_cas.read("/documents/data.txt")
        print(f"   Retrieved {len(retrieved)} bytes")
        print(f"   Content matches: {retrieved == content1}")
        print(f"   Content: {retrieved.decode()[:50]}...")
        print("   ‚úì CAS backend is transparent to user!")

        # Delete with reference counting
        print("\n45. Testing automatic reference counting on delete...")
        print(f"   Current ref count for shared content: {nx_cas.backend.get_ref_count(hash1)}")

        nx_cas.delete("/documents/data.txt")  # First delete
        print("   After deleting /documents/data.txt...")
        print(f"   Ref count: {nx_cas.backend.get_ref_count(hash1)}")
        print(f"   Content still exists: {nx_cas.backend.content_exists(hash1)}")
        print(f"   Other file still readable: {nx_cas.exists('/reports/summary.txt')}")

        nx_cas.delete("/reports/summary.txt")  # Second delete
        print("\n   After deleting /reports/summary.txt...")
        print(f"   Content exists in CAS: {nx_cas.backend.content_exists(hash1)}")
        print("   ‚úì Content automatically removed when last reference deleted!")

        # Inspect CAS directory structure
        print("\n46. Inspecting CAS directory structure...")
        cas_files = list((data_dir / "cas-mode" / "cas").rglob("*"))
        content_files = [f for f in cas_files if f.is_file() and f.suffix != ".meta"]
        meta_files = [f for f in cas_files if f.suffix == ".meta"]
        print(f"   Content files: {len(content_files)}")
        print(f"   Metadata files: {len(meta_files)}")
        print("\n   Directory tree (CAS storage):")
        for f in sorted(cas_files)[:10]:  # Show first 10
            if f.is_file():
                rel_path = f.relative_to(data_dir / "cas-mode" / "cas")
                print(f"   {rel_path}")

        # Demonstrate hash collision resistance
        print("\n47. Hash collision resistance...")
        test_contents = [
            (b"Content A", "/test/a.txt"),
            (b"Content B", "/test/b.txt"),
            (b"Similar content 1", "/test/c.txt"),
            (b"Similar content 2", "/test/d.txt"),
            (b"x" * 1000, "/test/e.txt"),
            (b"y" * 1000, "/test/f.txt"),
        ]
        for content, path in test_contents:
            nx_cas.write(path, content)

        hashes = [nx_cas.metadata.get(path).etag for _, path in test_contents]
        unique_hashes = set(hashes)
        print(f"   Wrote {len(test_contents)} different contents")
        print(f"   Got {len(unique_hashes)} unique hashes")
        print(f"   No collisions: {len(hashes) == len(unique_hashes)}")

        # Show storage efficiency
        print("\n48. Storage efficiency demonstration...")
        # Write same content 100 times to different paths
        repeated_content = b"This content will be written 100 times"
        nx_cas.write("/efficiency/test0.txt", repeated_content)
        repeated_meta = nx_cas.metadata.get("/efficiency/test0.txt")
        repeated_hash = repeated_meta.etag

        for i in range(1, 100):
            nx_cas.write(f"/efficiency/test{i}.txt", repeated_content)

        print("   Content written: 100 times (different paths)")
        print(f"   Ref count: {nx_cas.backend.get_ref_count(repeated_hash)}")
        print("   Physical copies: 1")
        print(f"   Space saved: ~{len(repeated_content) * 99} bytes")
        print("   ‚úì Automatic deduplication saves storage!")

        # List some files
        print("\n   Files exist in metadata:")
        files = nx_cas.list("/efficiency")
        print(f"   Total files: {len(files)}")
        print("   But only 1 physical copy in CAS storage!")

        nx_cas.close()

        # ============================================================
        # Part 9: File Discovery Operations (v0.1.0 - NEW!)
        # ============================================================
        print("\n" + "=" * 70)
        print("PART 9: File Discovery Operations - NEW in v0.1.0!")
        print("=" * 70)
        print("Issue #6 - Implement file discovery operations (list, glob, grep)")

        print("\n49. Setting up test files for discovery...")
        nx_discover = nexus.connect(config={"data_dir": str(data_dir / "discovery-demo")})

        # Create a structured file hierarchy for testing
        test_files = {
            "/src/main.py": b"def main():\n    print('Hello from main')\n    # TODO: Add logging\n",
            "/src/utils/helper.py": b"def helper():\n    return 42\n# TODO: Add tests\n",
            "/src/utils/validator.py": b"def validate(data):\n    # TODO: Add validation logic\n    return True\n",
            "/tests/test_main.py": b"import pytest\ndef test_main():\n    assert True\n",
            "/tests/test_helper.py": b"def test_helper():\n    # TODO: Implement tests\n    pass\n",
            "/data/config.json": b'{"setting": "enabled"}',
            "/data/users.csv": b"name,email\nAlice,alice@example.com\nBob,bob@example.com",
            "/docs/README.md": b"# Project Documentation\n## Overview\nThis is a test project.",
            "/docs/API.md": b"# API Reference\n## Endpoints\n- GET /api/users",
        }

        for path, content in test_files.items():
            nx_discover.write(path, content)

        print(f"   ‚úì Created {len(test_files)} test files")

        # Test list() with recursive and details options
        print("\n50. Testing list() with recursive option...")
        all_files = nx_discover.list("/", recursive=True)
        print(f"   All files (recursive): {len(all_files)}")
        for f in sorted(all_files)[:5]:
            print(f"   - {f}")

        print("\n51. Testing list() non-recursive...")
        root_files = nx_discover.list("/", recursive=False)
        print(f"   Root directory only: {len(root_files)}")
        for f in sorted(root_files):
            print(f"   - {f}")

        src_files = nx_discover.list("/src", recursive=False)
        print(f"\n   /src directory only: {len(src_files)}")
        for f in sorted(src_files):
            print(f"   - {f}")

        print("\n52. Testing list() with details...")
        detailed_files = nx_discover.list("/data", recursive=True, details=True)
        print("   Files in /data with metadata:")
        for file_info in detailed_files:
            print(f"   - {file_info['path']}")
            print(f"     Size: {file_info['size']} bytes")
            print(f"     Modified: {file_info['modified_at']}")
            print(f"     ETag: {file_info['etag'][:16]}...")

        # Test glob() with various patterns
        print("\n53. Testing glob() with simple patterns...")
        py_files = nx_discover.glob("*.py")
        print(f"   Pattern '*.py' (root only): {len(py_files)}")
        for f in sorted(py_files):
            print(f"   - {f}")

        csv_files = nx_discover.glob("*.csv", path="/data")
        print(f"\n   Pattern '*.csv' in /data: {len(csv_files)}")
        for f in sorted(csv_files):
            print(f"   - {f}")

        print("\n54. Testing glob() with recursive patterns...")
        all_py = nx_discover.glob("**/*.py")
        print(f"   Pattern '**/*.py' (all Python files): {len(all_py)}")
        for f in sorted(all_py):
            print(f"   - {f}")

        all_md = nx_discover.glob("**/*.md")
        print(f"\n   Pattern '**/*.md' (all Markdown files): {len(all_md)}")
        for f in sorted(all_md):
            print(f"   - {f}")

        test_files_glob = nx_discover.glob("test_*.py", path="/tests")
        print(f"\n   Pattern 'test_*.py' in /tests: {len(test_files_glob)}")
        for f in sorted(test_files_glob):
            print(f"   - {f}")

        print("\n55. Testing glob() with question mark wildcard...")
        all_files_glob = nx_discover.glob("**/*")
        print(f"   Pattern '**/*' (all files): {len(all_files_glob)}")

        # Test grep() for searching content
        print("\n56. Testing grep() for content search...")
        todo_matches = nx_discover.grep("TODO")
        print(f"   Searching for 'TODO': {len(todo_matches)} matches")
        for match in todo_matches:
            print(f"   - {match['file']}:{match['line']}")
            print(f"     {match['content'].strip()}")

        print("\n57. Testing grep() with regex patterns...")
        function_matches = nx_discover.grep(r"def \w+\(")
        print(f"   Searching for function definitions: {len(function_matches)} matches")
        for match in function_matches[:5]:  # Show first 5
            print(f"   - {match['file']}:{match['line']}")
            print(f"     {match['content'].strip()}")
            print(f"     Match: '{match['match']}'")

        print("\n58. Testing grep() with file pattern filtering...")
        todo_in_py = nx_discover.grep("TODO", file_pattern="**/*.py")
        print(f"   Searching 'TODO' in Python files only: {len(todo_in_py)} matches")
        for match in todo_in_py:
            print(f"   - {match['file']}:{match['line']}")

        print("\n59. Testing grep() case-insensitive search...")
        api_matches_sensitive = nx_discover.grep("api")
        api_matches_insensitive = nx_discover.grep("api", ignore_case=True)
        print(f"   Case-sensitive 'api': {len(api_matches_sensitive)} matches")
        print(f"   Case-insensitive 'api': {len(api_matches_insensitive)} matches")
        for match in api_matches_insensitive:
            print(f"   - {match['file']}:{match['line']}")
            print(f"     {match['content'].strip()}")

        print("\n60. Testing grep() with result limiting...")
        # Create a file with many matches
        repeated_content = "\n".join([f"Line {i} with KEYWORD here" for i in range(50)])
        nx_discover.write("/test/repeated.txt", repeated_content.encode())

        limited_results = nx_discover.grep("KEYWORD", max_results=5)
        print(f"   Limited to 5 results: {len(limited_results)} matches returned")
        for match in limited_results:
            print(f"   - Line {match['line']}: {match['content'][:40]}...")

        print("\n61. Practical example: Finding all test files...")
        # Combine glob and grep for powerful file discovery
        all_test_files = nx_discover.glob("**/test_*.py")
        print(f"   Found {len(all_test_files)} test files:")
        for f in sorted(all_test_files):
            print(f"   - {f}")

        print("\n62. Practical example: Finding unimplemented tests...")
        unimplemented = nx_discover.grep("pass|TODO", file_pattern="**/test_*.py")
        print(f"   Found {len(unimplemented)} potential unimplemented tests:")
        for match in unimplemented:
            print(f"   - {match['file']}:{match['line']}")
            print(f"     {match['content'].strip()}")

        # NEW in v0.2.0 - search_mode parameter
        print("\n62a. Testing grep() with search_mode parameter (v0.2.0)...")
        print("   search_mode='auto': Try parsed text first, fallback to raw (default)")
        auto_matches = nx_discover.grep("TODO", search_mode="auto")
        print(f"   Found {len(auto_matches)} matches with auto mode")
        for match in auto_matches[:2]:
            source = match.get("source", "raw")
            print(f"   - {match['file']}:{match['line']} (source: {source})")

        print("\n   search_mode='raw': Only search raw file content (skip parsing)")
        raw_matches = nx_discover.grep("TODO", search_mode="raw")
        print(f"   Found {len(raw_matches)} matches with raw mode")

        print("\n   search_mode='parsed': Only search parsed text (for PDFs/docs)")
        parsed_matches = nx_discover.grep("TODO", search_mode="parsed")
        print(f"   Found {len(parsed_matches)} matches with parsed mode")
        print("   (Only includes files with parsed content)")

        print("\n63. Summary of file discovery operations:")
        print("   list() enhancements:")
        print("   ‚úì recursive parameter - control depth of listing")
        print("   ‚úì details parameter - get file metadata (size, dates, etag)")
        print("   ‚úì Backward compatible with old prefix parameter")
        print()
        print("   glob() patterns supported:")
        print("   ‚úì * - matches any characters except /")
        print("   ‚úì ** - matches any characters including / (recursive)")
        print("   ‚úì ? - matches single character")
        print("   ‚úì [...] - character classes")
        print()
        print("   grep() capabilities:")
        print("   ‚úì Regex pattern matching in file contents")
        print("   ‚úì File filtering with glob patterns")
        print("   ‚úì Case-insensitive search option")
        print("   ‚úì Result limiting for large result sets")
        print("   ‚úì Automatic binary file detection and skipping")
        print("   ‚úì Returns file path, line number, matched line, and match text")

        nx_discover.close()

        # ============================================================
        # Part 10: Metadata Export/Import (v0.1.0 - NEW!)
        # ============================================================
        print("\n" + "=" * 70)
        print("PART 10: Metadata Export/Import - NEW in v0.1.0!")
        print("=" * 70)
        print("Issue #35 - Advanced metadata export/import with filters and conflict resolution")

        print("\n64. Setting up test data for export/import...")
        export_dir = data_dir / "export-demo"
        # Use is_admin=True since this demo focuses on export/import, not access control
        nx_export = nexus.connect(config={"data_dir": str(export_dir), "is_admin": True})

        # Create test files with various metadata
        import time

        test_export_files = {
            "/workspace/project1/main.py": b"def main():\n    print('Hello World')\n",
            "/workspace/project1/utils.py": b"def helper():\n    return 42\n",
            "/workspace/project2/app.py": b"# Application entry point\n",
            "/shared/models/v1.pkl": b"mock ML model data",
            "/shared/datasets/train.csv": b"col1,col2\n1,2\n3,4\n",
        }

        for path, content in test_export_files.items():
            nx_export.write(path, content)

        # Add custom metadata to some files
        print(f"   ‚úì Created {len(test_export_files)} test files")
        print("\n   Adding custom metadata to files...")
        nx_export.metadata.set_file_metadata("/workspace/project1/main.py", "author", "Alice")
        nx_export.metadata.set_file_metadata("/workspace/project1/main.py", "version", "1.0")
        nx_export.metadata.set_file_metadata("/shared/models/v1.pkl", "model_type", "classifier")
        nx_export.metadata.set_file_metadata("/shared/models/v1.pkl", "accuracy", 0.95)
        print("   ‚úì Added custom metadata to 2 files")

        # Test metadata export
        print("\n65. Exporting all metadata to JSONL file...")
        export_file = export_dir / "metadata-export.jsonl"
        exported_count = nx_export.export_metadata(export_file)
        print(f"   ‚úì Exported {exported_count} file metadata records")
        print(f"   Output: {export_file}")
        print("   ‚úì Output is sorted by path for clean git diffs!")

        # Show sample of exported JSONL
        print("\n66. Sample of exported JSONL content...")
        with open(export_file) as f:
            lines = f.readlines()
            for i, line in enumerate(lines[:2], 1):
                import json

                data = json.loads(line)
                print(f"\n   Record {i}:")
                print(f"     Path: {data['path']}")
                print(f"     Size: {data['size']} bytes")
                print(f"     ETag: {data['etag'][:16]}...")
                if "custom_metadata" in data:
                    print(f"     Custom metadata: {data['custom_metadata']}")

        # Test selective export with ExportFilter
        print("\n67. Testing ExportFilter with path prefix...")
        from nexus.core.export_import import ExportFilter

        workspace_export = export_dir / "workspace-export.jsonl"
        workspace_filter = ExportFilter(path_prefix="/workspace")
        workspace_count = nx_export.export_metadata(workspace_export, filter=workspace_filter)
        print(f"   ‚úì Exported {workspace_count} workspace files")
        print(f"   Output: {workspace_export}")

        # Test export with time filter
        print("\n68. Testing ExportFilter with time filter...")
        cutoff_time = datetime.now(UTC)
        time.sleep(0.1)  # Ensure different timestamp

        # Create new files after cutoff
        nx_export.write("/workspace/recent1.py", b"# Recent file 1")
        nx_export.write("/workspace/recent2.py", b"# Recent file 2")

        recent_export = export_dir / "recent-export.jsonl"
        recent_filter = ExportFilter(after_time=cutoff_time)
        recent_count = nx_export.export_metadata(recent_export, filter=recent_filter)
        print(f"   ‚úì Exported {recent_count} files modified after {cutoff_time.isoformat()}")

        # Test metadata import to a new instance
        print("\n69. Testing basic import to new instance...")
        import_dir = data_dir / "import-demo"
        # Use is_admin=True since this demo focuses on export/import, not access control
        nx_import = nexus.connect(config={"data_dir": str(import_dir), "is_admin": True})

        # Import metadata using new API
        from nexus.core.export_import import ImportOptions

        print("   Importing metadata from export file...")
        result = nx_import.import_metadata(export_file)
        print(f"   ‚úì Created: {result.created}")
        print(f"   ‚úì Updated: {result.updated}")
        print(f"   ‚úì Skipped: {result.skipped}")
        print(f"   ‚úì Total: {result.total_processed}")

        # Verify imported metadata
        print("\n70. Verifying imported metadata...")
        imported_files = nx_import.list()
        print(f"   Total files in new instance: {len(imported_files)}")
        for path in sorted(imported_files)[:3]:
            meta = nx_import.metadata.get(path)
            print(f"\n   {path}")
            print(f"     Size: {meta.size} bytes")
            print(f"     ETag: {meta.etag[:16]}...")

        # Verify custom metadata was imported
        print("\n   Checking custom metadata preservation...")
        author = nx_import.metadata.get_file_metadata("/workspace/project1/main.py", "author")
        version = nx_import.metadata.get_file_metadata("/workspace/project1/main.py", "version")
        print(f"   main.py author: {author}")
        print(f"   main.py version: {version}")
        print("   ‚úì Custom metadata preserved during import!")

        # Test import with conflict resolution modes
        print("\n71. Testing conflict resolution modes...")

        # Create conflict by modifying a file
        nx_import.write("/workspace/project1/main.py", b"# Modified content")
        print("   Modified /workspace/project1/main.py in import instance")

        # Test skip mode (default)
        print("\n   Testing 'skip' conflict mode...")
        skip_options = ImportOptions(conflict_mode="skip")
        skip_result = nx_import.import_metadata(export_file, options=skip_options)
        print(
            f"   Created: {skip_result.created}, Updated: {skip_result.updated}, Skipped: {skip_result.skipped}"
        )
        print(f"   Collisions detected: {len(skip_result.collisions)}")
        if skip_result.collisions:
            for collision in skip_result.collisions[:2]:
                print(f"   - {collision.path}: {collision.resolution}")

        # Test overwrite mode
        print("\n   Testing 'overwrite' conflict mode...")
        overwrite_options = ImportOptions(conflict_mode="overwrite")
        overwrite_result = nx_import.import_metadata(export_file, options=overwrite_options)
        print(
            f"   Created: {overwrite_result.created}, Updated: {overwrite_result.updated}, Skipped: {overwrite_result.skipped}"
        )
        print(f"   ‚úì Overwrote {overwrite_result.updated} files")

        # Test remap mode
        print("\n72. Testing 'remap' conflict mode...")
        remap_dir = data_dir / "remap-demo"
        nx_remap = nexus.connect(config={"data_dir": str(remap_dir)})

        # Create initial file
        nx_remap.write("/test.txt", b"original content")

        # Import with different content for same path
        import json

        conflict_data = {
            "path": "/test.txt",
            "backend_name": "local",
            "physical_path": "differenthash",
            "size": 20,
            "etag": "a" * 64,
            "created_at": datetime.now(UTC).isoformat(),
            "modified_at": datetime.now(UTC).isoformat(),
            "version": 1,
        }

        remap_export = remap_dir / "conflict-export.jsonl"
        with open(remap_export, "w") as f:
            f.write(json.dumps(conflict_data) + "\n")

        remap_options = ImportOptions(conflict_mode="remap")
        remap_result = nx_remap.import_metadata(remap_export, options=remap_options)
        print(f"   Remapped: {remap_result.remapped} files")
        if remap_result.collisions:
            collision = remap_result.collisions[0]
            print(f"   {collision.path} ‚Üí {collision.message}")

        # Verify remapped file exists
        all_files = nx_remap.list()
        print(f"   Files in instance: {all_files}")
        print("   ‚úì Original and remapped files both exist!")

        nx_remap.close()

        # Test auto mode
        print("\n73. Testing 'auto' conflict mode (newer wins)...")
        auto_dir = data_dir / "auto-demo"
        nx_auto = nexus.connect(config={"data_dir": str(auto_dir)})

        # Create old file
        nx_auto.write("/test.txt", b"old content")
        time.sleep(0.1)

        # Create export with newer timestamp
        future_data = {
            "path": "/test.txt",
            "backend_name": "local",
            "physical_path": "newhash",
            "size": 15,
            "etag": "b" * 64,
            "created_at": (datetime.now(UTC) + timedelta(hours=1)).isoformat(),
            "modified_at": (datetime.now(UTC) + timedelta(hours=1)).isoformat(),
            "version": 1,
        }

        auto_export = auto_dir / "newer-export.jsonl"
        with open(auto_export, "w") as f:
            f.write(json.dumps(future_data) + "\n")

        auto_options = ImportOptions(conflict_mode="auto")
        auto_result = nx_auto.import_metadata(auto_export, options=auto_options)
        print(f"   Updated: {auto_result.updated} (imported is newer)")
        if auto_result.collisions:
            collision = auto_result.collisions[0]
            print(f"   Resolution: {collision.resolution}")
            print(f"   Message: {collision.message}")

        nx_auto.close()

        # Test dry-run mode
        print("\n74. Testing dry-run mode (no changes)...")
        dry_run_dir = data_dir / "dry-run-demo"
        nx_dry = nexus.connect(config={"data_dir": str(dry_run_dir)})

        dry_run_options = ImportOptions(dry_run=True)
        dry_result = nx_dry.import_metadata(export_file, options=dry_run_options)
        print(f"   Would create: {dry_result.created} files")
        print(f"   Would update: {dry_result.updated} files")
        print(f"   Would skip: {dry_result.skipped} files")

        # Verify no files were actually created
        actual_files = nx_dry.list()
        print(f"   Actual files created: {len(actual_files)}")
        print("   ‚úì Dry-run did not modify database!")

        nx_dry.close()
        nx_export.close()
        nx_import.close()

        print("\n75. Summary of metadata export/import:")
        print("   Export capabilities (Issue #35):")
        print("   ‚úì Export all metadata to JSONL format")
        print("   ‚úì ExportFilter with path_prefix filtering")
        print("   ‚úì ExportFilter with after_time (time-based) filtering")
        print("   ‚úì Sorted output for clean git diffs")
        print("   ‚úì Includes file metadata (path, size, timestamps, etag)")
        print("   ‚úì Includes custom key-value metadata")
        print("   ‚úì Human-readable JSON format (one file per line)")
        print()
        print("   Import capabilities (Issue #35):")
        print("   ‚úì Import metadata from JSONL file")
        print("   ‚úì ImportOptions with 4 conflict resolution modes:")
        print("     - skip: Keep existing (default)")
        print("     - overwrite: Replace with imported")
        print("     - remap: Rename imported to avoid collision")
        print("     - auto: Smart resolution (newer wins)")
        print("   ‚úì Dry-run mode (simulate without changes)")
        print("   ‚úì Collision detection and tracking (CollisionDetail)")
        print("   ‚úì Returns ImportResult with detailed counts")
        print("   ‚úì Restore custom metadata")
        print("   ‚úì Validate required fields during import")
        print("   ‚úì Backward compatible with old API")
        print()
        print("   Use cases:")
        print("   ‚Ä¢ Git-friendly metadata backups (sorted output)")
        print("   ‚Ä¢ Zero-downtime migrations between instances")
        print("   ‚Ä¢ Disaster recovery with conflict resolution")
        print("   ‚Ä¢ Audit and inspect file metadata externally")
        print("   ‚Ä¢ Time-based incremental backups (after_time filter)")
        print("   ‚Ä¢ Safe trial runs with dry-run mode")

        # ============================================================
        # Part 11: Work Detection with SQL Views (v0.1.0 - NEW!)
        # ============================================================
        print("\n" + "=" * 70)
        print("PART 11: Work Detection with SQL Views - NEW in v0.1.0!")
        print("=" * 70)
        print("Issue #69 - SQL views for efficient work detection")

        print("\n72. Setting up work queue demonstration...")
        work_dir = data_dir / "work-demo"
        nx_work = nexus.connect(config={"data_dir": str(work_dir)})

        # Create work items (files representing tasks/jobs)
        print("   Creating work items as files...")
        work_items = [
            ("/jobs/task1.json", b'{"task": "process_data", "dataset": "users"}'),
            ("/jobs/task2.json", b'{"task": "train_model", "model": "classifier"}'),
            ("/jobs/task3.json", b'{"task": "generate_report", "format": "pdf"}'),
            ("/jobs/task4.json", b'{"task": "analyze_results", "experiment": "exp_001"}'),
            ("/jobs/task5.json", b'{"task": "cleanup_temp", "path": "/tmp"}'),
        ]

        for path, content in work_items:
            nx_work.write(path, content)

        print(f"   ‚úì Created {len(work_items)} work items")

        # Set work metadata using the metadata store
        print("\n73. Setting work status and priority metadata...")

        # Task 1: Ready to process (high priority)
        nx_work.metadata.set_file_metadata("/jobs/task1.json", "status", "ready")
        nx_work.metadata.set_file_metadata("/jobs/task1.json", "priority", 1)
        nx_work.metadata.set_file_metadata("/jobs/task1.json", "tags", ["urgent", "data"])
        print("   task1: status=ready, priority=1 (high)")

        # Task 2: In progress
        nx_work.metadata.set_file_metadata("/jobs/task2.json", "status", "in_progress")
        nx_work.metadata.set_file_metadata("/jobs/task2.json", "priority", 2)
        nx_work.metadata.set_file_metadata("/jobs/task2.json", "worker_id", "worker-001")
        nx_work.metadata.set_file_metadata(
            "/jobs/task2.json", "started_at", datetime.now(UTC).isoformat()
        )
        print("   task2: status=in_progress, worker=worker-001")

        # Task 3: Pending (waiting to start)
        nx_work.metadata.set_file_metadata("/jobs/task3.json", "status", "pending")
        nx_work.metadata.set_file_metadata("/jobs/task3.json", "priority", 3)
        print("   task3: status=pending, priority=3")

        # Task 4: Blocked (depends on task 2)
        # Get path_id for task2 (the blocker)
        task2_path_id = nx_work.metadata.get_path_id("/jobs/task2.json")
        nx_work.metadata.set_file_metadata("/jobs/task4.json", "status", "blocked")
        nx_work.metadata.set_file_metadata("/jobs/task4.json", "priority", 2)
        nx_work.metadata.set_file_metadata("/jobs/task4.json", "depends_on", task2_path_id)
        print(
            f"   task4: status=blocked, depends_on={task2_path_id[:8]}..."
            if task2_path_id
            else "   task4: status=blocked"
        )

        # Task 5: Ready (low priority)
        nx_work.metadata.set_file_metadata("/jobs/task5.json", "status", "ready")
        nx_work.metadata.set_file_metadata("/jobs/task5.json", "priority", 5)
        nx_work.metadata.set_file_metadata("/jobs/task5.json", "tags", ["cleanup"])
        print("   task5: status=ready, priority=5 (low)")

        # Query ready work using SQL views
        print("\n74. Querying ready work items (no blockers)...")
        ready_work = nx_work.metadata.get_ready_work(limit=10)
        print(f"   Found {len(ready_work)} ready work items:")
        for item in ready_work:
            print(f"   - {item['virtual_path']}")
            print(f"     Priority: {item['priority']}, Status: {item['status']}")

        # Query pending work
        print("\n75. Querying pending work items...")
        pending_work = nx_work.metadata.get_pending_work()
        print(f"   Found {len(pending_work)} pending work items:")
        for item in pending_work:
            print(f"   - {item['virtual_path']}")

        # Query blocked work
        print("\n76. Querying blocked work items...")
        blocked_work = nx_work.metadata.get_blocked_work()
        print(f"   Found {len(blocked_work)} blocked work items:")
        for item in blocked_work:
            print(f"   - {item['virtual_path']}")
            print(f"     Blocker count: {item['blocker_count']}")

        # Query in-progress work
        print("\n77. Querying in-progress work items...")
        in_progress_work = nx_work.metadata.get_in_progress_work()
        print(f"   Found {len(in_progress_work)} in-progress work items:")
        for item in in_progress_work:
            print(f"   - {item['virtual_path']}")
            if item.get("worker_id"):
                import json

                try:
                    worker_id = json.loads(item["worker_id"])
                    print(f"     Worker: {worker_id}")
                except (json.JSONDecodeError, TypeError):
                    print(f"     Worker: {item['worker_id']}")

        # Query all work by priority
        print("\n78. Querying all work items sorted by priority...")
        all_work = nx_work.metadata.get_work_by_priority(limit=10)
        print(f"   Found {len(all_work)} work items (top 10 by priority):")
        for item in all_work:
            import json

            status = "N/A"
            priority = "N/A"
            if item.get("status"):
                try:
                    status = json.loads(item["status"])
                except (json.JSONDecodeError, TypeError):
                    status = str(item["status"])
            if item.get("priority"):
                try:
                    priority = json.loads(item["priority"])
                except (json.JSONDecodeError, TypeError):
                    priority = str(item["priority"])
            print(f"   - {item['virtual_path']}: status={status}, priority={priority}")

        # Simulate work processing
        print("\n79. Simulating work processing...")
        print("   Picking highest priority ready work item...")
        if ready_work:
            work_item = ready_work[0]
            work_path = work_item["virtual_path"]
            print(f"   Selected: {work_path}")

            # Mark as in_progress
            print(f"\n   Marking {work_path} as in_progress...")
            nx_work.metadata.set_file_metadata(work_path, "status", "in_progress")
            nx_work.metadata.set_file_metadata(work_path, "worker_id", "worker-002")
            nx_work.metadata.set_file_metadata(
                work_path, "started_at", datetime.now(UTC).isoformat()
            )

            # Check ready work again
            ready_after = nx_work.metadata.get_ready_work()
            print(f"   Ready work items after starting: {len(ready_after)}")

            # Simulate completion
            print(f"\n   Completing {work_path}...")
            nx_work.metadata.set_file_metadata(work_path, "status", "completed")
            nx_work.metadata.set_file_metadata(
                work_path, "completed_at", datetime.now(UTC).isoformat()
            )

            # Check if any blocked work became ready
            print("\n   Checking if any blocked work became unblocked...")
            ready_final = nx_work.metadata.get_ready_work()
            blocked_final = nx_work.metadata.get_blocked_work()
            print(f"   Ready work: {len(ready_final)} items")
            print(f"   Blocked work: {len(blocked_final)} items")

        nx_work.close()

        # ============================================================
        # Part 12: Type-Level Validation (v0.1.0 - NEW!)
        # ============================================================
        print("\n" + "=" * 70)
        print("PART 12: Type-Level Validation - NEW in v0.1.0!")
        print("=" * 70)
        print("Issue #37 - Type-level validation for all domain types")

        print("\n81. Testing type-level validation on FileMetadata...")
        from nexus.core.exceptions import ValidationError
        from nexus.core.metadata import FileMetadata

        # Test invalid path (doesn't start with /)
        print("   Testing invalid path (must start with /)...")
        try:
            invalid_meta = FileMetadata(
                path="data/file.txt",  # Invalid: doesn't start with /
                backend_name="local",
                physical_path="/storage/file.txt",
                size=1024,
            )
            invalid_meta.validate()
            print("   ‚úó Should have raised ValidationError!")
        except ValidationError as e:
            print(f"   ‚úì Correctly rejected: {e}")

        # Test negative size
        print("\n   Testing negative size...")
        try:
            invalid_meta = FileMetadata(
                path="/data/file.txt",
                backend_name="local",
                physical_path="/storage/file.txt",
                size=-100,  # Invalid: negative size
            )
            invalid_meta.validate()
            print("   ‚úó Should have raised ValidationError!")
        except ValidationError as e:
            print(f"   ‚úì Correctly rejected: {e}")

        # Test path with null bytes
        print("\n   Testing path with null bytes...")
        try:
            invalid_meta = FileMetadata(
                path="/data/file\x00.txt",  # Invalid: contains null byte
                backend_name="local",
                physical_path="/storage/file.txt",
                size=1024,
            )
            invalid_meta.validate()
            print("   ‚úó Should have raised ValidationError!")
        except ValidationError as e:
            print(f"   ‚úì Correctly rejected: {e}")

        # Test missing required fields
        print("\n   Testing missing backend_name...")
        try:
            invalid_meta = FileMetadata(
                path="/data/file.txt",
                backend_name="",  # Invalid: empty backend_name
                physical_path="/storage/file.txt",
                size=1024,
            )
            invalid_meta.validate()
            print("   ‚úó Should have raised ValidationError!")
        except ValidationError as e:
            print(f"   ‚úì Correctly rejected: {e}")

        print("\n82. Testing validation in metadata store operations...")
        validation_dir = data_dir / "validation-demo"
        nx_validation = nexus.connect(config={"data_dir": str(validation_dir)})

        # Valid metadata - should succeed
        print("   Creating valid file metadata...")
        try:
            nx_validation.write("/test/valid.txt", b"Valid content")
            print("   ‚úì Valid file created successfully")
        except ValidationError as e:
            print(f"   ‚úó Unexpected validation error: {e}")

        # Test that validation happens at metadata store level
        print("\n   Testing validation at metadata store level...")
        from nexus.core.metadata import FileMetadata

        invalid_metadata = FileMetadata(
            path="invalid-path",  # Doesn't start with /
            backend_name="local",
            physical_path="/storage/file.txt",
            size=100,
        )

        try:
            nx_validation.metadata.put(invalid_metadata)
            print("   ‚úó Should have raised ValidationError!")
        except ValidationError as e:
            print(f"   ‚úì Validation enforced at store level: {e}")

        nx_validation.close()

        print("\n83. Testing SQLAlchemy model validation...")
        from nexus.storage.models import ContentChunkModel, FileMetadataModel, FilePathModel

        # Test FilePathModel validation
        print("   Testing FilePathModel validation...")
        try:
            invalid_file_path = FilePathModel(
                virtual_path="no-leading-slash",  # Invalid
                backend_id="local",
                physical_path="/storage/file.txt",
                size_bytes=100,
                tenant_id="test-tenant",
            )
            invalid_file_path.validate()
            print("   ‚úó Should have raised ValidationError!")
        except ValidationError as e:
            print(f"   ‚úì FilePathModel validation works: {e}")

        # Test FileMetadataModel validation
        print("\n   Testing FileMetadataModel validation...")
        try:
            invalid_metadata = FileMetadataModel(
                path_id="test-id",
                key="a" * 300,  # Invalid: too long (> 255)
                value='{"test": "value"}',
                created_at=datetime.now(UTC),
            )
            invalid_metadata.validate()
            print("   ‚úó Should have raised ValidationError!")
        except ValidationError as e:
            print(f"   ‚úì FileMetadataModel validation works: {e}")

        # Test ContentChunkModel validation
        print("\n   Testing ContentChunkModel validation...")
        try:
            invalid_chunk = ContentChunkModel(
                content_hash="tooshort",  # Invalid: must be 64 chars
                size_bytes=1024,
                storage_path="/storage/chunk",
                ref_count=1,
            )
            invalid_chunk.validate()
            print("   ‚úó Should have raised ValidationError!")
        except ValidationError as e:
            print(f"   ‚úì ContentChunkModel validation works: {e}")

        # Test negative ref_count
        print("\n   Testing negative ref_count...")
        try:
            invalid_chunk = ContentChunkModel(
                content_hash="a" * 64,  # Valid hash
                size_bytes=1024,
                storage_path="/storage/chunk",
                ref_count=-1,  # Invalid: negative
            )
            invalid_chunk.validate()
            print("   ‚úó Should have raised ValidationError!")
        except ValidationError as e:
            print(f"   ‚úì Correctly rejected negative ref_count: {e}")

        print("\n84. Summary of type-level validation:")
        print("   Validation features:")
        print("   ‚úì Automatic validation before database operations")
        print("   ‚úì Clear, actionable error messages")
        print("   ‚úì Fail fast (before expensive DB operations)")
        print("   ‚úì Consistent validation across all code paths")
        print()
        print("   Domain types with validation:")
        print("   ‚Ä¢ FileMetadata (path, size, backend_name, etc.)")
        print("   ‚Ä¢ FilePathModel (virtual_path, size_bytes, etc.)")
        print("   ‚Ä¢ FileMetadataModel (key length, path_id)")
        print("   ‚Ä¢ ContentChunkModel (content_hash, ref_count, size)")
        print()
        print("   Validation rules:")
        print("   ‚Ä¢ Paths must start with '/' and not contain null bytes")
        print("   ‚Ä¢ Sizes and counts must be non-negative")
        print("   ‚Ä¢ Required fields must not be empty")
        print("   ‚Ä¢ Content hashes must be valid 64-char SHA-256")
        print("   ‚Ä¢ Metadata keys must be ‚â§ 255 characters")
        print()
        print("   Benefits:")
        print("   ‚Ä¢ Prevents invalid data in database")
        print("   ‚Ä¢ Improves developer experience with clear errors")
        print("   ‚Ä¢ Reduces debugging time (fail fast)")
        print("   ‚Ä¢ Enables better API error responses (400 Bad Request)")

        # ============================================================
        # Part 13: Batch Get Content IDs for CAS Deduplication (v0.1.0 - Issue #34)
        # ============================================================
        print("\n" + "=" * 70)
        print("PART 13: Batch Get Content IDs - CAS Deduplication")
        print("=" * 70)
        print("Issue #34 - Batch operations to avoid N+1 queries")

        print("\n85. Creating files with duplicate content for deduplication demo...")
        dedup_dir = data_dir / "dedup-demo"
        nx_dedup = nexus.connect(config={"data_dir": str(dedup_dir)})

        # Create files with some duplicates
        files_to_create = [
            ("/docs/report1.txt", b"This is a unique report"),
            ("/docs/report2.txt", b"Another unique document"),
            ("/docs/report3.txt", b"This is a unique report"),  # Duplicate of report1
            ("/docs/report4.txt", b"Fourth report content"),
            ("/docs/report5.txt", b"Another unique document"),  # Duplicate of report2
            ("/docs/backup/report1.txt", b"This is a unique report"),  # Duplicate of report1 & 3
            ("/docs/backup/report4.txt", b"Fourth report content"),  # Duplicate of report4
        ]

        for path, content in files_to_create:
            nx_dedup.write(path, content)

        print(f"   Created {len(files_to_create)} files")

        print("\n86. Using batch_get_content_ids() to detect duplicates...")
        paths = [f[0] for f in files_to_create]

        # Single query to get all content hashes
        # Now available at top level for convenience!
        content_ids = nx_dedup.batch_get_content_ids(paths)

        # Also available at metadata level: nx_dedup.metadata.batch_get_content_ids(paths)

        print(f"   Retrieved {len(content_ids)} content hashes in single query")
        print("   ‚úì Avoided N+1 query problem (1 query vs 7 queries)")

        # Find duplicates
        from collections import defaultdict

        by_hash = defaultdict(list)
        for path, content_hash in content_ids.items():
            if content_hash:
                by_hash[content_hash].append(path)

        duplicates = {h: paths for h, paths in by_hash.items() if len(paths) > 1}

        print("\n87. Deduplication analysis:")
        print(f"   Total files: {len(files_to_create)}")
        print(f"   Unique content hashes: {len(by_hash)}")
        print(f"   Duplicate groups: {len(duplicates)}")

        print("\n   Duplicate content detected:")
        for i, (content_hash, duplicate_paths) in enumerate(duplicates.items(), 1):
            print(f"\n   Group {i} (hash: {content_hash[:16]}...):")
            for path in duplicate_paths:
                print(f"     ‚Ä¢ {path}")

        # Calculate space savings
        total_bytes = sum(len(f[1]) for f in files_to_create)
        unique_bytes = sum(
            len(files_to_create[paths.index(paths_list[0])][1]) for paths_list in by_hash.values()
        )
        saved_bytes = total_bytes - unique_bytes

        print("\n88. Storage optimization (CAS deduplication):")
        print(f"   Total bytes (without dedup): {total_bytes}")
        print(f"   Unique bytes (with dedup): {unique_bytes}")
        print(f"   Space saved: {saved_bytes} bytes ({saved_bytes / total_bytes * 100:.1f}%)")

        print("\n89. Performance comparison - batch vs individual queries:")
        print("   Without batch_get_content_ids() (N+1 problem):")
        print(f"     ‚Ä¢ {len(paths)} files √ó 2ms/query = ~{len(paths) * 2}ms")
        print("   With batch_get_content_ids():")
        print("     ‚Ä¢ 1 query = ~2ms")
        print(f"   ‚úì Performance improvement: ~{len(paths)}√ó faster!")

        print("\n90. Summary of batch_get_content_ids():")
        print("   Features:")
        print("   ‚úì Single SQL query with IN clause (not N queries)")
        print("   ‚úì Returns dict mapping path ‚Üí content_hash")
        print("   ‚úì Efficient for CAS deduplication scenarios")
        print("   ‚úì Only fetches content_hash field (not full metadata)")
        print()
        print("   Use cases:")
        print("   ‚Ä¢ Content-addressable storage (CAS) systems")
        print("   ‚Ä¢ Finding duplicate files for deduplication")
        print("   ‚Ä¢ Efficient backup systems (only backup unique content)")
        print("   ‚Ä¢ Data lake optimization (avoid storing duplicate datasets)")
        print("   ‚Ä¢ Media asset management (detect duplicate images/videos)")

        nx_dedup.close()

        print("\n80. Summary of work detection SQL views:")
        print("   SQL Views created:")
        print("   ‚Ä¢ ready_work_items    - Files ready for processing (no blockers)")
        print("   ‚Ä¢ pending_work_items  - Files waiting to start")
        print("   ‚Ä¢ blocked_work_items  - Files blocked by dependencies")
        print("   ‚Ä¢ in_progress_work    - Files currently being processed")
        print("   ‚Ä¢ work_by_priority    - All work items ordered by priority")
        print()
        print("   Python API methods:")
        print("   ‚Ä¢ get_ready_work()       - Get ready items (sorted by priority)")
        print("   ‚Ä¢ get_pending_work()     - Get pending items")
        print("   ‚Ä¢ get_blocked_work()     - Get blocked items with blocker count")
        print("   ‚Ä¢ get_in_progress_work() - Get active work items")
        print("   ‚Ä¢ get_work_by_priority() - Get all work sorted by priority")
        print()
        print("   Performance:")
        print("   ‚úì O(n) query performance using indexed SQL views")
        print("   ‚úì Efficient dependency checking with EXISTS subqueries")
        print("   ‚úì < 100ms query time for 10,000+ work items")
        print()
        print("   Use cases:")
        print("   ‚Ä¢ Work queue systems (distributed task processing)")
        print("   ‚Ä¢ Dependency resolution (DAG execution)")
        print("   ‚Ä¢ Priority-based scheduling (high-priority first)")
        print("   ‚Ä¢ Monitoring dashboards (real-time work status)")
        print("   ‚Ä¢ Worker assignment (load balancing)")

        # ============================================================
        # Summary
        # ============================================================
        print("\n" + "=" * 70)
        print("SUMMARY: How It Works")
        print("=" * 70)
        print(
            """
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                   USER APPLICATION                      ‚îÇ
‚îÇ                 (your Python code)                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ
                     ‚îÇ import nexus
                     ‚îÇ nx = nexus.connect()  ‚Üê THE RIGHT WAY
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              nexus.connect()                            ‚îÇ
‚îÇ              (auto-detects mode)                        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ Returns Embedded instance
                     ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Embedded Filesystem Class                  ‚îÇ
‚îÇ              (nexus.core.embedded)                      ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ  ‚Ä¢ Path validation                                      ‚îÇ
‚îÇ  ‚Ä¢ ETag computation                                     ‚îÇ
‚îÇ  ‚Ä¢ Automatic metadata tracking                          ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
      ‚îÇ                                  ‚îÇ
      ‚îÇ Store metadata                   ‚îÇ Read/write data
      ‚ñº                                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ SQLAlchemy Metadata  ‚îÇ      ‚îÇ   Storage Backend        ‚îÇ
‚îÇ      Store           ‚îÇ      ‚îÇ   (LocalBackend)         ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§      ‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ ‚Ä¢ FilePathModel      ‚îÇ      ‚îÇ ‚Ä¢ Physical file I/O      ‚îÇ
‚îÇ ‚Ä¢ FileMetadataModel  ‚îÇ      ‚îÇ ‚Ä¢ Local filesystem       ‚îÇ
‚îÇ ‚Ä¢ ContentChunkModel  ‚îÇ      ‚îÇ   operations             ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚îÇ
       ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   SQLite Database    ‚îÇ
‚îÇ   (metadata.db)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

Key Points:
‚Ä¢ Use nexus.connect() - it auto-detects mode
‚Ä¢ Embedded API provides simple file operations
‚Ä¢ Metadata Store tracks all file information
‚Ä¢ Custom metadata can be added at low level
‚Ä¢ Both views access the same SQLite database
‚Ä¢ Changes are immediately persisted

NEW in v0.1.0:
‚Ä¢ Path Router maps virtual paths to backends
‚Ä¢ Directory operations (mkdir, rmdir, is_directory)
‚Ä¢ Multi-mount support (different paths ‚Üí different backends)
‚Ä¢ Backend-agnostic interface (LocalFS today, S3/GDrive future)
‚Ä¢ Longest-prefix matching for mount points
‚Ä¢ Namespace & Tenant Isolation (workspace, shared, external, system, archives)
‚Ä¢ Path parsing & validation (security checks)
‚Ä¢ Access control (tenant isolation, admin-only, read-only namespaces)
        """
        )

        print("\nüìä Feature Summary:")
        print("   ‚úì File operations (read/write/delete)")
        print("   ‚úì Metadata tracking (SQLite)")
        print("   ‚úì Custom metadata (key-value)")
        print("   ‚úì Directory operations (mkdir/rmdir/is_directory)")
        print("   ‚úì Path routing (virtual ‚Üí physical)")
        print("   ‚úì Multi-mount support (multiple backends)")
        print("   ‚úì Namespace & tenant isolation (workspace/shared/external/system/archives)")
        print("   ‚úì Path validation & security (null bytes, control chars, path traversal)")
        print("   ‚úì Access control (tenant isolation, admin-only, read-only)")
        print("   ‚úì Persistence (survives restarts)")
        print("   ‚úì Data integrity (ETags)")
        print("   ‚úì Content-addressable storage (CAS)")
        print("   ‚úì Content deduplication (save space)")
        print("   ‚úì Reference counting (safe deletion)")
        print("   ‚úì Atomic writes (data integrity)")
        print("   ‚úì SHA-256 content hashing")
        print("   ‚úì File discovery operations (list/glob/grep) - NEW in v0.1.0!")
        print("     - list() with recursive and details options")
        print("     - glob() with ** recursive patterns")
        print("     - grep() with regex and file filtering")
        print("   ‚úì Metadata export/import (JSONL format) - NEW in v0.1.0!")
        print("     - Export metadata to JSONL for backup/migration")
        print("     - Import metadata from JSONL with validation")
        print("     - Selective export with path prefix filtering")
        print("     - Preserve custom metadata during export/import")
        print()
        print("üìÅ Files created:")
        workspace_backend_files = list((data_dir / "workspace-isolated").rglob("*"))
        main_backend_files = list((data_dir / "files").rglob("*"))
        print(f"   Main backend: {len([f for f in main_backend_files if f.is_file()])} files")
        print(
            f"   Workspace backend: {len([f for f in workspace_backend_files if f.is_file()])} files"
        )
        print()
        print("üéØ Key Capabilities:")
        print("   ‚Ä¢ Unified API across different storage backends")
        print("   ‚Ä¢ Automatic directory creation on write")
        print("   ‚Ä¢ Mount-based path routing with priority")
        print("   ‚Ä¢ Cache-friendly design (path resolution)")
        print()
        print("üîÆ Future Backends (same API!):")
        print("   ‚Ä¢ S3: Flat key-value (path ‚Üí key)")
        print("   ‚Ä¢ Google Drive: ID-based (path ‚Üí file ID with caching)")
        print("   ‚Ä¢ SharePoint: Site/Library structure")
        print()
        print("Example multi-backend config:")
        print("   /workspace ‚Üí LocalFS (fast, local)")
        print("   /shared ‚Üí S3 (scalable, remote)")
        print("   /external/gdrive ‚Üí Google Drive (collaborative)")
        print()
        print("All using the same nx.write(path, content) API!")
        print()
        print("=" * 70)

        # ============================================================
        # Part 14: Automatic Document Parsing (v0.2.0 - NEW!)
        # ============================================================
        print("\n" + "=" * 70)
        print("PART 14: Automatic Document Parsing - NEW in v0.2.0!")
        print("=" * 70)
        print("Issue #17 - Transparent document parsing with high-level primitives")

        print("\n91. Setting up files for auto-parsing demo...")
        parse_dir = data_dir / "parse-demo"
        nx_parse = nexus.connect(config={"data_dir": str(parse_dir), "auto_parse": True})

        print("   ‚úì Connected with auto_parse=True (default)")
        print("   Parser automatically extracts text from 23+ file formats")

        # Upload a PDF if available
        print("\n92. Uploading PDF file (auto-parsing in background)...")
        pdf_file = Path("examples/sample-local-pdf.pdf")
        if pdf_file.exists():
            with open(pdf_file, "rb") as f:
                pdf_content = f.read()

            # Just write - parsing happens automatically!
            nx_parse.write("/documents/sample.pdf", pdf_content)
            print(f"   ‚úì Uploaded PDF ({len(pdf_content)} bytes)")
            print("   ‚Üí Parsing triggered automatically in background thread")
        else:
            print("   ‚ö† PDF file not found, using markdown instead")

        # Upload markdown
        print("\n93. Uploading Markdown file (also auto-parsed)...")
        markdown_content = b"""# Project Report

## Executive Summary
This report demonstrates **automatic document parsing** in Nexus.

## Key Features
- Transparent parsing on write()
- grep() searches parsed text automatically
- Supports PDF, Office, Markdown, JSON, CSV, and 20+ formats

## Implementation Details
The parser system uses MarkItDown for initial parsing, with an
extensible architecture for adding custom parsers.

## Performance Metrics
- Parse time: < 2 seconds for typical documents
- TODO: Add benchmarking results
- ERROR handling is robust with graceful fallbacks

## Conclusion
Auto-parsing makes document search seamless and transparent.
"""

        nx_parse.write("/docs/report.md", markdown_content)
        print("   ‚úì Uploaded Markdown")
        print("   ‚Üí Parsing triggered automatically")

        # Upload JSON
        nx_parse.write(
            "/data/config.json", b'{"project": "nexus", "version": "0.2.0", "auto_parse": true}'
        )
        print("   ‚úì Uploaded JSON")

        # Wait for background parsing to complete
        print("\n94. Waiting for background parsing to complete...")
        import time

        time.sleep(3)  # Give parsers time to finish
        print("   ‚úì Parsing complete")

        # Check what was parsed
        print("\n95. Checking parsed files...")
        all_files = nx_parse.list()
        for file_path in all_files:
            parsed_text = nx_parse.metadata.get_file_metadata(file_path, "parsed_text")
            if parsed_text:
                parser_name = nx_parse.metadata.get_file_metadata(file_path, "parser_name")
                print(f"   ‚úì {file_path}")
                print(f"     Parser: {parser_name}")
                print(f"     Extracted: {len(parsed_text)} characters")
            else:
                print(f"   - {file_path} (not parsed)")

        # Demonstrate grep searching parsed content
        print("\n96. Using grep() - automatically searches parsed text!")
        print("   Searching for 'TODO' across all files...")
        todo_matches = nx_parse.grep("TODO")
        print(f"   Found {len(todo_matches)} matches:")
        for match in todo_matches:
            print(f"   - {match['file']}:{match['line']}")
            print(f"     {match['content'].strip()}")

        print("\n97. Case-insensitive search for 'ERROR'...")
        error_matches = nx_parse.grep("ERROR", ignore_case=True)
        print(f"   Found {len(error_matches)} matches:")
        for match in error_matches:
            print(f"   - {match['file']}:{match['line']}")

        # Search in PDF (if uploaded)
        if pdf_file.exists():
            print("\n98. Searching PDF content (binary file!)...")
            pdf_matches = nx_parse.grep("PDF", file_pattern="**/*.pdf")
            print(f"   Found {len(pdf_matches)} matches for 'PDF':")
            for match in pdf_matches[:3]:  # Show first 3
                print(f"   - {match['file']}:{match['line']}")
                print(f"     {match['content'][:60]}...")
            print("   ‚úì Searched extracted text, not binary data!")

        # Show that you can disable auto_parse
        print("\n99. Auto-parsing can be disabled if needed...")
        nx_no_parse = nexus.connect(
            config={"data_dir": str(parse_dir / "no-parse"), "auto_parse": False}
        )
        nx_no_parse.write("/test.txt", b"This won't be auto-parsed")
        parsed = nx_no_parse.metadata.get_file_metadata("/test.txt", "parsed_text")
        print(f"   auto_parse=False: parsed_text = {parsed}")
        nx_no_parse.close()

        # Show supported formats
        print("\n100. Checking supported file formats...")
        supported = nx_parse.parser_registry.get_supported_formats()
        print(f"   Total formats supported: {len(supported)}")
        print(f"   Formats: {', '.join(supported[:15])}...")

        print("\n101. Summary of automatic document parsing:")
        print("   Features:")
        print("   ‚úì Auto-parse on write() (default behavior)")
        print("   ‚úì grep() searches parsed text automatically")
        print("   ‚úì Transparent - no explicit parse() calls needed")
        print("   ‚úì Background processing (non-blocking)")
        print("   ‚úì 23+ file format support (PDF, Office, Markdown, etc.)")
        print("   ‚úì Extensible parser architecture")
        print()
        print("   Supported formats:")
        print("   ‚Ä¢ Documents: PDF, DOCX, DOC, PPTX, PPT")
        print("   ‚Ä¢ Spreadsheets: XLSX, XLS, CSV")
        print("   ‚Ä¢ Text: TXT, MD, Markdown")
        print("   ‚Ä¢ Data: JSON, XML")
        print("   ‚Ä¢ Images: PNG, JPG, GIF, BMP (with OCR)")
        print("   ‚Ä¢ Archives: EPUB, ZIP")
        print()
        print("   High-level primitives that use parsed content:")
        print("   ‚Ä¢ grep() - searches parsed text when available")
        print("   ‚Ä¢ (future) read() with parse flag")
        print("   ‚Ä¢ (future) vector embedding generation")
        print()
        print("   Configuration:")
        print("   ‚Ä¢ auto_parse=True (default) - parse on write")
        print("   ‚Ä¢ auto_parse=False - disable auto-parsing")
        print("   ‚Ä¢ Can also call parse() explicitly when needed")

        # ============================================================
        # PART 10: Advanced Parser Features (New in v0.2.0)
        # ============================================================
        print("\n" + "=" * 70)
        print("PART 10: Advanced Parser Features")
        print("=" * 70)

        # 102. Parser Auto-Discovery
        print("\n102. Testing parser auto-discovery...")
        from nexus.parsers import ParserRegistry

        registry = ParserRegistry()
        discovered_count = registry.discover_parsers("nexus.parsers")
        print(f"   ‚úì Auto-discovered {discovered_count} parser(s)")
        print(f"   Parsers: {[p.name for p in registry.get_parsers()]}")

        # 103. MIME Type Detection
        print("\n103. Testing MIME type detection...")
        from nexus.parsers import detect_mime_type

        test_content = b'{"key": "value"}'
        mime_type = detect_mime_type(test_content, "test.json")
        print(f"   ‚úì Detected MIME type: {mime_type}")

        pdf_content_sample = b"%PDF-1.4"
        mime_type_pdf = detect_mime_type(pdf_content_sample, "test.pdf")
        print(f"   ‚úì PDF MIME type: {mime_type_pdf}")

        # 104. Encoding Detection
        print("\n104. Testing text encoding detection...")
        from nexus.parsers import detect_encoding

        utf8_text = "Hello, ‰∏ñÁïå! üåç".encode()
        encoding = detect_encoding(utf8_text)
        print(f"   ‚úì Detected encoding: {encoding}")

        ascii_text = b"Hello, world!"
        encoding_ascii = detect_encoding(ascii_text)
        print(f"   ‚úì ASCII encoding: {encoding_ascii}")

        # 105. Compressed File Handling
        print("\n105. Testing compressed file handling...")
        import gzip

        from nexus.parsers import decompress_content, is_compressed

        # Create a compressed file
        original_text = b"This is a test document with important content."
        compressed_data = gzip.compress(original_text)

        print(f"   Original size: {len(original_text)} bytes")
        print(f"   Compressed size: {len(compressed_data)} bytes")
        print(f"   Compression ratio: {len(compressed_data) / len(original_text) * 100:.1f}%")

        # Check compression detection
        if is_compressed("document.txt.gz"):
            print("   ‚úì Compression detected for .gz file")

        # Decompress
        decompressed, inner_name = decompress_content(compressed_data, "document.txt.gz")
        print("   ‚úì Decompressed successfully")
        print(f"   Inner filename: {inner_name}")
        print(f"   Decompressed size: {len(decompressed)} bytes")
        assert decompressed == original_text, "Decompression failed!"

        # 106. Unified Content Preparation
        print("\n106. Testing unified content preparation...")
        from nexus.parsers import prepare_content_for_parsing

        # Test with compressed JSON
        json_content = b'{"project": "nexus", "version": "0.2.0"}'
        compressed_json = gzip.compress(json_content)

        processed, effective_path, metadata = prepare_content_for_parsing(
            compressed_json, "config.json.gz"
        )

        print("   ‚úì Original file: config.json.gz")
        print(f"   ‚úì Effective path: {effective_path}")
        print(f"   ‚úì Was compressed: {metadata.get('compressed', False)}")
        print(f"   ‚úì Inner filename: {metadata.get('inner_filename')}")
        print(f"   ‚úì MIME type: {metadata.get('mime_type')}")
        print(f"   ‚úì Content size: {len(processed)} bytes")

        # 107. Test with actual compressed file write
        print("\n107. Writing compressed file to Nexus...")
        compressed_report = gzip.compress(b"""# Compressed Report

This document was compressed with gzip before upload.

## Key Points
- Nexus automatically detects compression
- Parsers can handle compressed formats
- Transparent decompression during parsing
""")

        nx_parse.write("/documents/compressed-report.md.gz", compressed_report)
        print(f"   ‚úì Uploaded compressed file ({len(compressed_report)} bytes)")
        print("   ‚Üí Auto-parsing will decompress and parse automatically")

        # Wait for parsing
        import time

        time.sleep(2)

        # Check if parsed
        parsed_compressed = nx_parse.metadata.get_file_metadata(
            "/documents/compressed-report.md.gz", "parsed_text"
        )
        if parsed_compressed:
            print("   ‚úì Compressed file parsed successfully")
            print(f"   ‚úì Extracted {len(parsed_compressed)} characters")
        else:
            print("   ‚ö† Compressed file parsing still in progress...")

        print("\n108. Summary of advanced parser features:")
        print("   New features in v0.2.0:")
        print("   ‚úì Auto-discovery of parsers from packages")
        print("   ‚úì MIME type detection (python-magic + fallback)")
        print("   ‚úì Text encoding detection (chardet + fallback)")
        print("   ‚úì Compressed file support (.gz, .zip, .bz2, .xz)")
        print("   ‚úì Unified content preprocessing pipeline")
        print()
        print("   Supported compression formats:")
        print("   ‚Ä¢ .gz / .gzip - gzip compression")
        print("   ‚Ä¢ .zip - ZIP archives (single file)")
        print("   ‚Ä¢ .bz2 - bzip2 compression")
        print("   ‚Ä¢ .xz - LZMA compression")
        print()
        print("   These features work transparently with auto_parse!")

        nx_parse.close()

        # ============================================================
        # Part 14b: Permission Inheritance (v0.3.0 - Issue #111)
        # ============================================================
        print("\n" + "=" * 70)
        print("PART 14b: Permission Inheritance (NEW in v0.3.0)")
        print("=" * 70)
        print("Demonstrating automatic permission inheritance for new files")

        print("\n53b. Setting up permission inheritance demo...")
        nx_inherit = nexus.connect(config={"data_dir": str(data_dir)})

        # Create parent directory and set its permissions
        print("\n   Creating parent directory with permissions...")
        nx_inherit.write("/inherit-parent/.keep", b"")

        # Get parent metadata and set permissions
        parent_meta = nx_inherit.metadata.get("/inherit-parent")
        if parent_meta:
            parent_meta.owner = "alice"
            parent_meta.group = "developers"
            parent_meta.mode = 0o755  # rwxr-xr-x
            nx_inherit.metadata.put(parent_meta)
            print(
                f"   ‚úì Parent permissions: owner={parent_meta.owner}, group={parent_meta.group}, mode={oct(parent_meta.mode)}"
            )

        # Create new file - should automatically inherit permissions
        print("\n   Creating child file (should inherit permissions)...")
        nx_inherit.write("/inherit-parent/child-file.txt", b"This file inherits permissions")

        # Check inherited permissions
        child_meta = nx_inherit.metadata.get("/inherit-parent/child-file.txt")
        if child_meta:
            print(
                f"   ‚úì Child permissions: owner={child_meta.owner}, group={child_meta.group}, mode={oct(child_meta.mode) if child_meta.mode else 'None'}"
            )

            if child_meta.owner == "alice":
                print("   ‚úì Owner inherited from parent: alice")
            if child_meta.group == "developers":
                print("   ‚úì Group inherited from parent: developers")
            if child_meta.mode == 0o644:  # Execute bits cleared for files
                print("   ‚úì Mode inherited with execute bits cleared: 0o644 (rw-r--r--)")
                print("   ‚úì Parent was 0o755 (rwxr-xr-x) ‚Üí child is 0o644 (rw-r--r--)")

        # Create another parent with strict permissions
        print("\n   Creating strict parent directory (0o700)...")
        nx_inherit.write("/strict-parent/.keep", b"")
        strict_parent = nx_inherit.metadata.get("/strict-parent")
        if strict_parent:
            strict_parent.owner = "bob"
            strict_parent.group = "admins"
            strict_parent.mode = 0o700  # rwx------
            nx_inherit.metadata.put(strict_parent)
            print(
                f"   ‚úì Strict parent: owner={strict_parent.owner}, group={strict_parent.group}, mode={oct(strict_parent.mode)}"
            )

        # Create file in strict parent
        print("\n   Creating file in strict parent...")
        nx_inherit.write("/strict-parent/secret.txt", b"Secret data")
        strict_child = nx_inherit.metadata.get("/strict-parent/secret.txt")
        if strict_child:
            print(
                f"   ‚úì Strict child: owner={strict_child.owner}, group={strict_child.group}, mode={oct(strict_child.mode) if strict_child.mode else 'None'}"
            )
            if strict_child.mode == 0o600:  # 0o700 with execute bits cleared
                print("   ‚úì Strict permissions inherited: 0o600 (rw-------)")
                print("   ‚úì Parent was 0o700 (rwx------) ‚Üí child is 0o600 (rw-------)")

        # Update existing file - should preserve permissions
        print("\n   Updating existing file (should preserve permissions)...")
        nx_inherit.write("/inherit-parent/child-file.txt", b"Updated content")
        updated_child = nx_inherit.metadata.get("/inherit-parent/child-file.txt")
        if updated_child and updated_child.owner == "alice":
            print("   ‚úì Permissions preserved on update: owner still alice")

        print("\n   Permission inheritance demo complete!")
        print("   Key points:")
        print("   ‚Ä¢ New files inherit owner, group, and mode from parent directory")
        print("   ‚Ä¢ Execute bits are cleared for files (0o755 ‚Üí 0o644)")
        print("   ‚Ä¢ Updating existing files preserves their permissions")
        print("   ‚Ä¢ Works automatically - no user action required!")

        nx_inherit.close()

        # ============================================================
        # Part 15: rclone-style CLI Operations (v0.2.0)
        # ============================================================
        print("\n" + "=" * 70)
        print("PART 15: rclone-style CLI Operations (NEW in v0.2.0)")
        print("=" * 70)
        print("Demonstrating sync, copy, tree, and size operations")

        # Create test data for sync/copy operations
        print("\n54. Setting up test data for file operations...")
        nx_ops = nexus.connect(config={"data_dir": str(data_dir)})

        # Create source directory with test files
        nx_ops.write("/source/file1.txt", b"Content for file 1")
        nx_ops.write("/source/file2.txt", b"Content for file 2")
        nx_ops.write("/source/subdir/file3.txt", b"Content for file 3")
        nx_ops.write("/source/subdir/file4.txt", b"Content for file 4")
        print("   ‚úì Created test files in /source/")

        # These operations are typically done via CLI, but we can demonstrate
        # the underlying functionality programmatically
        from nexus.sync import copy_recursive, sync_directories

        print("\n55. Demonstrating sync operation (hash-based)...")
        stats = sync_directories(
            nx_ops,
            "/source",
            "/dest",
            delete=False,
            dry_run=False,
            checksum=True,
            progress=False,  # Disable progress bar for demo
        )
        print(f"   Files checked: {stats.files_checked}")
        print(f"   Files copied: {stats.files_copied}")
        print(f"   Files skipped: {stats.files_skipped}")
        print(f"   Bytes transferred: {stats.bytes_transferred:,}")
        print("   ‚úì Sync completed successfully")

        print("\n56. Re-syncing (should skip identical files)...")
        stats2 = sync_directories(
            nx_ops,
            "/source",
            "/dest",
            delete=False,
            dry_run=False,
            checksum=True,
            progress=False,
        )
        print(f"   Files checked: {stats2.files_checked}")
        print(f"   Files copied: {stats2.files_copied}")
        print(f"   Files skipped: {stats2.files_skipped} (all files identical!)")
        print("   ‚úì Smart deduplication works!")

        print("\n57. Testing sync with delete flag...")
        # Add an extra file to destination
        nx_ops.write("/dest/extra_file.txt", b"This will be deleted")
        stats3 = sync_directories(
            nx_ops,
            "/source",
            "/dest",
            delete=True,  # Delete extra files
            dry_run=False,
            checksum=True,
            progress=False,
        )
        print(f"   Files deleted: {stats3.files_deleted}")
        print("   ‚úì Extra files removed from destination")

        print("\n58. Demonstrating copy operation...")
        stats4 = copy_recursive(
            nx_ops,
            "/source",
            "/backup",
            checksum=True,
            progress=False,
        )
        print(f"   Files copied: {stats4.files_copied}")
        print("   ‚úì Directory copied to /backup")

        print("\n59. Listing files to verify operations...")
        dest_files = nx_ops.list("/dest", recursive=True)
        backup_files = nx_ops.list("/backup", recursive=True)
        print(f"   Files in /dest: {len(dest_files)}")
        print(f"   Files in /backup: {len(backup_files)}")

        print("\n60. Demonstrating ReBAC (Relationship-Based Access Control)...")
        import sqlite3

        from nexus.core.rebac_manager import ReBACManager

        # Get database path (same as the data_dir used throughout this demo)
        rebac_db_path = data_dir / "metadata.db"

        # Create ReBAC tables manually (since demo doesn't run Alembic migrations)
        print("   Setting up ReBAC database tables...")
        conn = sqlite3.connect(str(rebac_db_path))
        cursor = conn.cursor()

        # Create tables if they don't exist
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS rebac_tuples (
                tuple_id TEXT PRIMARY KEY,
                subject_type TEXT NOT NULL,
                subject_id TEXT NOT NULL,
                subject_relation TEXT,
                relation TEXT NOT NULL,
                object_type TEXT NOT NULL,
                object_id TEXT NOT NULL,
                created_at TEXT NOT NULL,
                expires_at TEXT,
                conditions TEXT
            )
        """)

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS rebac_namespaces (
                namespace_id TEXT PRIMARY KEY,
                object_type TEXT NOT NULL UNIQUE,
                config TEXT NOT NULL,
                created_at TEXT NOT NULL,
                updated_at TEXT NOT NULL
            )
        """)

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS rebac_check_cache (
                cache_id TEXT PRIMARY KEY,
                subject_type TEXT NOT NULL,
                subject_id TEXT NOT NULL,
                permission TEXT NOT NULL,
                object_type TEXT NOT NULL,
                object_id TEXT NOT NULL,
                result INTEGER NOT NULL,
                computed_at TEXT NOT NULL,
                expires_at TEXT NOT NULL
            )
        """)

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS rebac_changelog (
                change_id INTEGER PRIMARY KEY AUTOINCREMENT,
                change_type TEXT NOT NULL,
                tuple_id TEXT,
                subject_type TEXT NOT NULL,
                subject_id TEXT NOT NULL,
                relation TEXT NOT NULL,
                object_type TEXT NOT NULL,
                object_id TEXT NOT NULL,
                created_at TEXT NOT NULL
            )
        """)

        conn.commit()
        conn.close()

        rebac_mgr = ReBACManager(db_path=str(rebac_db_path))

        print("   Creating relationship tuples...")
        # alice is member of engineering team
        tuple1 = rebac_mgr.rebac_write(
            subject=("agent", "alice"),
            relation="member-of",
            object=("group", "engineering"),
        )
        print(f"   ‚úì Created: alice member-of engineering (ID: {tuple1[:8]}...)")

        # engineering team owns the /projects directory
        tuple2 = rebac_mgr.rebac_write(
            subject=("group", "engineering"),
            relation="owner-of",
            object=("file", "/projects"),
        )
        print(f"   ‚úì Created: engineering owner-of /projects (ID: {tuple2[:8]}...)")

        # parent folder relationship
        tuple3 = rebac_mgr.rebac_write(
            subject=("file", "/projects"),
            relation="parent-of",
            object=("file", "/projects/backend"),
        )
        print(f"   ‚úì Created: /projects parent-of /projects/backend (ID: {tuple3[:8]}...)")

        print("\n   Checking permissions (with graph traversal)...")
        # Direct check: alice is member of engineering
        has_perm1 = rebac_mgr.rebac_check(
            subject=("agent", "alice"),
            permission="member-of",
            object=("group", "engineering"),
        )
        print(f"   alice member-of engineering? {has_perm1} ‚úì")

        # Indirect check: alice owns /projects (via group membership)
        # Note: This requires namespace config for "file" type to support this traversal
        # For now, we just check direct ownership
        has_perm2 = rebac_mgr.rebac_check(
            subject=("group", "engineering"),
            permission="owner-of",
            object=("file", "/projects"),
        )
        print(f"   engineering owner-of /projects? {has_perm2} ‚úì")

        print("\n   Expanding permissions (find all subjects)...")
        # Find all members of engineering
        members = rebac_mgr.rebac_expand(
            permission="member-of",
            object=("group", "engineering"),
        )
        print(f"   Members of engineering: {[s[1] for s in members]}")

        # Find all owners of /projects
        owners = rebac_mgr.rebac_expand(
            permission="owner-of",
            object=("file", "/projects"),
        )
        print(f"   Owners of /projects: {[(s[0], s[1]) for s in owners]}")

        print("\n   Creating temporary access (expires after 1 second)...")
        expires_at = datetime.utcnow() + timedelta(seconds=1)
        rebac_mgr.rebac_write(
            subject=("agent", "bob"),
            relation="viewer-of",
            object=("file", "/projects/temp-doc"),
            expires_at=expires_at,
        )
        print("   ‚úì Created: bob viewer-of /projects/temp-doc (expires in 1s)")

        # Check immediately
        has_temp = rebac_mgr.rebac_check(
            subject=("agent", "bob"),
            permission="viewer-of",
            object=("file", "/projects/temp-doc"),
        )
        print(f"   bob can view temp-doc (now)? {has_temp} ‚úì")

        # Wait for expiration
        import time

        print("   Waiting 1.5 seconds for expiration...")
        time.sleep(1.5)

        # Check after expiration
        has_temp_after = rebac_mgr.rebac_check(
            subject=("agent", "bob"),
            permission="viewer-of",
            object=("file", "/projects/temp-doc"),
        )
        print(f"   bob can view temp-doc (after expiry)? {has_temp_after} ‚úì (expired)")

        print("\n   Deleting relationships...")
        deleted = rebac_mgr.rebac_delete(tuple1)
        print(f"   ‚úì Deleted tuple {tuple1[:8]}... (deleted: {deleted})")

        rebac_mgr.close()
        print("   ‚úì ReBAC demo completed!")

        # Note: tree and size commands are typically CLI-only
        # but we can show the underlying data they would display
        print("\n61. CLI commands available for:")
        print("   ‚Ä¢ nexus tree /workspace - ASCII tree visualization")
        print("   ‚Ä¢ nexus size /workspace --human - Calculate directory sizes")
        print("   ‚Ä¢ nexus sync ./local/ /workspace/ - One-way sync")
        print("   ‚Ä¢ nexus copy ./data/ /backup/ --recursive - Smart copy")
        print("   ‚Ä¢ nexus move /old /new - Efficient move/rename")
        print()
        print("   All commands support:")
        print("   ‚Ä¢ Progress bars (tqdm) for long operations")
        print("   ‚Ä¢ Hash-based change detection")
        print("   ‚Ä¢ Dry-run mode (--dry-run)")
        print("   ‚Ä¢ Cross-platform paths (local ‚Üî Nexus)")

        nx_ops.close()

        # ============================================================
        # Part 62: Permission Policies (v0.3.0 - Default permissions per namespace)
        # ============================================================
        print("\n" + "=" * 70)
        print("PART 62: Permission Policies (Automatic Permission Assignment)")
        print("=" * 70)

        # Re-connect with agent context
        nx_policy = nexus.connect(
            config={
                "data_dir": str(data_dir),
                "tenant_id": "acme-corp",
                "agent_id": "alice",
            }
        )

        print("\n62. Testing automatic permission assignment...")
        print("   Default policies apply permissions based on namespace:")
        print("   ‚Ä¢ /workspace/* ‚Üí owner=${agent_id}, group=agents, mode=0o644")
        print("   ‚Ä¢ /shared/*    ‚Üí owner=root, group=${tenant_id}, mode=0o664")
        print("   ‚Ä¢ /archives/*  ‚Üí owner=root, group=${tenant_id}, mode=0o444")
        print("   ‚Ä¢ /system/*    ‚Üí owner=root, group=root, mode=0o600")

        # Create files in different namespaces
        print("\n   Creating files in /workspace namespace...")
        nx_policy.write("/workspace/acme-corp/alice/project.py", b"# Alice's code")

        # Get metadata and check permissions
        meta = nx_policy.metadata.get("/workspace/acme-corp/alice/project.py")
        print(f"   ‚úì File created: {meta.path}")
        print(f"     Owner: {meta.owner} (substituted ${'{agent_id}'})")
        print(f"     Group: {meta.group}")
        print(f"     Mode: {oct(meta.mode) if meta.mode else 'None'} (rw-r--r--)")

        print("\n   Creating files in /shared namespace...")
        nx_policy.write("/shared/acme-corp/team-data.json", b'{"team": "engineering"}')

        meta_shared = nx_policy.metadata.get("/shared/acme-corp/team-data.json")
        print(f"   ‚úì File created: {meta_shared.path}")
        print(f"     Owner: {meta_shared.owner}")
        print(f"     Group: {meta_shared.group} (substituted ${'{tenant_id}'})")
        print(f"     Mode: {oct(meta_shared.mode) if meta_shared.mode else 'None'} (rw-rw-r--)")

        # Test permission preservation on update
        print("\n   Testing permission preservation on file update...")
        nx_policy.write("/workspace/acme-corp/alice/project.py", b"# Updated code")

        meta_updated = nx_policy.metadata.get("/workspace/acme-corp/alice/project.py")
        print(f"   ‚úì File updated: {meta_updated.path}")
        print(f"     Owner: {meta_updated.owner} (preserved)")
        print(f"     Group: {meta_updated.group} (preserved)")
        print(f"     Mode: {oct(meta_updated.mode) if meta_updated.mode else 'None'} (preserved)")

        # Test with different agent
        nx_policy.close()
        nx_bob = nexus.connect(
            config={
                "data_dir": str(data_dir),
                "tenant_id": "acme-corp",
                "agent_id": "bob",
            }
        )

        print("\n   Creating file as different agent (bob)...")
        nx_bob.write("/workspace/acme-corp/bob/report.md", b"# Bob's report")

        meta_bob = nx_bob.metadata.get("/workspace/acme-corp/bob/report.md")
        print(f"   ‚úì File created: {meta_bob.path}")
        print(f"     Owner: {meta_bob.owner} (substituted with bob)")
        print(f"     Group: {meta_bob.group}")
        print(f"     Mode: {oct(meta_bob.mode) if meta_bob.mode else 'None'}")

        # Access policy store directly (advanced usage)
        print("\n   Inspecting permission policies in database...")
        from nexus.storage.policy_store import PolicyStore

        with nx_bob.metadata.SessionLocal() as session:
            policy_store = PolicyStore(session)
            policies = policy_store.list_policies(tenant_id=None)  # System-wide policies

            print(f"   Total policies: {len(policies)}")
            for policy in policies[:2]:  # Show first 2
                print(f"\n   Policy: {policy.namespace_pattern}")
                print(f"     Default owner: {policy.default_owner}")
                print(f"     Default group: {policy.default_group}")
                print(f"     Default mode: {oct(policy.default_mode)}")
                print(f"     Priority: {policy.priority}")

        nx_bob.close()
        print("\n   ‚úì Permission policies work automatically!")

        # ============================================================
        # Part 63: Permission Enforcement (v0.3.0 - Multi-layer permission checking)
        # ============================================================
        print("\n" + "=" * 70)
        print("PART 63: Permission Enforcement (Multi-Layer Access Control)")
        print("=" * 70)

        from nexus.core.permissions import (
            OperationContext,
            Permission,
            PermissionEnforcer,
        )

        print("\n63. Testing OperationContext (user/agent context for operations)...")

        # Regular user context
        ctx_alice = OperationContext(user="alice", groups=["developers"])
        print(f"   ‚úì Created context for alice: {ctx_alice.user}, groups={ctx_alice.groups}")
        print(f"     is_admin={ctx_alice.is_admin}, is_system={ctx_alice.is_system}")

        # Admin context
        ctx_admin = OperationContext(user="admin", groups=["admins"], is_admin=True)
        print(f"\n   ‚úì Created admin context: {ctx_admin.user}")
        print(f"     is_admin={ctx_admin.is_admin} (bypasses all permission checks)")

        # System context
        ctx_system = OperationContext(user="system", groups=[], is_system=True)
        print(f"\n   ‚úì Created system context: {ctx_system.user}")
        print(f"     is_system={ctx_system.is_system} (bypasses all permission checks)")

        print("\n64. Testing PermissionEnforcer (multi-layer permission checking)...")

        # Create permission enforcer with metadata store
        nx_enforce = nexus.connect(config={"data_dir": str(data_dir)})
        enforcer = PermissionEnforcer(metadata_store=nx_enforce.metadata)

        print("   Permission checking uses 3 layers:")
        print("   1. ReBAC (Relationship-Based) - Check graph relationships")
        print("   2. ACL (Access Control Lists) - Check explicit allow/deny")
        print("   3. UNIX Permissions - Check owner/group/other bits")

        # Create test files with different permissions
        print("\n   Creating test files with different permissions...")
        nx_enforce.write("/test-perms/public.txt", b"Public file")
        meta_public = nx_enforce.metadata.get("/test-perms/public.txt")
        meta_public.owner = "alice"
        meta_public.group = "developers"
        meta_public.mode = 0o644  # rw-r--r--
        nx_enforce.metadata.put(meta_public)

        nx_enforce.write("/test-perms/secret.txt", b"Secret file")
        meta_secret = nx_enforce.metadata.get("/test-perms/secret.txt")
        meta_secret.owner = "alice"
        meta_secret.group = "developers"
        meta_secret.mode = 0o600  # rw-------
        nx_enforce.metadata.put(meta_secret)

        print("   ‚úì Created /test-perms/public.txt (mode=0o644, rw-r--r--)")
        print("   ‚úì Created /test-perms/secret.txt (mode=0o600, rw-------)")

        # Test owner can read/write both files
        print("\n   Testing owner permissions (alice)...")
        can_read_public = enforcer.check("/test-perms/public.txt", Permission.READ, ctx_alice)
        can_write_public = enforcer.check("/test-perms/public.txt", Permission.WRITE, ctx_alice)
        can_read_secret = enforcer.check("/test-perms/secret.txt", Permission.READ, ctx_alice)
        can_write_secret = enforcer.check("/test-perms/secret.txt", Permission.WRITE, ctx_alice)

        print(f"   alice can READ public.txt: {can_read_public} ‚úì")
        print(f"   alice can WRITE public.txt: {can_write_public} ‚úì")
        print(f"   alice can READ secret.txt: {can_read_secret} ‚úì")
        print(f"   alice can WRITE secret.txt: {can_write_secret} ‚úì")

        # Test group member can only read public file
        print("\n   Testing group member permissions (bob in developers)...")
        ctx_bob = OperationContext(user="bob", groups=["developers"])
        can_read_public_bob = enforcer.check("/test-perms/public.txt", Permission.READ, ctx_bob)
        can_write_public_bob = enforcer.check("/test-perms/public.txt", Permission.WRITE, ctx_bob)
        can_read_secret_bob = enforcer.check("/test-perms/secret.txt", Permission.READ, ctx_bob)
        can_write_secret_bob = enforcer.check("/test-perms/secret.txt", Permission.WRITE, ctx_bob)

        print(f"   bob can READ public.txt: {can_read_public_bob} ‚úì")
        print(f"   bob can WRITE public.txt: {can_write_public_bob} ‚úó (group read-only)")
        print(f"   bob can READ secret.txt: {can_read_secret_bob} ‚úó (owner-only)")
        print(f"   bob can WRITE secret.txt: {can_write_secret_bob} ‚úó (owner-only)")

        # Test other user permissions
        print("\n   Testing other user permissions (charlie, not in developers)...")
        ctx_charlie = OperationContext(user="charlie", groups=["designers"])
        can_read_public_charlie = enforcer.check(
            "/test-perms/public.txt", Permission.READ, ctx_charlie
        )
        can_write_public_charlie = enforcer.check(
            "/test-perms/public.txt", Permission.WRITE, ctx_charlie
        )
        can_read_secret_charlie = enforcer.check(
            "/test-perms/secret.txt", Permission.READ, ctx_charlie
        )

        print(f"   charlie can READ public.txt: {can_read_public_charlie} ‚úì (world-readable)")
        print(f"   charlie can WRITE public.txt: {can_write_public_charlie} ‚úó (owner-only write)")
        print(f"   charlie can READ secret.txt: {can_read_secret_charlie} ‚úó (owner-only)")

        # Test admin bypass
        print("\n   Testing admin bypass (admin sees everything)...")
        can_read_admin = enforcer.check("/test-perms/secret.txt", Permission.READ, ctx_admin)
        can_write_admin = enforcer.check("/test-perms/secret.txt", Permission.WRITE, ctx_admin)

        print(f"   admin can READ secret.txt: {can_read_admin} ‚úì (admin bypass)")
        print(f"   admin can WRITE secret.txt: {can_write_admin} ‚úì (admin bypass)")

        # Test filter_list (used by list operations)
        print("\n65. Testing filter_list (used by ls operations)...")

        all_test_paths = ["/test-perms/public.txt", "/test-perms/secret.txt"]

        # Bob can only see public file
        filtered_bob = enforcer.filter_list(all_test_paths, ctx_bob)
        print(f"   bob sees: {filtered_bob}")
        print("     ‚Üí Only public.txt (secret.txt filtered out)")

        # Alice sees both
        filtered_alice = enforcer.filter_list(all_test_paths, ctx_alice)
        print(f"\n   alice sees: {filtered_alice}")
        print("     ‚Üí Both files (owner)")

        # Admin sees all
        filtered_admin = enforcer.filter_list(all_test_paths, ctx_admin)
        print(f"\n   admin sees: {filtered_admin}")
        print("     ‚Üí All files (admin bypass)")

        # Test backward compatibility (files without permissions)
        print("\n66. Testing backward compatibility (files without permissions)...")
        nx_enforce.write("/test-perms/old-file.txt", b"Old file without permissions")
        # Don't set owner/group/mode - simulates pre-v0.3.0 file
        # (default is already None for these fields)

        can_read_old = enforcer.check("/test-perms/old-file.txt", Permission.READ, ctx_charlie)
        can_write_old = enforcer.check("/test-perms/old-file.txt", Permission.WRITE, ctx_charlie)

        print("   Created old-file.txt without permissions (owner=None, mode=None)")
        print(f"   charlie can READ: {can_read_old} ‚úì (backward compat: allow)")
        print(f"   charlie can WRITE: {can_write_old} ‚úì (backward compat: allow)")
        print("   ‚Üí Files without permissions allow all access (v0.2.x compatibility)")

        nx_enforce.close()
        print("\n   ‚úì Permission enforcement works across all layers!")

        # ============================================================
        # Part 64: Version Tracking & History (v0.3.5)
        # ============================================================
        print("\n" + "=" * 70)
        print("PART 64: Version Tracking & History (CAS-Backed)")
        print("=" * 70)

        print("\n67. Testing automatic version tracking on writes...")
        nx_version = nexus.connect(config={"data_dir": str(data_dir), "agent_id": "vtest"})

        # Create a file and modify it multiple times
        print("\n   Creating file and making multiple edits...")
        nx_version.write("/docs/document.txt", b"Version 1: Initial draft")
        print("   ‚úì Wrote v1: Initial draft")

        nx_version.write("/docs/document.txt", b"Version 2: Added introduction")
        print("   ‚úì Wrote v2: Added introduction")

        nx_version.write("/docs/document.txt", b"Version 3: Added conclusion")
        print("   ‚úì Wrote v3: Added conclusion")

        nx_version.write("/docs/document.txt", b"Version 4: Final version")
        print("   ‚úì Wrote v4: Final version")

        print("\n68. Listing version history...")
        versions = nx_version.list_versions("/docs/document.txt")
        print(f"   Total versions: {len(versions)}")
        for v in versions:
            print(f"   - v{v['version']}: {v['size']} bytes, created {v['created_at']}")

        print("\n69. Retrieving specific version content...")
        v1_content = nx_version.get_version("/docs/document.txt", version=1)
        v2_content = nx_version.get_version("/docs/document.txt", version=2)
        print(f"   v1 content: {v1_content.decode()}")
        print(f"   v2 content: {v2_content.decode()}")

        print("\n70. Comparing versions (metadata diff)...")
        diff_meta = nx_version.diff_versions("/docs/document.txt", v1=1, v2=4, mode="metadata")
        print(f"   Size change: {diff_meta['size_v1']} ‚Üí {diff_meta['size_v2']} bytes")
        print(f"   Hash changed: {diff_meta['content_changed']}")
        print(f"   v1 hash: {diff_meta['content_hash_v1']}")
        print(f"   v4 hash: {diff_meta['content_hash_v2']}")

        print("\n71. Comparing versions (content diff)...")
        diff_content = nx_version.diff_versions("/docs/document.txt", v1=1, v2=2, mode="content")
        print("   Unified diff:")
        print("   " + "\n   ".join(diff_content.split("\n")[:10]))  # First 10 lines

        print("\n72. Testing rollback to previous version...")
        current_content = nx_version.read("/docs/document.txt")
        print(f"   Current (v4): {current_content.decode()}")

        # Rollback to v2
        nx_version.rollback("/docs/document.txt", version=2)
        rollback_content = nx_version.read("/docs/document.txt")
        print(f"\n   After rollback to v2: {rollback_content.decode()}")

        # Check that rollback created a new version
        versions_after = nx_version.list_versions("/docs/document.txt")
        print(f"\n   Version count after rollback: {len(versions_after)}")
        print("   ‚úì Rollback creates new version pointing to old content (no data loss!)")

        print("\n73. Testing version tracking with Skills...")
        # Create a skill and update it
        skill_path = "/skills/my-skill/SKILL.md"
        nx_version.write(
            skill_path,
            b"---\nname: my-skill\nversion: 1.0.0\n---\n# My Skill\nInitial version",
        )
        print("   ‚úì Created skill v1")

        nx_version.write(
            skill_path,
            b"---\nname: my-skill\nversion: 1.1.0\n---\n# My Skill\nAdded feature A",
        )
        print("   ‚úì Updated skill to v1.1.0")

        nx_version.write(
            skill_path,
            b"---\nname: my-skill\nversion: 2.0.0\n---\n# My Skill\nMajor refactoring",
        )
        print("   ‚úì Updated skill to v2.0.0")

        # List skill versions
        skill_versions = nx_version.list_versions(skill_path)
        print(f"\n   Skill version history: {len(skill_versions)} versions")
        for sv in skill_versions:
            print(f"   - v{sv['version']}: {sv['size']} bytes")

        # Compare skill versions
        skill_diff = nx_version.diff_versions(skill_path, v1=1, v2=3, mode="content")
        print("\n   Diff between v1 and v3 (first 5 lines):")
        print("   " + "\n   ".join(skill_diff.split("\n")[:5]))

        print("\n74. Testing CAS deduplication with versions...")
        # Write the same content again (should reuse existing hash)
        nx_version.write("/docs/duplicate.txt", b"Version 2: Added introduction")
        print("   ‚úì Wrote duplicate.txt with same content as document.txt v2")
        print("   ‚Üí CAS automatically deduplicates content (zero storage overhead!)")

        nx_version.close()
        print("\n   ‚úì Version tracking works for files and skills!")
        print("   ‚úì Complete history preserved with CAS deduplication")

        print("\n‚úì Integrated demo completed successfully!")
        print("=" * 70)


if __name__ == "__main__":
    main()
