# AI News Collector Library ä½¿ç”¨æŒ‡å—

## ğŸ“š ç›®å½•ç»“æ„

```
ai_news_collector_lib/
â”œâ”€â”€ __init__.py                 # åº“å…¥å£
â”œâ”€â”€ core/                       # æ ¸å¿ƒæ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ collector.py           # åŸºç¡€æœé›†å™¨
â”‚   â””â”€â”€ advanced_collector.py  # é«˜çº§æœé›†å™¨
â”œâ”€â”€ models/                     # æ•°æ®æ¨¡å‹
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ article.py             # æ–‡ç« æ¨¡å‹
â”‚   â””â”€â”€ result.py              # ç»“æœæ¨¡å‹
â”œâ”€â”€ config/                     # é…ç½®æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ settings.py            # é…ç½®è®¾ç½®
â”œâ”€â”€ tools/                      # æœç´¢å·¥å…·
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ search_tools.py        # æœç´¢å·¥å…·å®ç°
â”œâ”€â”€ utils/                      # å·¥å…·æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ content_extractor.py   # å†…å®¹æå–å™¨
â”‚   â”œâ”€â”€ keyword_extractor.py   # å…³é”®è¯æå–å™¨
â”‚   â”œâ”€â”€ cache.py               # ç¼“å­˜ç®¡ç†å™¨
â”‚   â”œâ”€â”€ scheduler.py           # å®šæ—¶è°ƒåº¦å™¨
â”‚   â””â”€â”€ reporter.py            # æŠ¥å‘Šç”Ÿæˆå™¨
â”œâ”€â”€ examples/                   # ç¤ºä¾‹ä»£ç 
â”‚   â”œâ”€â”€ basic_usage.py         # åŸºç¡€ä½¿ç”¨ç¤ºä¾‹
â”‚   â””â”€â”€ advanced_usage.py      # é«˜çº§ä½¿ç”¨ç¤ºä¾‹
â”œâ”€â”€ tests/                      # æµ‹è¯•ä»£ç 
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_collector.py      # æœé›†å™¨æµ‹è¯•
â”œâ”€â”€ setup.py                   # å®‰è£…è„šæœ¬
â”œâ”€â”€ requirements.txt           # ä¾èµ–åˆ—è¡¨
â”œâ”€â”€ README.md                  # é¡¹ç›®è¯´æ˜
â”œâ”€â”€ LICENSE                    # è®¸å¯è¯
â””â”€â”€ pyproject.toml             # é¡¹ç›®é…ç½®
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. å®‰è£…åº“

```bash
# åŸºç¡€å®‰è£…
pip install ai-news-collector-lib

# é«˜çº§åŠŸèƒ½å®‰è£…
pip install ai-news-collector-lib[advanced]

# å¼€å‘å®‰è£…
git clone https://github.com/ai-news-collector/ai-news-collector-lib.git
cd ai-news-collector-lib
pip install -e .
```

### 2. åŸºç¡€ä½¿ç”¨

```python
import asyncio
from ai_news_collector_lib import AINewsCollector, SearchConfig

# åˆ›å»ºé…ç½®
config = SearchConfig(
    enable_hackernews=True,
    enable_arxiv=True,
    enable_duckduckgo=True,
    max_articles_per_source=10
)

# åˆ›å»ºæœé›†å™¨
collector = AINewsCollector(config)

# æ”¶é›†æ–°é—»
async def main():
    result = await collector.collect_news("artificial intelligence")
    print(f"æ”¶é›†åˆ° {result.total_articles} ç¯‡æ–‡ç« ")
    return result.articles

# è¿è¡Œ
articles = asyncio.run(main())
```

### 3. é«˜çº§ä½¿ç”¨

```python
from ai_news_collector_lib import AdvancedAINewsCollector, AdvancedSearchConfig

# åˆ›å»ºé«˜çº§é…ç½®
config = AdvancedSearchConfig(
    enable_hackernews=True,
    enable_arxiv=True,
    enable_duckduckgo=True,
    enable_content_extraction=True,
    enable_keyword_extraction=True,
    cache_results=True
)

# åˆ›å»ºé«˜çº§æœé›†å™¨
collector = AdvancedAINewsCollector(config)

# æ”¶é›†å¢å¼ºæ–°é—»
async def main():
    result = await collector.collect_news_advanced("machine learning")
    
    # åˆ†æç»“æœ
    total_words = sum(article['word_count'] for article in result['articles'])
    print(f"æ€»å­—æ•°: {total_words}")
    
    return result

# è¿è¡Œ
enhanced_result = asyncio.run(main())
```

## ğŸ”§ é…ç½®è¯¦è§£

### åŸºç¡€é…ç½® (SearchConfig)

```python
from ai_news_collector_lib import SearchConfig

config = SearchConfig(
    # ä¼ ç»Ÿæº
    enable_hackernews=True,      # HackerNews
    enable_arxiv=True,           # ArXivå­¦æœ¯è®ºæ–‡
    enable_newsapi=False,        # NewsAPI (éœ€è¦APIå¯†é’¥)
    enable_rss_feeds=True,       # RSSè®¢é˜…æº
    
    # æœç´¢å¼•æ“æº
    enable_duckduckgo=True,      # DuckDuckGoæœç´¢
    enable_tavily=False,         # Tavily API (éœ€è¦APIå¯†é’¥)
    enable_google_search=False,  # Googleæœç´¢ (éœ€è¦APIå¯†é’¥)
    enable_bing_search=False,    # Bingæœç´¢ (éœ€è¦APIå¯†é’¥)
    enable_serper=False,         # Serper API (éœ€è¦APIå¯†é’¥)
    enable_brave_search=False,   # Braveæœç´¢ (éœ€è¦APIå¯†é’¥)
    
    # æœç´¢å‚æ•°
    max_articles_per_source=10,  # æ¯ä¸ªæºæœ€å¤§æ–‡ç« æ•°
    days_back=7,                 # æœç´¢å¤©æ•°
    similarity_threshold=0.85    # å»é‡ç›¸ä¼¼åº¦é˜ˆå€¼
)
```

### é«˜çº§é…ç½® (AdvancedSearchConfig)

```python
from ai_news_collector_lib import AdvancedSearchConfig

config = AdvancedSearchConfig(
    # ç»§æ‰¿åŸºç¡€é…ç½®
    enable_hackernews=True,
    enable_arxiv=True,
    enable_duckduckgo=True,
    
    # é«˜çº§åŠŸèƒ½
    enable_content_extraction=True,    # å†…å®¹æå–
    enable_sentiment_analysis=False,  # æƒ…æ„Ÿåˆ†æ
    enable_keyword_extraction=True,   # å…³é”®è¯æå–
    cache_results=True,                # ç»“æœç¼“å­˜
    cache_duration_hours=24,          # ç¼“å­˜æ—¶é•¿(å°æ—¶)
    
    # å†…å®¹å¤„ç†å‚æ•°
    max_content_length=5000,          # æœ€å¤§å†…å®¹é•¿åº¦
    keyword_count=10                  # å…³é”®è¯æ•°é‡
)
```

## ğŸ“Š æ”¯æŒçš„æœç´¢æº

### å…è´¹æºï¼ˆæ— éœ€APIå¯†é’¥ï¼‰

| æº | æè¿° | ç‰¹ç‚¹ |
|---|---|---|
| ğŸ”¥ HackerNews | æŠ€æœ¯ç¤¾åŒºè®¨è®º | é«˜è´¨é‡æŠ€æœ¯å†…å®¹ |
| ğŸ“š ArXiv | å­¦æœ¯è®ºæ–‡å’Œé¢„å°æœ¬ | æœ€æ–°ç ”ç©¶æˆæœ |
| ğŸ¦† DuckDuckGo | éšç§ä¿æŠ¤çš„ç½‘é¡µæœç´¢ | æ— è¿½è¸ªæœç´¢ |

> æ³¨ï¼šArXiv æ—¥æœŸè§£æä¸å›é€€
- ä¼˜å…ˆä½¿ç”¨ `BeautifulSoup` è§£æ `published`ï¼›è‹¥ XML è§£æå¤±è´¥åˆ™å›é€€åˆ° `feedparser`ã€‚
- åœ¨ `feedparser` åˆ†æ”¯ä¸­ï¼Œæ—¥æœŸå­—æ®µå¯èƒ½ä»…æä¾›å…¶ä¸€ï¼š`published_parsed` æˆ– `updated_parsed`ï¼ˆå‡ä¸º `time.struct_time`ï¼‰ã€‚
- å›é€€é¡ºåºï¼š`published_parsed` â†’ `updated_parsed` â†’ `datetime.now()`ï¼Œä»¥æœ€å¤§ç¨‹åº¦è´´è¿‘çœŸå®å‘å¸ƒæ—¶é—´ã€‚
- `struct_time` è½¬æ¢ç¤ºä¾‹ï¼š`datetime(*entry.published_parsed[:6])`ã€‚

### ä»˜è´¹æºï¼ˆéœ€è¦APIå¯†é’¥ï¼‰

| æº | æè¿° | APIå¯†é’¥ | ç‰¹ç‚¹ |
|---|---|---|---|
| ğŸ“¡ NewsAPI | å¤šæºæ–°é—»èšåˆ | NEWS_API_KEY | ä¸°å¯Œçš„æ–°é—»æº |
| ğŸ” Tavily | AIé©±åŠ¨çš„æœç´¢API | TAVILY_API_KEY | æ™ºèƒ½æœç´¢ |
| ğŸŒ Google Search | Googleè‡ªå®šä¹‰æœç´¢API | GOOGLE_SEARCH_API_KEY + GOOGLE_SEARCH_ENGINE_ID | é«˜è´¨é‡æœç´¢ç»“æœ |
| ğŸ”µ Bing Search | å¾®è½¯Bingæœç´¢API | BING_SEARCH_API_KEY | å¾®è½¯æœç´¢æŠ€æœ¯ |
| âš¡ Serper | å¿«é€ŸGoogleæœç´¢API | SERPER_API_KEY | å¿«é€ŸGoogleç»“æœ |
| ğŸ¦ Brave Search | ç‹¬ç«‹éšç§æœç´¢API | BRAVE_SEARCH_API_KEY | ç‹¬ç«‹æœç´¢ç´¢å¼• |

## ğŸ› ï¸ é«˜çº§åŠŸèƒ½

### 1. å®šæ—¶ä»»åŠ¡

```python
from ai_news_collector_lib import DailyScheduler

# å®šä¹‰æ”¶é›†å‡½æ•°
async def collect_news_task():
    config = AdvancedSearchConfig(enable_hackernews=True, enable_arxiv=True)
    collector = AdvancedAINewsCollector(config)
    result = await collector.collect_news_advanced("AI news")
    print(f"æ”¶é›†å®Œæˆ: {result['unique_articles']} ç¯‡æ–‡ç« ")
    return result

# åˆ›å»ºè°ƒåº¦å™¨
scheduler = DailyScheduler(
    collector_func=collect_news_task,
    schedule_time="09:00",
    timezone="Asia/Shanghai"
)

# å¯åŠ¨è°ƒåº¦å™¨
scheduler.start()
```

### 2. ç¼“å­˜ç®¡ç†

```python
from ai_news_collector_lib import CacheManager

# åˆ›å»ºç¼“å­˜ç®¡ç†å™¨
cache = CacheManager(cache_dir="./cache", default_ttl_hours=24)

# ç”Ÿæˆç¼“å­˜é”®
cache_key = cache.get_cache_key("artificial intelligence", ["hackernews", "arxiv"])

# æ£€æŸ¥ç¼“å­˜
cached_result = cache.get_cached_result(cache_key)
if cached_result:
    print("ä½¿ç”¨ç¼“å­˜ç»“æœ")
else:
    # æ‰§è¡Œæœç´¢å¹¶ç¼“å­˜ç»“æœ
    result = await collector.collect_news("artificial intelligence")
    cache.cache_result(cache_key, result)

# è·å–ç¼“å­˜ä¿¡æ¯
cache_info = cache.get_cache_info()
print(f"ç¼“å­˜æ–‡ä»¶æ•°: {cache_info['total_files']}")
print(f"ç¼“å­˜å¤§å°: {cache_info['total_size_mb']} MB")
```

### 3. æŠ¥å‘Šç”Ÿæˆ

```python
from ai_news_collector_lib import ReportGenerator

# åˆ›å»ºæŠ¥å‘Šç”Ÿæˆå™¨
reporter = ReportGenerator(output_dir="./reports")

# ç”ŸæˆMarkdownæŠ¥å‘Š
markdown_report = reporter.generate_daily_report(result, format="markdown")
print(markdown_report)

# ç”ŸæˆHTMLæŠ¥å‘Š
html_report = reporter.generate_daily_report(result, format="html")

# ä¿å­˜æŠ¥å‘Šåˆ°æ–‡ä»¶
markdown_file = reporter.save_report(result, format="markdown")
html_file = reporter.save_report(result, format="html")

print(f"MarkdownæŠ¥å‘Š: {markdown_file}")
print(f"HTMLæŠ¥å‘Š: {html_file}")
```

### 4. å¤šä¸»é¢˜æ”¶é›†

```python
# æ”¶é›†å¤šä¸ªä¸»é¢˜
topics = [
    "artificial intelligence",
    "machine learning",
    "deep learning",
    "neural networks",
    "computer vision"
]

result = await collector.collect_multiple_topics(topics)

print(f"ä¸»é¢˜æ•°: {len(result['topics_searched'])}")
print(f"æ€»æ–‡ç« æ•°: {result['total_articles']}")
print(f"ç‹¬ç‰¹æ–‡ç« æ•°: {result['unique_articles']}")
print(f"æ€»å­—æ•°: {result['total_words']:,}")
print(f"å¹³å‡é˜…è¯»æ—¶é—´: {result['average_reading_time']} åˆ†é’Ÿ")

# æ˜¾ç¤ºå„ä¸»é¢˜ç»Ÿè®¡
for topic, stats in result['topic_results'].items():
    if 'error' in stats:
        print(f"âŒ {topic}: {stats['error']}")
    else:
        print(f"âœ… {topic}: {stats['unique']} ç¯‡ç‹¬ç‰¹æ–‡ç« ")
```

## ğŸ”Œ é›†æˆç¤ºä¾‹

### 1. ä½œä¸ºç‹¬ç«‹æœåŠ¡

```python
# åœ¨ä½ çš„é¡¹ç›®ä¸­
from ai_news_collector_lib import AINewsCollector, SearchConfig

class YourProjectService:
    def __init__(self):
        self.news_collector = AINewsCollector(SearchConfig())
    
    async def get_ai_news(self):
        result = await self.news_collector.collect_news("AI news")
        return result.articles
```

### 2. ä½œä¸ºå®šæ—¶ä»»åŠ¡

```python
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from ai_news_collector_lib import AdvancedAINewsCollector, AdvancedSearchConfig

scheduler = AsyncIOScheduler()

@scheduler.scheduled_job('cron', hour=9, minute=0)
async def daily_news_job():
    config = AdvancedSearchConfig(enable_hackernews=True, enable_arxiv=True)
    collector = AdvancedAINewsCollector(config)
    result = await collector.collect_news_advanced("AI")
    # å¤„ç†ç»“æœ...

scheduler.start()
```

### 3. ä½œä¸ºAPIç«¯ç‚¹

```python
from fastapi import FastAPI
from ai_news_collector_lib import AINewsCollector, SearchConfig

app = FastAPI()
collector = AINewsCollector(SearchConfig())

@app.get("/ai-news")
async def get_ai_news(query: str = "artificial intelligence"):
    result = await collector.collect_news(query)
    return {
        "total": result.total_articles,
        "unique": result.unique_articles,
        "articles": [article.to_dict() for article in result.articles]
    }
```

## ğŸ§ª æµ‹è¯•

### è¿è¡Œæµ‹è¯•

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
pytest

# è¿è¡Œç‰¹å®šæµ‹è¯•
pytest tests/test_collector.py

# è¿è¡Œå¼‚æ­¥æµ‹è¯•
pytest -v

# è¿è¡Œå¸¦æ ‡è®°çš„æµ‹è¯•
pytest -m "not slow"
```

### ç¼–å†™æµ‹è¯•

```python
import pytest
from ai_news_collector_lib import AINewsCollector, SearchConfig

@pytest.mark.asyncio
async def test_collect_news():
    config = SearchConfig(enable_hackernews=True)
    collector = AINewsCollector(config)
    
    result = await collector.collect_news("test query")
    assert result.total_articles >= 0
    assert result.unique_articles >= 0
```

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–

### 1. å¹¶å‘æ§åˆ¶

```python
# é™åˆ¶å¹¶å‘æ•°
import asyncio
from asyncio import Semaphore

class LimitedCollector:
    def __init__(self, max_concurrent=5):
        self.semaphore = Semaphore(max_concurrent)
    
    async def search_with_limit(self, source, query):
        async with self.semaphore:
            return await self._search_source(source, query)
```

### 2. ç¼“å­˜ç­–ç•¥

```python
# ä½¿ç”¨Redisç¼“å­˜
import redis
import json

class RedisCache:
    def __init__(self):
        self.redis = redis.Redis(host='localhost', port=6379, db=0)
    
    def get_cached_result(self, key):
        cached = self.redis.get(key)
        return json.loads(cached) if cached else None
    
    def cache_result(self, key, result, ttl=3600):
        self.redis.setex(key, ttl, json.dumps(result))
```

### 3. é”™è¯¯å¤„ç†

```python
# é‡è¯•æœºåˆ¶
import asyncio
from functools import wraps

def retry(max_attempts=3, delay=1):
    def decorator(func):
        @wraps(func)
        async def wrapper(*args, **kwargs):
            for attempt in range(max_attempts):
                try:
                    return await func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_attempts - 1:
                        raise e
                    await asyncio.sleep(delay * (2 ** attempt))
            return None
        return wrapper
    return decorator

@retry(max_attempts=3)
async def search_with_retry(self, source, query):
    return await self._search_source(source, query)
```

## ğŸš€ éƒ¨ç½²

### 1. Dockeréƒ¨ç½²

```dockerfile
FROM python:3.9-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install -r requirements.txt

COPY . .
RUN pip install -e .

CMD ["python", "examples/daily_collector.py"]
```

### 2. ç¯å¢ƒå˜é‡

```bash
# .envæ–‡ä»¶
NEWS_API_KEY=your_newsapi_key
TAVILY_API_KEY=your_tavily_key
GOOGLE_SEARCH_API_KEY=your_google_key
GOOGLE_SEARCH_ENGINE_ID=your_engine_id
BING_SEARCH_API_KEY=your_bing_key
SERPER_API_KEY=your_serper_key
BRAVE_SEARCH_API_KEY=your_brave_key
```

### 3. ç”Ÿäº§ç¯å¢ƒé…ç½®

```python
# ç”Ÿäº§ç¯å¢ƒé…ç½®
config = AdvancedSearchConfig(
    enable_hackernews=True,
    enable_arxiv=True,
    enable_duckduckgo=True,
    enable_content_extraction=True,
    enable_keyword_extraction=True,
    cache_results=True,
    cache_duration_hours=24,
    max_articles_per_source=20
)
```

## ğŸ“ æ”¯æŒ

- [é—®é¢˜æŠ¥å‘Š](https://github.com/ai-news-collector/ai-news-collector-lib/issues)
- [è®¨è®ºåŒº](https://github.com/ai-news-collector/ai-news-collector-lib/discussions)
- [é‚®ä»¶æ”¯æŒ](mailto:support@ai-news-collector.com)

---

**ç¥ä½ ä½¿ç”¨æ„‰å¿«ï¼** ğŸ‰
