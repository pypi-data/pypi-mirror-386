# -*- coding: utf-8 -*-
"""Memor parameters and constants."""
from enum import Enum
MEMOR_VERSION = "0.9"

DATE_TIME_FORMAT = "%Y-%m-%d %H:%M:%S %z"

INVALID_PATH_MESSAGE = "Invalid path: must be a string and refer to an existing location. Given path: {path}"
INVALID_STR_VALUE_MESSAGE = "Invalid value. `{parameter_name}` must be a string."
INVALID_BOOL_VALUE_MESSAGE = "Invalid value. `{parameter_name}` must be a boolean."
INVALID_POSFLOAT_VALUE_MESSAGE = "Invalid value. `{parameter_name}` must be a positive number."
INVALID_POSINT_VALUE_MESSAGE = "Invalid value. `{parameter_name}` must be a positive integer."
INVALID_PROB_VALUE_MESSAGE = "Invalid value. `{parameter_name}` must be a value between 0 and 1."
INVALID_LIST_OF_X_MESSAGE = "Invalid value. `{parameter_name}` must be a list of {type_name}."
INVALID_INT_OR_STR_MESSAGE = "Invalid value. `{parameter_name}` must be an integer or a string."
INVALID_INT_OR_STR_SLICE_MESSAGE = "Invalid value. `{parameter_name}` must be an integer, string or a slice."
INVALID_DATETIME_MESSAGE = "Invalid value. `{parameter_name}` must be a datetime object that includes timezone information."
INVALID_TEMPLATE_MESSAGE = "Invalid template. It must be an instance of `PromptTemplate` or `PresetPromptTemplate`."
INVALID_RESPONSE_MESSAGE = "Invalid response. It must be an instance of `Response`."
INVALID_MESSAGE = "Invalid message. It must be an instance of `Prompt` or `Response`."
INVALID_MESSAGE_STATUS_LEN_MESSAGE = "Invalid message status length. It must be equal to the number of messages."
INVALID_CUSTOM_MAP_MESSAGE = "Invalid custom map: it must be a dictionary with keys and values that can be converted to strings."
INVALID_ROLE_MESSAGE = "Invalid role. It must be an instance of Role enum."
INVALID_ID_MESSAGE = "Invalid message ID. It must be a valid UUIDv4."
INVALID_MODEL_MESSAGE = "Invalid model. It must be an instance of LLMModel enum or a string."
INVALID_TEMPLATE_STRUCTURE_MESSAGE = "Invalid template structure. It should be a JSON object with proper fields."
INVALID_PROMPT_STRUCTURE_MESSAGE = "Invalid prompt structure. It should be a JSON object with proper fields."
INVALID_RESPONSE_STRUCTURE_MESSAGE = "Invalid response structure. It should be a JSON object with proper fields."
INVALID_SESSION_STRUCTURE_MESSAGE = "Invalid session structure. It should be a JSON object with proper fields."
INVALID_RENDER_FORMAT_MESSAGE = "Invalid render format. It must be an instance of RenderFormat enum."
INVALID_WARNINGS_STRUCTURE_MESSAGE = "Invalid `warnings` structure. It must be a valid dictionary."
PROMPT_RENDER_ERROR_MESSAGE = "Prompt template and properties are incompatible."
UNSUPPORTED_OPERAND_ERROR_MESSAGE = "Unsupported operand type(s) for {operator}: `{operand1}` and `{operand2}`"
AI_STUDIO_SYSTEM_WARNING = "Google AI Studio models may not support content with a system role."
DATA_SAVE_SUCCESS_MESSAGE = "Everything seems good."
MESSAGE_SIZE_WARNING = "Message {message_id} exceeded size threshold ({current_size} > {threshold})."
SESSION_SIZE_WARNING = "Session exceeded size threshold ({current_size} > {threshold})."

XML_PATTERN = r"<([a-zA-Z_][\w\-\.]*)(\s[^<>]*?)?>.*?</\1\s*>|<([a-zA-Z_][\w\-\.]*)(\s[^<>]*?)?/?>"


class Role(Enum):
    """Role enum."""

    SYSTEM = "system"
    USER = "user"
    ASSISTANT = "assistant"
    DEFAULT = USER


class RenderFormat(Enum):
    """Render format."""

    STRING = "STRING"
    OPENAI = "OPENAI"
    AI_STUDIO = "AI STUDIO"
    DICTIONARY = "DICTIONARY"
    ITEMS = "ITEMS"
    DEFAULT = STRING


class LLMModel(Enum):
    """LLM model enum."""

    GPT_5 = "gpt-5"
    GPT_5_MINI = "gpt-5-mini"
    GPT_5_NANO = "gpt-5-nano"
    GPT_4 = "gpt-4"
    GPT_4_TURBO = "gpt-4-turbo"
    GPT_4_VISION = "gpt-4-vision"
    GPT_4O = "gpt-4o"
    GPT_4O_MINI = "gpt-4o-mini"
    GPT_4O_SEARCH = "gpt-4o-search"
    GPT_4O_MINI_SEARCH = "gpt-4o-mini-search"
    GPT_4_1 = "gpt-4.1"
    GPT_4_1_MINI = "gpt-4.1-mini"
    GPT_4_1_NANO = "gpt-4.1-nano"
    GPT_O1 = "gpt-o1"
    GPT_O1_MINI = "gpt-o1-mini"
    GPT_O1_PRO = "gpt-o1-pro"
    GPT_O3 = "gpt-o3"
    GPT_O3_MINI = "gpt-o3-mini"
    GPT_O3_MINI_HIGH = "gpt-o3-mini-high"
    GPT_O3_DEEP_RESEARCH = "gpt-o3-deep-research"
    GPT_O4_MINI = "gpt-o4-mini"
    GPT_O4_MINI_DEEP_RESEARCH = "gpt-o4-mini-deep-research"
    GPT_3_5_TURBO = "gpt-3.5-turbo"
    GPT_OSS_20B = "gpt-oss-20b"
    GPT_OSS_120B = "gpt-oss-120b"
    DAVINCI = "davinci"
    BABBAGE = "babbage"

    CLAUDE_1 = "claude-1"
    CLAUDE_INSTANT = "claude-instant"
    CLAUDE_2 = "claude-2"
    CLAUDE_2_1 = "claude-2.1"
    CLAUDE_3_HAIKU = "claude-3-haiku"
    CLAUDE_3_SONNET = "claude-3-sonnet"
    CLAUDE_3_OPUS = "claude-3-opus"
    CLAUDE_3_5_SONNET = "claude-3.5-sonnet"
    CLAUDE_3_5_HAIKU = "claude-3.5-haiku"
    CLAUDE_3_7_SONNET = "claude-3.7-sonnet"
    CLAUDE_4_OPUS = "claude-4-opus"
    CLAUDE_4_SONNET = "claude-4-sonnet"
    CLAUDE_4_1_OPUS = "claude-4.1-opus"

    LLAMA_2_7B = "llama-2-7b"
    LLAMA_2_13B = "llama-2-13b"
    LLAMA_2_70B = "llama-2-70b"
    LLAMA_3_8B = "llama-3-8b"
    LLAMA_3_8B_INSTRUCT = "llama-3-8b-instruct"
    LLAMA_3_70B = "llama-3-70b"
    LLAMA_3_70B_INSTRUCT = "llama-3-70b-instruct"
    LLAMA_3_1_8B = "llama-3.1-8b"
    LLAMA_3_1_8B_INSTRUCT = "llama-3.1-8b-instruct"
    LLAMA_3_1_70B = "llama-3.1-70b"
    LLAMA_3_1_70B_INSTRUCT = "llama-3.1-70b-instruct"
    LLAMA_3_1_405B = "llama-3.1-405b"
    LLAMA_3_1_405B_INSTRUCT = "llama-3.1-405b-instruct"
    LLAMA_3_2_1B = "llama-3.2-1b"
    LLAMA_3_2_1B_INSTRUCT = "llama-3.2-1b-instruct"
    LLAMA_3_2_3B = "llama-3.2-3b"
    LLAMA_3_2_3B_INSTRUCT = "llama-3.2-3b-instruct"
    LLAMA_3_2_11B_VISION = "llama-3.2-11b-vision"
    LLAMA_3_2_11B_VISION_INSTRUCT = "llama-3.2-11b-vision-instruct"
    LLAMA_3_2_90B_VISION = "llama-3.2-90b-vision"
    LLAMA_3_2_90B_VISION_INSTRUCT = "llama-3.2-90b-vision-instruct"
    LLAMA_3_3_70B_INSTRUCT = "llama-3.3-70b-instruct"
    LLAMA_4_MAVERICK_17B_128E = "llama-4-maverick-17b-128e"
    LLAMA_4_MAVERICK_17B_128E_INSTRUCT = "llama-4-maverick-17b-128e-instruct"
    LLAMA_GUARD_7B = "llama-guard-7b"
    LLAMA_GUARD_2_8B = "llama-guard-2-8b"
    LLAMA_GUARD_3_1B = "llama-guard-3-1b"
    LLAMA_GUARD_3_8B = "llama-guard-3-8b"
    LLAMA_GUARD_3_11B_VISION = "llama-guard-3-11b-vision"
    LLAMA_GUARD_4_12B = "llama-guard-4-12b"
    LLAMA_PROMPT_GUARD_86M = "llama-prompt-guard-86m"
    LLAMA_PROMPT_GUARD_2_22M = "llama-prompt-guard-2-22m"
    LLAMA_PROMPT_GUARD_2_86M = "llama-prompt-guard-2-86m"
    LLAMA_4_SCOUT_17B_16E = "llama-4-scout-17b-16e"
    LLAMA_4_SCOUT_17B_16E_INSTRUCT = "llama-4-scout-17b-16e-instruct"
    CODE_LLAMA_7B = "code-llama-7b"
    CODE_LLAMA_7B_INSTRUCT = "code-llama-7b-instruct"
    CODE_LLAMA_7B_PYTHON = "code-llama-7b-python"
    CODE_LLAMA_13B = "code-llama-13b"
    CODE_LLAMA_13B_INSTRUCT = "code-llama-13b-instruct"
    CODE_LLAMA_13B_PYTHON = "code-llama-13b-python"
    CODE_LLAMA_34B = "code-llama-34b"
    CODE_LLAMA_34B_INSTRUCT = "code-llama-34b-instruct"
    CODE_LLAMA_34B_PYTHON = "code-llama-34b-python"
    CODE_LLAMA_70B = "code-llama-70b"
    CODE_LLAMA_70B_INSTRUCT = "code-llama-70b-instruct"
    CODE_LLAMA_70B_PYTHON = "code-llama-70b-python"

    MISTRAL_7B = "mistral-7b"
    MISTRAL_7B_INSTRUCT = "mistral-7b-instruct"
    MIXTRAL_8X7B = "mixtral-8x7b"
    MIXTRAL_8X7B_INSTRUCT = "mixtral-8x7b-instruct"
    MIXTRAL_8X22B = "mixtral-8x22b"
    MIXTRAL_8X22B_INSTRUCT = "mixtral-8x22b-instruct"
    MISTRAL_NEMO = "mistral-nemo"
    MISTRAL_NEMO_INSTRUCT = "mistral-nemo-instruct"
    MISTRAL_SMALL_INSTRUCT = "mistral-small-instruct"
    MISTRAL_SMALL_24B = "mistral-small-24b"
    MISTRAL_SMALL_24B_INSTRUCT = "mistral-small-24b-instruct"
    MISTRAL_SMALL_3_1_24B = "mistral-small-3.1-24b"
    MISTRAL_SMALL_3_1_24B_INSTRUCT = "mistral-small-3.1-24b-instruct"
    MISTRAL_SMALL_3_2_24B_INSTRUCT = "mistral-small-3.2-24b-instruct"
    MISTRAL_LARGE_INSTRUCT = "mistral-large-instruct"
    MINISTRAL_8B_INSTRUCT = "ministral-8b-instruct"
    CODESTRAL_22B = "codestral-22b"
    MAMBA_CODESTRAL_7B = "mamba-codestral-7b"
    DEVSTRAL_SMALL = "devstral-small"
    MAGISTRAL_SMALL = "magistral-small"
    MATHSTRAL_7B = "mathstral-7b"
    PIXTRAL_12B = "pixtral-12b"
    PIXTRAL_LARGE_INSTRUCT = "pixtral-large-instruct"
    VOXTRAL_MINI_3B = "voxtral-mini-3b"
    VOXTRAL_SMALL_24B = "voxtral-small-24b"

    GEMMA_7B = "gemma-7b"
    GEMMA_2_9B = "gemma-2-9b"
    GEMMA_3_1B = "gemma-3-1b"
    GEMMA_3_4B = "gemma-3-4b"
    GEMMA_3_12B = "gemma-3-12b"
    GEMMA_3_27B = "gemma-3-27b"
    GEMMA_3N_E2B = "gemma-3n-e2b"
    GEMMA_3N_E4B = "gemma-3n-e4b"
    CODEGEMMA_2B = "codegemma-2b"
    CODEGEMMA_7B = "codegemma-7b"
    RECURRENTGEMMA_2B = "recurrentgemma-2b"
    RECURRENTGEMMA_9B = "recurrentgemma-9b"
    PALIGEMMA_1_3B = "paligemma-1-3b"
    PALIGEMMA_2_3B = "paligemma-2-3b"
    PALIGEMMA_2_10B = "paligemma-2-10b"
    PALIGEMMA_2_28B = "paligemma-2-28b"
    TXGEMMA = "txgemma"
    DATAGEMMA = "datagemma"
    SHIELDGEMMA_1_2B = "shieldgemma-1-2b"
    SHIELDGEMMA_1_9B = "shieldgemma-1-9b"
    SHIELDGEMMA_1_27B = "shieldgemma-1-27b"
    SHIELDGEMMA_2_4B = "shieldgemma-2-4b"
    GEMINI_1_PRO = "gemini-1-pro"
    GEMINI_1_ULTRA = "gemini-1-ultra"
    GEMINI_1_5_PRO = "gemini-1.5-pro"
    GEMINI_1_5_ULTRA = "gemini-1.5-ultra"
    GEMINI_1_5_FLASH = "gemini-1.5-flash"
    GEMINI_2_FLASH = "gemini-2-flash"
    GEMINI_2_FLASH_LITE = "gemini-2-flash-lite"
    GEMINI_2_PRO = "gemini-2-pro"
    GEMINI_2_5_FLASH = "gemini-2.5-flash"
    GEMINI_2_5_FLASH_LITE = "gemini-2.5-flash-lite"
    GEMINI_2_5_PRO = "gemini-2.5-pro"

    DEEPSEEK_LLM_7B_BASE = "deepseek-llm-7b-base"
    DEEPSEEK_LLM_7B_CHAT = "deepseek-llm-7b-chat"
    DEEPSEEK_LLM_67B_BASE = "deepseek-llm-67b-base"
    DEEPSEEK_LLM_67B_CHAT = "deepseek-llm-67b-chat"
    DEEPSEEK_MOE_16B_BASE = "deepseek-moe-16b-base"
    DEEPSEEK_MOE_16B_CHAT = "deepseek-moe-16b-chat"
    DEEPSEEK_CODER = "deepseek-coder"
    DEEPSEEK_CODER_1_3B_BASE = "deepseek-coder-1.3b-base"
    DEEPSEEK_CODER_1_3B_INSTRUCT = "deepseek-coder-1.3b-instruct"
    DEEPSEEK_CODER_6_7B_BASE = "deepseek-coder-6.7b-base"
    DEEPSEEK_CODER_6_7B_INSTRUCT = "deepseek-coder-6.7b-instruct"
    DEEPSEEK_CODER_7B_BASE = "deepseek-coder-7b-base"
    DEEPSEEK_CODER_7B_INSTRUCT = "deepseek-coder-7b-instruct"
    DEEPSEEK_CODER_33B_BASE = "deepseek-coder-33b-base"
    DEEPSEEK_CODER_33B_INSTRUCT = "deepseek-coder-33b-instruct"
    DEEPSEEK_VL_7B_CHAT = "deepseek-vl-7b-chat"
    DEEPSEEK_VL_1_3B_BASE = "deepseek-vl-1.3b-base"
    DEEPSEEK_VL_7B_BASE = "deepseek-vl-7b-base"
    DEEPSEEK_VL_1_3B_CHAT = "deepseek-vl-1.3b-chat"
    DEEPSEEK_VL2_TINY = "deepseek-vl2-tiny"
    DEEPSEEK_VL2_SMALL = "deepseek-vl2-small"
    DEEPSEEK_VL2_BASE = "deepseek-vl2-base"
    DEEPSEEK_MATH_7B_BASE = "deepseek-math-7b-base"
    DEEPSEEK_MATH_7B_INSTRUCT = "deepseek-math-7b-instruct"
    DEEPSEEK_MATH_7B_RL = "deepseek-math-7b-rl"
    DEEPSEEK_CODER_V2_LITE_BASE = "deepseek-coder-v2-lite-base"
    DEEPSEEK_CODER_V2_LITE_INSTRUCT = "deepseek-coder-v2-lite-instruct"
    DEEPSEEK_CODER_V2_BASE = "deepseek-coder-v2-base"
    DEEPSEEK_CODER_V2_INSTRUCT = "deepseek-coder-v2-instruct"
    DEEPSEEK_V2 = "deepseek-v2"
    DEEPSEEK_V2_5 = "deepseek-v2.5"
    DEEPSEEK_V2_CHAT = "deepseek-v2-chat"
    DEEPSEEK_V2_LITE = "deepseek-v2-lite"
    DEEPSEEK_V2_LITE_CHAT = "deepseek-v2-lite-chat"
    DEEPSEEK_V3_BASE = "deepseek-v3-base"
    DEEPSEEK_V3 = "deepseek-v3"
    DEEPSEEK_PROVER_V1 = "deepseek-prover-v1"
    DEEPSEEK_PROVER_V1_5_BASE = "deepseek-prover-v1.5-base"
    DEEPSEEK_PROVER_V1_5_SFT = "deepseek-prover-v1.5-sft"
    DEEPSEEK_PROVER_V1_5_RL = "deepseek-prover-v1.5-rl"
    DEEPSEEK_PROVER_V2_7B = "deepseek-prover-v2-7b"
    DEEPSEEK_PROVER_V2_671B = "deepseek-prover-v2-671b"
    DEEPSEEK_R1_ZERO = "deepseek-r1-zero"
    DEEPSEEK_R1 = "deepseek-r1"
    DEEPSEEK_R1_DISTILL_QWEN_1_5B = "deepseek-r1-distill-qwen-1.5b"
    DEEPSEEK_R1_DISTILL_QWEN_7B = "deepseek-r1-distill-qwen-7b"
    DEEPSEEK_R1_DISTILL_QWEN_14B = "deepseek-r1-distill-qwen-14b"
    DEEPSEEK_R1_DISTILL_QWEN_32B = "deepseek-r1-distill-qwen-32b"
    DEEPSEEK_R1_DISTILL_LLAMA_8B = "deepseek-r1-distill-llama-8b"
    DEEPSEEK_R1_DISTILL_LLAMA_70B = "deepseek-r1-distill-llama-70b"
    JANUS_1B = "janus-1b"
    JANUS_PRO_7B = "janus-pro-7b"
    JANUS_1_3B = "janus-1.3b"
    JANUS_FLOW_1_3B = "janus-flow-1.3b"
    ESFT_VANILLA_LITE = "esft-vanilla-lite"
    ESFT_TOKEN_LAW_LITE = "esft-token-law-lite"
    ESFT_TOKEN_SUMMARY_LITE = "esft-token-summary-lite"
    ESFT_TOKEN_CODE_LITE = "esft-token-code-lite"
    ESFT_TOKEN_INTENT_LITE = "esft-token-intent-lite"
    ESFT_TOKEN_TRANSLATION_LITE = "esft-token-translation-lite"
    ESFT_TOKEN_MATH_LITE = "esft-token-math-lite"
    ESFT_GATE_LAW_LITE = "esft-gate-law-lite"
    ESFT_GATE_SUMMARY_LITE = "esft-gate-summary-lite"
    ESFT_GATE_TRANSLATION_LITE = "esft-gate-translation-lite"
    ESFT_GATE_CODE_LITE = "esft-gate-code-lite"
    ESFT_GATE_INTENT_LITE = "esft-gate-intent-lite"
    ESFT_GATE_MATH_LITE = "esft-gate-math-lite"

    LTS_GPT2_SM = "lts-gpt2-sm"
    DIALO_SMALL = "dialo-small"
    DIALO_MEDIUM = "dialo-medium"
    DIALO_LARGE = "dialo-large"
    BIOGPT = "biogpt"
    BIOGPT_LARGE = "biogpt-large"
    BIOGPT_LARGE_PUBMEDQA = "biogpt-large-pubmedqa"
    CODEGPT_SMALL_PY = "codegpt-small-py"
    CODEGPT_SMALL_JAVA = "codegpt-small-java"
    DOLLY_V2_7B_OLIVE_OPTIMIZED = "dolly-v2-7b-olive-optimized"
    GRIN_MOE = "grin-moe"
    PHI_TINY_MOE_INSTRUCT = "phi-tiny-moe-instruct"
    PHI_1 = "phi-1"
    PHI_1_5 = "phi-1.5"
    PHI_2 = "phi-2"
    PHI_3_MINI_INSTRUCT = "phi-3-mini-instruct"
    PHI_3_SMALL_INSTRUCT = "phi-3-small-instruct"
    PHI_3_MEDIUM_INSTRUCT = "phi-3-medium-instruct"
    PHI_3_VISION_INSTRUCT = "phi-3-vision-instruct"
    PHI_3_5_MINI_INSTRUCT = "phi-3.5-mini-instruct"
    PHI_3_5_MOE_INSTRUCT = "phi-3.5-moe-instruct"
    PHI_3_5_VISION_INSTRUCT = "phi-3.5-vision-instruct"
    PHI_4 = "phi-4"
    PHI_4_MINI_INSTRUCT = "phi-4-mini-instruct"
    PHI_4_MINI_FLASH_REASONING = "phi-4-mini-flash-reasoning"
    PHI_4_MINI_REASONING = "phi-4-mini-reasoning"
    PHI_4_REASONING = "phi-4-reasoning"
    PHI_4_REASONING_PLUS = "phi-4-reasoning-plus"
    PHI_4_MULTIMODAL_INSTRUCT = "phi-4-multimodal-instruct"
    BITNET_B1_58_2B_4T = "bitnet-b1.58-2b-4t"
    MAI_DS_R1 = "mai-ds-r1"
    MEDIPHI = "mediphi"
    MEDIPHI_INSTRUCT = "mediphi-instruct"
    MEDIPHI_PUBMED = "mediphi-pubmed"
    MEDIPHI_CLINICAL = "mediphi-clinical"
    MEDIPHI_MEDWIKI = "mediphi-medwiki"
    MEDIPHI_GUIDELINES = "mediphi-guidelines"
    MEDIPHI_MEDCODE = "mediphi-medcode"
    NATURELM_8x7b = "naturelm-8x7b"
    NATURELM_8x7b_INSTRUCT = "naturelm-8x7b-instruct"
    NEXTCODER_7B = "nextcoder-7b"
    NEXTCODER_14B = "nextcoder-14b"
    NEXTCODER_32B = "nextcoder-32b"
    MAIRA_2 = "maira-2"
    LLAVA_RAD = "llava-rad"
    LLAVA_MED_V1_5_MISTRAL_7B = "llava-med-v1.5-mistral-7b"
    LLAVA_MED_7B_DELTA = "llava-med-7b-delta"
    ORCA_2_7B = "orca-2-7b"
    ORCA_2_13B = "orca-2-13b"
    UDOP_LARGE = "udop-large"
    FLORENCE_2 = "florence-2"
    FLORENCE_2_LARGE = "florence-2-large"
    LLAMA_2_GTL_DELTA_7B = "llama-2-gtl-delta-7b"
    LLAMA_2_GTL_DELTA_13B = "llama-2-gtl-delta-13b"
    WAVECODER_ULTRA_6_7B = "wavecoder-ultra-6.7b"
    WAVECODER_PRO_6_7B = "wavecoder-pro-6.7b"
    WAVECODER_DS_6_7B = "wavecoder-ds-6.7b"
    RHO_MATH_1B = "rho-math-1b"
    RHO_MATH_1B_INTERPRETER = "rho-math-1b-interpreter"
    RHO_MATH_7B = "rho-math-7b"
    RHO_MATH_7B_INTERPRETER = "rho-math-7b-interpreter"

    QWEN_1_8B = "qwen-1.8b"
    QWEN_7B = "qwen-7b"
    QWEN_14B = "qwen-14b"
    QWEN_72B = "qwen-72b"
    QWEN_1_5_0_5B = "qwen-1.5-0.5b"
    QWEN_1_5_1_8B = "qwen-1.5-1.8b"
    QWEN_1_5_4B = "qwen-1.5-4b"
    QWEN_1_5_7B = "qwen-1.5-7b"
    QWEN_1_5_14B = "qwen-1.5-14b"
    QWEN_1_5_32B = "qwen-1.5-32b"
    QWEN_1_5_72B = "qwen-1.5-72b"
    QWEN_1_5_110B = "qwen-1.5-110b"
    QWEN_1_5_MOE = "qwen-1.5-moe"
    QWEN_2_0_5B = "qwen-2-0.5b"
    QWEN_2_0_5B_INSTRUCT = "qwen-2-0.5b-instruct"
    QWEN_2_1_5B = "qwen-2-1.5b"
    QWEN_2_1_5B_INSTRUCT = "qwen-2-1.5b-instruct"
    QWEN_2_7B = "qwen-2-7b"
    QWEN_2_7B_INSTRUCT = "qwen-2-7b-instruct"
    QWEN_2_57B = "qwen-2-57b"
    QWEN_2_57B_INSTRUCT = "qwen-2-57b-instruct"
    QWEN_2_72B = "qwen-2-72b"
    QWEN_2_72B_INSTRUCT = "qwen-2-72b-instruct"
    QWEN_2_5_0_5B = "qwen-2.5-0.5b"
    QWEN_2_5_0_5B_INSTRUCT = "qwen-2.5-0.5b-instruct"
    QWEN_2_5_1_5B = "qwen-2.5-1.5b"
    QWEN_2_5_1_5B_INSTRUCT = "qwen-2.5-1.5b-instruct"
    QWEN_2_5_3B = "qwen-2.5-3b"
    QWEN_2_5_3B_INSTRUCT = "qwen-2.5-3b-instruct"
    QWEN_2_5_7B = "qwen-2.5-7b"
    QWEN_2_5_7B_INSTRUCT = "qwen-2.5-7b-instruct"
    QWEN_2_5_14B = "qwen-2.5-14b"
    QWEN_2_5_14B_INSTRUCT = "qwen-2.5-14b-instruct"
    QWEN_2_5_32B = "qwen-2.5-32b"
    QWEN_2_5_32B_INSTRUCT = "qwen-2.5-32b-instruct"
    QWEN_2_5_72B = "qwen-2.5-72b"
    QWEN_2_5_72B_INSTRUCT = "qwen-2.5-72b-instruct"
    QWEN_2_5_CODER_0_5B = "qwen-2.5-coder-0.5b"
    QWEN_2_5_CODER_0_5B_INSTRUCT = "qwen-2.5-coder-0.5b-instruct"
    QWEN_2_5_CODER_1_5B = "qwen-2.5-coder-1.5b"
    QWEN_2_5_CODER_1_5B_INSTRUCT = "qwen-2.5-coder-1.5b-instruct"
    QWEN_2_5_CODER_3B = "qwen-2.5-coder-3b"
    QWEN_2_5_CODER_3B_INSTRUCT = "qwen-2.5-coder-3b-instruct"
    QWEN_2_5_CODER_7B = "qwen-2.5-coder-7b"
    QWEN_2_5_CODER_7B_INSTRUCT = "qwen-2.5-coder-7b-instruct"
    QWEN_2_5_CODER_14B = "qwen-2.5-coder-14b"
    QWEN_2_5_CODER_14B_INSTRUCT = "qwen-2.5-coder-14b-instruct"
    QWEN_2_5_CODER_32B = "qwen-2.5-coder-32b"
    QWEN_2_5_CODER_32B_INSTRUCT = "qwen-2.5-coder-32b-instruct"
    QWEN_2_5_MATH_1_5B = "qwen-2.5-math-1.5b"
    QWEN_2_5_MATH_1_5B_INSTRUCT = "qwen-2.5-math-1.5b-instruct"
    QWEN_2_5_MATH_7B = "qwen-2.5-math-7b"
    QWEN_2_5_MATH_7B_INSTRUCT = "qwen-2.5-math-7b-instruct"
    QWEN_2_5_MATH_72B = "qwen-2.5-math-72b"
    QWEN_2_5_MATH_72B_INSTRUCT = "qwen-2.5-math-72b-instruct"
    QWEN_2_5_MATH_RM_72B = "qwen-2.5-math-rm-72b"
    QWEN_2_5_MATH_PRM_72B = "qwen-2.5-math-prm-72b"
    QWEN_2_5_MATH_PRM_7B = "qwen-2.5-math-prm-7b"
    QWEN_2_VL_2B = "qwen-2-vl-2b"
    QWEN_2_VL_2B_INSTRUCT = "qwen-2-vl-2b-instruct"
    QWEN_2_VL_7B = "qwen-2-vl-7b"
    QWEN_2_VL_7B_INSTRUCT = "qwen-2-vl-7b-instruct"
    QWEN_2_VL_72B = "qwen-2-vl-72b"
    QWEN_2_VL_72B_INSTRUCT = "qwen-2-vl-72b-instruct"
    QWEN_2_5_VL_3B_INSTRUCT = "qwen-2.5-vl-3b-instruct"
    QWEN_2_5_VL_7B_INSTRUCT = "qwen-2.5-vl-7b-instruct"
    QWEN_2_5_VL_32B_INSTRUCT = "qwen-2.5-vl-32b-instruct"
    QWEN_2_5_VL_72B_INSTRUCT = "qwen-2.5-vl-72b-instruct"
    QWEN_2_5_OMNI_3B = "qwen-2.5-omni-3b"
    QWEN_2_5_OMNI_7B = "qwen-2.5-omni-7b"
    QWEN_2_AUDIO_7B = "qwen-2-audio-7b"
    QWEN_2_AUDIO_7B_INSTRUCT = "qwen-2-audio-7b-instruct"
    QWEN_2_MATH_1_5B = "qwen-2-math-1.5b"
    QWEN_2_MATH_1_5B_INSTRUCT = "qwen-2-math-1.5b-instruct"
    QWEN_2_MATH_7B = "qwen-2-math-7b"
    QWEN_2_MATH_7B_INSTRUCT = "qwen-2-math-7b-instruct"
    QWEN_2_MATH_72B = "qwen-2-math-72b"
    QWEN_2_MATH_72B_INSTRUCT = "qwen-2-math-72b-instruct"
    QWEN_3_0_6B = "qwen-3-0.6b"
    QWEN_3_1_7B = "qwen-3-1.7b"
    QWEN_3_4B = "qwen-3-4b"
    QWEN_3_4B_THINKING = "qwen-3-4b-thinking"
    QWEN_3_4B_INSTRUCT = "qwen-3-4b-instruct"
    QWEN_3_8B = "qwen-3-8b"
    QWEN_3_14B = "qwen-3-14b"
    QWEN_3_30B = "qwen-3-30b"
    QWEN_3_30B_THINKING = "qwen-3-30b-thinking"
    QWEN_3_30B_INSTRUCT = "qwen-3-30b-instruct"
    QWEN_3_32B = "qwen-3-32b"
    QWEN_3_235B = "qwen-3-235b"
    QWEN_3_235B_THINKING = "qwen-3-235b-thinking"
    QWEN_3_235B_INSTRUCT = "qwen-3-235b-instruct"
    QWEN_3_CODER_30B_INSTRUCT = "qwen-3-coder-30b-instruct"
    QWEN_3_CODER_480B_INSTRUCT = "qwen-3-coder-480b-instruct"
    QWQ_32B = "qwq-32b"
    QVQ_72B = "qvq-72b"
    CODEQWEN_1_5_7B = "codeqwen-1.5-7b"

    YI_34B = "yi-34b"
    YI_34B_CHAT = "yi-34b-chat"
    YI_34B_CHAT_8BITS = "yi-34b-chat-8bits"
    YI_34B_CHAT_4BITS = "yi-34b-chat-4bits"
    YI_9B = "yi-9b"
    YI_6B_CHAT = "yi-6b-chat"
    YI_6B = "yi-6b"
    YI_1_5_34B = "yi-1.5-34b"
    YI_1_5_34B_CHAT = "yi-1.5-34b-chat"
    YI_1_5_9B = "yi-1.5-9b"
    YI_1_5_9B_CHAT = "yi-1.5-9b-chat"
    YI_1_5_6B = "yi-1.5-6b"
    YI_1_5_6B_CHAT = "yi-1.5-6b-chat"
    YI_VL_34B = "yi-vl-34b"
    YI_VL_6B = "yi-vl-6b"
    YI_CODER_9B_CHAT = "yi-coder-9b-chat"
    YI_CODER_9B = "yi-coder-9b"
    YI_CODER_1_5B_CHAT = "yi-coder-1.5b-chat"
    YI_CODER_1_5B = "yi-coder-1.5b"

    DEFAULT = "unknown"
