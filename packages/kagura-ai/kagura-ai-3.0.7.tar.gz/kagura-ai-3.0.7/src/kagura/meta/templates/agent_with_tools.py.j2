"""{{ spec.description }}

Auto-generated by Kagura Meta Agent
Created: {{ timestamp }}
Kagura Version: {{ kagura_version }}
"""

from kagura import agent
{% if "code_executor" in spec.tools %}
from kagura.core.executor import CodeExecutor
{% endif %}


@agent(
    model="{{ spec.model | default('gpt-5-mini') }}",
    temperature=0.7,
    {% if "code_executor" in spec.tools %}
    tools=[CodeExecutor()],
    {% endif %}
)
async def {{ spec.name }}(input_data: {{ spec.input_type }}) -> {{ spec.output_type }}:
    """{{ spec.description }}

    Args:
        input_data: {{ spec.input_type }} - Input data

    Returns:
        {{ spec.output_type }} - Generated result

    Tools:
    {% for tool in spec.tools %}
    - {{ tool }}: {{ tool_descriptions.get(tool, "Tool functionality") }}
    {% endfor %}
    """
    # System prompt with tool instructions
    system_prompt = """{{ spec.system_prompt }}

Available tools:
{% for tool in spec.tools %}
- {{ tool }}: {{ tool_descriptions.get(tool, "Tool functionality") }}
{% endfor %}
"""

    # Template variable for LLM (will be rendered at runtime)
    return f"{system_prompt}\n\nInput: {input_data}"

{% if spec.examples %}

# Example usage:
if __name__ == "__main__":
    import asyncio

    async def main():
        # Examples
        {% for example in spec.examples %}
        result = await {{ spec.name }}("{{ example.input }}")
        print(f"Input: {{ example.input }}")
        print(f"Output: {result}")
        print()
        {% endfor %}

    asyncio.run(main())
{% endif %}
