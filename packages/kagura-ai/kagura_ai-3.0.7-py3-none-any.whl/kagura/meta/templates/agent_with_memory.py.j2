"""{{ spec.description }}

Auto-generated by Kagura Meta Agent
Created: {{ timestamp }}
Kagura Version: {{ kagura_version }}
"""

from kagura import agent
from kagura.core.memory import MemoryManager


@agent(
    model="{{ spec.model | default('gpt-5-mini') }}",
    temperature=0.7,
    enable_memory=True,
    max_messages=100,
)
async def {{ spec.name }}(
    input_data: {{ spec.input_type }},
    memory: MemoryManager
) -> {{ spec.output_type }}:
    """{{ spec.description }}

    This agent has conversation memory enabled and can remember
    previous interactions.

    Args:
        input_data: {{ spec.input_type }} - Input data
        memory: MemoryManager - Conversation memory

    Returns:
        {{ spec.output_type }} - Generated result
    """
    # Add user message to memory
    memory.add_message("user", str(input_data))

    # System prompt for this agent
    system_prompt = """{{ spec.system_prompt }}

You have access to conversation history through the memory system.
Use this to maintain context across interactions.
"""

    # Template variable for LLM (will be rendered at runtime)
    return f"{system_prompt}\n\nInput: {input_data}"

{% if spec.examples %}

# Example usage:
if __name__ == "__main__":
    import asyncio

    async def main():
        # Examples with memory
        {% for example in spec.examples %}
        result = await {{ spec.name }}("{{ example.input }}")
        print(f"Input: {{ example.input }}")
        print(f"Output: {result}")
        print()
        {% endfor %}

    asyncio.run(main())
{% endif %}
