"""{{ spec.description }}

Auto-generated by Kagura Meta Agent
Created: {{ timestamp }}
Kagura Version: {{ kagura_version }}
"""

from kagura import agent


@agent(
    model="{{ spec.model | default('gpt-5-mini') }}",
    temperature=0.7,
)
async def {{ spec.name }}(input_data: {{ spec.input_type }}) -> {{ spec.output_type }}:
    """{{ spec.description }}

    Args:
        input_data: {{ spec.input_type }} - Input data

    Returns:
        {{ spec.output_type }} - Generated result
    """
    # System prompt for this agent
    system_prompt = """{{ spec.system_prompt }}"""

    # Template variable for LLM (will be rendered at runtime)
    return f"{system_prompt}\n\nInput: {input_data}"

{% if spec.examples %}

# Example usage:
if __name__ == "__main__":
    import asyncio

    async def main():
        # Examples
        {% for example in spec.examples %}
        result = await {{ spec.name }}("{{ example.input }}")
        print(f"Input: {{ example.input }}")
        print(f"Output: {result}")
        print()
        {% endfor %}

    asyncio.run(main())
{% endif %}
