# ğŸš€ FloTorch-core

**FloTorch-core** is a modular and extensible Python framework for building LLM-powered RAG (Retrieval-Augmented Generation) pipelines. It offers plug-and-play components for embeddings, chunking, retrieval, gateway-based LLM calls, and RAG evaluation.

---

## âœ¨ Features

- ğŸ§© Text Chunking (Fixed-size, Hierarchical)
- ğŸ§  Embedding Models (Titan, Cohere, Bedrock)
- ğŸ” Document Retrieval (OpenSearch + Vector Storage)
- ğŸ’» Bedrock/sagemaker/gateway inferencer
- ğŸ”Œ Unified LLM Gateway (OpenAI, Bedrock, Ollama, etc.)
- ğŸ“ RAG Evaluation (RAGAS Metrics)
- â˜ï¸ AWS Integration (S3, DynamoDB, Lambda)
- ğŸ§¢ Built-in Testing Support

---

## ğŸ“† Installation

```bash
pip install FloTorch-core
```

To install development dependencies:

```bash
pip install FloTorch-core[dev]
```

---


## ğŸ“‚ Project Structure

```
flotorch/
â”œâ”€â”€ inferencer/         # LLM gateway/bedrock/sagemaker interface
â”œâ”€â”€ embedding/          # Embedding models
â”œâ”€â”€ chunking/           # Text chunking logic
â”œâ”€â”€ evaluator/          # RAG evaluation (RAGAS)
â”œâ”€â”€ storage/            # Vector DB, S3, DynamoDB
â”œâ”€â”€ util/               # Utilities and helpers
â”œâ”€â”€ rerank/             # Ranking documents
â”œâ”€â”€ guardrails/         # Enabling guardrails
â”œâ”€â”€ reader/             # reader for json/pdf
```

---

## ğŸ“– Usage Example

### Reader

```
from flotorch_core.reader.json_reader import JSONReader
from flotorch_core.storage.s3_storage import S3StorageProvider

json_reader = JSONReader(S3StorageProvider(<S3 bucket>))
json_reader.read(<path>)
```

### Embedding
```
from flotorch_core.embedding.embedding_registry import embedding_registry

embedding_class = embedding_registry.get_model(<model id>)

# model id example: amazon.titan-text-express-v1, amazon.titan-embed-text-v2:0, cohere.embed-multilingual-v3
```

### Vector storage (opensearch)
```
from flotorch_core.storage.db.vector.open_search import OpenSearchClient

vector_storage_object = OpenSearchClient(
    <opensearch_host>, 
    <opensearch_port>, 
    <opensearch_username>, 
    <opensearch_password>, 
    <index_id>, 
    <embedding object>
)
```

### Vector storage (bedrock knowledgebase)
```
from flotorch_core.storage.db.vector.bedrock_knowledgebase_storage import BedrockKnowledgeBaseStorage

vector_storage_object = BedrockKnowledgeBaseStorage(
    knowledge_base_id=<knowledge_base_id>,
    region=<aws_region>
)
```

### Guardrails over vector storage
```
from flotorch_core.storage.db.vector.guardrails_vector_storage import GuardRailsVectorStorage

base_guardrails = BedrockGuardrail(<guardrail_id>, <guardrail_version>, <aws_region>)            
vector_storage_object = GuardRailsVectorStorage(
    vector_storage_object, 
    base_guardrails,
    <enable_prompt_guardrails(True/False)>,
    <enable_context_guardrails(True/False)>
)
```

### Inferencer
```
from flotorch_core.inferencer.bedrock_inferencer import BedrockInferencer
from flotorch_core.inferencer.gateway_inferencer import GatewayInferencer
from flotorch_core.inferencer.sagemaker_inferencer import SageMakerInferencer

inferencer = BedrockInferencer(
    <model_id>, 
    <region>, 
    <number of n_shot_prompts>, 
    <temperature>, 
    <n_shot_prompt_guide_obj>
)

inferencer = GatewayInferencer(
    model_id=<model_id>, 
    api_key=<api_key>, 
    base_url=<base_url>, 
    n_shot_prompts=<n_shot_prompts>, 
    n_shot_prompt_guide_obj=<n_shot_prompt_guide_obj>
)

inferencer = SageMakerInferencer(
    <model_id>, 
    <region>, 
    <arn_role>, 
    <n_shot_prompts>, 
    <temperature>, 
    <n_shot_prompt_guide_obj>
)
```

### GuardRail over inferencer

```
from flotorch_core.inferencer.guardrails.guardrails_inferencer import GuardRailsInferencer

inferencer = GuardRailsInferencer(inferencer, base_guardrails)
```

---


## ğŸ“¬ Maintainer

**Shiva Krishna**  
ğŸ“§ Email: shiva.krishnaah@gmail.com

**Adil Raza**  
ğŸ“§ Email: adilraza.9752@gmail.com

---

## ğŸ“„ License

This project is licensed under the [MIT License](LICENSE).

---

## ğŸŒ Links

- GitHub: [https://github.com/FissionAI/flotorch-core](https://github.com/FissionAI/flotorch-core)

