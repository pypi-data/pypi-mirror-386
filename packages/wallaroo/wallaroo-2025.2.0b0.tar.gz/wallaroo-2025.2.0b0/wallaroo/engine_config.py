import json
from abc import ABC, abstractmethod
from enum import Enum
from typing import Any, Dict, Literal, Optional, Union

import pydantic
from pydantic import BaseModel, ConfigDict, Field
from typing_extensions import Protocol, runtime_checkable

import wallaroo.config as wallaroo_config
from wallaroo.wallaroo_ml_ops_api_client.types import Unset

from .wallaroo_ml_ops_api_client.models.acceleration_type_4 import (
    AccelerationType4 as OpenapiQaicWithConfig,
)
from .wallaroo_ml_ops_api_client.models.qaic_config import (
    QaicConfig as OpenapiQaicConfig,
)


class Architecture(str, Enum):
    """
    An Enum to represent the supported processor architecture.
    """

    X86 = "x86"
    ARM = "arm"
    Power10 = "power10"

    @classmethod
    def default(cls) -> "Architecture":
        _config = wallaroo_config._config
        if _config is None or _config.get("default_arch") is None:
            raise Exception(
                "Your Client hasn't been properly set up. Try creating it again or contact Wallaroo support."
            )
        return Architecture(_config["default_arch"])

    def __str__(self) -> str:
        return str(self.value)


class Acceleration(str, Enum):
    """
    An Enum to represent the supported acceleration options.
    """

    _None = "none"
    CUDA = "cuda"
    Jetson = "jetson"
    OpenVINO = "openvino"
    QAIC = "qaic"

    def __str__(self) -> str:
        return str(self.value)

    @classmethod
    def default(cls) -> "Acceleration":
        return cls._None

    def default_acceleration_with_config(self) -> "AccelerationWithConfig":
        if self == Acceleration.QAIC:
            return QaicWithConfig()
        raise ModelOptimizationConfigError()

    def is_applicable(self, arch: Architecture) -> bool:
        match self:
            case Acceleration._None | Acceleration.CUDA:
                return True
            case Acceleration.Jetson:
                return arch == Architecture.ARM
            case Acceleration.OpenVINO | Acceleration.QAIC:
                return arch == Architecture.X86
            case _:
                return False

    def requires_config(self) -> bool:
        return self == Acceleration.QAIC

    def with_config(self, config: "QaicConfig") -> "AccelerationWithConfig":
        """Create an acceleration with a config. Required only for the Qaic acceleration for now.

        :param config: QaicConfig The config to use for the acceleration.
        :return: AccelerationWithConfig The acceleration with the given config.

        :raise ModelOptimizationConfigError: If the acceleration is not supported.
        """
        if self == Acceleration.QAIC:
            try:
                return QaicWithConfig(accel=self, config=config)
            except pydantic.ValidationError:
                raise ModelOptimizationConfigError()
        raise ModelOptimizationConfigError()


@runtime_checkable
class OpenapiAccelerationWithConfig(Protocol):
    def to_dict(self) -> Dict[str, Any]: ...

    @classmethod
    def from_dict(cls, data: Dict[str, Any]) -> "OpenapiAccelerationWithConfig": ...


class AccelerationWithConfig(BaseModel, ABC):
    """
    A base class for all acceleration that require a config.
    """

    model_config = ConfigDict(
        arbitrary_types_allowed=True,
        extra="forbid",
        protected_namespaces=(),
    )

    accel: Acceleration
    config: BaseModel

    def is_applicable(self, arch: Architecture) -> bool:
        return self.accel.is_applicable(arch)

    @abstractmethod
    def _to_openapi_acceleration_with_config(self) -> OpenapiAccelerationWithConfig:
        """Convert the config to the autogenerated OpenAPI version for validation."""

    def to_dict(self) -> Dict[str, Any]:
        """Convert the config to a dictionary."""
        return self._to_openapi_acceleration_with_config().to_dict()

    def _parse_to_openapi(self) -> Dict[str, Any]:
        """Parse the config to the autogenerated OpenAPI version."""
        return {
            self.accel.value: self.config.model_dump(),
        }


# initialize the equivalent OpenAPI config so we can get the default values
_open_api_qaic_config: OpenapiQaicConfig = OpenapiQaicConfig()


class QaicConfig(BaseModel):
    """
    A config for the Qaic acceleration.
    """

    model_config = ConfigDict(
        arbitrary_types_allowed=True,
        extra="forbid",
        use_enum_values=True,
        protected_namespaces=(),
    )

    num_cores: Union[Unset, int] = Field(
        default=_open_api_qaic_config.num_cores,
        ge=1,
        validate_default=True,
        description="Number of cores used to compile the model. Defaults to `16`.",
    )
    num_devices: Union[Unset, int] = Field(
        default=_open_api_qaic_config.num_devices,
        ge=1,
        validate_default=True,
        description="Number of SoCs in a given card to compile the model for. Each card (e.g. AI100) has 4 SoCs. Defaults to `1`.",
    )
    ctx_len: Union[Unset, int] = Field(
        default=_open_api_qaic_config.ctx_len,
        ge=1,
        validate_default=True,
        description="Maximum context that the compiled model can remember. Defaults to `128`.",
    )
    prefill_seq_len: Union[Unset, int] = Field(
        default=_open_api_qaic_config.prefill_seq_len,
        ge=1,
        validate_default=True,
        description="The length of the Prefill prompt. Defaults to `32`.",
    )
    full_batch_size: Union[Unset, int] = Field(
        default=_open_api_qaic_config.full_batch_size,
        ge=1,
        description="Maximum number of sequences per iteration. Set to enable continuous batching mode. Defaults to `None`.",
    )
    mxfp6_matmul: Union[Unset, bool] = Field(
        default=_open_api_qaic_config.mxfp6_matmul,
        validate_default=True,
        description="Enable compilation for MXFP6 precision. Defaults to `False`.",
    )
    mxint8_kv_cache: Union[Unset, bool] = Field(
        default=_open_api_qaic_config.mxint8_kv_cache,
        validate_default=True,
        description="Compress Present/Past KV to MXINT8. Defaults to `False`.",
    )
    aic_enable_depth_first: Union[Unset, bool] = Field(
        default=_open_api_qaic_config.aic_enable_depth_first,
        validate_default=True,
        description="Enables DFS with default memory size. Defaults to `False`.",
    )


class QaicWithConfig(AccelerationWithConfig):
    accel: Literal[Acceleration.QAIC] = Acceleration.QAIC
    config: QaicConfig = QaicConfig()

    def _to_openapi_acceleration_with_config(self) -> OpenapiQaicWithConfig:
        """Convert the config to the autogenerated OpenAPI version for validation."""
        return OpenapiQaicWithConfig.from_dict(self._parse_to_openapi())


class EngineConfig:
    """Wraps an engine config."""

    # NOTE: refer to /conductor/helm/payloads/deployment-manager/helm/default-values and
    # /conductor/helm/payloads/deployment-manager/helm/orchestra-deployment.yaml
    # for reasonable defaults
    def __init__(
        self,
        cpus: int,
        gpus: Optional[int] = 0,
        inference_channel_size: Optional[int] = None,
        model_concurrency: Optional[int] = None,
        pipeline_config_directory: Optional[str] = None,
        model_config_directory: Optional[str] = None,
        model_directory: Optional[str] = None,
        audit_logging: bool = False,
        arch: Architecture = Architecture.X86,
        accel: Acceleration = Acceleration._None,
    ) -> None:
        self._cpus = cpus
        self._gpus = gpus
        self._inference_channel_size = (
            inference_channel_size if inference_channel_size else 10000
        )
        self._model_concurrency = model_concurrency if model_concurrency else 1
        self._audit_logging = audit_logging
        self._pipeline_config_directory = pipeline_config_directory
        self._model_config_directory = model_config_directory
        self._model_directory = model_directory
        self._arch = arch
        self._accel = accel

    # TODO: Is there a better way to keep this in sync with our helm chart?
    def _to_dict(self) -> Dict[str, Any]:
        """Generate a dictionary representation for use to coversion to json or yaml"""
        config: Dict[str, Any] = {
            "cpus": self._cpus,
            "gpus": self._gpus,
            "arch": str(self._arch),
            "accel": str(self._accel),
        }
        if self._inference_channel_size:
            config["inference_channel_size"] = self._inference_channel_size
        if self._model_concurrency:
            config["model_server"] = {"model_concurrency": self._model_concurrency}
        config["audit_logging"] = {"enabled": self._audit_logging}
        return config

    def to_json(self) -> str:
        """Returns a json representation of this object"""
        return json.dumps(self._to_dict())


class InvalidAccelerationError(Exception):
    """Raised when the specified acceleration is incompatible with the given platform architecture."""

    def __init__(self):
        super().__init__(
            "The specified model architecture configuration is not available. "
            "Please try this operation again using a different configuration "
            "or contact support@wallaroo.ai for questions or help."
        )


class ModelOptimizationConfigError(Exception):
    """Raised when the specified model optimization configuration is not available."""

    def __init__(self):
        super().__init__(
            "The specified model optimization configuration is not available. "
            "Please try this operation again using a different configuration "
            "or contact Wallaroo at support@wallaroo.ai for questions or help."
        )
