{"guide": {"name": "object-detection-from-video", "category": "streaming", "pretty_category": "Streaming", "guide_index": 3, "absolute_index": 43, "pretty_name": "Object Detection From Video", "content": "# Streaming Object Detection from Video\n\n\n\nIn this guide we'll use the [RT-DETR](https://huggingface.co/docs/transformers/en/model_doc/rt_detr) model to detect objects in a user uploaded video. We'll stream the results from the server using the new video streaming features introduced in Gradio 5.0.\n\n![video_object_detection_stream_latest](https://github.com/user-attachments/assets/4e27ac58-5ded-495d-9e0d-5e87e68b1355)\n\n## Setting up the Model\n\nFirst, we'll install the following requirements in our system:\n\n```\nopencv-python\ntorch\ntransformers>=4.43.0\nspaces\n```\n\nThen, we'll download the model from the Hugging Face Hub:\n\n```python\nfrom transformers import RTDetrForObjectDetection, RTDetrImageProcessor\n\nimage_processor = RTDetrImageProcessor.from_pretrained(\"PekingU/rtdetr_r50vd\")\nmodel = RTDetrForObjectDetection.from_pretrained(\"PekingU/rtdetr_r50vd\").to(\"cuda\")\n```\nWe're moving the model to the GPU. We'll be deploying our model to Hugging Face Spaces and running the inference in the [free ZeroGPU cluster](https://huggingface.co/zero-gpu-explorers). \n\n\n## The Inference Function\n\nOur inference function will accept a video and a desired confidence threshold.\nObject detection models identify many objects and assign a confidence score to each object. The lower the confidence, the higher the chance of a false positive. So we will let our users set the confidence threshold.\n\nOur function will iterate over the frames in the video and run the RT-DETR model over each frame.\nWe will then draw the bounding boxes for each detected object in the frame and save the frame to a new output video.\nThe function will yield each output video in chunks of two seconds.\n\nIn order to keep inference times as low as possible on ZeroGPU (there is a time-based quota),\nwe will halve the original frames-per-second in the output video and resize the input frames to be half the original \nsize before running the model.\n\nThe code for the inference function is below - we'll go over it piece by piece.\n\n```python\nimport spaces\nimport cv2\nfrom PIL import Image\nimport torch\nimport time\nimport numpy as np\nimport uuid\n\nfrom draw_boxes import draw_bounding_boxes\n\nSUBSAMPLE = 2\n\n@spaces.GPU\ndef stream_object_detection(video, conf_threshold):\n    cap = cv2.VideoCapture(video)\n\n    # This means we will output mp4 videos\n    video_codec = cv2.VideoWriter_fourcc(*\"mp4v\") # type: ignore\n    fps = int(cap.get(cv2.CAP_PROP_FPS))\n\n    desired_fps = fps // SUBSAMPLE\n    width  = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) // 2\n    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) // 2\n\n    iterating, frame = cap.read()\n\n    n_frames = 0\n\n    # Use UUID to create a unique video file\n    output_video_name = f\"output_{uuid.uuid4()}.mp4\"\n\n    # Output Video\n    output_video = cv2.VideoWriter(output_video_name, video_codec, desired_fps, (width, height)) # type: ignore\n    batch = []\n\n    while iterating:\n        frame = cv2.resize( frame, (0,0), fx=0.5, fy=0.5)\n        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n        if n_frames % SUBSAMPLE == 0:\n            batch.append(frame)\n        if len(batch) == 2 * desired_fps:\n            inputs = image_processor(images=batch, return_tensors=\"pt\").to(\"cuda\")\n\n            with torch.no_grad():\n                outputs = model(**inputs)\n\n            boxes = image_processor.post_process_object_detection(\n                outputs,\n                target_sizes=torch.tensor([(height, width)] * len(batch)),\n                threshold=conf_threshold)\n            \n            for i, (array, box) in enumerate(zip(batch, boxes)):\n                pil_image = draw_bounding_boxes(Image.fromarray(array), box, model, conf_threshold)\n                frame = np.array(pil_image)\n                # Convert RGB to BGR\n                frame = frame[:, :, ::-1].copy()\n                output_video.write(frame)\n\n            batch = []\n            output_video.release()\n            yield output_video_name\n            output_video_name = f\"output_{uuid.uuid4()}.mp4\"\n            output_video = cv2.VideoWriter(output_video_name, video_codec, desired_fps, (width, height)) # type: ignore\n\n        iterating, frame = cap.read()\n        n_frames += 1\n```\n\n1. **Reading from the Video**\n\nOne of the industry standards for creating videos in python is OpenCV so we will use it in this app.\n\nThe `cap` variable is how we will read from the input video. Whenever we call `cap.read()`, we are reading the next frame in the video.\n\nIn order to stream video in Gradio, we need to yield a different video file for each \"chunk\" of the output video.\nWe create the next video file to write to with the `output_video = cv2.VideoWriter(output_video_name, video_codec, desired_fps, (width, height))` line. The `video_codec` is how we specify the type of video file. Only \"mp4\" and \"ts\" files are supported for video sreaming at the moment.\n\n\n2. **The Inference Loop**\n\nFor each frame in the video, we will resize it to be half the size. OpenCV reads files in `BGR` format, so will convert to the expected `RGB` format of transfomers. That's what the first two lines of the while loop are doing. \n\nWe take every other frame and add it to a `batch` list so that the output video is half the original FPS. When the batch covers two seconds of video, we will run the model. The two second threshold was chosen to keep the processing time of each batch small enough so that video is smoothly displayed in the server while not requiring too many separate forward passes. In order for video streaming to work properly in Gradio, the batch size should be at least 1 second. \n\nWe run the forward pass of the model and then use the `post_process_object_detection` method of the model to scale the detected bounding boxes to the size of the input frame.\n\nWe make use of a custom function to draw the bounding boxes (source [here](https://huggingface.co/spaces/gradio/rt-detr-object-detection/blob/main/draw_boxes.py#L14)). We then have to convert from `RGB` to `BGR` before writing back to the output video.\n\nOnce we have finished processing the batch, we create a new output video file for the next batch.\n\n## The Gradio Demo\n\nThe UI code is pretty similar to other kinds of Gradio apps. \nWe'll use a standard two-column layout so that users can see the input and output videos side by side.\n\nIn order for streaming to work, we have to set `streaming=True` in the output video. Setting the video\nto autoplay is not necessary but it's a better experience for users.\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as app:\n    gr.HTML(\n        \"\"\"\n    <h1 style='text-align: center'>\n    Video Object Detection with <a href='https://huggingface.co/PekingU/rtdetr_r101vd_coco_o365' target='_blank'>RT-DETR</a>\n    </h1>\n    \"\"\")\n    with gr.Row():\n        with gr.Column():\n            video = gr.Video(label=\"Video Source\")\n            conf_threshold = gr.Slider(\n                label=\"Confidence Threshold\",\n                minimum=0.0,\n                maximum=1.0,\n                step=0.05,\n                value=0.30,\n            )\n        with gr.Column():\n            output_video = gr.Video(label=\"Processed Video\", streaming=True, autoplay=True)\n\n    video.upload(\n        fn=stream_object_detection,\n        inputs=[video, conf_threshold],\n        outputs=[output_video],\n    )\n\n\n```\n\n\n## Conclusion\n\nYou can check out our demo hosted on Hugging Face Spaces [here](https://huggingface.co/spaces/gradio/rt-detr-object-detection). \n\nIt is also embedded on this page below\n\n<gradio-app space='gradio/rt-detr-object-detection'></gradio-app>", "tags": ["VISION", "STREAMING", "VIDEO"], "spaces": [], "url": "/guides/object-detection-from-video/", "contributor": null}}