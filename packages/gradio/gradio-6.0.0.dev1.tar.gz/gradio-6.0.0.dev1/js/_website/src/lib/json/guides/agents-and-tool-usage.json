{"guide": {"name": "agents-and-tool-usage", "category": "chatbots", "pretty_category": "Chatbots", "guide_index": 3, "absolute_index": 31, "pretty_name": "Agents And Tool Usage", "content": "# Building a UI for an LLM Agent\n\n\n\nThe Gradio Chatbot can natively display intermediate thoughts and tool usage in a collapsible accordion next to a chat message. This makes it perfect for creating UIs for LLM agents and chain-of-thought (CoT) or reasoning demos. This guide will show you how to display thoughts and tool usage with `gr.Chatbot` and `gr.ChatInterface`.\n\n![](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/gradio-guides/nested-thoughts.png)\n\n## The `ChatMessage` dataclass\n\nEvery element of the chatbot value is a dictionary of `role` and `content` keys. You can always use plain python dictionaries to add new values to the chatbot but Gradio also provides the `ChatMessage` dataclass to help you with IDE autocompletion. The schema of `ChatMessage` is as follows:\n\n ```py\nMessageContent = Union[str, FileDataDict, FileData, Component]\n\n@dataclass\nclass ChatMessage:\n    content: MessageContent | [MessageContent]\n    role: Literal[\"user\", \"assistant\"]\n    metadata: MetadataDict = None\n    options: list[OptionDict] = None\n\nclass MetadataDict(TypedDict):\n    title: NotRequired[str]\n    id: NotRequired[int | str]\n    parent_id: NotRequired[int | str]\n    log: NotRequired[str]\n    duration: NotRequired[float]\n    status: NotRequired[Literal[\"pending\", \"done\"]]\n\nclass OptionDict(TypedDict):\n    label: NotRequired[str]\n    value: str\n ```\n\n\nFor our purposes, the most important key is the `metadata` key, which accepts a dictionary. If this dictionary includes a `title` for the message, it will be displayed in a collapsible accordion representing a thought. It's that simple! Take a look at this example:\n\n\n```python\nimport gradio as gr\n\nwith gr.Blocks() as demo:\n    chatbot = gr.Chatbot(\n        value=[\n            gr.ChatMessage(\n                role=\"user\", \n                content=\"What is the weather in San Francisco?\"\n            ),\n            gr.ChatMessage(\n                role=\"assistant\", \n                content=\"I need to use the weather API tool?\",\n                metadata={\"title\":  \"\ud83e\udde0 Thinking\"}\n            )\n        ]\n    )\n\ndemo.launch()\n```\n\n\n\nIn addition to `title`, the dictionary provided to `metadata` can take several optional keys:\n\n* `log`: an optional string value to be displayed in a subdued font next to the thought title.\n* `duration`: an optional numeric value representing the duration of the thought/tool usage, in seconds. Displayed in a subdued font next inside parentheses next to the thought title.\n* `status`: if set to `\"pending\"`, a spinner appears next to the thought title and the accordion is initialized open.  If `status` is `\"done\"`, the thought accordion is initialized closed. If `status` is not provided, the thought accordion is initialized open and no spinner is displayed.\n* `id` and `parent_id`: if these are provided, they can be used to nest thoughts inside other thoughts.\n\nBelow, we show several complete examples of using `gr.Chatbot` and `gr.ChatInterface` to display tool use or thinking UIs.\n\n## Building with Agents\n\n### A real example using transformers.agents\n\nWe'll create a Gradio application simple agent that has access to a text-to-image tool.\n            <div class='tip'>\n                <span class=\"inline-flex\" style=\"align-items: baseline\">\n                    <svg class=\"self-center w-5 h-5 mx-1\" xmlns=\"http://www.w3.org/2000/svg\" width=\"800px\" height=\"800px\" viewBox=\"0 0 24 24\" fill=\"currentColor\">\n                        <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M9.25 18.7089C9.25 18.2894 9.58579 17.9494 10 17.9494H14C14.4142 17.9494 14.75 18.2894 14.75 18.7089C14.75 19.1283 14.4142 19.4684 14 19.4684H10C9.58579 19.4684 9.25 19.1283 9.25 18.7089ZM9.91667 21.2405C9.91667 20.821 10.2525 20.481 10.6667 20.481H13.3333C13.7475 20.481 14.0833 20.821 14.0833 21.2405C14.0833 21.66 13.7475 22 13.3333 22H10.6667C10.2525 22 9.91667 21.66 9.91667 21.2405Z\"/>\n                        <path d=\"M7.41058 13.8283L8.51463 14.8807C8.82437 15.1759 9 15.5875 9 16.0182C9 16.6653 9.518 17.1899 10.157 17.1899H13.843C14.482 17.1899 15 16.6653 15 16.0182C15 15.5875 15.1756 15.1759 15.4854 14.8807L16.5894 13.8283C18.1306 12.3481 18.9912 10.4034 18.9999 8.3817L19 8.29678C19 4.84243 15.866 2 12 2C8.13401 2 5 4.84243 5 8.29678L5.00007 8.3817C5.00875 10.4034 5.86939 12.3481 7.41058 13.8283Z\"/>\n                    </svg>\n                <span><strong>Tip:</strong></span>\n                </span>\n                <p>Make sure you read the <a href=\"https://huggingface.co/docs/smolagents/index\">smolagents documentation</a> first</p>\n            </div>\n                \n\nWe'll start by importing the necessary classes from transformers and gradio. \n\n```python\nimport gradio as gr\nfrom gradio import ChatMessage\nfrom transformers import Tool, ReactCodeAgent  # type: ignore\nfrom transformers.agents import stream_to_gradio, HfApiEngine  # type: ignore\n\n# Import tool from Hub\nimage_generation_tool = Tool.from_space(\n    space_id=\"black-forest-labs/FLUX.1-schnell\",\n    name=\"image_generator\",\n    description=\"Generates an image following your prompt. Returns a PIL Image.\",\n    api_name=\"/infer\",\n)\n\nllm_engine = HfApiEngine(\"Qwen/Qwen2.5-Coder-32B-Instruct\")\n# Initialize the agent with both tools and engine\nagent = ReactCodeAgent(tools=[image_generation_tool], llm_engine=llm_engine)\n```\n\nThen we'll build the UI:\n\n```python\ndef interact_with_agent(prompt, history):\n    messages = []\n    yield messages\n    for msg in stream_to_gradio(agent, prompt):\n        messages.append(asdict(msg))\n        yield messages\n    yield messages\n\n\ndemo = gr.ChatInterface(\n    interact_with_agent,\n    chatbot= gr.Chatbot(\n        label=\"Agent\",\n        avatar_images=(\n            None,\n            \"https://em-content.zobj.net/source/twitter/53/robot-face_1f916.png\",\n        ),\n    ),\n    examples=[\n        [\"Generate an image of an astronaut riding an alligator\"],\n        [\"I am writing a children's book for my daughter. Can you help me with some illustrations?\"],\n    ],\n)\n```\n\nYou can see the full demo code [here](https://huggingface.co/spaces/gradio/agent_chatbot/blob/main/app.py).\n\n\n![transformers_agent_code](https://github.com/freddyaboulton/freddyboulton/assets/41651716/c8d21336-e0e6-4878-88ea-e6fcfef3552d)\n\n\n### A real example using langchain agents\n\nWe'll create a UI for langchain agent that has access to a search engine.\n\nWe'll begin with imports and setting up the langchain agent. Note that you'll need an .env file with the following environment variables set - \n\n```\nSERPAPI_API_KEY=\nHF_TOKEN=\nOPENAI_API_KEY=\n```\n\n```python\nfrom langchain import hub\nfrom langchain.agents import AgentExecutor, create_openai_tools_agent, load_tools\nfrom langchain_openai import ChatOpenAI\nfrom gradio import ChatMessage\nimport gradio as gr\n\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nmodel = ChatOpenAI(temperature=0, streaming=True)\n\ntools = load_tools([\"serpapi\"])\n\n# Get the prompt to use - you can modify this!\nprompt = hub.pull(\"hwchase17/openai-tools-agent\")\nagent = create_openai_tools_agent(\n    model.with_config({\"tags\": [\"agent_llm\"]}), tools, prompt\n)\nagent_executor = AgentExecutor(agent=agent, tools=tools).with_config(\n    {\"run_name\": \"Agent\"}\n)\n```\n\nThen we'll create the Gradio UI\n\n```python\nasync def interact_with_langchain_agent(prompt, messages):\n    messages.append(ChatMessage(role=\"user\", content=prompt))\n    yield messages\n    async for chunk in agent_executor.astream(\n        {\"input\": prompt}\n    ):\n        if \"steps\" in chunk:\n            for step in chunk[\"steps\"]:\n                messages.append(ChatMessage(role=\"assistant\", content=step.action.log,\n                                  metadata={\"title\": f\"\ud83d\udee0\ufe0f Used tool {step.action.tool}\"}))\n                yield messages\n        if \"output\" in chunk:\n            messages.append(ChatMessage(role=\"assistant\", content=chunk[\"output\"]))\n            yield messages\n\n\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# Chat with a LangChain Agent \ud83e\udd9c\u26d3\ufe0f and see its thoughts \ud83d\udcad\")\n    chatbot = gr.Chatbot(\n        label=\"Agent\",\n        avatar_images=(\n            None,\n            \"https://em-content.zobj.net/source/twitter/141/parrot_1f99c.png\",\n        ),\n    )\n    input = gr.Textbox(lines=1, label=\"Chat Message\")\n    input.submit(interact_with_langchain_agent, [input_2, chatbot_2], [chatbot_2])\n\ndemo.launch()\n```\n\n![langchain_agent_code](https://github.com/freddyaboulton/freddyboulton/assets/41651716/762283e5-3937-47e5-89e0-79657279ea67)\n\nThat's it! See our finished langchain demo [here](https://huggingface.co/spaces/gradio/langchain-agent).\n\n\n## Building with Visibly Thinking LLMs\n\n\nThe Gradio Chatbot can natively display intermediate thoughts of a _thinking_ LLM. This makes it perfect for creating UIs that show how an AI model \"thinks\" while generating responses. Below guide will show you how to build a chatbot that displays Gemini AI's thought process in real-time.\n\n\n### A real example using Gemini 2.0 Flash Thinking API\n\nLet's create a complete chatbot that shows its thoughts and responses in real-time. We'll use Google's Gemini API for accessing Gemini 2.0 Flash Thinking LLM and Gradio for the UI.\n\nWe'll begin with imports and setting up the gemini client. Note that you'll need to [acquire a Google Gemini API key](https://aistudio.google.com/apikey) first -\n\n```python\nimport gradio as gr\nfrom gradio import ChatMessage\nfrom typing import Iterator\nimport google.generativeai as genai\n\ngenai.configure(api_key=\"your-gemini-api-key\")\nmodel = genai.GenerativeModel(\"gemini-2.0-flash-thinking-exp-1219\")\n```\n\nFirst, let's set up our streaming function that handles the model's output:\n\n```python\ndef stream_gemini_response(user_message: str, messages: list) -> Iterator[list]:\n    \"\"\"\n    Streams both thoughts and responses from the Gemini model.\n    \"\"\"\n    # Initialize response from Gemini\n    response = model.generate_content(user_message, stream=True)\n    \n    # Initialize buffers\n    thought_buffer = \"\"\n    response_buffer = \"\"\n    thinking_complete = False\n    \n    # Add initial thinking message\n    messages.append(\n        ChatMessage(\n            role=\"assistant\",\n            content=\"\",\n            metadata={\"title\": \"\u23f3Thinking: *The thoughts produced by the Gemini2.0 Flash model are experimental\"}\n        )\n    )\n    \n    for chunk in response:\n        parts = chunk.candidates[0].content.parts\n        current_chunk = parts[0].text\n        \n        if len(parts) == 2 and not thinking_complete:\n            # Complete thought and start response\n            thought_buffer += current_chunk\n            messages[-1] = ChatMessage(\n                role=\"assistant\",\n                content=thought_buffer,\n                metadata={\"title\": \"\u23f3Thinking: *The thoughts produced by the Gemini2.0 Flash model are experimental\"}\n            )\n            \n            # Add response message\n            messages.append(\n                ChatMessage(\n                    role=\"assistant\",\n                    content=parts[1].text\n                )\n            )\n            thinking_complete = True\n            \n        elif thinking_complete:\n            # Continue streaming response\n            response_buffer += current_chunk\n            messages[-1] = ChatMessage(\n                role=\"assistant\",\n                content=response_buffer\n            )\n            \n        else:\n            # Continue streaming thoughts\n            thought_buffer += current_chunk\n            messages[-1] = ChatMessage(\n                role=\"assistant\",\n                content=thought_buffer,\n                metadata={\"title\": \"\u23f3Thinking: *The thoughts produced by the Gemini2.0 Flash model are experimental\"}\n            )\n        \n        yield messages\n```\n\nThen, let's create the Gradio interface:\n\n```python\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# Chat with Gemini 2.0 Flash and See its Thoughts \ud83d\udcad\")\n    \n    chatbot = gr.Chatbot(\n        label=\"Gemini2.0 'Thinking' Chatbot\",\n        render_markdown=True,\n    )\n    \n    input_box = gr.Textbox(\n        lines=1,\n        label=\"Chat Message\",\n        placeholder=\"Type your message here and press Enter...\"\n    )\n    \n    # Set up event handlers\n    msg_store = gr.State(\"\")  # Store for preserving user message\n    \n    input_box.submit(\n        lambda msg: (msg, msg, \"\"),  # Store message and clear input\n        inputs=[input_box],\n        outputs=[msg_store, input_box, input_box],\n        queue=False\n    ).then(\n        user_message,  # Add user message to chat\n        inputs=[msg_store, chatbot],\n        outputs=[input_box, chatbot],\n        queue=False\n    ).then(\n        stream_gemini_response,  # Generate and stream response\n        inputs=[msg_store, chatbot],\n        outputs=chatbot\n    )\n\ndemo.launch()\n```\n\nThis creates a chatbot that:\n\n- Displays the model's thoughts in a collapsible section\n- Streams the thoughts and final response in real-time\n- Maintains a clean chat history\n\n That's it! You now have a chatbot that not only responds to users but also shows its thinking process, creating a more transparent and engaging interaction. See our finished Gemini 2.0 Flash Thinking demo [here](https://huggingface.co/spaces/ysharma/Gemini2-Flash-Thinking).\n\n\n ## Building with Citations \n\nThe Gradio Chatbot can display citations from LLM responses, making it perfect for creating UIs that show source documentation and references. This guide will show you how to build a chatbot that displays Claude's citations in real-time.\n\n### A real example using Anthropic's Citations API\nLet's create a complete chatbot that shows both responses and their supporting citations. We'll use Anthropic's Claude API with citations enabled and Gradio for the UI.\n\nWe'll begin with imports and setting up the Anthropic client. Note that you'll need an `ANTHROPIC_API_KEY` environment variable set:\n\n```python\nimport gradio as gr\nimport anthropic\nimport base64\nfrom typing import List, Dict, Any\n\nclient = anthropic.Anthropic()\n```\n\nFirst, let's set up our message formatting functions that handle document preparation:\n\n```python\ndef encode_pdf_to_base64(file_obj) -> str:\n    \"\"\"Convert uploaded PDF file to base64 string.\"\"\"\n    if file_obj is None:\n        return None\n    with open(file_obj.name, 'rb') as f:\n        return base64.b64encode(f.read()).decode('utf-8')\n\ndef format_message_history(\n    history: list, \n    enable_citations: bool,\n    doc_type: str,\n    text_input: str,\n    pdf_file: str\n) -> List[Dict]:\n    \"\"\"Convert Gradio chat history to Anthropic message format.\"\"\"\n    formatted_messages = []\n    \n    # Add previous messages\n    for msg in history[:-1]:\n        if msg[\"role\"] == \"user\":\n            formatted_messages.append({\"role\": \"user\", \"content\": msg[\"content\"]})\n    \n    # Prepare the latest message with document\n    latest_message = {\"role\": \"user\", \"content\": []}\n    \n    if enable_citations:\n        if doc_type == \"plain_text\":\n            latest_message[\"content\"].append({\n                \"type\": \"document\",\n                \"source\": {\n                    \"type\": \"text\",\n                    \"media_type\": \"text/plain\",\n                    \"data\": text_input.strip()\n                },\n                \"title\": \"Text Document\",\n                \"citations\": {\"enabled\": True}\n            })\n        elif doc_type == \"pdf\" and pdf_file:\n            pdf_data = encode_pdf_to_base64(pdf_file)\n            if pdf_data:\n                latest_message[\"content\"].append({\n                    \"type\": \"document\",\n                    \"source\": {\n                        \"type\": \"base64\",\n                        \"media_type\": \"application/pdf\",\n                        \"data\": pdf_data\n                    },\n                    \"title\": pdf_file.name,\n                    \"citations\": {\"enabled\": True}\n                })\n    \n    # Add the user's question\n    latest_message[\"content\"].append({\"type\": \"text\", \"text\": history[-1][\"content\"]})\n    \n    formatted_messages.append(latest_message)\n    return formatted_messages\n```\n\nThen, let's create our bot response handler that processes citations:\n\n```python\ndef bot_response(\n    history: list,\n    enable_citations: bool,\n    doc_type: str,\n    text_input: str,\n    pdf_file: str\n) -> List[Dict[str, Any]]:\n    try:\n        messages = format_message_history(history, enable_citations, doc_type, text_input, pdf_file)\n        response = client.messages.create(model=\"claude-3-5-sonnet-20241022\", max_tokens=1024, messages=messages)\n        \n        # Initialize main response and citations\n        main_response = \"\"\n        citations = []\n        \n        # Process each content block\n        for block in response.content:\n            if block.type == \"text\":\n                main_response += block.text\n                if enable_citations and hasattr(block, 'citations') and block.citations:\n                    for citation in block.citations:\n                        if citation.cited_text not in citations:\n                            citations.append(citation.cited_text)\n        \n        # Add main response\n        history.append({\"role\": \"assistant\", \"content\": main_response})\n        \n        # Add citations in a collapsible section\n        if enable_citations and citations:\n            history.append({\n                \"role\": \"assistant\",\n                \"content\": \"\\n\".join([f\"\u2022 {cite}\" for cite in citations]),\n                \"metadata\": {\"title\": \"\ud83d\udcda Citations\"}\n            })\n        \n        return history\n            \n    except Exception as e:\n        history.append({\n            \"role\": \"assistant\",\n            \"content\": \"I apologize, but I encountered an error while processing your request.\"\n        })\n        return history\n```\n\nFinally, let's create the Gradio interface:\n\n```python\nwith gr.Blocks() as demo:\n    gr.Markdown(\"# Chat with Citations\")\n    \n    with gr.Row(scale=1):\n        with gr.Column(scale=4):\n            chatbot = gr.Chatbot(bubble_full_width=False, show_label=False, scale=1)\n            msg = gr.Textbox(placeholder=\"Enter your message here...\", show_label=False, container=False)\n            \n        with gr.Column(scale=1):\n            enable_citations = gr.Checkbox(label=\"Enable Citations\", value=True, info=\"Toggle citation functionality\" )\n            doc_type_radio = gr.Radio( choices=[\"plain_text\", \"pdf\"], value=\"plain_text\", label=\"Document Type\", info=\"Choose the type of document to use\")\n            text_input = gr.Textbox(label=\"Document Content\", lines=10, info=\"Enter the text you want to reference\")\n            pdf_input = gr.File(label=\"Upload PDF\", file_types=[\".pdf\"], file_count=\"single\", visible=False)\n    \n    # Handle message submission\n    msg.submit(\n        user_message,\n        [msg, chatbot, enable_citations, doc_type_radio, text_input, pdf_input],\n        [msg, chatbot]\n    ).then(\n        bot_response,\n        [chatbot, enable_citations, doc_type_radio, text_input, pdf_input],\n        chatbot\n    )\n\ndemo.launch()\n```\n\nThis creates a chatbot that:\n- Supports both plain text and PDF documents for Claude to cite from \n- Displays Citations in collapsible sections using our `metadata` feature\n- Shows source quotes directly from the given documents\n\nThe citations feature works particularly well with the Gradio Chatbot's `metadata` support, allowing us to create collapsible sections that keep the chat interface clean while still providing easy access to source documentation.\n\nThat's it! You now have a chatbot that not only responds to users but also shows its sources, creating a more transparent and trustworthy interaction. See our finished Citations demo [here](https://huggingface.co/spaces/ysharma/anthropic-citations-with-gradio-metadata-key).\n\n", "tags": ["LLM", "AGENTS", "CHAT"], "spaces": [], "url": "/guides/agents-and-tool-usage/", "contributor": null}}