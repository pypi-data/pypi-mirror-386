{"guide": {"name": "chatinterface-examples", "category": "chatbots", "pretty_category": "Chatbots", "guide_index": 2, "absolute_index": 30, "pretty_name": "Chatinterface Examples", "content": "# Using Popular LLM libraries and APIs\n\n\n\nIn this Guide, we go through several examples of how to use `gr.ChatInterface` with popular LLM libraries and API providers.\n\nWe will cover the following libraries and API providers:\n\n* [Llama Index](#llama-index)\n* [LangChain](#lang-chain)\n* [OpenAI](#open-ai)\n* [Hugging Face `transformers`](#hugging-face-transformers)\n* [SambaNova](#samba-nova)\n* [Hyperbolic](#hyperbolic)\n* [Anthropic's Claude](#anthropics-claude)\n\nFor many LLM libraries and providers, there exist community-maintained integration libraries that make it even easier to spin up Gradio apps. We reference these libraries in the appropriate sections below.\n\n## Llama Index\n\nLet's start by using `llama-index` on top of `openai` to build a RAG chatbot on any text or PDF files that you can demo and share in less than 30 lines of code. You'll need to have an OpenAI key for this example (keep reading for the free, open-source equivalent!)\n\n```python\n# This is a simple RAG chatbot built on top of Llama Index and Gradio. It allows you to upload any text or PDF files and ask questions about them!\n# Before running this, make sure you have exported your OpenAI API key as an environment variable:\n# export OPENAI_API_KEY=\"your-openai-api-key\"\n\nfrom llama_index.core import VectorStoreIndex, SimpleDirectoryReader  \nimport gradio as gr\n\ndef answer(message, history):\n    files = []\n    for msg in history:\n        if msg['role'] == \"user\" and isinstance(msg['content'], tuple):\n            files.append(msg['content'][0])\n    for file in message[\"files\"]:\n        files.append(file)\n\n    documents = SimpleDirectoryReader(input_files=files).load_data()\n    index = VectorStoreIndex.from_documents(documents)\n    query_engine = index.as_query_engine()\n    return str(query_engine.query(message[\"text\"]))\n\ndemo = gr.ChatInterface(\n    answer,\n    title=\"Llama Index RAG Chatbot\",\n    description=\"Upload any text or pdf files and ask questions about them!\",\n    textbox=gr.MultimodalTextbox(file_types=[\".pdf\", \".txt\"]),\n    multimodal=True,\n    api_name=\"chat\",\n)\n\ndemo.launch()\n\n```\n\n## LangChain\n\nHere's an example using `langchain` on top of `openai` to build a general-purpose chatbot. As before, you'll need to have an OpenAI key for this example.\n\n```python\n# This is a simple general-purpose chatbot built on top of LangChain and Gradio.\n# Before running this, make sure you have exported your OpenAI API key as an environment variable:\n# export OPENAI_API_KEY=\"your-openai-api-key\"\n\nfrom langchain_openai import ChatOpenAI  \nfrom langchain.schema import AIMessage, HumanMessage  \nimport gradio as gr\n\nmodel = ChatOpenAI(model=\"gpt-4o-mini\")\n\ndef predict(message, history):\n    history_langchain_format = []\n    for msg in history:\n        if msg['role'] == \"user\":\n            history_langchain_format.append(HumanMessage(content=msg['content']))\n        elif msg['role'] == \"assistant\":\n            history_langchain_format.append(AIMessage(content=msg['content']))\n    history_langchain_format.append(HumanMessage(content=message))\n    gpt_response = model.invoke(history_langchain_format)\n    return gpt_response.content\n\ndemo = gr.ChatInterface(\n    predict,\n    api_name=\"chat\",\n)\n\ndemo.launch()\n\n```\n            <div class='tip'>\n                <span class=\"inline-flex\" style=\"align-items: baseline\">\n                    <svg class=\"self-center w-5 h-5 mx-1\" xmlns=\"http://www.w3.org/2000/svg\" width=\"800px\" height=\"800px\" viewBox=\"0 0 24 24\" fill=\"currentColor\">\n                        <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M9.25 18.7089C9.25 18.2894 9.58579 17.9494 10 17.9494H14C14.4142 17.9494 14.75 18.2894 14.75 18.7089C14.75 19.1283 14.4142 19.4684 14 19.4684H10C9.58579 19.4684 9.25 19.1283 9.25 18.7089ZM9.91667 21.2405C9.91667 20.821 10.2525 20.481 10.6667 20.481H13.3333C13.7475 20.481 14.0833 20.821 14.0833 21.2405C14.0833 21.66 13.7475 22 13.3333 22H10.6667C10.2525 22 9.91667 21.66 9.91667 21.2405Z\"/>\n                        <path d=\"M7.41058 13.8283L8.51463 14.8807C8.82437 15.1759 9 15.5875 9 16.0182C9 16.6653 9.518 17.1899 10.157 17.1899H13.843C14.482 17.1899 15 16.6653 15 16.0182C15 15.5875 15.1756 15.1759 15.4854 14.8807L16.5894 13.8283C18.1306 12.3481 18.9912 10.4034 18.9999 8.3817L19 8.29678C19 4.84243 15.866 2 12 2C8.13401 2 5 4.84243 5 8.29678L5.00007 8.3817C5.00875 10.4034 5.86939 12.3481 7.41058 13.8283Z\"/>\n                    </svg>\n                <span><strong>Tip:</strong></span>\n                </span>\n                <p>For quick prototyping, the community-maintained <a href='https://github.com/AK391/langchain-gradio'>langchain-gradio repo</a>  makes it even easier to build chatbots on top of LangChain.</p>\n            </div>\n                \n\n## OpenAI\n\nOf course, we could also use the `openai` library directy. Here a similar example to the LangChain , but this time with streaming as well:\n            <div class='tip'>\n                <span class=\"inline-flex\" style=\"align-items: baseline\">\n                    <svg class=\"self-center w-5 h-5 mx-1\" xmlns=\"http://www.w3.org/2000/svg\" width=\"800px\" height=\"800px\" viewBox=\"0 0 24 24\" fill=\"currentColor\">\n                        <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M9.25 18.7089C9.25 18.2894 9.58579 17.9494 10 17.9494H14C14.4142 17.9494 14.75 18.2894 14.75 18.7089C14.75 19.1283 14.4142 19.4684 14 19.4684H10C9.58579 19.4684 9.25 19.1283 9.25 18.7089ZM9.91667 21.2405C9.91667 20.821 10.2525 20.481 10.6667 20.481H13.3333C13.7475 20.481 14.0833 20.821 14.0833 21.2405C14.0833 21.66 13.7475 22 13.3333 22H10.6667C10.2525 22 9.91667 21.66 9.91667 21.2405Z\"/>\n                        <path d=\"M7.41058 13.8283L8.51463 14.8807C8.82437 15.1759 9 15.5875 9 16.0182C9 16.6653 9.518 17.1899 10.157 17.1899H13.843C14.482 17.1899 15 16.6653 15 16.0182C15 15.5875 15.1756 15.1759 15.4854 14.8807L16.5894 13.8283C18.1306 12.3481 18.9912 10.4034 18.9999 8.3817L19 8.29678C19 4.84243 15.866 2 12 2C8.13401 2 5 4.84243 5 8.29678L5.00007 8.3817C5.00875 10.4034 5.86939 12.3481 7.41058 13.8283Z\"/>\n                    </svg>\n                <span><strong>Tip:</strong></span>\n                </span>\n                <p>For quick prototyping, the  <a href='https://github.com/gradio-app/openai-gradio'>openai-gradio library</a> makes it even easier to build chatbots on top of OpenAI models.</p>\n            </div>\n                \n\n\n## Hugging Face `transformers`\n\nOf course, in many cases you want to run a chatbot locally. Here's the equivalent example using the SmolLM2-135M-Instruct model using the Hugging Face `transformers` library.\n\n```python\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nimport gradio as gr\n\ncheckpoint = \"HuggingFaceTB/SmolLM2-135M-Instruct\"\ndevice = \"cpu\"  # \"cuda\" or \"cpu\"\ntokenizer = AutoTokenizer.from_pretrained(checkpoint)\nmodel = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n\ndef predict(message, history):\n    history.append({\"role\": \"user\", \"content\": message})\n    input_text = tokenizer.apply_chat_template(history, tokenize=False)\n    inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)  \n    outputs = model.generate(inputs, max_new_tokens=100, temperature=0.2, top_p=0.9, do_sample=True)\n    decoded = tokenizer.decode(outputs[0])\n    response = decoded.split(\"<|im_start|>assistant\\n\")[-1].split(\"<|im_end|>\")[0]\n    return response\n\ndemo = gr.ChatInterface(predict, api_name=\"chat\")\n\ndemo.launch()\n\n```\n\n## SambaNova\n\nThe SambaNova Cloud API provides access to full-precision open-source models, such as the Llama family. Here's an example of how to build a Gradio app around the SambaNova API\n\n```python\n# This is a simple general-purpose chatbot built on top of SambaNova API. \n# Before running this, make sure you have exported your SambaNova API key as an environment variable:\n# export SAMBANOVA_API_KEY=\"your-sambanova-api-key\"\n\nimport os\nimport gradio as gr\nfrom openai import OpenAI\n\napi_key = os.getenv(\"SAMBANOVA_API_KEY\")\n\nclient = OpenAI(\n    base_url=\"https://api.sambanova.ai/v1/\",\n    api_key=api_key,\n)\n\ndef predict(message, history):\n    history.append({\"role\": \"user\", \"content\": message})\n    stream = client.chat.completions.create(messages=history, model=\"Meta-Llama-3.1-70B-Instruct-8k\", stream=True)\n    chunks = []\n    for chunk in stream:\n        chunks.append(chunk.choices[0].delta.content or \"\")\n        yield \"\".join(chunks)\n\ndemo = gr.ChatInterface(predict, api_name=\"chat\")\n\ndemo.launch()\n\n\n```\n            <div class='tip'>\n                <span class=\"inline-flex\" style=\"align-items: baseline\">\n                    <svg class=\"self-center w-5 h-5 mx-1\" xmlns=\"http://www.w3.org/2000/svg\" width=\"800px\" height=\"800px\" viewBox=\"0 0 24 24\" fill=\"currentColor\">\n                        <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M9.25 18.7089C9.25 18.2894 9.58579 17.9494 10 17.9494H14C14.4142 17.9494 14.75 18.2894 14.75 18.7089C14.75 19.1283 14.4142 19.4684 14 19.4684H10C9.58579 19.4684 9.25 19.1283 9.25 18.7089ZM9.91667 21.2405C9.91667 20.821 10.2525 20.481 10.6667 20.481H13.3333C13.7475 20.481 14.0833 20.821 14.0833 21.2405C14.0833 21.66 13.7475 22 13.3333 22H10.6667C10.2525 22 9.91667 21.66 9.91667 21.2405Z\"/>\n                        <path d=\"M7.41058 13.8283L8.51463 14.8807C8.82437 15.1759 9 15.5875 9 16.0182C9 16.6653 9.518 17.1899 10.157 17.1899H13.843C14.482 17.1899 15 16.6653 15 16.0182C15 15.5875 15.1756 15.1759 15.4854 14.8807L16.5894 13.8283C18.1306 12.3481 18.9912 10.4034 18.9999 8.3817L19 8.29678C19 4.84243 15.866 2 12 2C8.13401 2 5 4.84243 5 8.29678L5.00007 8.3817C5.00875 10.4034 5.86939 12.3481 7.41058 13.8283Z\"/>\n                    </svg>\n                <span><strong>Tip:</strong></span>\n                </span>\n                <p>For quick prototyping, the  <a href='https://github.com/gradio-app/sambanova-gradio'>sambanova-gradio library</a> makes it even easier to build chatbots on top of SambaNova models.</p>\n            </div>\n                \n\n## Hyperbolic\n\nThe Hyperbolic AI API provides access to many open-source models, such as the Llama family. Here's an example of how to build a Gradio app around the Hyperbolic\n\n```python\n# This is a simple general-purpose chatbot built on top of Hyperbolic API. \n# Before running this, make sure you have exported your Hyperbolic API key as an environment variable:\n# export HYPERBOLIC_API_KEY=\"your-hyperbolic-api-key\"\n\nimport os\nimport gradio as gr\nfrom openai import OpenAI\n\napi_key = os.getenv(\"HYPERBOLIC_API_KEY\")\n\nclient = OpenAI(\n    base_url=\"https://api.hyperbolic.xyz/v1/\",\n    api_key=api_key,\n)\n\ndef predict(message, history):\n    history.append({\"role\": \"user\", \"content\": message})\n    stream = client.chat.completions.create(messages=history, model=\"gpt-4o-mini\", stream=True)\n    chunks = []\n    for chunk in stream:\n        chunks.append(chunk.choices[0].delta.content or \"\")\n        yield \"\".join(chunks)\n\ndemo = gr.ChatInterface(predict, api_name=\"chat\")\n\ndemo.launch()\n\n\n```\n            <div class='tip'>\n                <span class=\"inline-flex\" style=\"align-items: baseline\">\n                    <svg class=\"self-center w-5 h-5 mx-1\" xmlns=\"http://www.w3.org/2000/svg\" width=\"800px\" height=\"800px\" viewBox=\"0 0 24 24\" fill=\"currentColor\">\n                        <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M9.25 18.7089C9.25 18.2894 9.58579 17.9494 10 17.9494H14C14.4142 17.9494 14.75 18.2894 14.75 18.7089C14.75 19.1283 14.4142 19.4684 14 19.4684H10C9.58579 19.4684 9.25 19.1283 9.25 18.7089ZM9.91667 21.2405C9.91667 20.821 10.2525 20.481 10.6667 20.481H13.3333C13.7475 20.481 14.0833 20.821 14.0833 21.2405C14.0833 21.66 13.7475 22 13.3333 22H10.6667C10.2525 22 9.91667 21.66 9.91667 21.2405Z\"/>\n                        <path d=\"M7.41058 13.8283L8.51463 14.8807C8.82437 15.1759 9 15.5875 9 16.0182C9 16.6653 9.518 17.1899 10.157 17.1899H13.843C14.482 17.1899 15 16.6653 15 16.0182C15 15.5875 15.1756 15.1759 15.4854 14.8807L16.5894 13.8283C18.1306 12.3481 18.9912 10.4034 18.9999 8.3817L19 8.29678C19 4.84243 15.866 2 12 2C8.13401 2 5 4.84243 5 8.29678L5.00007 8.3817C5.00875 10.4034 5.86939 12.3481 7.41058 13.8283Z\"/>\n                    </svg>\n                <span><strong>Tip:</strong></span>\n                </span>\n                <p>For quick prototyping, the  <a href='https://github.com/HyperbolicLabs/hyperbolic-gradio'>hyperbolic-gradio library</a> makes it even easier to build chatbots on top of Hyperbolic models.</p>\n            </div>\n                \n\n\n## Anthropic's Claude \n\nAnthropic's Claude model can also be used via API. Here's a simple 20 questions-style game built on top of the Anthropic API:\n\n```python\n# This is a simple 20 questions-style game built on top of the Anthropic API.\n# Before running this, make sure you have exported your Anthropic API key as an environment variable:\n# export ANTHROPIC_API_KEY=\"your-anthropic-api-key\"\n\nimport anthropic  \nimport gradio as gr\n\nclient = anthropic.Anthropic()\n\ndef predict(message, history):\n    keys_to_keep = [\"role\", \"content\"]\n    history = [{k: d[k] for k in keys_to_keep if k in d} for d in history]\n    history.append({\"role\": \"user\", \"content\": message})\n    if len(history) > 20:\n        history.append({\"role\": \"user\", \"content\": \"DONE\"})\n    output = client.messages.create(\n        messages=history,  \n        model=\"claude-3-5-sonnet-20241022\",\n        max_tokens=1000,\n        system=\"You are guessing an object that the user is thinking of. You can ask 10 yes/no questions. Keep asking questions until the user says DONE\"\n    )\n    return {\n        \"role\": \"assistant\",\n        \"content\": output.content[0].text,  \n        \"options\": [{\"value\": \"Yes\"}, {\"value\": \"No\"}]\n    }\n\nplaceholder = \"\"\"\n<center><h1>10 Questions</h1><br>Think of a person, place, or thing. I'll ask you 10 yes/no questions to try and guess it.\n</center>\n\"\"\"\n\ndemo = gr.ChatInterface(\n    predict,\n    examples=[\"Start!\"],\n    chatbot=gr.Chatbot(placeholder=placeholder),\n    api_name=\"chat\",\n)\n\ndemo.launch()\n\n```\n\n\n", "tags": ["LLM", "CHATBOT", "API"], "spaces": [], "url": "/guides/chatinterface-examples/", "contributor": null}}