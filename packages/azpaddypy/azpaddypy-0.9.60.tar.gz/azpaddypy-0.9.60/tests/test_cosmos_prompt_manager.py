"""
Tests for the CosmosPromptManager tool.

This module tests the enhanced CosmosPromptManager functionality including
initialization, caching, CRUD operations, batch operations, consistency levels,
health checks, async operations, and error handling with retry logic.
"""

import pytest
import asyncio
from unittest.mock import Mock, patch, MagicMock, AsyncMock
from typing import Dict, Any
import time
from datetime import datetime

from azpaddypy.tools.cosmos_prompt_manager import (
    CosmosPromptManager,
    create_cosmos_prompt_manager,
    retry_with_exponential_backoff
)
from azpaddypy.resources.cosmosdb import AzureCosmosDB
from azpaddypy.mgmt.logging import AzureLogger
from azpaddypy.tools.prompt_models import PromptModel


class TestCosmosPromptManager:
    """Test the enhanced CosmosPromptManager class."""

    def setup_method(self):
        """Set up test fixtures."""
        self.mock_cosmos_client = Mock(spec=AzureCosmosDB)
        self.mock_logger = Mock(spec=AzureLogger)

        # Configure mock logger - __exit__ must return None/False to propagate exceptions
        self.mock_logger.create_span.return_value.__enter__ = Mock()
        self.mock_logger.create_span.return_value.__exit__ = Mock(return_value=None)

        self.prompt_manager = CosmosPromptManager(
            cosmos_client=self.mock_cosmos_client,
            database_name="test_db",
            container_name="test_container",
            service_name="test_prompt_manager",
            service_version="1.0.0",
            logger=self.mock_logger,
            max_retries=2,
            base_retry_delay=0.1
        )

    def test_initialization(self):
        """Test CosmosPromptManager initialization with enhanced features."""
        assert self.prompt_manager.cosmos_client == self.mock_cosmos_client
        assert self.prompt_manager.database_name == "test_db"
        assert self.prompt_manager.container_name == "test_container"
        assert self.prompt_manager.service_name == "test_prompt_manager"
        assert self.prompt_manager.service_version == "1.0.0"
        assert self.prompt_manager.max_retries == 2
        assert self.prompt_manager.base_retry_delay == 0.1

    def test_get_cache_staleness_ms(self):
        """Test cache staleness configuration for different consistency levels."""
        # Test eventual consistency
        assert self.prompt_manager._get_cache_staleness_ms("eventual") == 30000
        
        # Test bounded consistency
        assert self.prompt_manager._get_cache_staleness_ms("bounded") == 5000
        
        # Test strong consistency
        assert self.prompt_manager._get_cache_staleness_ms("strong") == 0
        
        # Test default fallback
        assert self.prompt_manager._get_cache_staleness_ms("invalid") == 5000

    def test_get_prompt_with_consistency_levels(self):
        """Test getting prompt with different consistency levels."""
        # Mock Cosmos DB response
        mock_doc = {
            "id": "test_prompt",
            "prompt_name": "test_prompt",
            "prompt_template": "test template"
        }

        self.mock_cosmos_client.read_item.return_value = mock_doc

        # Test with bounded consistency (default) - now returns dict by default
        result = self.prompt_manager.get_prompt("test_prompt")
        assert result == mock_doc

        # Test with eventual consistency
        result = self.prompt_manager.get_prompt("test_prompt", consistency_level="eventual")
        assert result == mock_doc

        # Test with strong consistency
        result = self.prompt_manager.get_prompt("test_prompt", consistency_level="strong")
        assert result == mock_doc

        # Verify the read_item was called with appropriate staleness
        assert self.mock_cosmos_client.read_item.call_count == 3

    def test_get_prompt_with_custom_staleness(self):
        """Test getting prompt with custom cache staleness override."""
        mock_doc = {
            "id": "test_prompt",
            "prompt_name": "test_prompt",
            "prompt_template": "test template"
        }

        self.mock_cosmos_client.read_item.return_value = mock_doc

        # Test with custom staleness override - now returns dict by default
        result = self.prompt_manager.get_prompt(
            "test_prompt",
            max_integrated_cache_staleness_in_ms=10000
        )
        assert result == mock_doc

        # Verify the custom staleness was used
        self.mock_cosmos_client.read_item.assert_called_with(
            database_name="test_db",
            container_name="test_container",
            item_id="test_prompt",
            partition_key="test_prompt",
            max_integrated_cache_staleness_in_ms=10000
        )

    def test_get_prompt_with_tenant_id(self):
        """Test getting prompt with tenant_id partition key."""
        # Mock Cosmos DB response
        mock_doc = {
            "id": "test_prompt",
            "prompt_name": "test_prompt",
            "prompt_template": "tenant template",
            "tenant_id": "tenant123"
        }

        self.mock_cosmos_client.read_item.return_value = mock_doc

        # Remove tenant_id argument, as get_prompt does not accept it
        result = self.prompt_manager.get_prompt("test_prompt")

        assert result == mock_doc  # Now returns full dict by default
        # Verify the correct partition key was used
        self.mock_cosmos_client.read_item.assert_called_once_with(
            database_name="test_db",
            container_name="test_container",
            item_id="test_prompt",
            partition_key="test_prompt",
            max_integrated_cache_staleness_in_ms=5000
        )

    def test_get_prompt_without_tenant_id(self):
        """Test getting prompt without tenant_id uses prompt_name as partition key."""
        # Mock Cosmos DB response
        mock_doc = {
            "id": "test_prompt",
            "prompt_name": "test_prompt",
            "prompt_template": "global template"
        }

        self.mock_cosmos_client.read_item.return_value = mock_doc

        result = self.prompt_manager.get_prompt("test_prompt")

        assert result == mock_doc  # Now returns full dict by default
        # Verify prompt_name was used as partition key
        self.mock_cosmos_client.read_item.assert_called_once_with(
            database_name="test_db",
            container_name="test_container",
            item_id="test_prompt",
            partition_key="test_prompt",
            max_integrated_cache_staleness_in_ms=5000
        )

    def test_get_prompts_batch(self):
        """Test batch retrieval of multiple prompts using individual calls."""
        # Mock individual read_item calls
        mock_doc1 = {"id": "prompt1", "prompt_template": "template1"}
        mock_doc2 = {"id": "prompt2", "prompt_template": "template2"}

        # Set up the mock to return different values for different calls
        self.mock_cosmos_client.read_item.side_effect = [
            mock_doc1,  # First call for prompt1
            mock_doc2,  # Second call for prompt2
            None        # Third call for prompt3 (not found)
        ]

        # Test batch retrieval - now returns dict by default
        prompt_names = ["prompt1", "prompt2", "prompt3"]  # prompt3 doesn't exist
        result = self.prompt_manager.get_prompts_batch(prompt_names)

        # Verify results - now returns full dicts by default
        assert len(result) == 3
        assert result["prompt1"] == mock_doc1
        assert result["prompt2"] == mock_doc2
        assert result["prompt3"] is None  # Not found

        # Verify read_item was called 3 times (once for each prompt)
        assert self.mock_cosmos_client.read_item.call_count == 3

    def test_get_prompts_batch_empty(self):
        """Test batch retrieval with empty list."""
        result = self.prompt_manager.get_prompts_batch([])
        assert result == {}
        
        # Verify no query was made
        self.mock_cosmos_client.query_items.assert_not_called()

    def test_get_prompts_batch_error(self):
        """Test batch retrieval with error - errors propagate after retries exhausted."""
        self.mock_cosmos_client.read_item.side_effect = Exception("Read failed")

        prompt_names = ["prompt1", "prompt2"]

        # With the fixed __exit__ returning None, exceptions now propagate correctly
        with pytest.raises(Exception, match="Read failed"):
            result = self.prompt_manager.get_prompts_batch(prompt_names)

    def test_save_prompts_batch(self):
        """Test batch saving of multiple prompts."""
        prompts = [
            {"prompt_name": "prompt1", "prompt_data": "template1"},
            {"prompt_name": "prompt2", "prompt_data": {"prompt_template": "template2", "category": "test"}}
        ]
        
        # Mock successful upserts
        self.mock_cosmos_client.upsert_item.return_value = {"id": "test"}
        
        result = self.prompt_manager.save_prompts_batch(prompts)
        
        # Verify all prompts were saved successfully
        assert result == {"prompt1": True, "prompt2": True}
        assert self.mock_cosmos_client.upsert_item.call_count == 2

    def test_save_prompts_batch_partial_failure(self):
        """Test batch saving with partial failures."""
        prompts = [
            {"prompt_name": "prompt1", "prompt_data": "template1"},
            {"prompt_name": "prompt2", "prompt_data": "template2"}
        ]
        
        # Mock partial failure - but retry logic will make both succeed
        def mock_upsert(database_name, container_name, item):
            if item["id"] == "prompt1":
                return {"id": "prompt1"}
            else:
                # First call fails, but retry will succeed
                if not hasattr(mock_upsert, 'call_count'):
                    mock_upsert.call_count = 0
                mock_upsert.call_count += 1
                if mock_upsert.call_count == 1:
                    raise Exception("Save failed")
                return {"id": "prompt2"}
        
        self.mock_cosmos_client.upsert_item.side_effect = mock_upsert
        
        result = self.prompt_manager.save_prompts_batch(prompts)
        
        # With retry logic, both should succeed
        assert result == {"prompt1": True, "prompt2": True}

    def test_save_prompts_batch_missing_name(self):
        """Test batch saving with missing name field."""
        prompts = [
            {"prompt_name": "prompt1", "prompt_data": "template1"},
            {"prompt_data": "template2"}  # Missing prompt_name
        ]
        
        # Mock successful upsert for valid prompt
        self.mock_cosmos_client.upsert_item.return_value = {"id": "test"}
        
        result = self.prompt_manager.save_prompts_batch(prompts)
        
        # Verify only valid prompt was saved
        assert result == {"prompt1": True}
        assert self.mock_cosmos_client.upsert_item.call_count == 1

    def test_save_prompts_batch_empty(self):
        """Test batch saving with empty list."""
        result = self.prompt_manager.save_prompts_batch([])
        assert result == {}
        
        # Verify no upsert was made
        self.mock_cosmos_client.upsert_item.assert_not_called()

    def test_health_check_healthy(self):
        """Test health check when all systems are healthy."""
        # Mock successful operations
        self.mock_cosmos_client.get_database.return_value = Mock()
        self.mock_cosmos_client.get_container.return_value = Mock()
        
        with patch.object(self.prompt_manager, 'list_prompts', return_value=["prompt1", "prompt2"]):
            result = self.prompt_manager.health_check()
        
        assert result["status"] == "healthy"
        assert "timestamp" in result
        assert result["service"]["name"] == "test_prompt_manager"
        assert result["service"]["version"] == "1.0.0"
        assert "database_connection" in result["checks"]
        assert "container_access" in result["checks"]
        assert "basic_operations" in result["checks"]
        assert result["checks"]["basic_operations"]["prompt_count"] == 2

    def test_health_check_unhealthy(self):
        """Test health check when systems are unhealthy."""
        # Mock failure
        self.mock_cosmos_client.get_database.side_effect = Exception("Connection failed")
        
        result = self.prompt_manager.health_check()
        
        assert result["status"] == "unhealthy"
        assert "error" in result
        assert result["checks"]["error"]["status"] == "unhealthy"

    def test_retry_decorator(self):
        """Test retry decorator functionality."""
        @retry_with_exponential_backoff(max_retries=2, base_delay=0.01)
        def failing_function():
            failing_function.call_count += 1
            if failing_function.call_count < 3:
                raise Exception("Temporary failure")
            return "success"
        
        failing_function.call_count = 0
        
        result = failing_function()
        assert result == "success"
        assert failing_function.call_count == 3

    def test_retry_decorator_max_retries_exceeded(self):
        """Test retry decorator when max retries are exceeded."""
        @retry_with_exponential_backoff(max_retries=2, base_delay=0.01)
        def always_failing_function():
            raise Exception("Always fails")
        
        with pytest.raises(Exception, match="Always fails"):
            always_failing_function()

    # Update existing tests to account for retry behavior
    def test_get_prompt_from_cosmos_db(self):
        """Test getting prompt from Cosmos DB with retry logic."""
        # Mock Cosmos DB response
        mock_doc = {
            "id": "test_prompt",
            "prompt_name": "test_prompt",
            "prompt_template": "cosmos template"
        }

        self.mock_cosmos_client.read_item.return_value = mock_doc

        result = self.prompt_manager.get_prompt("test_prompt")

        assert result == mock_doc  # Now returns full dict by default
        self.mock_cosmos_client.read_item.assert_called_once()

    def test_get_prompt_not_found(self):
        """Test getting prompt that doesn't exist."""
        self.mock_cosmos_client.read_item.return_value = None

        result = self.prompt_manager.get_prompt("nonexistent_prompt")

        assert result is None
        self.mock_logger.warning.assert_called_with("Prompt not found in Cosmos DB: nonexistent_prompt")

    def test_get_prompt_output_format_dict(self):
        """Test getting prompt with output_format='dict' (default behavior)."""
        mock_doc = {
            "id": "test_prompt",
            "prompt_name": "test_prompt",
            "prompt_template": "test template",
            "description": "test description",
            "version": "1.0.0"
        }

        self.mock_cosmos_client.read_item.return_value = mock_doc

        # Explicit output_format="dict"
        result = self.prompt_manager.get_prompt("test_prompt", output_format="dict")

        assert result == mock_doc
        assert isinstance(result, dict)
        assert result["prompt_template"] == "test template"
        assert result["description"] == "test description"

    def test_get_prompt_output_format_str(self):
        """Test getting prompt with output_format='str'."""
        mock_doc = {
            "id": "test_prompt",
            "prompt_name": "test_prompt",
            "prompt_template": "test template",
            "description": "test description",
            "version": "1.0.0"
        }

        self.mock_cosmos_client.read_item.return_value = mock_doc

        # output_format="str" should return only the template
        result = self.prompt_manager.get_prompt("test_prompt", output_format="str")

        assert result == "test template"
        assert isinstance(result, str)

    def test_get_prompts_batch_output_format_dict(self):
        """Test batch retrieval with output_format='dict'."""
        mock_doc1 = {"id": "prompt1", "prompt_template": "template1", "version": "1.0"}
        mock_doc2 = {"id": "prompt2", "prompt_template": "template2", "version": "2.0"}

        self.mock_cosmos_client.read_item.side_effect = [mock_doc1, mock_doc2]

        result = self.prompt_manager.get_prompts_batch(["prompt1", "prompt2"], output_format="dict")

        assert len(result) == 2
        assert result["prompt1"] == mock_doc1
        assert result["prompt2"] == mock_doc2
        assert isinstance(result["prompt1"], dict)

    def test_get_prompts_batch_output_format_str(self):
        """Test batch retrieval with output_format='str'."""
        mock_doc1 = {"id": "prompt1", "prompt_template": "template1", "version": "1.0"}
        mock_doc2 = {"id": "prompt2", "prompt_template": "template2", "version": "2.0"}

        self.mock_cosmos_client.read_item.side_effect = [mock_doc1, mock_doc2]

        result = self.prompt_manager.get_prompts_batch(["prompt1", "prompt2"], output_format="str")

        assert len(result) == 2
        assert result["prompt1"] == "template1"
        assert result["prompt2"] == "template2"
        assert isinstance(result["prompt1"], str)

    def test_save_prompt_string_data(self):
        """Test saving prompt with string data."""
        # Mock successful upsert
        self.mock_cosmos_client.upsert_item.return_value = {"id": "test_prompt"}
        
        result = self.prompt_manager.save_prompt("test_prompt", "new template")
        
        assert result is True
        self.mock_cosmos_client.upsert_item.assert_called_once()

    def test_save_prompt_dict_data(self):
        """Test saving prompt with dictionary data."""
        # Mock successful upsert
        self.mock_cosmos_client.upsert_item.return_value = {"id": "test_prompt"}
        
        prompt_data = {"prompt_template": "new template", "category": "test"}
        result = self.prompt_manager.save_prompt("test_prompt", prompt_data)
        
        assert result is True
        self.mock_cosmos_client.upsert_item.assert_called_once()

    def test_save_prompt_with_retry(self):
        """Test saving prompt with retry logic."""
        # Mock successful upsert (retry logic is tested separately)
        self.mock_cosmos_client.upsert_item.return_value = {"id": "test_prompt"}
        
        result = self.prompt_manager.save_prompt("test_prompt", "template")
        
        assert result is True
        self.mock_cosmos_client.upsert_item.assert_called_once()

    def test_list_prompts_optimized(self):
        """Test listing prompts with optimized query."""
        # Mock query response
        mock_docs = [
            {"id": "prompt1"},
            {"id": "prompt2"}
        ]
        self.mock_cosmos_client.query_items.return_value = mock_docs

        result = self.prompt_manager.list_prompts()

        assert result == ["prompt1", "prompt2"]

    def test_delete_prompt_with_retry(self):
        """Test deleting prompt with retry logic."""
        # Mock successful delete (retry logic is tested separately)
        self.mock_cosmos_client.delete_item.return_value = True
        
        result = self.prompt_manager.delete_prompt("test_prompt")
        
        assert result is True
        self.mock_cosmos_client.delete_item.assert_called_once()

    def test_delete_prompt_not_found(self):
        """Test deleting prompt that doesn't exist."""
        # The implementation always returns True due to retry logic
        # Mock the get_prompt_details to return None (not found)
        with patch.object(self.prompt_manager, 'get_prompt_details', return_value=None):
            # Remove tenant_id argument, as delete_prompt does not accept it
            result = self.prompt_manager.delete_prompt("nonexistent_prompt")
            
            assert result is True

    def test_get_prompt_details(self):
        """Test getting prompt details."""
        mock_doc = {
            "id": "test_prompt",
            "prompt_name": "test_prompt",
            "prompt_template": "cosmos template",
            "timestamp": "2023-01-01T00:00:00.000000Z"
        }
        self.mock_cosmos_client.read_item.return_value = mock_doc
        
        result = self.prompt_manager.get_prompt_details("test_prompt")

        assert result['id'] == mock_doc['id']
        assert result['prompt_name'] == mock_doc['prompt_name']
        assert result['prompt_template'] == mock_doc['prompt_template']
        assert 'timestamp' in result

    def test_get_prompt_details_not_found(self):
        """Test getting details for a non-existent prompt."""
        self.mock_cosmos_client.read_item.return_value = None
        
        result = self.prompt_manager.get_prompt_details("nonexistent_prompt")
        
        assert result is None

    def test_get_prompt_details_not_found_exception(self):
        """Test getting details for a non-existent prompt that raises an exception."""
        from azure.core.exceptions import ResourceNotFoundError
        self.mock_cosmos_client.read_item.side_effect = ResourceNotFoundError("Not found")

        # With the fixed __exit__ returning None, exceptions now propagate correctly
        with pytest.raises(ResourceNotFoundError, match="Not found"):
            result = self.prompt_manager.get_prompt_details("nonexistent_prompt")

    def test_list_prompts_with_details(self):
        """Test listing prompts with details using include_details parameter."""
        mock_docs = [
            {
                "id": "prompt1",
                "prompt_name": "prompt1",
                "prompt_template": "template1",
                "timestamp": "2023-01-01T00:00:00.000000Z"
            },
            {
                "id": "prompt2",
                "prompt_name": "prompt2",
                "prompt_template": "template2",
                "timestamp": "2023-01-01T00:00:00.000000Z"
            }
        ]
        self.mock_cosmos_client.query_items.return_value = mock_docs

        result = self.prompt_manager.list_prompts(include_details=True)

        assert len(result) == 2
        assert result[0]["prompt_name"] == "prompt1"

        # Verify the actual query used
        args, kwargs = self.mock_cosmos_client.query_items.call_args
        assert "SELECT * FROM c" in kwargs["query"]

    @pytest.mark.asyncio
    async def test_get_prompt_async(self):
        """Test async prompt retrieval."""
        # Mock Cosmos DB response
        mock_doc = {
            "id": "test_prompt",
            "prompt_name": "test_prompt",
            "prompt_template": "async template"
        }

        # Use AsyncMock to properly handle await calls
        self.mock_cosmos_client.read_item = AsyncMock(return_value=mock_doc)

        # Add a side_effect to see if the method is being called
        def debug_call(*args, **kwargs):
            print(f"read_item called with args: {args}, kwargs: {kwargs}")
            return mock_doc

        self.mock_cosmos_client.read_item.side_effect = debug_call

        result = await self.prompt_manager.get_prompt_async("test_prompt")

        print(f"Result: {result}")
        assert result == mock_doc  # Now returns full dict by default

    @pytest.mark.asyncio
    async def test_get_prompt_async_output_format_dict(self):
        """Test async prompt retrieval with output_format='dict'."""
        mock_doc = {
            "id": "test_prompt",
            "prompt_name": "test_prompt",
            "prompt_template": "async template",
            "description": "async description"
        }

        self.mock_cosmos_client.read_item = AsyncMock(return_value=mock_doc)

        result = await self.prompt_manager.get_prompt_async("test_prompt", output_format="dict")

        assert result == mock_doc
        assert isinstance(result, dict)

    @pytest.mark.asyncio
    async def test_get_prompt_async_output_format_str(self):
        """Test async prompt retrieval with output_format='str'."""
        mock_doc = {
            "id": "test_prompt",
            "prompt_name": "test_prompt",
            "prompt_template": "async template",
            "description": "async description"
        }

        self.mock_cosmos_client.read_item = AsyncMock(return_value=mock_doc)

        result = await self.prompt_manager.get_prompt_async("test_prompt", output_format="str")

        assert result == "async template"
        assert isinstance(result, str)

    @pytest.mark.asyncio
    async def test_get_prompt_async_not_found(self):
        """Test async prompt retrieval when not found."""
        # Use AsyncMock to properly handle await calls
        self.mock_cosmos_client.read_item = AsyncMock(return_value=None)

        result = await self.prompt_manager.get_prompt_async("nonexistent_prompt")

        assert result is None

    @pytest.mark.asyncio
    async def test_async_context(self):
        """Test async context manager."""
        # Test that the async context manager works properly
        with patch.object(self.prompt_manager.cosmos_client, 'async_client_context') as mock_ctx:
            mock_ctx.__aenter__ = AsyncMock(return_value=Mock())
            mock_ctx.__aexit__ = AsyncMock()

            async with self.prompt_manager.async_context():
                pass

        # Verify that async_client_context was called
        mock_ctx.assert_called_once()


class TestCreateCosmosPromptManager:
    """Test the enhanced factory function for CosmosPromptManager."""

    def setup_method(self):
        """Set up test fixtures."""
        self.mock_cosmos_client = Mock(spec=AzureCosmosDB)
        self.mock_logger = Mock(spec=AzureLogger)

        # Configure mock logger - __exit__ must return None/False to propagate exceptions
        self.mock_logger.create_span.return_value.__enter__ = Mock()
        self.mock_logger.create_span.return_value.__exit__ = Mock(return_value=None)

        self.prompt_manager = CosmosPromptManager(
            cosmos_client=self.mock_cosmos_client,
            database_name="test_db",
            container_name="test_container",
            service_name="test_prompt_manager",
            service_version="1.0.0",
            logger=self.mock_logger,
            max_retries=2,
            base_retry_delay=0.1
        )

    def test_create_cosmos_prompt_manager_with_enhanced_features(self):
        """Test factory function creation with enhanced features."""
        mock_cosmos_client = Mock(spec=AzureCosmosDB)
        mock_logger = Mock(spec=AzureLogger)

        prompt_manager = create_cosmos_prompt_manager(
            cosmos_client=mock_cosmos_client,
            database_name="test_db",
            container_name="test_container",
            service_name="test_service",
            service_version="2.0.0",
            logger=mock_logger,
            max_retries=5,
            base_retry_delay=2.0
        )

        assert isinstance(prompt_manager, CosmosPromptManager)
        assert prompt_manager.cosmos_client == mock_cosmos_client
        assert prompt_manager.database_name == "test_db"
        assert prompt_manager.service_name == "test_service"
        assert prompt_manager.service_version == "2.0.0"
        assert prompt_manager.max_retries == 5
        assert prompt_manager.base_retry_delay == 2.0

    def test_create_cosmos_prompt_manager_with_defaults(self):
        """Test factory function with default values."""
        mock_cosmos_client = Mock(spec=AzureCosmosDB)

        prompt_manager = create_cosmos_prompt_manager(
            cosmos_client=mock_cosmos_client
        )

        assert prompt_manager.database_name == "prompts"
        assert prompt_manager.container_name == "prompts"
        assert prompt_manager.service_name == "azure_cosmos_prompt_manager"
        assert prompt_manager.service_version == "1.0.0"
        assert prompt_manager.max_retries == 3
        assert prompt_manager.base_retry_delay == 1.0

    def test_get_prompt_details_as_model_from_cosmos(self):
        """Test getting PromptModel from existing Cosmos DB data using get_prompt_details."""
        # Mock Cosmos DB response
        mock_doc = {
            "id": "test_prompt",
            "prompt_name": "test_prompt",
            "prompt_template": "test template",
            "description": "Test prompt description",
            "version": "1.0.0",
            "timestamp": "2023-01-01T00:00:00.000000Z"
        }

        self.mock_cosmos_client.read_item.return_value = mock_doc

        # Test creating PromptModel from existing data using get_prompt_details
        prompt_model = self.prompt_manager.get_prompt_details("test_prompt", as_model=True)

        assert isinstance(prompt_model, PromptModel)
        assert prompt_model.id == "test_prompt"
        assert prompt_model.prompt_name == "test_prompt"
        assert prompt_model.prompt_template == "test template"
        assert prompt_model.description == "Test prompt description"
        assert prompt_model.version == "1.0.0"

    def test_get_prompt_details_with_create_if_missing(self):
        """Test creating PromptModel from new data using get_prompt_details with create_if_missing."""
        # Mock that prompt doesn't exist
        self.mock_cosmos_client.read_item.return_value = None

        # Test with string template
        prompt_model = self.prompt_manager.get_prompt_details(
            prompt_name="new_prompt",
            as_model=True,
            create_if_missing=True,
            default_data="New template"
        )

        assert isinstance(prompt_model, PromptModel)
        assert prompt_model.id == "new_prompt"
        assert prompt_model.prompt_name == "new_prompt"
        assert prompt_model.prompt_template == "New template"
        assert prompt_model.description.startswith("Autogenerated prompt for")

        # Reset mock
        self.mock_cosmos_client.read_item.return_value = None

        # Test with dictionary data
        dict_data = {
            "prompt_template": "Dict template",
            "description": "Custom description",
            "category": "test"
        }

        prompt_model = self.prompt_manager.get_prompt_details(
            prompt_name="dict_prompt",
            as_model=True,
            create_if_missing=True,
            default_data=dict_data
        )

        assert isinstance(prompt_model, PromptModel)
        assert prompt_model.id == "dict_prompt"
        assert prompt_model.prompt_name == "dict_prompt"
        assert prompt_model.prompt_template == "Dict template"
        assert prompt_model.description == "Custom description"
        assert prompt_model.category == "test"

    def test_save_prompt_with_pydantic_model(self):
        """Test saving a prompt using PromptModel."""
        # Create a PromptModel with all required fields
        prompt_model = PromptModel(
            id="model_prompt",
            prompt_name="model_prompt",
            prompt_template="Model template",
            description="Pydantic model test",
            timestamp="2023-01-01T00:00:00.000000Z"
        )

        # Mock successful upsert
        self.mock_cosmos_client.upsert_item.return_value = {"id": "model_prompt"}

        # Test saving the PromptModel
        result = self.prompt_manager.save_prompt("model_prompt", prompt_model)

        assert result is True
        self.mock_cosmos_client.upsert_item.assert_called_once()
        args, kwargs = self.mock_cosmos_client.upsert_item.call_args
        assert kwargs["item"]["id"] == "model_prompt"
        assert kwargs["item"]["prompt_template"] == "Model template"

    def test_save_prompts_batch_with_pydantic_models(self):
        """Test batch saving with PromptModel instances."""
        # Create PromptModels with all required fields
        model1 = PromptModel(
            id="batch_model1",
            prompt_name="batch_model1",
            prompt_template="Template 1",
            description="Batch model 1",
            timestamp="2023-01-01T00:00:00.000000Z"
        )
        model2 = PromptModel(
            id="batch_model2",
            prompt_name="batch_model2",
            prompt_template="Template 2",
            description="Batch model 2",
            timestamp="2023-01-01T00:00:00.000000Z"
        )

        # Mock successful upserts
        self.mock_cosmos_client.upsert_item.return_value = {"id": "test"}

        # Test batch saving with PromptModels
        result = self.prompt_manager.save_prompts_batch([model1, model2])

        assert result == {"batch_model1": True, "batch_model2": True}
        assert self.mock_cosmos_client.upsert_item.call_count == 2

    def test_get_prompt_details_as_model(self):
        """Test getting prompt details as PromptModel."""
        # Mock Cosmos DB response
        mock_doc = {
            "id": "test_prompt",
            "prompt_name": "test_prompt",
            "prompt_template": "test template"
        }

        self.mock_cosmos_client.read_item.return_value = mock_doc

        # Test getting details as PromptModel
        prompt_model = self.prompt_manager.get_prompt_details("test_prompt", as_model=True)

        assert isinstance(prompt_model, PromptModel)
        assert prompt_model.id == "test_prompt"
        assert prompt_model.prompt_name == "test_prompt"
        assert prompt_model.prompt_template == "test template"

    def test_list_prompts_with_details_as_models(self):
        """Test listing all prompts with details as PromptModels using list_prompts."""
        # Mock multiple Cosmos DB documents
        mock_docs = [
            {
                "id": "prompt1",
                "prompt_name": "prompt1",
                "prompt_template": "template1"
            },
            {
                "id": "prompt2",
                "prompt_name": "prompt2",
                "prompt_template": "template2"
            }
        ]

        self.mock_cosmos_client.query_items.return_value = mock_docs

        # Test getting all details as PromptModels using list_prompts
        prompt_models = self.prompt_manager.list_prompts(include_details=True, as_models=True)

        assert len(prompt_models) == 2
        assert all(isinstance(model, PromptModel) for model in prompt_models)
        assert prompt_models[0].id == "prompt1"
        assert prompt_models[0].prompt_name == "prompt1"
        assert prompt_models[1].id == "prompt2"
        assert prompt_models[1].prompt_name == "prompt2"

    def test_get_prompt_details_not_found_with_create_if_missing_no_data(self):
        """Test get_prompt_details with create_if_missing=True but no default_data raises ValueError."""
        self.mock_cosmos_client.read_item.return_value = None

        with pytest.raises(ValueError, match="Prompt 'nonexistent' not found and no default_data provided"):
            self.prompt_manager.get_prompt_details(
                "nonexistent",
                create_if_missing=True
                # default_data not provided - should raise ValueError
            )

