Description,Type,DocumentationLink,Category,TableName,LastPingAt,CustomSyntax,ParamTable,SyntaxDescr,BodyStrNames
"Used to read CSV, Text, Pipe-Delimited, etc. files held within AWS S3",DirectProvider,,Utilities,AwsS3.Csv,2022-03-25 16:02:57.4680374Z,"Of the form:

use AwsS3.Csv [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

<OPTIONS>:

_______________________________________________________________________________________________________________________
 file (-f)         │ *Mandatory* The file to read. Should be text (any extension) or compressed (.zip).
                   │   It may also be a folder, in which case --folderFilter is also required to specify which files in
                   │   the folder to process. [String]
 folderFilter      │ Denotes this is searching an entire folder structure and provides a Regular Expression of
                   │   path/file names within it that should be processed.
                   │   All matches should be of the same format. [String]
 zipFilter (-z)    │ Denotes this is a Zip file and provides a Regular Expression of path/file names within it that
                   │   should be processed.
                   │   All matches should be of the same format. [String]
 addFileName       │ Adds a column (the first column) to the result set which contains the file the row came from.
                   │   [Boolean]
 names             │ Column Names either overrides the header row or steps in when there is no header row (comma
                   │   delimited list) [String]
 select            │ Column (by Name) that should be returned (comma delimited list) [String]
 types             │ Column types (comma delimited list of : Boolean, Date, DateTime, Decimal, Double, Int, BigInt,
                   │   Text, Table) [String]
 inferTypeRowCount │ If non-zero and 'types' is not specified this will look through N rows to attempt to work out the
                   │   column types [Int32]
 noHeader          │ Set this if there is no header row [Boolean]
 delimiter         │ The delimiter between values (\t for tab) [String, Default: ,]
 escape            │ Character used to escape the 'Quote' character when within a value [String, Default: ""]
 quote             │ Character used around any field containing the 'delimiter' or a line break. [String, Default: ""]
 valuesToMakeNull  │ Regex of values to map to 'null' in the returned data. [String, Default:
                   │   (^[nN][uU][lL]{1,2}$)|((\r\n|\n|\r)$)|(^(\r\n|\n|\r))|(^\s*$)]
 skipPreHeader     │ Number of rows to ignore before the header row [Int32]
 skipPostHeader    │ Number of rows to ignore after the header row [Int32]
 skipInvalidRows   │ Skip invalid data rows (totally invalid ones), 
                   │   This also allows for potentially wrong data if it can be handled somewhat e.g. embedded quotes
                   │   misused (and still returns such rows).
                   │   In either case a warning will show in the progress feedback. [Boolean]
"," file (-f)         │ *Mandatory* The file to read. Should be text (any extension) or compressed (.zip).
                   │   It may also be a folder, in which case --folderFilter is also required to specify which files in
                   │   the folder to process. [String]
 folderFilter      │ Denotes this is searching an entire folder structure and provides a Regular Expression of
                   │   path/file names within it that should be processed.
                   │   All matches should be of the same format. [String]
 zipFilter (-z)    │ Denotes this is a Zip file and provides a Regular Expression of path/file names within it that
                   │   should be processed.
                   │   All matches should be of the same format. [String]
 addFileName       │ Adds a column (the first column) to the result set which contains the file the row came from.
                   │   [Boolean]
 names             │ Column Names either overrides the header row or steps in when there is no header row (comma
                   │   delimited list) [String]
 select            │ Column (by Name) that should be returned (comma delimited list) [String]
 types             │ Column types (comma delimited list of : Boolean, Date, DateTime, Decimal, Double, Int, BigInt,
                   │   Text, Table) [String]
 inferTypeRowCount │ If non-zero and 'types' is not specified this will look through N rows to attempt to work out the
                   │   column types [Int32]
 noHeader          │ Set this if there is no header row [Boolean]
 delimiter         │ The delimiter between values (\t for tab) [String, Default: ,]
 escape            │ Character used to escape the 'Quote' character when within a value [String, Default: ""]
 quote             │ Character used around any field containing the 'delimiter' or a line break. [String, Default: ""]
 valuesToMakeNull  │ Regex of values to map to 'null' in the returned data. [String, Default:
                   │   (^[nN][uU][lL]{1,2}$)|((\r\n|\n|\r)$)|(^(\r\n|\n|\r))|(^\s*$)]
 skipPreHeader     │ Number of rows to ignore before the header row [Int32]
 skipPostHeader    │ Number of rows to ignore after the header row [Int32]
 skipInvalidRows   │ Skip invalid data rows (totally invalid ones), 
                   │   This also allows for potentially wrong data if it can be handled somewhat e.g. embedded quotes
                   │   misused (and still returns such rows).
                   │   In either case a warning will show in the progress feedback. [Boolean]","Of the form:

use AwsS3.Csv [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

",[]
Used to read Excel files held within AWS S3,DirectProvider,,Utilities,AwsS3.Excel,2022-03-25 16:02:57.4680374Z,"Of the form:

use AwsS3.Excel [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

<OPTIONS>:

_______________________________________________________________________________________________________________________
 file (-f)         │ *Mandatory* The file to read. Should be an Excel file (.xlsx or .xlsm)
                   │   It may also be a folder, in which case --folderFilter is also required to specify which files in
                   │   the folder to process. [String]
 folderFilter      │ Denotes this is searching an entire folder structure and provides a Regular Expression of
                   │   path/file names within it that should be processed.
                   │   All matches should be of the same format. [String]
 zipFilter (-z)    │ Denotes this is a Zip file and provides a Regular Expression of path/file names within it that
                   │   should be processed.
                   │   All matches should be of the same format. [String]
 addFileName       │ Adds a column (the first column) to the result set which contains the file the row came from.
                   │   [Boolean]
 names             │ Column Names either overrides the header row or steps in when there is no header row (comma
                   │   delimited list) [String]
 types             │ Column types (comma delimited list of : Boolean, Date, DateTime, Decimal, Double, Int, BigInt,
                   │   Text, Table) [String]
 inferTypeRowCount │ If non-zero and 'types' is not specified this will look through N rows to attempt to work out the
                   │   column types [Int32]
 noHeader          │ Set this if there is no header row [Boolean]
 calculate         │ Whether to attempt a calculation of the imported cell range prior to import [Boolean]
 password          │ If specified will be used as the password used for password protected workbooks [String]
 worksheet (-w)    │ The worksheet containing the cell range to import (name or index, will default to first) [String]
 range (-r)        │ The cell range to import as either a specified range or a table name.
                   │   Range formats can be any of: 
                   │   - 'A1:B3' (standard Excel range format)
                   │   - '1,3,7,9' (rows 1-7, columns 3-9)
                   │   - ',3,,9' (all populated rows, but only columns 3-9)
                   │   - ',3,,' (all populated rows, but only columns 3 to the last populated one)
                   │   Defaults to the used dimensions of the worksheet (equivalent to ',,,') [String]
 ignoreInvalid     │ If specified cells which can not be successfully converted to the target type will be ignored
                   │   [Boolean]
 ignoreBlankRows   │ If the entire rows has only blank cells it will be ignored will be ignored [Boolean]
"," file (-f)         │ *Mandatory* The file to read. Should be an Excel file (.xlsx or .xlsm)
                   │   It may also be a folder, in which case --folderFilter is also required to specify which files in
                   │   the folder to process. [String]
 folderFilter      │ Denotes this is searching an entire folder structure and provides a Regular Expression of
                   │   path/file names within it that should be processed.
                   │   All matches should be of the same format. [String]
 zipFilter (-z)    │ Denotes this is a Zip file and provides a Regular Expression of path/file names within it that
                   │   should be processed.
                   │   All matches should be of the same format. [String]
 addFileName       │ Adds a column (the first column) to the result set which contains the file the row came from.
                   │   [Boolean]
 names             │ Column Names either overrides the header row or steps in when there is no header row (comma
                   │   delimited list) [String]
 types             │ Column types (comma delimited list of : Boolean, Date, DateTime, Decimal, Double, Int, BigInt,
                   │   Text, Table) [String]
 inferTypeRowCount │ If non-zero and 'types' is not specified this will look through N rows to attempt to work out the
                   │   column types [Int32]
 noHeader          │ Set this if there is no header row [Boolean]
 calculate         │ Whether to attempt a calculation of the imported cell range prior to import [Boolean]
 password          │ If specified will be used as the password used for password protected workbooks [String]
 worksheet (-w)    │ The worksheet containing the cell range to import (name or index, will default to first) [String]
 range (-r)        │ The cell range to import as either a specified range or a table name.
                   │   Range formats can be any of: 
                   │   - 'A1:B3' (standard Excel range format)
                   │   - '1,3,7,9' (rows 1-7, columns 3-9)
                   │   - ',3,,9' (all populated rows, but only columns 3-9)
                   │   - ',3,,' (all populated rows, but only columns 3 to the last populated one)
                   │   Defaults to the used dimensions of the worksheet (equivalent to ',,,') [String]
 ignoreInvalid     │ If specified cells which can not be successfully converted to the target type will be ignored
                   │   [Boolean]
 ignoreBlankRows   │ If the entire rows has only blank cells it will be ignored will be ignored [Boolean]","Of the form:

use AwsS3.Excel [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

",[]
Used to read text files in a raw-content from held within AWS S3,DirectProvider,,Utilities,AwsS3.RawText,2022-03-25 16:02:57.4680374Z,"Of the form:

use AwsS3.RawText [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

<OPTIONS>:

_______________________________________________________________________________________________________________________
 file (-f)      │ *Mandatory* The file to read.
                │   It may also be a folder, in which case --folderFilter is also required to specify which files in the
                │   folder to process. [String]
 folderFilter   │ Denotes this is searching an entire folder structure and provides a Regular Expression of path/file
                │   names within it that should be processed.
                │   All matches should be of the same format. [String]
 zipFilter (-z) │ Denotes this is a Zip file and provides a Regular Expression of path/file names within it that should
                │   be processed.
                │   All matches should be of the same format. [String]
 addFileName    │ Adds a column (the first column) to the result set which contains the file the row came from.
                │   [Boolean]
"," file (-f)      │ *Mandatory* The file to read.
                │   It may also be a folder, in which case --folderFilter is also required to specify which files in the
                │   folder to process. [String]
 folderFilter   │ Denotes this is searching an entire folder structure and provides a Regular Expression of path/file
                │   names within it that should be processed.
                │   All matches should be of the same format. [String]
 zipFilter (-z) │ Denotes this is a Zip file and provides a Regular Expression of path/file names within it that should
                │   be processed.
                │   All matches should be of the same format. [String]
 addFileName    │ Adds a column (the first column) to the result set which contains the file the row came from.
                │   [Boolean]","Of the form:

use AwsS3.RawText [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

",[]
Used to write query results to S3 in a variety of formats,DirectProvider,,"Files, Utilities",AwsS3.SaveAs,2022-03-25 16:02:57.4680374Z,"Of the form:

use AwsS3.SaveAs [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

<OPTIONS>:

_______________________________________________________________________________________________________________________
 type             │ The file types to create - also controls file extensions if not specified [Csv, Excel, SqLite,
                  │   Json, Default: Csv]
 combineToOne     │ Combine all files to one for this export - and give them this name [String]
 ignoreOnZeroRows │ Flag to say, if a file would have zero rows, do not write it at all. [Boolean]
 fileNames        │ *Mandatory* File names (without a path).  One per table of data is required. [String]
 path             │ *Mandatory* The location to save this within the target file store. [String]
"," type             │ The file types to create - also controls file extensions if not specified [Csv, Excel, SqLite,
                  │   Json, Default: Csv]
 combineToOne     │ Combine all files to one for this export - and give them this name [String]
 ignoreOnZeroRows │ Flag to say, if a file would have zero rows, do not write it at all. [Boolean]
 fileNames        │ *Mandatory* File names (without a path).  One per table of data is required. [String]
 path             │ *Mandatory* The location to save this within the target file store. [String]","Of the form:

use AwsS3.SaveAs [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

",[]
Used to read tables from Sqlite files held within AWS S3,DirectProvider,,Utilities,AwsS3.Sqlite,2022-03-25 16:02:57.4680374Z,"Of the form:

use AwsS3.Sqlite [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

<OPTIONS>:

_______________________________________________________________________________________________________________________
 file (-f)      │ *Mandatory* The file to read. Should be a sqlite file.
                │   It may also be a folder, in which case --folderFilter is also required to specify which files in the
                │   folder to process. [String]
 folderFilter   │ Denotes this is searching an entire folder structure and provides a Regular Expression of path/file
                │   names within it that should be processed.
                │   All matches should be of the same format. [String]
 zipFilter (-z) │ Denotes this is a Zip file and provides a Regular Expression of path/file names within it that should
                │   be processed.
                │   All matches should be of the same format. [String]
 addFileName    │ Adds a column (the first column) to the result set which contains the file the row came from.
                │   [Boolean]
 table          │ Table name to read.  If missing then an error will be raised if there is any number of tables other
                │   than one. [String]
"," file (-f)      │ *Mandatory* The file to read. Should be a sqlite file.
                │   It may also be a folder, in which case --folderFilter is also required to specify which files in the
                │   folder to process. [String]
 folderFilter   │ Denotes this is searching an entire folder structure and provides a Regular Expression of path/file
                │   names within it that should be processed.
                │   All matches should be of the same format. [String]
 zipFilter (-z) │ Denotes this is a Zip file and provides a Regular Expression of path/file names within it that should
                │   be processed.
                │   All matches should be of the same format. [String]
 addFileName    │ Adds a column (the first column) to the result set which contains the file the row came from.
                │   [Boolean]
 table          │ Table name to read.  If missing then an error will be raised if there is any number of tables other
                │   than one. [String]","Of the form:

use AwsS3.Sqlite [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

",[]
Used to read XML files held within S3,DirectProvider,,Utilities,AwsS3.Xml,2022-03-25 16:02:57.4680374Z,"Of the form:

use AwsS3.Xml [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

<OPTIONS>:

_______________________________________________________________________________________________________________________
 file (-f)         │ *Mandatory* The file to read. Should be text (any extension) or compressed (.zip).
                   │   It may also be a folder, in which case --folderFilter is also required to specify which files in
                   │   the folder to process. [String]
 folderFilter      │ Denotes this is searching an entire folder structure and provides a Regular Expression of
                   │   path/file names within it that should be processed.
                   │   All matches should be of the same format. [String]
 zipFilter (-z)    │ Denotes this is a Zip file and provides a Regular Expression of path/file names within it that
                   │   should be processed.
                   │   All matches should be of the same format. [String]
 addFileName       │ Adds a column (the first column) to the result set which contains the file the row came from.
                   │   [Boolean]
 types             │ Column types (comma delimited list of : Boolean, Date, DateTime, Decimal, Double, Int, BigInt,
                   │   Text, Table) [String]
 inferTypeRowCount │ If non-zero and 'types' is not specified this will look through N rows to attempt to work out the
                   │   column types [Int32]
 valuesToMakeNull  │ Regex of values to map to 'null' in the returned data. [String, Default:
                   │   (^[nN][uU][lL]{1,2}$)|((\r\n|\n|\r)$)|(^(\r\n|\n|\r))|(^\s*$)]
 nodePath          │ XPath query that selects the nodes to map to rows [String, Default: //*]
 namespaces        │ Selected prefix(es) and
                   │   namespace(s):prefix1=namespace1-uri1,prefix2=namespace2-uri2,...prefixN=namespaceN-uriN [String]
 columns           │ Column names [String]
"," file (-f)         │ *Mandatory* The file to read. Should be text (any extension) or compressed (.zip).
                   │   It may also be a folder, in which case --folderFilter is also required to specify which files in
                   │   the folder to process. [String]
 folderFilter      │ Denotes this is searching an entire folder structure and provides a Regular Expression of
                   │   path/file names within it that should be processed.
                   │   All matches should be of the same format. [String]
 zipFilter (-z)    │ Denotes this is a Zip file and provides a Regular Expression of path/file names within it that
                   │   should be processed.
                   │   All matches should be of the same format. [String]
 addFileName       │ Adds a column (the first column) to the result set which contains the file the row came from.
                   │   [Boolean]
 types             │ Column types (comma delimited list of : Boolean, Date, DateTime, Decimal, Double, Int, BigInt,
                   │   Text, Table) [String]
 inferTypeRowCount │ If non-zero and 'types' is not specified this will look through N rows to attempt to work out the
                   │   column types [Int32]
 valuesToMakeNull  │ Regex of values to map to 'null' in the returned data. [String, Default:
                   │   (^[nN][uU][lL]{1,2}$)|((\r\n|\n|\r)$)|(^(\r\n|\n|\r))|(^\s*$)]
 nodePath          │ XPath query that selects the nodes to map to rows [String, Default: //*]
 namespaces        │ Selected prefix(es) and
                   │   namespace(s):prefix1=namespace1-uri1,prefix2=namespace2-uri2,...prefixN=namespaceN-uriN [String]
 columns           │ Column names [String]","Of the form:

use AwsS3.Xml [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

",[]
Provides the ability to send notifications to Slack,DirectProvider,,Utilities,Dev.Slack.Send,2022-03-25 16:02:54.26031Z,"Sends a message to Slack
Of the form:

@result = use Dev.Slack.Send [with @@x, @y, ...]
  <OPTIONS>
  [----]
  [<JSON MESSAGE>]
enduse;

<JSON MESSAGE>: 
  The message to send (including where to send it).
  This is allowed ONLY if --rawJason is used.
  This can take advantage of '@@scalar-variables' as well as making tables from '@table-variables' (but keep them short)
  For basic messages see: https://api.slack.com/docs/messages/builder
  For very complex ones see: https://app.slack.com/block-kit-builder
  
@@channel = select '#your_channel';
@@txt = select 'This is the basic message text';
-- some small amount of data here...
@data = select distinct ^ from sys.field order by 1 limit 5;

@resp = 
use Dev.Slack.Send with @@channel, @@txt, @data
--rawJson
----

{
    ""channel"": ""{@@channel}"",
    ""text"": ""{@@txt}"",
    ""attachments"": [
    {
        ""text"": ""
Some data as an ascii art table...
{@data}""
    }
    ]
}
enduse;

select * from @resp

<OPTIONS>:
_______________________________________________________________________________________________________________________
 topN                │ The maximum number of rows to show in any table (when not attaching them as files) [Int32,
                     │   Default: 5]
 attachAs            │ Attach files with this type as opposed to putting the data in a text message [Csv, Excel,
                     │   SqLite, Json]
 attachAsInThread    │ When attaching files should they be in a response to the thread or simply shared after the
                     │   message [Boolean]
 attachAsOneFileName │ When attaching files (and there are many) combine them into one file (the method for this varies
                     │   by --attachAs) using the name/title provided [String]
 maxWidth            │ The maximum column width for any column [Int32]
 json                │ Setting this flag means you must specify the full Json template for Slack.  Allows for much more
                     │   flexibility & complexity. [Boolean]
 allowFailure        │ Without this flag being set the query will error if the message cannot be sent in Slack.
                     │   [Boolean]
 ignoreOnZeroRows    │ Flag to say, if one or more tables are provided and they all have zero rows, then do not send a
                     │   message at all. [Boolean]
 channel             │ Name of the Channel to send to (generally ignored with --json, although you may use {channel} to
                     │   reference it).
                     │   This can be #some_channel_name, a user-uid, a user-display-name a user-full-name or a
                     │   user-email-address [String]
 text                │ The body text of the message (in Markdown) (ignored with --json). [String]
"," topN                │ The maximum number of rows to show in any table (when not attaching them as files) [Int32,
                     │   Default: 5]
 attachAs            │ Attach files with this type as opposed to putting the data in a text message [Csv, Excel,
                     │   SqLite, Json]
 attachAsInThread    │ When attaching files should they be in a response to the thread or simply shared after the
                     │   message [Boolean]
 attachAsOneFileName │ When attaching files (and there are many) combine them into one file (the method for this varies
                     │   by --attachAs) using the name/title provided [String]
 maxWidth            │ The maximum column width for any column [Int32]
 json                │ Setting this flag means you must specify the full Json template for Slack.  Allows for much more
                     │   flexibility & complexity. [Boolean]
 allowFailure        │ Without this flag being set the query will error if the message cannot be sent in Slack.
                     │   [Boolean]
 ignoreOnZeroRows    │ Flag to say, if one or more tables are provided and they all have zero rows, then do not send a
                     │   message at all. [Boolean]
 channel             │ Name of the Channel to send to (generally ignored with --json, although you may use {channel} to
                     │   reference it).
                     │   This can be #some_channel_name, a user-uid, a user-display-name a user-full-name or a
                     │   user-email-address [String]
 text                │ The body text of the message (in Markdown) (ignored with --json). [String]","Sends a message to Slack
Of the form:

@result = use Dev.Slack.Send [with @@x, @y, ...]
  <OPTIONS>
  [----]
  [<JSON MESSAGE>]
enduse;

<JSON MESSAGE>: 
  The message to send (including where to send it).
  This is allowed ONLY if --rawJason is used.
  This can take advantage of '@@scalar-variables' as well as making tables from '@table-variables' (but keep them short)
  For basic messages see: https://api.slack.com/docs/messages/builder
  For very complex ones see: https://app.slack.com/block-kit-builder
  
@@channel = select '#your_channel';
@@txt = select 'This is the basic message text';
-- some small amount of data here...
@data = select distinct ^ from sys.field order by 1 limit 5;

@resp = 
use Dev.Slack.Send with @@channel, @@txt, @data
--rawJson
----

{
    ""channel"": ""{@@channel}"",
    ""text"": ""{@@txt}"",
    ""attachments"": [
    {
        ""text"": ""
Some data as an ascii art table...
{@data}""
    }
    ]
}
enduse;

select * from @resp

",['json_message']
"Used to read CSV, Text, Pipe-Delimited, etc. files held within Lusid Drive",DirectProvider,https://support.lusid.com/knowledgebase/article/KA-01680/,"Lusid, Utilities",Drive.Csv,2022-03-25 16:02:56.4709206Z,"Of the form:

use Drive.Csv [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

<OPTIONS>:

_______________________________________________________________________________________________________________________
 file (-f)         │ *Mandatory* The file to read. Should be text (any extension) or compressed (.zip).
                   │   It may also be a folder, in which case --folderFilter is also required to specify which files in
                   │   the folder to process. [String]
 folderFilter      │ Denotes this is searching an entire folder structure and provides a Regular Expression of
                   │   path/file names within it that should be processed.
                   │   All matches should be of the same format. [String]
 zipFilter (-z)    │ Denotes this is a Zip file and provides a Regular Expression of path/file names within it that
                   │   should be processed.
                   │   All matches should be of the same format. [String]
 addFileName       │ Adds a column (the first column) to the result set which contains the file the row came from.
                   │   [Boolean]
 names             │ Column Names either overrides the header row or steps in when there is no header row (comma
                   │   delimited list) [String]
 select            │ Column (by Name) that should be returned (comma delimited list) [String]
 types             │ Column types (comma delimited list of : Boolean, Date, DateTime, Decimal, Double, Int, BigInt,
                   │   Text, Table) [String]
 inferTypeRowCount │ If non-zero and 'types' is not specified this will look through N rows to attempt to work out the
                   │   column types [Int32]
 noHeader          │ Set this if there is no header row [Boolean]
 delimiter         │ The delimiter between values (\t for tab) [String, Default: ,]
 escape            │ Character used to escape the 'Quote' character when within a value [String, Default: ""]
 quote             │ Character used around any field containing the 'delimiter' or a line break. [String, Default: ""]
 valuesToMakeNull  │ Regex of values to map to 'null' in the returned data. [String, Default:
                   │   (^[nN][uU][lL]{1,2}$)|((\r\n|\n|\r)$)|(^(\r\n|\n|\r))|(^\s*$)]
 skipPreHeader     │ Number of rows to ignore before the header row [Int32]
 skipPostHeader    │ Number of rows to ignore after the header row [Int32]
 skipInvalidRows   │ Skip invalid data rows (totally invalid ones), 
                   │   This also allows for potentially wrong data if it can be handled somewhat e.g. embedded quotes
                   │   misused (and still returns such rows).
                   │   In either case a warning will show in the progress feedback. [Boolean]
"," file (-f)         │ *Mandatory* The file to read. Should be text (any extension) or compressed (.zip).
                   │   It may also be a folder, in which case --folderFilter is also required to specify which files in
                   │   the folder to process. [String]
 folderFilter      │ Denotes this is searching an entire folder structure and provides a Regular Expression of
                   │   path/file names within it that should be processed.
                   │   All matches should be of the same format. [String]
 zipFilter (-z)    │ Denotes this is a Zip file and provides a Regular Expression of path/file names within it that
                   │   should be processed.
                   │   All matches should be of the same format. [String]
 addFileName       │ Adds a column (the first column) to the result set which contains the file the row came from.
                   │   [Boolean]
 names             │ Column Names either overrides the header row or steps in when there is no header row (comma
                   │   delimited list) [String]
 select            │ Column (by Name) that should be returned (comma delimited list) [String]
 types             │ Column types (comma delimited list of : Boolean, Date, DateTime, Decimal, Double, Int, BigInt,
                   │   Text, Table) [String]
 inferTypeRowCount │ If non-zero and 'types' is not specified this will look through N rows to attempt to work out the
                   │   column types [Int32]
 noHeader          │ Set this if there is no header row [Boolean]
 delimiter         │ The delimiter between values (\t for tab) [String, Default: ,]
 escape            │ Character used to escape the 'Quote' character when within a value [String, Default: ""]
 quote             │ Character used around any field containing the 'delimiter' or a line break. [String, Default: ""]
 valuesToMakeNull  │ Regex of values to map to 'null' in the returned data. [String, Default:
                   │   (^[nN][uU][lL]{1,2}$)|((\r\n|\n|\r)$)|(^(\r\n|\n|\r))|(^\s*$)]
 skipPreHeader     │ Number of rows to ignore before the header row [Int32]
 skipPostHeader    │ Number of rows to ignore after the header row [Int32]
 skipInvalidRows   │ Skip invalid data rows (totally invalid ones), 
                   │   This also allows for potentially wrong data if it can be handled somewhat e.g. embedded quotes
                   │   misused (and still returns such rows).
                   │   In either case a warning will show in the progress feedback. [Boolean]","Of the form:

use Drive.Csv [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

",[]
Used to read Excel files held within Lusid Drive,DirectProvider,https://support.lusid.com/knowledgebase/article/KA-01682/,"Lusid, Utilities",Drive.Excel,2022-03-25 16:02:56.4709206Z,"Of the form:

use Drive.Excel [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

<OPTIONS>:

_______________________________________________________________________________________________________________________
 file (-f)         │ *Mandatory* The file to read. Should be an Excel file (.xlsx or .xlsm)
                   │   It may also be a folder, in which case --folderFilter is also required to specify which files in
                   │   the folder to process. [String]
 folderFilter      │ Denotes this is searching an entire folder structure and provides a Regular Expression of
                   │   path/file names within it that should be processed.
                   │   All matches should be of the same format. [String]
 zipFilter (-z)    │ Denotes this is a Zip file and provides a Regular Expression of path/file names within it that
                   │   should be processed.
                   │   All matches should be of the same format. [String]
 addFileName       │ Adds a column (the first column) to the result set which contains the file the row came from.
                   │   [Boolean]
 names             │ Column Names either overrides the header row or steps in when there is no header row (comma
                   │   delimited list) [String]
 types             │ Column types (comma delimited list of : Boolean, Date, DateTime, Decimal, Double, Int, BigInt,
                   │   Text, Table) [String]
 inferTypeRowCount │ If non-zero and 'types' is not specified this will look through N rows to attempt to work out the
                   │   column types [Int32]
 noHeader          │ Set this if there is no header row [Boolean]
 calculate         │ Whether to attempt a calculation of the imported cell range prior to import [Boolean]
 password          │ If specified will be used as the password used for password protected workbooks [String]
 worksheet (-w)    │ The worksheet containing the cell range to import (name or index, will default to first) [String]
 range (-r)        │ The cell range to import as either a specified range or a table name.
                   │   Range formats can be any of: 
                   │   - 'A1:B3' (standard Excel range format)
                   │   - '1,3,7,9' (rows 1-7, columns 3-9)
                   │   - ',3,,9' (all populated rows, but only columns 3-9)
                   │   - ',3,,' (all populated rows, but only columns 3 to the last populated one)
                   │   Defaults to the used dimensions of the worksheet (equivalent to ',,,') [String]
 ignoreInvalid     │ If specified cells which can not be successfully converted to the target type will be ignored
                   │   [Boolean]
 ignoreBlankRows   │ If the entire rows has only blank cells it will be ignored will be ignored [Boolean]
"," file (-f)         │ *Mandatory* The file to read. Should be an Excel file (.xlsx or .xlsm)
                   │   It may also be a folder, in which case --folderFilter is also required to specify which files in
                   │   the folder to process. [String]
 folderFilter      │ Denotes this is searching an entire folder structure and provides a Regular Expression of
                   │   path/file names within it that should be processed.
                   │   All matches should be of the same format. [String]
 zipFilter (-z)    │ Denotes this is a Zip file and provides a Regular Expression of path/file names within it that
                   │   should be processed.
                   │   All matches should be of the same format. [String]
 addFileName       │ Adds a column (the first column) to the result set which contains the file the row came from.
                   │   [Boolean]
 names             │ Column Names either overrides the header row or steps in when there is no header row (comma
                   │   delimited list) [String]
 types             │ Column types (comma delimited list of : Boolean, Date, DateTime, Decimal, Double, Int, BigInt,
                   │   Text, Table) [String]
 inferTypeRowCount │ If non-zero and 'types' is not specified this will look through N rows to attempt to work out the
                   │   column types [Int32]
 noHeader          │ Set this if there is no header row [Boolean]
 calculate         │ Whether to attempt a calculation of the imported cell range prior to import [Boolean]
 password          │ If specified will be used as the password used for password protected workbooks [String]
 worksheet (-w)    │ The worksheet containing the cell range to import (name or index, will default to first) [String]
 range (-r)        │ The cell range to import as either a specified range or a table name.
                   │   Range formats can be any of: 
                   │   - 'A1:B3' (standard Excel range format)
                   │   - '1,3,7,9' (rows 1-7, columns 3-9)
                   │   - ',3,,9' (all populated rows, but only columns 3-9)
                   │   - ',3,,' (all populated rows, but only columns 3 to the last populated one)
                   │   Defaults to the used dimensions of the worksheet (equivalent to ',,,') [String]
 ignoreInvalid     │ If specified cells which can not be successfully converted to the target type will be ignored
                   │   [Boolean]
 ignoreBlankRows   │ If the entire rows has only blank cells it will be ignored will be ignored [Boolean]","Of the form:

use Drive.Excel [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

",[]
Used to read text files in a raw-content from held within Lusid Drive,DirectProvider,https://support.lusid.com/knowledgebase/article/KA-01754/,"Lusid, Utilities",Drive.RawText,2022-03-25 16:02:56.4709206Z,"Of the form:

use Drive.RawText [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

<OPTIONS>:

_______________________________________________________________________________________________________________________
 file (-f)      │ *Mandatory* The file to read.
                │   It may also be a folder, in which case --folderFilter is also required to specify which files in the
                │   folder to process. [String]
 folderFilter   │ Denotes this is searching an entire folder structure and provides a Regular Expression of path/file
                │   names within it that should be processed.
                │   All matches should be of the same format. [String]
 zipFilter (-z) │ Denotes this is a Zip file and provides a Regular Expression of path/file names within it that should
                │   be processed.
                │   All matches should be of the same format. [String]
 addFileName    │ Adds a column (the first column) to the result set which contains the file the row came from.
                │   [Boolean]
"," file (-f)      │ *Mandatory* The file to read.
                │   It may also be a folder, in which case --folderFilter is also required to specify which files in the
                │   folder to process. [String]
 folderFilter   │ Denotes this is searching an entire folder structure and provides a Regular Expression of path/file
                │   names within it that should be processed.
                │   All matches should be of the same format. [String]
 zipFilter (-z) │ Denotes this is a Zip file and provides a Regular Expression of path/file names within it that should
                │   be processed.
                │   All matches should be of the same format. [String]
 addFileName    │ Adds a column (the first column) to the result set which contains the file the row came from.
                │   [Boolean]","Of the form:

use Drive.RawText [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

",[]
Used to write query results to Lusid Drive in a variety of formats,DirectProvider,https://support.lusid.com/knowledgebase/article/KA-01693/,"Lusid, Files, Utilities",Drive.SaveAs,2022-03-25 16:02:56.4709206Z,"Of the form:

use Drive.SaveAs [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

<OPTIONS>:

_______________________________________________________________________________________________________________________
 type             │ The file types to create - also controls file extensions if not specified [Csv, Excel, SqLite,
                  │   Json, Default: Csv]
 combineToOne     │ Combine all files to one for this export - and give them this name [String]
 ignoreOnZeroRows │ Flag to say, if a file would have zero rows, do not write it at all. [Boolean]
 fileNames        │ *Mandatory* File names (without a path).  One per table of data is required. [String]
 path             │ *Mandatory* The location to save this within the target file store. [String]
"," type             │ The file types to create - also controls file extensions if not specified [Csv, Excel, SqLite,
                  │   Json, Default: Csv]
 combineToOne     │ Combine all files to one for this export - and give them this name [String]
 ignoreOnZeroRows │ Flag to say, if a file would have zero rows, do not write it at all. [Boolean]
 fileNames        │ *Mandatory* File names (without a path).  One per table of data is required. [String]
 path             │ *Mandatory* The location to save this within the target file store. [String]","Of the form:

use Drive.SaveAs [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

",[]
Used to read tables from Sqlite files held within Lusid Drive,DirectProvider,,"Lusid, Utilities",Drive.Sqlite,2022-03-25 16:02:56.4709206Z,"Of the form:

use Drive.Sqlite [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

<OPTIONS>:

_______________________________________________________________________________________________________________________
 file (-f)      │ *Mandatory* The file to read. Should be a sqlite file.
                │   It may also be a folder, in which case --folderFilter is also required to specify which files in the
                │   folder to process. [String]
 folderFilter   │ Denotes this is searching an entire folder structure and provides a Regular Expression of path/file
                │   names within it that should be processed.
                │   All matches should be of the same format. [String]
 zipFilter (-z) │ Denotes this is a Zip file and provides a Regular Expression of path/file names within it that should
                │   be processed.
                │   All matches should be of the same format. [String]
 addFileName    │ Adds a column (the first column) to the result set which contains the file the row came from.
                │   [Boolean]
 table          │ Table name to read.  If missing then an error will be raised if there is any number of tables other
                │   than one. [String]
"," file (-f)      │ *Mandatory* The file to read. Should be a sqlite file.
                │   It may also be a folder, in which case --folderFilter is also required to specify which files in the
                │   folder to process. [String]
 folderFilter   │ Denotes this is searching an entire folder structure and provides a Regular Expression of path/file
                │   names within it that should be processed.
                │   All matches should be of the same format. [String]
 zipFilter (-z) │ Denotes this is a Zip file and provides a Regular Expression of path/file names within it that should
                │   be processed.
                │   All matches should be of the same format. [String]
 addFileName    │ Adds a column (the first column) to the result set which contains the file the row came from.
                │   [Boolean]
 table          │ Table name to read.  If missing then an error will be raised if there is any number of tables other
                │   than one. [String]","Of the form:

use Drive.Sqlite [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

",[]
Used to read XML files held within Lusid Drive,DirectProvider,https://support.lusid.com/knowledgebase/article/KA-01753/,"Lusid, Utilities",Drive.Xml,2022-03-25 16:02:56.4709206Z,"Of the form:

use Drive.Xml [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

<OPTIONS>:

_______________________________________________________________________________________________________________________
 file (-f)         │ *Mandatory* The file to read. Should be text (any extension) or compressed (.zip).
                   │   It may also be a folder, in which case --folderFilter is also required to specify which files in
                   │   the folder to process. [String]
 folderFilter      │ Denotes this is searching an entire folder structure and provides a Regular Expression of
                   │   path/file names within it that should be processed.
                   │   All matches should be of the same format. [String]
 zipFilter (-z)    │ Denotes this is a Zip file and provides a Regular Expression of path/file names within it that
                   │   should be processed.
                   │   All matches should be of the same format. [String]
 addFileName       │ Adds a column (the first column) to the result set which contains the file the row came from.
                   │   [Boolean]
 types             │ Column types (comma delimited list of : Boolean, Date, DateTime, Decimal, Double, Int, BigInt,
                   │   Text, Table) [String]
 inferTypeRowCount │ If non-zero and 'types' is not specified this will look through N rows to attempt to work out the
                   │   column types [Int32]
 valuesToMakeNull  │ Regex of values to map to 'null' in the returned data. [String, Default:
                   │   (^[nN][uU][lL]{1,2}$)|((\r\n|\n|\r)$)|(^(\r\n|\n|\r))|(^\s*$)]
 nodePath          │ XPath query that selects the nodes to map to rows [String, Default: //*]
 namespaces        │ Selected prefix(es) and
                   │   namespace(s):prefix1=namespace1-uri1,prefix2=namespace2-uri2,...prefixN=namespaceN-uriN [String]
 columns           │ Column names [String]
"," file (-f)         │ *Mandatory* The file to read. Should be text (any extension) or compressed (.zip).
                   │   It may also be a folder, in which case --folderFilter is also required to specify which files in
                   │   the folder to process. [String]
 folderFilter      │ Denotes this is searching an entire folder structure and provides a Regular Expression of
                   │   path/file names within it that should be processed.
                   │   All matches should be of the same format. [String]
 zipFilter (-z)    │ Denotes this is a Zip file and provides a Regular Expression of path/file names within it that
                   │   should be processed.
                   │   All matches should be of the same format. [String]
 addFileName       │ Adds a column (the first column) to the result set which contains the file the row came from.
                   │   [Boolean]
 types             │ Column types (comma delimited list of : Boolean, Date, DateTime, Decimal, Double, Int, BigInt,
                   │   Text, Table) [String]
 inferTypeRowCount │ If non-zero and 'types' is not specified this will look through N rows to attempt to work out the
                   │   column types [Int32]
 valuesToMakeNull  │ Regex of values to map to 'null' in the returned data. [String, Default:
                   │   (^[nN][uU][lL]{1,2}$)|((\r\n|\n|\r)$)|(^(\r\n|\n|\r))|(^\s*$)]
 nodePath          │ XPath query that selects the nodes to map to rows [String, Default: //*]
 namespaces        │ Selected prefix(es) and
                   │   namespace(s):prefix1=namespace1-uri1,prefix2=namespace2-uri2,...prefixN=namespaceN-uriN [String]
 columns           │ Column names [String]","Of the form:

use Drive.Xml [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

",[]
Provides the ability to send emails vs SMTP,DirectProvider,,Utilities,Email.Send,2022-03-25 16:02:56.4709206Z,"Sends an email message
Of the form:

@result = use Email.Send [with @@x, @y, ...]
  <OPTIONS>
  ----
  <MARKDOWN MESSAGE>
enduse;

<MARKDOWN MESSAGE>: 
  The message to send in Markdown form (this will be converted into HTML for emailing purposes)

<OPTIONS>:
_______________________________________________________________________________________________________________________
 addressTo           │ The TO address in the form: abc@xyz.com or [Abc](abc@xyz.com) [String]
 addressCc           │ The CC address in the form: abc@xyz.com or [Abc](abc@xyz.com) [String]
 addressBcc          │ The BCC address in the form: abc@xyz.com or [Abc](abc@xyz.com) [String]
 topN                │ The maximum number of rows to show in any table (when not attaching them as files) [Int32,
                     │   Default: 5]
 attachAs            │ Attach files with this type as opposed to putting the data in a text message [Csv, Excel,
                     │   SqLite, Json]
 attachAsOneFileName │ When attaching files (and there are many) combine them into one file (the method for this varies
                     │   by --attachAs) using the name/title provided [String]
 allowFailure        │ Without this flag being set the query will error if the message cannot be sent. [Boolean]
 ignoreOnZeroRows    │ Flag to say, if one or more tables are provided and they all have zero rows, then do not send a
                     │   message at all. [Boolean]
 subject             │ *Mandatory* Subject for the email [String]
"," addressTo           │ The TO address in the form: abc@xyz.com or [Abc](abc@xyz.com) [String]
 addressCc           │ The CC address in the form: abc@xyz.com or [Abc](abc@xyz.com) [String]
 addressBcc          │ The BCC address in the form: abc@xyz.com or [Abc](abc@xyz.com) [String]
 topN                │ The maximum number of rows to show in any table (when not attaching them as files) [Int32,
                     │   Default: 5]
 attachAs            │ Attach files with this type as opposed to putting the data in a text message [Csv, Excel,
                     │   SqLite, Json]
 attachAsOneFileName │ When attaching files (and there are many) combine them into one file (the method for this varies
                     │   by --attachAs) using the name/title provided [String]
 allowFailure        │ Without this flag being set the query will error if the message cannot be sent. [Boolean]
 ignoreOnZeroRows    │ Flag to say, if one or more tables are provided and they all have zero rows, then do not send a
                     │   message at all. [Boolean]
 subject             │ *Mandatory* Subject for the email [String]","Sends an email message
Of the form:

@result = use Email.Send [with @@x, @y, ...]
  <OPTIONS>
  ----
  <MARKDOWN MESSAGE>
enduse;

<MARKDOWN MESSAGE>: 
  The message to send in Markdown form (this will be converted into HTML for emailing purposes)

",['markdown_message']
Provides the ability to execute SQL against a Database directly and promote that to being a Provider,DirectProvider,,DatabaseView,Sql.Db,2022-03-25 16:02:56.7070954Z,"Runs SQL given a set of options & Some SQL.
Of the form:

use Sql.Db
  <OPTIONS>
  ----
  <SQL FOR READING>
  ----
  <SQL FOR WRITING>
  ----
  [<SQL FOR SETUP WRITING>]
enduse;


<SQL FOR READING>: 
  The SQL to run against the raw data source.
  This should ideally contain the following macros (at least if you intend to use --provider)
  You may have as many of each as you need in a given query.
    - #SELECT
      - Used to only select the columns the user requests.
      - Becomes '1' if no columns are selected.  
      - Will not include the word 'SELECT' itself.
      - If a column has a + after the name, this will be considered a PrimaryKey in the result set (if applicable, e.g. creating a provider)
      - If a column has a ^ after the name, this will be considered a Main field in the result set (if applicable, e.g. creating a provider)
      #SELECT
      {
         { some-column-name+^ : sql-to-give-that-column },
         { next-column-name^ : more-sql-main-column },
         { yet-next-column-name : more-sql-normal-column },
         ...
      }

    - #SELECT_AGG *
       - The same as #SELECT, except it means ""this allows for SQL-Aggregations to be performed"", and must follow these rules:
         - There can be only one (if there isn't one no aggregations will be passed down to the underlying SQL implementation)
         - It must contain all the fields that can possibly be returned

    - #PARAMETERVALUE(ParameterName) : 
      - Used to pass a user-supplied, initial or default value of the parameter with name 'ParameterName' to the raw data source.
      - If a parameter of the required name cannot be found, an exception will be thrown.
      - Parameters are specified through the command line option --parameters, with parameters defined on successive lines in the format:
            Name,Type,InitialValue[,SetAsDefaultValue=true] 
      - Parameter types must be one of: 
            Boolean, Int, BigInt, Double, Decimal, Text, Date, DateTime
      - An initial value must be provided for all parameters.
      - Optionally, this value may be set as the default value for parameters in any created Provider using the 'SetAsDefaultValue' argument.
      - If no default value is set, an exception will be thrown when calling created Providers without a user-supplied value for Text types,
        and otherwise the type-specific default will be used.
        
    - #RESTRICT : 
      - Used to pass down the user-filter given to the Provider to the raw data source
      - Any user filter requiring more fields than are used in a given macro block will be reduced as required.
        So there can be benefits in using the macro multiple times, where some earlier blocks may only be able to filter on certain fields
        and later blocks that could filter on more fields, but perhaps not as efficiently.
      - Becomes '(1 = 1)' if the user-filter wasn't given or it required fields not in the macro.  
      - Will not include the word 'WHERE' / 'ON' itself (may be used in joins or where clause as a result, even if only part of the clause).

      #RESTRICT
      {
         { some-column-name : sql-to-give-that-column },
         { next-column-name : more-sql },
         ...
      }

    - #RESTRICT_AGG *
      - Rather like restrict, but you do not specify any members, simply provide this token.
      - Members will be pulled from #SELECT_AGG, which must exist (so using this is more concise than #RESTRICT)
      - All these members must be filterable, and it must be all the fields returned.

    - #LIMIT
      - Used to fill in the 'LIMIT / TOP / ROWNUM' as required by the Sql Dialect 

    - #DISTINCT
      - Used to fill in the 'DISTINCT' as required by the Sql Dialect 

    - #GROUP_BY *
      - Will be replaced with a group by clause at that location (or removed)

    * = Required in the case when you wish to support Aggregations being passed down to the underlying SQL engine (when possible)

  Full example for SQL READING:

      SELECT #DISTINCT #SELECT_AGG {
        { x : a.x },
        { y : a.y },
        { z : b.f },
        }
      FROM
        abc a
        LEFT OUTER join b
         on a.id = b.id
         and #RESTRICT { { z : b.f } }
      WHERE #RESTRICT_AGG
      #GROUP_BY
      #LIMIT


<SQL FOR WRITING>:
  You must specify --withWriter for this, and thus the intent is to WRITE (as well as read) data.
  If you do that and specify --provider, you will spin up two providers `A.B.C` for reading and `A.B.C.Writer` for writing.
  These may have different license requirements and connections strings.

  This SQL may be of the following form (and is best done by example and varies by DbType heavily):

       INSERT INTO my_table (#INSERT_COLUMN_NAMES)
       VALUES (#INSERT_COLUMN_VALUES)
       ON CONFLICT (name)
           DO UPDATE SET
           updated_at = CURRENT_TIMESTAMP,
           #UPDATE_COLUMN_ASSIGNMENTS
           {
           { name : name = excluded.name },
           { email : email = case 
               when (my_table.email like '%' || excluded.email || '%') 
               then my_table.email 
               else excluded.email || ';' || my_table.email 
               end },
           }
           WHERE
           #INSERT_COLUMN_CHANGED_FILTER

    - #INSERT_COLUMN_NAMES
      - Expands to the columns the user provides which overlaps with those returned
      - Also supports aliasing, e.g.: #INSERT_COLUMN_NAMES { { x : xxx }, { b : b1 } }
      - The { { x : y }, ... } syntax is required when column names do not match the actual database field names.
    - #INSERT_COLUMN_VALUES
      - Expands to @p1, @p2, etc. (for the number of columns named)
    - #UPDATE_COLUMN_ASSIGNMENTS
      - used to describe how to assign values
    - #INSERT_COLUMN_CHANGED_FILTER
      - converts the #UPDATE_COLUMN_ASSIGNMENTS assignments into a 'where something has changed' filter

<SQL FOR SETUP WRITING>
  This will only be executed by 'Sql.Db' never by the created providers.
  Used for the likes of initial table creation, e.g.:

       CREATE TABLE IF NOT EXISTS some_schema.my_table (
           name VARCHAR(50) UNIQUE NOT NULL,
           email VARCHAR(1000) NOT NULL,
           updated_at TIMESTAMP without time zone NOT NULL default (now() at time zone 'utc'),
           created_at TIMESTAMP without time zone NOT NULL default (now() at time zone 'utc')
       );

<OPTIONS>:
_______________________________________________________________________________________________________________________
 licenceCode       │ The licence code to apply to the created provider, if any. [String]
 allDomains        │ This sets up reader/writer provider(s) which are for all domains (if they have the correct licence
                   │   of course), not ones only for the current domain as is normally done.  This has extra licencing
                   │   and entitlement requirements. [Boolean]
 otherSingleDomain │ This sets up reader/writer provider(s) which are for a different single domain (if is has the
                   │   correct licence of course), not ones only for the current domain as is normally done.  This has
                   │   extra licencing and entitlement requirements (as per --allDomains). [String]
 provider          │ The Provider Name which should be created, e.g. 'A.B.C'. This will save a .sql file to the HcFs
                   │   (e.g. 'RootPath/A/B/C.sql'). This is ignored until after the point at which a query runs
                   │   successfully after which a provider of that name will be created / updated / deleted. [String]
 rootPath          │ The root HcFs path under which to save the given provider.  This should be organized by how it
                   │   will later be used. [String, Default: DatabaseProviders]
 change            │ The 'commit message' for this change. [String, Default: Updated]
 deleteProvider    │ Instead of creating a provider by that name, remove any pre-existing one.  Requires --provider be
                   │   set. [Boolean]
 limit             │ A Row Limit to apply.  This will not be a part of the saved view definition (that will come from
                   │   how the view is used). [Int32, Default: 100]
 distinct          │ If Distinct should be applied.  This will not be a part of the saved view definition (that will
                   │   come from how the view is used). [Boolean]
 filter            │ A filter expression to be applied.  Any usage of fields not in the #RESTRICT macro(s) are pruned
                   │   out and this is not re-applied to the final data set (so invalid fields are OK).  This will not be
                   │   a part of the saved view definition (that will come from how the view is used). [String]
 select            │ Pipe (|) delimited set of expressions to select, this allows for testing Aggregations.  This will
                   │   not be a part of the saved view definition (that will come from how the view is used). [String]
 groupBy           │ Pipe (|) delimited set of expressions to group by, this allows for testing Aggregations.  This
                   │   will not be a part of the saved view definition (that will come from how the view is used).
                   │   [String]
 filterIsInWebForm │ True means the --filter is a `Finbourne WebAPI Filter`.  This will not be a part of the saved view
                   │   definition (that is not possible in this case). [Boolean]
 parameters        │ Any scalar parameters to use. These should be of the form
                   │   Name,Type,InitialValue[,SetAsDefaultValue=true]. Place one on each line after --parameters
                   │   [String]
 type              │ *Mandatory* The type of connection (which sort of Database) to use [MySql, Odbc, Oracle,
                   │   PostgreSql, SqLite, SqlServer, GoogleBigQuery, AwsAthena, Honeycomb]
 maxSize           │ The overall maximum allowed size of the database in KB, MB, GB etc. [String]
 connection        │ *Mandatory* The connection string for the database.  This may be an alias defined in the
                   │   'database-provider-settings.json'. [String]
 writerConnection  │ The connection string for the database, when writing data.  This may be an alias defined in the
                   │   'database-provider-settings.json' and defaults to '--connection'.  Only used if --withWriter is
                   │   specified. [String]
 writerLicenceCode │ The licence code to apply to the created writer-provider, if any. [String]
 withWriter        │ Creates a provider XX.Writer, when --provider is specified. Also allows for a Table of data to be
                   │   passed to a Sql.Db of data to write. [Boolean]
 commitEvery       │ When writing data commit every N rows; this can be used with OFFSET on the source data set to
                   │   incrementally load data if there are intermittent failures in the Database itself.  Default of 0
                   │   means only at the end of all rows. [Int32]
"," licenceCode       │ The licence code to apply to the created provider, if any. [String]
 allDomains        │ This sets up reader/writer provider(s) which are for all domains (if they have the correct licence
                   │   of course), not ones only for the current domain as is normally done.  This has extra licencing
                   │   and entitlement requirements. [Boolean]
 otherSingleDomain │ This sets up reader/writer provider(s) which are for a different single domain (if is has the
                   │   correct licence of course), not ones only for the current domain as is normally done.  This has
                   │   extra licencing and entitlement requirements (as per --allDomains). [String]
 provider          │ The Provider Name which should be created, e.g. 'A.B.C'. This will save a .sql file to the HcFs
                   │   (e.g. 'RootPath/A/B/C.sql'). This is ignored until after the point at which a query runs
                   │   successfully after which a provider of that name will be created / updated / deleted. [String]
 rootPath          │ The root HcFs path under which to save the given provider.  This should be organized by how it
                   │   will later be used. [String, Default: DatabaseProviders]
 change            │ The 'commit message' for this change. [String, Default: Updated]
 deleteProvider    │ Instead of creating a provider by that name, remove any pre-existing one.  Requires --provider be
                   │   set. [Boolean]
 limit             │ A Row Limit to apply.  This will not be a part of the saved view definition (that will come from
                   │   how the view is used). [Int32, Default: 100]
 distinct          │ If Distinct should be applied.  This will not be a part of the saved view definition (that will
                   │   come from how the view is used). [Boolean]
 filter            │ A filter expression to be applied.  Any usage of fields not in the #RESTRICT macro(s) are pruned
                   │   out and this is not re-applied to the final data set (so invalid fields are OK).  This will not be
                   │   a part of the saved view definition (that will come from how the view is used). [String]
 select            │ Pipe (|) delimited set of expressions to select, this allows for testing Aggregations.  This will
                   │   not be a part of the saved view definition (that will come from how the view is used). [String]
 groupBy           │ Pipe (|) delimited set of expressions to group by, this allows for testing Aggregations.  This
                   │   will not be a part of the saved view definition (that will come from how the view is used).
                   │   [String]
 filterIsInWebForm │ True means the --filter is a `Finbourne WebAPI Filter`.  This will not be a part of the saved view
                   │   definition (that is not possible in this case). [Boolean]
 parameters        │ Any scalar parameters to use. These should be of the form
                   │   Name,Type,InitialValue[,SetAsDefaultValue=true]. Place one on each line after --parameters
                   │   [String]
 type              │ *Mandatory* The type of connection (which sort of Database) to use [MySql, Odbc, Oracle,
                   │   PostgreSql, SqLite, SqlServer, GoogleBigQuery, AwsAthena, Honeycomb]
 maxSize           │ The overall maximum allowed size of the database in KB, MB, GB etc. [String]
 connection        │ *Mandatory* The connection string for the database.  This may be an alias defined in the
                   │   'database-provider-settings.json'. [String]
 writerConnection  │ The connection string for the database, when writing data.  This may be an alias defined in the
                   │   'database-provider-settings.json' and defaults to '--connection'.  Only used if --withWriter is
                   │   specified. [String]
 writerLicenceCode │ The licence code to apply to the created writer-provider, if any. [String]
 withWriter        │ Creates a provider XX.Writer, when --provider is specified. Also allows for a Table of data to be
                   │   passed to a Sql.Db of data to write. [Boolean]
 commitEvery       │ When writing data commit every N rows; this can be used with OFFSET on the source data set to
                   │   incrementally load data if there are intermittent failures in the Database itself.  Default of 0
                   │   means only at the end of all rows. [Int32]","Runs SQL given a set of options & Some SQL.
Of the form:

use Sql.Db
  <OPTIONS>
  ----
  <SQL FOR READING>
  ----
  <SQL FOR WRITING>
  ----
  [<SQL FOR SETUP WRITING>]
enduse;


<SQL FOR READING>: 
  The SQL to run against the raw data source.
  This should ideally contain the following macros (at least if you intend to use --provider)
  You may have as many of each as you need in a given query.
    - #SELECT
      - Used to only select the columns the user requests.
      - Becomes '1' if no columns are selected.  
      - Will not include the word 'SELECT' itself.
      - If a column has a + after the name, this will be considered a PrimaryKey in the result set (if applicable, e.g. creating a provider)
      - If a column has a ^ after the name, this will be considered a Main field in the result set (if applicable, e.g. creating a provider)
      #SELECT
      {
         { some-column-name+^ : sql-to-give-that-column },
         { next-column-name^ : more-sql-main-column },
         { yet-next-column-name : more-sql-normal-column },
         ...
      }

    - #SELECT_AGG *
       - The same as #SELECT, except it means ""this allows for SQL-Aggregations to be performed"", and must follow these rules:
         - There can be only one (if there isn't one no aggregations will be passed down to the underlying SQL implementation)
         - It must contain all the fields that can possibly be returned

    - #PARAMETERVALUE(ParameterName) : 
      - Used to pass a user-supplied, initial or default value of the parameter with name 'ParameterName' to the raw data source.
      - If a parameter of the required name cannot be found, an exception will be thrown.
      - Parameters are specified through the command line option --parameters, with parameters defined on successive lines in the format:
            Name,Type,InitialValue[,SetAsDefaultValue=true] 
      - Parameter types must be one of: 
            Boolean, Int, BigInt, Double, Decimal, Text, Date, DateTime
      - An initial value must be provided for all parameters.
      - Optionally, this value may be set as the default value for parameters in any created Provider using the 'SetAsDefaultValue' argument.
      - If no default value is set, an exception will be thrown when calling created Providers without a user-supplied value for Text types,
        and otherwise the type-specific default will be used.
        
    - #RESTRICT : 
      - Used to pass down the user-filter given to the Provider to the raw data source
      - Any user filter requiring more fields than are used in a given macro block will be reduced as required.
        So there can be benefits in using the macro multiple times, where some earlier blocks may only be able to filter on certain fields
        and later blocks that could filter on more fields, but perhaps not as efficiently.
      - Becomes '(1 = 1)' if the user-filter wasn't given or it required fields not in the macro.  
      - Will not include the word 'WHERE' / 'ON' itself (may be used in joins or where clause as a result, even if only part of the clause).

      #RESTRICT
      {
         { some-column-name : sql-to-give-that-column },
         { next-column-name : more-sql },
         ...
      }

    - #RESTRICT_AGG *
      - Rather like restrict, but you do not specify any members, simply provide this token.
      - Members will be pulled from #SELECT_AGG, which must exist (so using this is more concise than #RESTRICT)
      - All these members must be filterable, and it must be all the fields returned.

    - #LIMIT
      - Used to fill in the 'LIMIT / TOP / ROWNUM' as required by the Sql Dialect 

    - #DISTINCT
      - Used to fill in the 'DISTINCT' as required by the Sql Dialect 

    - #GROUP_BY *
      - Will be replaced with a group by clause at that location (or removed)

    * = Required in the case when you wish to support Aggregations being passed down to the underlying SQL engine (when possible)

  Full example for SQL READING:

      SELECT #DISTINCT #SELECT_AGG {
        { x : a.x },
        { y : a.y },
        { z : b.f },
        }
      FROM
        abc a
        LEFT OUTER join b
         on a.id = b.id
         and #RESTRICT { { z : b.f } }
      WHERE #RESTRICT_AGG
      #GROUP_BY
      #LIMIT


<SQL FOR WRITING>:
  You must specify --withWriter for this, and thus the intent is to WRITE (as well as read) data.
  If you do that and specify --provider, you will spin up two providers `A.B.C` for reading and `A.B.C.Writer` for writing.
  These may have different license requirements and connections strings.

  This SQL may be of the following form (and is best done by example and varies by DbType heavily):

       INSERT INTO my_table (#INSERT_COLUMN_NAMES)
       VALUES (#INSERT_COLUMN_VALUES)
       ON CONFLICT (name)
           DO UPDATE SET
           updated_at = CURRENT_TIMESTAMP,
           #UPDATE_COLUMN_ASSIGNMENTS
           {
           { name : name = excluded.name },
           { email : email = case 
               when (my_table.email like '%' || excluded.email || '%') 
               then my_table.email 
               else excluded.email || ';' || my_table.email 
               end },
           }
           WHERE
           #INSERT_COLUMN_CHANGED_FILTER

    - #INSERT_COLUMN_NAMES
      - Expands to the columns the user provides which overlaps with those returned
      - Also supports aliasing, e.g.: #INSERT_COLUMN_NAMES { { x : xxx }, { b : b1 } }
      - The { { x : y }, ... } syntax is required when column names do not match the actual database field names.
    - #INSERT_COLUMN_VALUES
      - Expands to @p1, @p2, etc. (for the number of columns named)
    - #UPDATE_COLUMN_ASSIGNMENTS
      - used to describe how to assign values
    - #INSERT_COLUMN_CHANGED_FILTER
      - converts the #UPDATE_COLUMN_ASSIGNMENTS assignments into a 'where something has changed' filter

<SQL FOR SETUP WRITING>
  This will only be executed by 'Sql.Db' never by the created providers.
  Used for the likes of initial table creation, e.g.:

       CREATE TABLE IF NOT EXISTS some_schema.my_table (
           name VARCHAR(50) UNIQUE NOT NULL,
           email VARCHAR(1000) NOT NULL,
           updated_at TIMESTAMP without time zone NOT NULL default (now() at time zone 'utc'),
           created_at TIMESTAMP without time zone NOT NULL default (now() at time zone 'utc')
       );

","['sql_for_reading', 'sql_for_writing', 'sql_for_setup_writing']"
"Used to write query results to the HcFs in a variety of formats.
Such files are for configuration purposes not general data result storage (for that use Drive.SaveAs).",DirectProvider,,"System, Administration",Sys.Admin.File.SaveAs,2022-03-25 16:02:57.5133765Z,"Of the form:

use Sys.Admin.File.SaveAs [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

<OPTIONS>:

_______________________________________________________________________________________________________________________
 type             │ The file types to create - also controls file extensions if not specified [Csv, Excel, SqLite,
                  │   Json, Default: Csv]
 combineToOne     │ Combine all files to one for this export - and give them this name [String]
 ignoreOnZeroRows │ Flag to say, if a file would have zero rows, do not write it at all. [Boolean]
 fileNames        │ *Mandatory* File names (without a path).  One per table of data is required. [String]
 path             │ *Mandatory* The location to save this within the target file store. [String]
 removeFiles      │ Flag to say, do not write these files, but rather remove them if present [Boolean]
"," type             │ The file types to create - also controls file extensions if not specified [Csv, Excel, SqLite,
                  │   Json, Default: Csv]
 combineToOne     │ Combine all files to one for this export - and give them this name [String]
 ignoreOnZeroRows │ Flag to say, if a file would have zero rows, do not write it at all. [Boolean]
 fileNames        │ *Mandatory* File names (without a path).  One per table of data is required. [String]
 path             │ *Mandatory* The location to save this within the target file store. [String]
 removeFiles      │ Flag to say, do not write these files, but rather remove them if present [Boolean]","Of the form:

use Sys.Admin.File.SaveAs [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

",[]
Provides the ability to execute Luminesce SQL in order to promote that to being a View (a Provider),DirectProvider,,DatabaseView,Sys.Admin.SetupView,2022-03-25 16:02:56.7070954Z,"Runs SQL given a set of options & Some SQL.
Of the form:

use Sys.Admin.SetupView [with @@var1, ...]
  <OPTIONS *>
  ----
  <SQL FOR READING *>
enduse;


<SQL FOR READING>: 
  The SQL to run against the raw data source.
  This should ideally contain the following macros (at least if you intend to use --provider)
  You may have as many of each as you need in a given query.
    - #SELECT
      - Used to only select the columns the user requests.
      - Becomes '1' if no columns are selected.  
      - Will not include the word 'SELECT' itself.
      - If a column has a + after the name, this will be considered a PrimaryKey in the result set (if applicable, e.g. creating a provider)
      - If a column has a ^ after the name, this will be considered a Main field in the result set (if applicable, e.g. creating a provider)
      #SELECT
      {
         { some-column-name+^ : sql-to-give-that-column },
         { next-column-name^ : more-sql-main-column },
         { yet-next-column-name : more-sql-normal-column },
         ...
      }

    - #SELECT_AGG *
       - The same as #SELECT, except it means ""this allows for SQL-Aggregations to be performed"", and must follow these rules:
         - There can be only one (if there isn't one no aggregations will be passed down to the underlying SQL implementation)
         - It must contain all the fields that can possibly be returned

    - #PARAMETERVALUE(ParameterName) : 
      - Used to pass a user-supplied, initial or default value of the parameter with name 'ParameterName' to the raw data source.
      - If a parameter of the required name cannot be found, an exception will be thrown.
      - Parameters are specified through the command line option --parameters, with parameters defined on successive lines in the format:
            Name,Type,InitialValue[,SetAsDefaultValue=true] 
      - Parameter types must be one of: 
            Boolean, Int, BigInt, Double, Decimal, Text, Date, DateTime
      - An initial value must be provided for all parameters.
      - Optionally, this value may be set as the default value for parameters in any created Provider using the 'SetAsDefaultValue' argument.
      - If no default value is set, an exception will be thrown when calling created Providers without a user-supplied value for Text types,
        and otherwise the type-specific default will be used.
        
    - #RESTRICT : 
      - Used to pass down the user-filter given to the Provider to the raw data source
      - Any user filter requiring more fields than are used in a given macro block will be reduced as required.
        So there can be benefits in using the macro multiple times, where some earlier blocks may only be able to filter on certain fields
        and later blocks that could filter on more fields, but perhaps not as efficiently.
      - Becomes '(1 = 1)' if the user-filter wasn't given or it required fields not in the macro.  
      - Will not include the word 'WHERE' / 'ON' itself (may be used in joins or where clause as a result, even if only part of the clause).

      #RESTRICT
      {
         { some-column-name : sql-to-give-that-column },
         { next-column-name : more-sql },
         ...
      }

    - #RESTRICT_AGG *
      - Rather like restrict, but you do not specify any members, simply provide this token.
      - Members will be pulled from #SELECT_AGG, which must exist (so using this is more concise than #RESTRICT)
      - All these members must be filterable, and it must be all the fields returned.

    - #LIMIT
      - Used to fill in the 'LIMIT / TOP / ROWNUM' as required by the Sql Dialect 

    - #DISTINCT
      - Used to fill in the 'DISTINCT' as required by the Sql Dialect 

    - #GROUP_BY *
      - Will be replaced with a group by clause at that location (or removed)

    * = Required in the case when you wish to support Aggregations being passed down to the underlying SQL engine (when possible)

  Full example for SQL READING:

      SELECT #DISTINCT #SELECT_AGG {
        { x : a.x },
        { y : a.y },
        { z : b.f },
        }
      FROM
        abc a
        LEFT OUTER join b
         on a.id = b.id
         and #RESTRICT { { z : b.f } }
      WHERE #RESTRICT_AGG
      #GROUP_BY
      #LIMIT

<OPTIONS>:

<NOTES>
  * = Scalar Parameters:
    - You may pass in any number of scalar parameters and they can be auto-replaced within the options and/or SQL body
    - `with @@v1, @@v2` and somewhere within the `use-body` {@@v1} {@@v2} will be directly replaced with those values
    - This provides an additional use of this provider, namely to operate as a `sql-exec` function, e.g.

        @@sql = select 'select 1 as abc';
        @x = use Sys.Admin.SetupView with @@sql
           ----
           {@@sql}
        enduse;
        select * from @x;

_______________________________________________________________________________________________________________________
 licenceCode       │ The licence code to apply to the created provider, if any. [String]
 allDomains        │ This sets up reader/writer provider(s) which are for all domains (if they have the correct licence
                   │   of course), not ones only for the current domain as is normally done.  This has extra licencing
                   │   and entitlement requirements. [Boolean]
 otherSingleDomain │ This sets up reader/writer provider(s) which are for a different single domain (if is has the
                   │   correct licence of course), not ones only for the current domain as is normally done.  This has
                   │   extra licencing and entitlement requirements (as per --allDomains). [String]
 provider          │ The Provider Name which should be created, e.g. 'A.B.C'. This will save a .sql file to the HcFs
                   │   (e.g. 'RootPath/A/B/C.sql'). This is ignored until after the point at which a query runs
                   │   successfully after which a provider of that name will be created / updated / deleted. [String]
 rootPath          │ The root HcFs path under which to save the given provider.  This should be organized by how it
                   │   will later be used. [String, Default: DatabaseProviders]
 change            │ The 'commit message' for this change. [String, Default: Updated]
 deleteProvider    │ Instead of creating a provider by that name, remove any pre-existing one.  Requires --provider be
                   │   set. [Boolean]
 limit             │ A Row Limit to apply.  This will not be a part of the saved view definition (that will come from
                   │   how the view is used). [Int32, Default: 100]
 distinct          │ If Distinct should be applied.  This will not be a part of the saved view definition (that will
                   │   come from how the view is used). [Boolean]
 filter            │ A filter expression to be applied.  Any usage of fields not in the #RESTRICT macro(s) are pruned
                   │   out and this is not re-applied to the final data set (so invalid fields are OK).  This will not be
                   │   a part of the saved view definition (that will come from how the view is used). [String]
 select            │ Pipe (|) delimited set of expressions to select, this allows for testing Aggregations.  This will
                   │   not be a part of the saved view definition (that will come from how the view is used). [String]
 groupBy           │ Pipe (|) delimited set of expressions to group by, this allows for testing Aggregations.  This
                   │   will not be a part of the saved view definition (that will come from how the view is used).
                   │   [String]
 filterIsInWebForm │ True means the --filter is a `Finbourne WebAPI Filter`.  This will not be a part of the saved view
                   │   definition (that is not possible in this case). [Boolean]
 parameters        │ Any scalar parameters to use. These should be of the form
                   │   Name,Type,InitialValue[,SetAsDefaultValue=true]. Place one on each line after --parameters
                   │   [String]
"," licenceCode       │ The licence code to apply to the created provider, if any. [String]
 allDomains        │ This sets up reader/writer provider(s) which are for all domains (if they have the correct licence
                   │   of course), not ones only for the current domain as is normally done.  This has extra licencing
                   │   and entitlement requirements. [Boolean]
 otherSingleDomain │ This sets up reader/writer provider(s) which are for a different single domain (if is has the
                   │   correct licence of course), not ones only for the current domain as is normally done.  This has
                   │   extra licencing and entitlement requirements (as per --allDomains). [String]
 provider          │ The Provider Name which should be created, e.g. 'A.B.C'. This will save a .sql file to the HcFs
                   │   (e.g. 'RootPath/A/B/C.sql'). This is ignored until after the point at which a query runs
                   │   successfully after which a provider of that name will be created / updated / deleted. [String]
 rootPath          │ The root HcFs path under which to save the given provider.  This should be organized by how it
                   │   will later be used. [String, Default: DatabaseProviders]
 change            │ The 'commit message' for this change. [String, Default: Updated]
 deleteProvider    │ Instead of creating a provider by that name, remove any pre-existing one.  Requires --provider be
                   │   set. [Boolean]
 limit             │ A Row Limit to apply.  This will not be a part of the saved view definition (that will come from
                   │   how the view is used). [Int32, Default: 100]
 distinct          │ If Distinct should be applied.  This will not be a part of the saved view definition (that will
                   │   come from how the view is used). [Boolean]
 filter            │ A filter expression to be applied.  Any usage of fields not in the #RESTRICT macro(s) are pruned
                   │   out and this is not re-applied to the final data set (so invalid fields are OK).  This will not be
                   │   a part of the saved view definition (that will come from how the view is used). [String]
 select            │ Pipe (|) delimited set of expressions to select, this allows for testing Aggregations.  This will
                   │   not be a part of the saved view definition (that will come from how the view is used). [String]
 groupBy           │ Pipe (|) delimited set of expressions to group by, this allows for testing Aggregations.  This
                   │   will not be a part of the saved view definition (that will come from how the view is used).
                   │   [String]
 filterIsInWebForm │ True means the --filter is a `Finbourne WebAPI Filter`.  This will not be a part of the saved view
                   │   definition (that is not possible in this case). [Boolean]
 parameters        │ Any scalar parameters to use. These should be of the form
                   │   Name,Type,InitialValue[,SetAsDefaultValue=true]. Place one on each line after --parameters
                   │   [String]","Runs SQL given a set of options & Some SQL.
Of the form:

use Sys.Admin.SetupView [with @@var1, ...]
  <OPTIONS *>
  ----
  <SQL FOR READING *>
enduse;


<SQL FOR READING>: 
  The SQL to run against the raw data source.
  This should ideally contain the following macros (at least if you intend to use --provider)
  You may have as many of each as you need in a given query.
    - #SELECT
      - Used to only select the columns the user requests.
      - Becomes '1' if no columns are selected.  
      - Will not include the word 'SELECT' itself.
      - If a column has a + after the name, this will be considered a PrimaryKey in the result set (if applicable, e.g. creating a provider)
      - If a column has a ^ after the name, this will be considered a Main field in the result set (if applicable, e.g. creating a provider)
      #SELECT
      {
         { some-column-name+^ : sql-to-give-that-column },
         { next-column-name^ : more-sql-main-column },
         { yet-next-column-name : more-sql-normal-column },
         ...
      }

    - #SELECT_AGG *
       - The same as #SELECT, except it means ""this allows for SQL-Aggregations to be performed"", and must follow these rules:
         - There can be only one (if there isn't one no aggregations will be passed down to the underlying SQL implementation)
         - It must contain all the fields that can possibly be returned

    - #PARAMETERVALUE(ParameterName) : 
      - Used to pass a user-supplied, initial or default value of the parameter with name 'ParameterName' to the raw data source.
      - If a parameter of the required name cannot be found, an exception will be thrown.
      - Parameters are specified through the command line option --parameters, with parameters defined on successive lines in the format:
            Name,Type,InitialValue[,SetAsDefaultValue=true] 
      - Parameter types must be one of: 
            Boolean, Int, BigInt, Double, Decimal, Text, Date, DateTime
      - An initial value must be provided for all parameters.
      - Optionally, this value may be set as the default value for parameters in any created Provider using the 'SetAsDefaultValue' argument.
      - If no default value is set, an exception will be thrown when calling created Providers without a user-supplied value for Text types,
        and otherwise the type-specific default will be used.
        
    - #RESTRICT : 
      - Used to pass down the user-filter given to the Provider to the raw data source
      - Any user filter requiring more fields than are used in a given macro block will be reduced as required.
        So there can be benefits in using the macro multiple times, where some earlier blocks may only be able to filter on certain fields
        and later blocks that could filter on more fields, but perhaps not as efficiently.
      - Becomes '(1 = 1)' if the user-filter wasn't given or it required fields not in the macro.  
      - Will not include the word 'WHERE' / 'ON' itself (may be used in joins or where clause as a result, even if only part of the clause).

      #RESTRICT
      {
         { some-column-name : sql-to-give-that-column },
         { next-column-name : more-sql },
         ...
      }

    - #RESTRICT_AGG *
      - Rather like restrict, but you do not specify any members, simply provide this token.
      - Members will be pulled from #SELECT_AGG, which must exist (so using this is more concise than #RESTRICT)
      - All these members must be filterable, and it must be all the fields returned.

    - #LIMIT
      - Used to fill in the 'LIMIT / TOP / ROWNUM' as required by the Sql Dialect 

    - #DISTINCT
      - Used to fill in the 'DISTINCT' as required by the Sql Dialect 

    - #GROUP_BY *
      - Will be replaced with a group by clause at that location (or removed)

    * = Required in the case when you wish to support Aggregations being passed down to the underlying SQL engine (when possible)

  Full example for SQL READING:

      SELECT #DISTINCT #SELECT_AGG {
        { x : a.x },
        { y : a.y },
        { z : b.f },
        }
      FROM
        abc a
        LEFT OUTER join b
         on a.id = b.id
         and #RESTRICT { { z : b.f } }
      WHERE #RESTRICT_AGG
      #GROUP_BY
      #LIMIT

",['sql_for_reading']
Get the log lines for a Grafana Loki LogQL query with a given time window and resolution.,DirectProvider,,"System, Logs",Sys.GrafanaLoki.LogData,2022-03-25 16:02:56.9830716Z,"Get the log lines for a Grafana Loki LogQL query with a given time window and resolution.
Of the form:

@result = use Sys.GrafanaLoki.LogData [limit N]
  <OPTIONS>
  [----]
  [LogQL]
enduse;

_______________________________________________________________________________________________________________________
 logQL                    │ The Grafana Loki LogQL query string which must end with a semicolon. 
                          │   To escape a character use double \\, e.g. \\\\""
                          │   This is somewhat deprecated. You may omit this and instead have a `----` after all other
                          │   options at which point all remaining text is LogQL.
                          │   That requires no escaping and supports SPACES, BACKSLASHES and LINE BREAK. [String]
 startAt                  │ Start datetime of the query. Expected format: yyyy-MM-ddThh:mm:ss. Defaults to 24 hours
                          │   before endAt. [DateTime]
 endAt                    │ End datetime of the query. Expected format: yyyy-MM-ddThh:mm:ss. Defaults to current date
                          │   and time. [DateTime]
 step                     │ Query resolution in seconds. [Int32, Default: 60]
 defaultLimit             │ Max number of log messages to pull back from Loki by default (the LIMIT from the context
                          │   will override this, if there is one.) [Int32, Default: 1000]
 direction                │ Sets the sorting order of the logs: can be 'forward' or 'backward'. [String, Default:
                          │   backward]
 explicitLabelColumns     │ Always return these label-columns (a comma delimited list), regardless of what exists in
                          │   Loki for the selected data (without this the returned shape of data can vary by date range
                          │   and loose all labels if there is no matching data). [String]
 onlyExplicitLabelColumns │ Only the explicitly mentioned labels will be returned, regardless of what exists in Loki
                          │   for the selected data (otherwise it will be explicit + dynamic). [Boolean]
"," logQL                    │ The Grafana Loki LogQL query string which must end with a semicolon. 
                          │   To escape a character use double \\, e.g. \\\\""
                          │   This is somewhat deprecated. You may omit this and instead have a `----` after all other
                          │   options at which point all remaining text is LogQL.
                          │   That requires no escaping and supports SPACES, BACKSLASHES and LINE BREAK. [String]
 startAt                  │ Start datetime of the query. Expected format: yyyy-MM-ddThh:mm:ss. Defaults to 24 hours
                          │   before endAt. [DateTime]
 endAt                    │ End datetime of the query. Expected format: yyyy-MM-ddThh:mm:ss. Defaults to current date
                          │   and time. [DateTime]
 step                     │ Query resolution in seconds. [Int32, Default: 60]
 defaultLimit             │ Max number of log messages to pull back from Loki by default (the LIMIT from the context
                          │   will override this, if there is one.) [Int32, Default: 1000]
 direction                │ Sets the sorting order of the logs: can be 'forward' or 'backward'. [String, Default:
                          │   backward]
 explicitLabelColumns     │ Always return these label-columns (a comma delimited list), regardless of what exists in
                          │   Loki for the selected data (without this the returned shape of data can vary by date range
                          │   and loose all labels if there is no matching data). [String]
 onlyExplicitLabelColumns │ Only the explicitly mentioned labels will be returned, regardless of what exists in Loki
                          │   for the selected data (otherwise it will be explicit + dynamic). [Boolean]","Get the log lines for a Grafana Loki LogQL query with a given time window and resolution.
Of the form:

@result = use Sys.GrafanaLoki.LogData [limit N]
  <OPTIONS>
  [----]
  [LogQL]
enduse;
",['log_ql']
Get the time series values for a promQL query with a given time window and granularity.,DirectProvider,,"System, Logs",Sys.Prometheus.Series.Data,2022-03-25 16:02:56.9830716Z,"Get the time series values for a promQL query with a given time window and granularity.

Of the form:

use Sys.Prometheus.Series.Data [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

<OPTIONS>:


_______________________________________________________________________________________________________________________
 promQL                    │ The PromQL query string to run. [String]
 granularity               │ The granularity in seconds for the queried time series (defaults to 60). [Int32]
 startAt                   │ Start datetime for prometheus series data query. Expected format: yyyy-MM-ddThh:mm:ss.
                           │   Defaults to 1 day before endAt time. [DateTime]
 endAt                     │ End datetime for prometheus series data query. Expected format: yyyy-MM-ddThh:mm:ss.
                           │   Defaults to the current date and time. [DateTime]
 explicitMetricColumns     │ Always return these metric-columns (a comma delimited list), regardless of what exists in
                           │   Prometheus for the selected data (without this the returned shape of data can vary by date
                           │   range and loose all metric if there is no matching data). [String]
 onlyExplicitMetricColumns │ Only the explicitly mentioned metric will be returned, regardless of what exists in
                           │   Prometheus for the selected data (otherwise it will be explicit + dynamic). [Boolean]
"," promQL                    │ The PromQL query string to run. [String]
 granularity               │ The granularity in seconds for the queried time series (defaults to 60). [Int32]
 startAt                   │ Start datetime for prometheus series data query. Expected format: yyyy-MM-ddThh:mm:ss.
                           │   Defaults to 1 day before endAt time. [DateTime]
 endAt                     │ End datetime for prometheus series data query. Expected format: yyyy-MM-ddThh:mm:ss.
                           │   Defaults to the current date and time. [DateTime]
 explicitMetricColumns     │ Always return these metric-columns (a comma delimited list), regardless of what exists in
                           │   Prometheus for the selected data (without this the returned shape of data can vary by date
                           │   range and loose all metric if there is no matching data). [String]
 onlyExplicitMetricColumns │ Only the explicitly mentioned metric will be returned, regardless of what exists in
                           │   Prometheus for the selected data (otherwise it will be explicit + dynamic). [Boolean]","Get the time series values for a promQL query with a given time window and granularity.

Of the form:

use Sys.Prometheus.Series.Data [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

",[]
Gets Prometheus time series metadata (labels and values that label a time series) for a given metric in a time window.,DirectProvider,,"System, Logs",Sys.Prometheus.Series.Metadata,2022-03-25 16:02:56.9830716Z,"Gets Prometheus time series metadata (labels and values that label a time series) for a given metric in a time window.

Of the form:

use Sys.Prometheus.Series.Metadata [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

<OPTIONS>:


_______________________________________________________________________________________________________________________
 metricName │ Prometheus Metric Name to get series metadata data for. [String]
 startAt    │ Start datetime for prometheus series metadata query. Expected format: yy-mm-ddThh:mm:ss. Defaults to 1
            │   day before endAt time. [DateTime]
 endAt      │ End datetime for prometheus series metadata query. Expected format: yy-mm-ddThh:mm:ss. Defaults to the
            │   current date and time. [DateTime]
"," metricName │ Prometheus Metric Name to get series metadata data for. [String]
 startAt    │ Start datetime for prometheus series metadata query. Expected format: yy-mm-ddThh:mm:ss. Defaults to 1
            │   day before endAt time. [DateTime]
 endAt      │ End datetime for prometheus series metadata query. Expected format: yy-mm-ddThh:mm:ss. Defaults to the
            │   current date and time. [DateTime]","Gets Prometheus time series metadata (labels and values that label a time series) for a given metric in a time window.

Of the form:

use Sys.Prometheus.Series.Metadata [with @@var1, ...]
  <OPTIONS *>
enduse;

`@@variables` are simple scalar values usable within the the options via simple substitution, e.g.: `--someOption=abc{@@var1}123`

",[]
Provider for performing inference with Sklearn models and table variable inputs. Models must have been converted to ONNX and uploaded to Drive using the upload_model function in lumipy.experimental.,DirectProvider,,Utilities,Tools.ML.Inference.Sklearn,2022-03-25 16:02:56.4709206Z,"
Runs inference for a given machine learning model over a single input table. 
The columns of the table must correspond to the input features of the model. 
The model itself must be stored in Drive and have been converted to ONNX format using the corresponding lumipy.ml converter function. 

Example usage:
use Tools.ML.Inference.Sklearn with @features
    --onnxFilePath=/path/to/model.onnx
enduse

_______________________________________________________________________________________________________________________
 onnxFilePath │ Path to the onnx file in drive to be served. [String]
", onnxFilePath │ Path to the onnx file in drive to be served. [String],"
Runs inference for a given machine learning model over a single input table. 
The columns of the table must correspond to the input features of the model. 
The model itself must be stored in Drive and have been converted to ONNX format using the corresponding lumipy.ml converter function. 

Example usage:
use Tools.ML.Inference.Sklearn with @features
    --onnxFilePath=/path/to/model.onnx
enduse
",[]
