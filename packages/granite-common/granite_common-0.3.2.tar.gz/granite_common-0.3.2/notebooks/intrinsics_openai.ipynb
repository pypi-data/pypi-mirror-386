{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d197889",
   "metadata": {},
   "source": [
    "# Using Intrinsics on OpenAI-Compatible Inference Backends\n",
    "\n",
    "This notebook demonstrates how to use the shared input and output processing code for\n",
    "intrinsics when performing model inference with an OpenAI-compatible backend such as\n",
    "vLLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071ccbb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports go in this cell\n",
    "import os\n",
    "import openai\n",
    "import json\n",
    "import granite_common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5162c7c9",
   "metadata": {},
   "source": [
    "## Constants\n",
    "\n",
    "Change the value of the constants `intrinsic_name` and `base_model` in the cell that \n",
    "follows to change which intrinsic will be demonstrated in the remainder of this notebook. Other constants will automatically adjust accordingly.\n",
    "\n",
    "You may also need to adjust the values of `openai_base_url` and `openai_api_key` to correspond to the location of the server where you are hosting the LoRA adapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44000e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the following constants to select a different intrinsic for the remainder\n",
    "# of this notebook.\n",
    "intrinsic_name = \"answerability\"\n",
    "base_model_name = \"granite-3.3-8b-instruct\"\n",
    "use_alora = False\n",
    "\n",
    "# Change the following two constants as needed to reflect the location of the inference\n",
    "# server.\n",
    "openai_base_url = \"http://localhost:55555/v1\"\n",
    "openai_api_key = \"rag_intrinsics_1234\"\n",
    "\n",
    "\n",
    "#######################################################################################\n",
    "# The code below adjusts the remaining constants according to the chosen intrinsic.\n",
    "\n",
    "TESTDATA_DIR = \"../tests/granite_common/intrinsics/rag/testdata\"\n",
    "KNOWN_INTRINSICS = [\n",
    "    \"answerability\",\n",
    "    \"answer_relevance_classifier\",\n",
    "    \"answer_relevance_rewriter\",\n",
    "    \"citations\",\n",
    "    \"context_relevance\",\n",
    "    \"hallucination_detection\",\n",
    "    \"query_rewrite\",\n",
    "    \"requirement_check\",\n",
    "    \"uncertainty\",\n",
    "]\n",
    "INTRINSICS_WITH_LOCAL_YAML_FILES = []\n",
    "\n",
    "io_yaml_file = None  # None -> load from Hugging Face Hub\n",
    "request_json_file = f\"{TESTDATA_DIR}/input_json/{intrinsic_name}.json\"\n",
    "\n",
    "# Include local JSON file with arguments if that file is present.\n",
    "maybe_arg_file = f\"{TESTDATA_DIR}/input_args/{intrinsic_name}.json\"\n",
    "arg_file = maybe_arg_file if os.path.exists(maybe_arg_file) else None\n",
    "\n",
    "# Selectively override defaults\n",
    "if intrinsic_name == \"answerability\":\n",
    "    request_json_file = f\"{TESTDATA_DIR}/input_json/answerable.json\"\n",
    "elif intrinsic_name in INTRINSICS_WITH_LOCAL_YAML_FILES:\n",
    "    # Some io.yaml files not yet delivered to Hugging Face Hub\n",
    "    io_yaml_file = f\"{TESTDATA_DIR}/input_yaml/{intrinsic_name}.yaml\"\n",
    "elif intrinsic_name not in KNOWN_INTRINSICS:\n",
    "    raise ValueError(f\"Unrecognized intrinsic name '{intrinsic_name}'\")\n",
    "\n",
    "if io_yaml_file is None:\n",
    "    # Fetch IO configuration file from Hugging Face Hub\n",
    "    io_yaml_file = granite_common.intrinsics.util.obtain_io_yaml(\n",
    "        intrinsic_name, base_model_name, alora=use_alora\n",
    "    )\n",
    "\n",
    "# Print the variables we just set\n",
    "print(f\"{io_yaml_file=}\")\n",
    "print(f\"{request_json_file=}\")\n",
    "print(f\"{arg_file=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f3a8e7",
   "metadata": {},
   "source": [
    "## Instantiate the input and output processing classes\n",
    "\n",
    "The constructors for the classes `IntrinsicsRewriter` and `IntrinsicsResultProcessor`\n",
    "serve as factory methods to produce input and output processors, respectively, for \n",
    "a given intrinsic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3292e4c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    f\"Instantiating input and output processing from configuration file:\\n\"\n",
    "    f\"{io_yaml_file}\"\n",
    ")\n",
    "\n",
    "rewriter = granite_common.IntrinsicsRewriter(config_file=io_yaml_file)\n",
    "result_processor = granite_common.IntrinsicsResultProcessor(config_file=io_yaml_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfe9bba",
   "metadata": {},
   "source": [
    "## Perform input processing\n",
    "\n",
    "The cells that follow load an example OpenAI-compatible chat completion request from\n",
    "a local file, then show how to apply input processing to the request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57e2e812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read original request from the appropriate file\n",
    "print(f\"Loading request data from {request_json_file}\")\n",
    "with open(request_json_file, encoding=\"utf-8\") as f:\n",
    "    request_json_str = f.read()\n",
    "request_json = json.loads(request_json_str)\n",
    "\n",
    "# Some parameters like model name aren't kept in the JSON files that we use for testing.\n",
    "# Apply appropriate values for those parameters.\n",
    "request_json[\"model\"] = intrinsic_name\n",
    "request_json[\"temperature\"] = 0.0\n",
    "\n",
    "print(\"Original request:\")\n",
    "print(json.dumps(request_json, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb7ee9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some intrinsics take one or more additional arguments besides the target chat\n",
    "# completion request. Load the additional arguments from a file if that is the case.\n",
    "intrinsic_kwargs = {}\n",
    "if arg_file is not None:\n",
    "    with open(arg_file, encoding=\"utf8\") as file:\n",
    "        intrinsic_kwargs = json.load(file)\n",
    "    print(f\"Using additional arguments:\\n{intrinsic_kwargs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2f345c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run request through input processing.\n",
    "rewritten_request = rewriter.transform(request_json, **intrinsic_kwargs)\n",
    "\n",
    "print(\"Request after input processing:\")\n",
    "print(rewritten_request.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f628799",
   "metadata": {},
   "source": [
    "## Run inference\n",
    "\n",
    "Passing a request through the input processing `RagAgentLibRewriter.transform()` \n",
    "turns the request into something that can be sent directly to an OpenAI-compatible\n",
    "inference endpoint for the intrinsic.\n",
    "\n",
    "The cells that follow show how to perform inference using the chat completions API.\n",
    "\n",
    "To run these cells, you'll need to start a server such as vLLM that serves the intrinsic\n",
    "under the appropriate model name at the base URL specified by `openai_base_url` in the constants cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea587240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the local inference server\n",
    "client = openai.OpenAI(base_url=openai_base_url, api_key=openai_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db75967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pass our rewritten request directly to `chat.completions.create()`\n",
    "chat_completion = client.chat.completions.create(**rewritten_request.model_dump())\n",
    "\n",
    "print(\"Immediately after low-level inference, first completion is:\")\n",
    "print(chat_completion.choices[0].model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e598f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the raw JSON for the convenience of developers who might need that data.\n",
    "print(chat_completion.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e1157a",
   "metadata": {},
   "source": [
    "## Post-process inference results\n",
    "\n",
    "The raw output of some intrinsics requires some additional postprocessing to turn it \n",
    "into a form that is easy to consume in an application. This postprocessing occurs in\n",
    "the method `RagAgentLibResultProcessor.transform()`. \n",
    "\n",
    "The cells that follow show how to use this method to transform the raw output of the\n",
    "`chat.completions.create()` API call into the intrinsic's application-level output\n",
    "value.\n",
    "\n",
    "By convention, this application-level output value is returned in the same format as a\n",
    "chat completions request result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ff1b2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_chat_completion = result_processor.transform(\n",
    "    chat_completion, rewritten_request\n",
    ")\n",
    "\n",
    "print(\"After post-processing, first completion is:\")\n",
    "print(processed_chat_completion.choices[0].model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6216c57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that the contents of the completion is valid JSON and pretty-print the JSON.\n",
    "parsed_contents = json.loads(processed_chat_completion.choices[0].message.content)\n",
    "print(\"JSON contents of first completion:\")\n",
    "print(json.dumps(parsed_contents, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b7254f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full JSON output of post-processing\n",
    "print(processed_chat_completion.model_dump_json(indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd48fcf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
