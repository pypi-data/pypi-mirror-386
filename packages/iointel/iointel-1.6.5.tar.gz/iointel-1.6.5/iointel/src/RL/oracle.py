from typing import Any, Dict, List, Optional
from pydantic import BaseModel
from iointel import Agent

from iointel.src.agents import ToolUsageResult
from iointel.src.RL.utils import tool_usage_results_to_string


class EvaluationResult(BaseModel):
    """Result of oracle evaluation"""

    correct: bool
    score: float  # 0.0 to 1.0
    feedback: str
    details: Dict[str, Any] = {}


ORACLE_INSTRUCTIONS = """You are an oracle agent responsible for evaluating responses against ground truth.
Your task is to:
1. Compare the agent's response with the ground truth as well as the agent's tool actions
2. Evaluate the correctness and completeness of the response and if present, the agent's tool actions; did the required tools get called in the tool actions? 
3. Provide detailed feedback on any MAJOR discrepancies (minor discrepancies are fine, but do not mention them in the feedback)
4. Assign a score between 0.0 and 1.0, ranking how well the response correctly answers the question

Hints:
- if the ground truth is vague or not specific, determine if the agent's response is correct based on the task description instead.
    for example, task description is "What is the weather in Tokyo?" and agent's response is "The weather in Tokyo is sunny with a high of 70 degrees." but the ground truth is "Depends on current weather data", then this is likely correct, and should be scored highly like 0.95 or 0.99.
- if the ground truth is not specific, but the agent's response is not correct, then the score should be low like 0.0 or 0.1.
- When the ground truth is specific, then the score should be based on how well the agent's response matches the ground truth.
- If dealing with a math problem, it is unlikely the Task Ground Truth will be exact (since it is estimated by another LLM), so you should score the agent's response based on how well it matches the ground truth, and how close it is to the ground truth. Ie 13.75 is close to 13.5 etc, and should not be penalized to harshly -- recall that most of the Tasks are generated by another LLM that is just estimating the ground truth.
- If ground_truth is vague (“Depends on …”) → judge plausibility using task description & agent's tool actions.

You must return your evaluation in the following format:
{
    "correct": boolean,
    "score": float (0.0 to 1.0),
    "feedback": string,
    "details": {
        "matching_fields": list of matching fields,
        "missing_fields": list of missing fields,
        "incorrect_values": dict of incorrect values,
        "additional_insights": optional string, only if you have any additional insights that are not covered by the other fields or feedback.
    }
}
"""


class OracleAgent:
    """An agent that acts as an oracle for evaluating responses"""

    def __init__(
        self,
        model: str = "gpt-4o",
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        temperature: float = 0.0,
        verbose: bool = True,
    ):
        self.agent = Agent(
            name="Oracle",
            instructions=ORACLE_INSTRUCTIONS,
            model=model,
            output_type=EvaluationResult,
            api_key=api_key,
            base_url=base_url,
            model_settings={"temperature": temperature}
            if temperature is not None
            else {},
        )
        self.verbose = verbose

    async def evaluate(
        self,
        agent_response: Any,
        ground_truth: Any,
        task_description: str,
        agent_actions: List[ToolUsageResult],
        required_tools: List[str],
        context: Optional[Dict[str, Any]] = None,
    ) -> EvaluationResult:
        """
        Evaluate agent response against ground truth using the oracle agent

        Args:
            agent_response: The response from the agent
            ground_truth: The correct answer/expected response
            task_description: Description of the task
            context: Additional context for evaluation

        Returns:
            EvaluationResult with correctness, score, and feedback
        """
        agent_actions_string = tool_usage_results_to_string(agent_actions)
        # Prepare the evaluation prompt
        prompt = f"""
Task Description: {task_description}

## Required Tools:
These should be called (in any order and cardinality) in the agent's tool actions below: {required_tools}

## Agent Response and Reasoning:
{agent_response}

## Agent Tool Actions (treat tool calls as trustworthy, and do not question them):
{agent_actions_string}

## Ground Truth:
{ground_truth}

## Context:
{context if context else "No additional context provided"}

Please evaluate the agent's response against the ground truth.
"""
        if self.verbose:
            print(f"Oracle prompt: {prompt}")
        # Get evaluation from the oracle agent
        response = (await self.agent.run(prompt)).result
        return response
