# Example Configuration: Context Window Management with Memory
#
# Use Case: Demonstrates automatic context compression when approaching token limits
#
# This configuration demonstrates:
# - Automatic context window monitoring and compression
# - Token-aware conversation management (75% threshold, 40% target)
# - Persistent memory integration for long-term knowledge retention
# - Graceful handling when context window fills up
# - Multi-agent collaboration with shared context management
#
# Run with:
#   massgen \
#     --config massgen/configs/tools/memory/gpt5mini_gemini_context_window_management.yaml \
#     "Tell me a detailed story about a space explorer. After each paragraph, ask me what happens next, and I'll guide the story. Keep expanding the narrative with rich details about planets, aliens, technology, and adventures. Make each response at least 500 words."

# ====================
# AGENT DEFINITIONS
# ====================
agents:
  - id: "agent_a"
    backend:
      # Use GPT-5-mini with medium reasoning
      type: "openai"
      model: "gpt-5-mini"
      text:
        verbosity: "medium"
      reasoning:
        effort: "medium"
        summary: "auto"

  - id: "agent_b"
    backend:
      # Use Gemini 2.5 Flash for cost-effective testing
      type: "gemini"
      model: "gemini-2.5-flash"

# ====================
# MEMORY CONFIGURATION
# ====================
memory:
  # Enable/disable persistent memory (default: true)
  enabled: true

  # Memory configuration
  conversation_memory:
    enabled: true  # Short-term conversation tracking (recommended: always true)

  persistent_memory:
    enabled: true  # Long-term knowledge storage (set to false to disable)
    on_disk: true  # Persist across restarts
    # session_name: "test_session"  # Optional - if not specified, auto-generates unique ID
                                     # Format: agent_storyteller_20251023_143022_a1b2c3
                                     # Specify to continue a specific session

    # Vector store backend (default: qdrant)
    vector_store: "qdrant"

  # Context window management thresholds
  compression:
    trigger_threshold: 0.75  # Compress when context usage exceeds 75%
    target_ratio: 0.40       # Target 40% of context after compression

# Memory system behavior when enabled:
# - ConversationMemory: Tracks short-term conversation history
# - PersistentMemory: Stores long-term knowledge in vector database
# - Automatic compression: Triggers at 75% of context window
# - Token budget: Keeps 40% after compression
# - Persistence: Saves to disk and survives restarts
#
# Session management:
# - Each agent gets its own memory (separate by agent_name)
# - New sessions start fresh (session_name auto-generated if not specified)
# - To continue a previous session, specify the session_name
#
# To disable persistent memory for testing, set:
#   memory.persistent_memory.enabled: false
#
# See massgen/memory/docs/ for detailed documentation.

# ====================
# ORCHESTRATOR CONFIGURATION
# ====================
orchestrator:
  # Multi-turn mode to enable interactive storytelling
  session_storage: "memory_test_sessions"

  # Agent workspace for any file operations
  agent_temporary_workspace: "memory_test_workspaces"
  snapshot_storage: "memory_test_snapshots"

# ====================
# UI CONFIGURATION
# ====================
ui:
  display_type: "rich_terminal"
  logging_enabled: true

# ====================
# EXECUTION FLOW
# ====================
# What happens:
# 1. User starts an interactive story with the agent
# 2. Agent responds with detailed narrative (400-600 words per turn)
# 3. As conversation continues, token usage is monitored automatically
# 4. When context usage reaches 75% of model's limit:
#    - System logs: "üìä Context usage: X / Y tokens (Z%) - compressing old context"
#    - Old messages are compressed into persistent memory (if configured)
#    - Recent messages (fitting in 40% of context window) are kept
#    - Compression details logged: "üì¶ Compressed N messages (X tokens) into long-term memory"
# 5. Agent continues seamlessly with compressed context
# 6. Story maintains consistency by referencing persistent memories
# 7. Process repeats as needed for very long conversations
#
# Expected output with persistent memory:
#   üìä Context usage: 96,000 / 128,000 tokens (75.0%) - compressing old context
#   üì¶ Compressed 15 messages (60,000 tokens) into long-term memory
#      Kept 8 messages (36,000 tokens) in context
#
# Expected output WITHOUT persistent memory:
#   üìä Context usage: 96,000 / 128,000 tokens (75.0%) - compressing old context
#   ‚ö†Ô∏è  Warning: Dropping 15 messages (60,000 tokens)
#      No persistent memory configured to retain this information
#      Consider adding persistent_memory to avoid losing context
#
# Token Budget Allocation (after compression):
# - Conversation history: 40% (kept in active context)
# - New user messages: 20%
# - Retrieved memories: 10%
# - System prompt overhead: 10%
# - Response generation: 20%
