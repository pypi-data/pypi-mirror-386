{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enterprise Workflows with Egnyte-LangChain\n",
    "\n",
    "This notebook demonstrates production-ready enterprise workflows using Egnyte-LangChain integration for real-world business scenarios.\n",
    "\n",
    "## Enterprise Use Cases Covered\n",
    "\n",
    "1. **Document Intelligence Pipeline**: Automated document analysis and insights\n",
    "2. **Compliance Monitoring**: Regulatory document tracking and analysis\n",
    "3. **Knowledge Management**: Intelligent knowledge base creation\n",
    "4. **Executive Reporting**: Automated executive summary generation\n",
    "5. **Risk Assessment**: Document-based risk analysis workflows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enterprise-grade imports\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Optional\n",
    "from dataclasses import dataclass\n",
    "from enum import Enum\n",
    "\n",
    "# LangChain enterprise components\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "# Egnyte integration\n",
    "from langchain_egnyte import (\n",
    "    EgnyteRetriever, \n",
    "    EgnyteSearchOptions,\n",
    "    create_date_range_search_options,\n",
    "    create_folder_search_options\n",
    ")\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print(\"🏢 Enterprise Workflows Setup Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Document Intelligence Pipeline\n",
    "\n",
    "Automated pipeline for document analysis and insight extraction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DocumentType(Enum):\n",
    "    FINANCIAL = \"financial\"\n",
    "    LEGAL = \"legal\"\n",
    "    TECHNICAL = \"technical\"\n",
    "    MARKETING = \"marketing\"\n",
    "    HR = \"hr\"\n",
    "    GENERAL = \"general\"\n",
    "\n",
    "@dataclass\n",
    "class DocumentInsight:\n",
    "    document_name: str\n",
    "    document_path: str\n",
    "    document_type: DocumentType\n",
    "    key_topics: List[str]\n",
    "    sentiment: str\n",
    "    risk_level: str\n",
    "    action_items: List[str]\n",
    "    summary: str\n",
    "    confidence_score: float\n",
    "    processing_timestamp: datetime\n",
    "\n",
    "class DocumentIntelligencePipeline:\n",
    "    \"\"\"Enterprise document intelligence pipeline.\"\"\"\n",
    "    \n",
    "    def __init__(self, egnyte_domain: str, egnyte_token: str, openai_key: str):\n",
    "        self.retriever = EgnyteRetriever(domain=egnyte_domain, user_token=egnyte_token)\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4\", temperature=0.1, openai_api_key=openai_key)\n",
    "        \n",
    "        # Document classification prompt\n",
    "        self.classification_prompt = PromptTemplate(\n",
    "            template=\"\"\"\n",
    "            Analyze this document and provide structured insights:\n",
    "            \n",
    "            Document: {document_name}\n",
    "            Content: {content}\n",
    "            \n",
    "            Provide analysis in this JSON format:\n",
    "            {{\n",
    "                \"document_type\": \"financial|legal|technical|marketing|hr|general\",\n",
    "                \"key_topics\": [\"topic1\", \"topic2\", \"topic3\"],\n",
    "                \"sentiment\": \"positive|neutral|negative\",\n",
    "                \"risk_level\": \"low|medium|high\",\n",
    "                \"action_items\": [\"action1\", \"action2\"],\n",
    "                \"summary\": \"Brief summary of key points\",\n",
    "                \"confidence_score\": 0.95\n",
    "            }}\n",
    "            \n",
    "            JSON Response:\n",
    "            \"\"\",\n",
    "            input_variables=[\"document_name\", \"content\"]\n",
    "        )\n",
    "        \n",
    "        self.analysis_chain = LLMChain(llm=self.llm, prompt=self.classification_prompt)\n",
    "    \n",
    "    def process_documents(self, query: str, max_docs: int = 10) -> List[DocumentInsight]:\n",
    "        \"\"\"Process documents through intelligence pipeline.\"\"\"\n",
    "        \n",
    "        logger.info(f\"Starting document intelligence pipeline for query: {query}\")\n",
    "        \n",
    "        # Retrieve documents\n",
    "        search_options = EgnyteSearchOptions(limit=max_docs)\n",
    "        self.retriever.search_options = search_options\n",
    "        documents = self.retriever.invoke(query)\n",
    "        \n",
    "        logger.info(f\"Retrieved {len(documents)} documents for analysis\")\n",
    "        \n",
    "        insights = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            try:\n",
    "                # Analyze document\n",
    "                analysis_result = self.analysis_chain.invoke({\n",
    "                    \"document_name\": doc.metadata.get('name', 'Unknown'),\n",
    "                    \"content\": doc.page_content[:4000]  # Limit content for API\n",
    "                })\n",
    "                \n",
    "                # Parse JSON response\n",
    "                analysis_data = json.loads(analysis_result['text'])\n",
    "                \n",
    "                # Create insight object\n",
    "                insight = DocumentInsight(\n",
    "                    document_name=doc.metadata.get('name', 'Unknown'),\n",
    "                    document_path=doc.metadata.get('path', 'Unknown'),\n",
    "                    document_type=DocumentType(analysis_data['document_type']),\n",
    "                    key_topics=analysis_data['key_topics'],\n",
    "                    sentiment=analysis_data['sentiment'],\n",
    "                    risk_level=analysis_data['risk_level'],\n",
    "                    action_items=analysis_data['action_items'],\n",
    "                    summary=analysis_data['summary'],\n",
    "                    confidence_score=analysis_data['confidence_score'],\n",
    "                    processing_timestamp=datetime.now()\n",
    "                )\n",
    "                \n",
    "                insights.append(insight)\n",
    "                logger.info(f\"Processed: {insight.document_name} - Type: {insight.document_type.value}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing document {doc.metadata.get('name', 'Unknown')}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        logger.info(f\"Pipeline completed. Generated {len(insights)} insights\")\n",
    "        return insights\n",
    "    \n",
    "    def generate_intelligence_report(self, insights: List[DocumentInsight]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate comprehensive intelligence report.\"\"\"\n",
    "        \n",
    "        # Aggregate insights\n",
    "        doc_types = {}\n",
    "        risk_distribution = {\"low\": 0, \"medium\": 0, \"high\": 0}\n",
    "        sentiment_distribution = {\"positive\": 0, \"neutral\": 0, \"negative\": 0}\n",
    "        all_topics = []\n",
    "        all_actions = []\n",
    "        \n",
    "        for insight in insights:\n",
    "            # Document types\n",
    "            doc_type = insight.document_type.value\n",
    "            doc_types[doc_type] = doc_types.get(doc_type, 0) + 1\n",
    "            \n",
    "            # Risk distribution\n",
    "            risk_distribution[insight.risk_level] += 1\n",
    "            \n",
    "            # Sentiment distribution\n",
    "            sentiment_distribution[insight.sentiment] += 1\n",
    "            \n",
    "            # Collect topics and actions\n",
    "            all_topics.extend(insight.key_topics)\n",
    "            all_actions.extend(insight.action_items)\n",
    "        \n",
    "        # Calculate averages\n",
    "        avg_confidence = sum(i.confidence_score for i in insights) / len(insights) if insights else 0\n",
    "        \n",
    "        return {\n",
    "            \"report_timestamp\": datetime.now().isoformat(),\n",
    "            \"total_documents\": len(insights),\n",
    "            \"document_types\": doc_types,\n",
    "            \"risk_distribution\": risk_distribution,\n",
    "            \"sentiment_distribution\": sentiment_distribution,\n",
    "            \"average_confidence\": avg_confidence,\n",
    "            \"top_topics\": list(set(all_topics))[:10],\n",
    "            \"action_items\": list(set(all_actions)),\n",
    "            \"high_risk_documents\": [\n",
    "                {\"name\": i.document_name, \"path\": i.document_path, \"summary\": i.summary}\n",
    "                for i in insights if i.risk_level == \"high\"\n",
    "            ]\n",
    "        }\n",
    "\n",
    "print(\"Document Intelligence Pipeline created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and run document intelligence pipeline\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "pipeline = DocumentIntelligencePipeline(\n",
    "    egnyte_domain=os.getenv(\"EGNYTE_DOMAIN\"),\n",
    "    egnyte_token=os.getenv(\"EGNYTE_USER_TOKEN\"),\n",
    "    openai_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Process documents\n",
    "query = \"quarterly report financial analysis\"\n",
    "insights = pipeline.process_documents(query, max_docs=5)\n",
    "\n",
    "# Generate intelligence report\n",
    "intelligence_report = pipeline.generate_intelligence_report(insights)\n",
    "\n",
    "print(\"DOCUMENT INTELLIGENCE REPORT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total Documents Analyzed: {intelligence_report['total_documents']}\")\n",
    "print(f\"Average Confidence Score: {intelligence_report['average_confidence']:.2f}\")\n",
    "print(f\"\\n Document Types: {intelligence_report['document_types']}\")\n",
    "print(f\" Risk Distribution: {intelligence_report['risk_distribution']}\")\n",
    "print(f\" Sentiment Distribution: {intelligence_report['sentiment_distribution']}\")\n",
    "print(f\"\\n Top Topics: {intelligence_report['top_topics'][:5]}\")\n",
    "print(f\"\\n Action Items ({len(intelligence_report['action_items'])}):\")\n",
    "for action in intelligence_report['action_items'][:3]:\n",
    "    print(f\"  • {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Compliance Monitoring Workflow\n",
    "\n",
    "Automated compliance monitoring and regulatory document tracking:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ComplianceRule:\n",
    "    def __init__(self, name: str, description: str, keywords: List[str], severity: str):\n",
    "        self.name = name\n",
    "        self.description = description\n",
    "        self.keywords = keywords\n",
    "        self.severity = severity  # low, medium, high, critical\n",
    "\n",
    "@dataclass\n",
    "class ComplianceViolation:\n",
    "    rule_name: str\n",
    "    document_name: str\n",
    "    document_path: str\n",
    "    violation_text: str\n",
    "    severity: str\n",
    "    confidence: float\n",
    "    detected_at: datetime\n",
    "    remediation_suggestion: str\n",
    "\n",
    "class ComplianceMonitor:\n",
    "    \"\"\"Enterprise compliance monitoring system.\"\"\"\n",
    "    \n",
    "    def __init__(self, egnyte_domain: str, egnyte_token: str, openai_key: str):\n",
    "        self.retriever = EgnyteRetriever(domain=egnyte_domain, user_token=egnyte_token)\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4\", temperature=0, openai_api_key=openai_key)\n",
    "        \n",
    "        # Define compliance rules\n",
    "        self.compliance_rules = [\n",
    "            ComplianceRule(\n",
    "                name=\"PII_EXPOSURE\",\n",
    "                description=\"Personal Identifiable Information exposure\",\n",
    "                keywords=[\"SSN\", \"social security\", \"credit card\", \"passport\", \"driver license\"],\n",
    "                severity=\"critical\"\n",
    "            ),\n",
    "            ComplianceRule(\n",
    "                name=\"FINANCIAL_DISCLOSURE\",\n",
    "                description=\"Improper financial information disclosure\",\n",
    "                keywords=[\"insider trading\", \"material information\", \"earnings\", \"confidential financial\"],\n",
    "                severity=\"high\"\n",
    "            ),\n",
    "            ComplianceRule(\n",
    "                name=\"DATA_RETENTION\",\n",
    "                description=\"Data retention policy violations\",\n",
    "                keywords=[\"delete after\", \"retention period\", \"archive\", \"permanent storage\"],\n",
    "                severity=\"medium\"\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Compliance analysis prompt\n",
    "        self.compliance_prompt = PromptTemplate(\n",
    "            template=\"\"\"\n",
    "            Analyze this document for compliance violations based on the given rule:\n",
    "            \n",
    "            Rule: {rule_name}\n",
    "            Description: {rule_description}\n",
    "            Keywords: {keywords}\n",
    "            \n",
    "            Document: {document_name}\n",
    "            Content: {content}\n",
    "            \n",
    "            Provide analysis in JSON format:\n",
    "            {{\n",
    "                \"violation_detected\": true/false,\n",
    "                \"violation_text\": \"specific text that violates the rule\",\n",
    "                \"confidence\": 0.95,\n",
    "                \"remediation_suggestion\": \"specific steps to address the violation\"\n",
    "            }}\n",
    "            \n",
    "            JSON Response:\n",
    "            \"\"\",\n",
    "            input_variables=[\"rule_name\", \"rule_description\", \"keywords\", \"document_name\", \"content\"]\n",
    "        )\n",
    "    \n",
    "    def scan_documents(self, folder_path: str = None, days_back: int = 30) -> List[ComplianceViolation]:\n",
    "        \"\"\"Scan documents for compliance violations.\"\"\"\n",
    "        \n",
    "        logger.info(f\"Starting compliance scan for folder: {folder_path}, days back: {days_back}\")\n",
    "        \n",
    "        # Create search options\n",
    "        search_options = create_date_range_search_options(\n",
    "            created_after=datetime.now() - timedelta(days=days_back),\n",
    "            limit=20\n",
    "        )\n",
    "        \n",
    "        if folder_path:\n",
    "            search_options.folder_path = folder_path\n",
    "        \n",
    "        self.retriever.search_options = search_options\n",
    "        \n",
    "        # Get recent documents\n",
    "        documents = self.retriever.invoke(\"*\")  # Get all recent documents\n",
    "        logger.info(f\"Scanning {len(documents)} documents for compliance\")\n",
    "        \n",
    "        violations = []\n",
    "        \n",
    "        for doc in documents:\n",
    "            for rule in self.compliance_rules:\n",
    "                try:\n",
    "                    # Check if document contains rule keywords\n",
    "                    content_lower = doc.page_content.lower()\n",
    "                    if any(keyword.lower() in content_lower for keyword in rule.keywords):\n",
    "                        \n",
    "                        # Detailed analysis with LLM\n",
    "                        analysis = self.llm.invoke(\n",
    "                            self.compliance_prompt.format(\n",
    "                                rule_name=rule.name,\n",
    "                                rule_description=rule.description,\n",
    "                                keywords=\", \".join(rule.keywords),\n",
    "                                document_name=doc.metadata.get('name', 'Unknown'),\n",
    "                                content=doc.page_content[:3000]\n",
    "                            )\n",
    "                        )\n",
    "                        \n",
    "                        # Parse analysis\n",
    "                        analysis_data = json.loads(analysis.content)\n",
    "                        \n",
    "                        if analysis_data['violation_detected']:\n",
    "                            violation = ComplianceViolation(\n",
    "                                rule_name=rule.name,\n",
    "                                document_name=doc.metadata.get('name', 'Unknown'),\n",
    "                                document_path=doc.metadata.get('path', 'Unknown'),\n",
    "                                violation_text=analysis_data['violation_text'],\n",
    "                                severity=rule.severity,\n",
    "                                confidence=analysis_data['confidence'],\n",
    "                                detected_at=datetime.now(),\n",
    "                                remediation_suggestion=analysis_data['remediation_suggestion']\n",
    "                            )\n",
    "                            violations.append(violation)\n",
    "                            logger.warning(f\"Compliance violation detected: {rule.name} in {doc.metadata.get('name')}\")\n",
    "                \n",
    "                except Exception as e:\n",
    "                    logger.error(f\"Error analyzing document {doc.metadata.get('name')} for rule {rule.name}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        logger.info(f\"Compliance scan completed. Found {len(violations)} violations\")\n",
    "        return violations\n",
    "    \n",
    "    def generate_compliance_report(self, violations: List[ComplianceViolation]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate compliance report.\"\"\"\n",
    "        \n",
    "        # Group violations by severity\n",
    "        severity_groups = {\"critical\": [], \"high\": [], \"medium\": [], \"low\": []}\n",
    "        rule_counts = {}\n",
    "        \n",
    "        for violation in violations:\n",
    "            severity_groups[violation.severity].append(violation)\n",
    "            rule_counts[violation.rule_name] = rule_counts.get(violation.rule_name, 0) + 1\n",
    "        \n",
    "        return {\n",
    "            \"report_timestamp\": datetime.now().isoformat(),\n",
    "            \"total_violations\": len(violations),\n",
    "            \"severity_breakdown\": {\n",
    "                severity: len(viols) for severity, viols in severity_groups.items()\n",
    "            },\n",
    "            \"rule_breakdown\": rule_counts,\n",
    "            \"critical_violations\": [\n",
    "                {\n",
    "                    \"rule\": v.rule_name,\n",
    "                    \"document\": v.document_name,\n",
    "                    \"path\": v.document_path,\n",
    "                    \"confidence\": v.confidence,\n",
    "                    \"remediation\": v.remediation_suggestion\n",
    "                }\n",
    "                for v in severity_groups[\"critical\"]\n",
    "            ],\n",
    "            \"recommendations\": self._generate_recommendations(violations)\n",
    "        }\n",
    "    \n",
    "    def _generate_recommendations(self, violations: List[ComplianceViolation]) -> List[str]:\n",
    "        \"\"\"Generate compliance recommendations.\"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        critical_count = sum(1 for v in violations if v.severity == \"critical\")\n",
    "        if critical_count > 0:\n",
    "            recommendations.append(f\"URGENT: Address {critical_count} critical compliance violations immediately\")\n",
    "        \n",
    "        pii_violations = sum(1 for v in violations if v.rule_name == \"PII_EXPOSURE\")\n",
    "        if pii_violations > 0:\n",
    "            recommendations.append(f\"Implement PII scanning and redaction for {pii_violations} documents\")\n",
    "        \n",
    "        recommendations.append(\"Schedule regular compliance training for document handling\")\n",
    "        recommendations.append(\"Implement automated compliance monitoring for new documents\")\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "print(\"Compliance Monitor created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run compliance monitoring\n",
    "compliance_monitor = ComplianceMonitor(\n",
    "    egnyte_domain=os.getenv(\"EGNYTE_DOMAIN\"),\n",
    "    egnyte_token=os.getenv(\"EGNYTE_USER_TOKEN\"),\n",
    "    openai_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Scan for violations\n",
    "violations = compliance_monitor.scan_documents(days_back=7)\n",
    "\n",
    "# Generate compliance report\n",
    "compliance_report = compliance_monitor.generate_compliance_report(violations)\n",
    "\n",
    "print(\"COMPLIANCE MONITORING REPORT\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Total Violations Found: {compliance_report['total_violations']}\")\n",
    "print(f\"\\n Severity Breakdown:\")\n",
    "for severity, count in compliance_report['severity_breakdown'].items():\n",
    "    if count > 0:\n",
    "        print(f\"  {severity.upper()}: {count}\")\n",
    "\n",
    "print(f\"\\n Rule Breakdown:\")\n",
    "for rule, count in compliance_report['rule_breakdown'].items():\n",
    "    print(f\"  {rule}: {count}\")\n",
    "\n",
    "if compliance_report['critical_violations']:\n",
    "    print(f\"\\n CRITICAL VIOLATIONS:\")\n",
    "    for violation in compliance_report['critical_violations']:\n",
    "        print(f\"  • {violation['rule']} in {violation['document']}\")\n",
    "        print(f\"    Confidence: {violation['confidence']:.2f}\")\n",
    "        print(f\"    Action: {violation['remediation']}\")\n",
    "\n",
    "print(f\"\\n RECOMMENDATIONS:\")\n",
    "for rec in compliance_report['recommendations']:\n",
    "    print(f\"  • {rec}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Executive Reporting Workflow\n",
    "\n",
    "Automated executive summary generation from enterprise documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExecutiveReportGenerator:\n",
    "    \"\"\"Generate executive reports from enterprise documents.\"\"\"\n",
    "    \n",
    "    def __init__(self, egnyte_domain: str, egnyte_token: str, openai_key: str):\n",
    "        self.retriever = EgnyteRetriever(domain=egnyte_domain, user_token=egnyte_token)\n",
    "        self.llm = ChatOpenAI(model=\"gpt-4\", temperature=0.2, openai_api_key=openai_key)\n",
    "        \n",
    "        # Executive summary prompt\n",
    "        self.executive_prompt = PromptTemplate(\n",
    "            template=\"\"\"\n",
    "            Create an executive summary from the following business documents.\n",
    "            Focus on key insights, strategic implications, and actionable recommendations.\n",
    "            \n",
    "            Documents:\n",
    "            {documents}\n",
    "            \n",
    "            Generate a comprehensive executive summary with:\n",
    "            \n",
    "            ## EXECUTIVE SUMMARY\n",
    "            \n",
    "            ### Key Highlights\n",
    "            - [3-5 most important findings]\n",
    "            \n",
    "            ### Strategic Insights\n",
    "            - [Strategic implications and trends]\n",
    "            \n",
    "            ### Financial Impact\n",
    "            - [Financial implications and metrics]\n",
    "            \n",
    "            ### Risk Assessment\n",
    "            - [Key risks and mitigation strategies]\n",
    "            \n",
    "            ### Recommendations\n",
    "            - [Specific, actionable recommendations]\n",
    "            \n",
    "            ### Next Steps\n",
    "            - [Immediate actions required]\n",
    "            \n",
    "            Keep the summary concise but comprehensive, suitable for C-level executives.\n",
    "            \"\"\",\n",
    "            input_variables=[\"documents\"]\n",
    "        )\n",
    "    \n",
    "    def generate_executive_report(\n",
    "        self, \n",
    "        topic: str, \n",
    "        folder_paths: List[str] = None,\n",
    "        days_back: int = 30\n",
    "    ) -> Dict[str, Any]:\n",
    "        \"\"\"Generate executive report for a specific topic.\"\"\"\n",
    "        \n",
    "        logger.info(f\"Generating executive report for topic: {topic}\")\n",
    "        \n",
    "        all_documents = []\n",
    "        \n",
    "        # Search in specified folders or globally\n",
    "        if folder_paths:\n",
    "            for folder in folder_paths:\n",
    "                search_options = create_folder_search_options(\n",
    "                    folder_path=folder,\n",
    "                    limit=10\n",
    "                )\n",
    "                search_options.created_after = datetime.now() - timedelta(days=days_back)\n",
    "                \n",
    "                self.retriever.search_options = search_options\n",
    "                docs = self.retriever.invoke(topic)\n",
    "                all_documents.extend(docs)\n",
    "        else:\n",
    "            search_options = create_date_range_search_options(\n",
    "                created_after=datetime.now() - timedelta(days=days_back),\n",
    "                limit=15\n",
    "            )\n",
    "            self.retriever.search_options = search_options\n",
    "            all_documents = self.retriever.invoke(topic)\n",
    "        \n",
    "        logger.info(f\"Retrieved {len(all_documents)} documents for executive report\")\n",
    "        \n",
    "        # Prepare document content for analysis\n",
    "        document_summaries = []\n",
    "        for doc in all_documents[:10]:  # Limit to top 10 documents\n",
    "            summary = f\"\"\"\n",
    "            Document: {doc.metadata.get('name', 'Unknown')}\n",
    "            Path: {doc.metadata.get('path', 'Unknown')}\n",
    "            Content: {doc.page_content[:1500]}...\n",
    "            \"\"\"\n",
    "            document_summaries.append(summary)\n",
    "        \n",
    "        # Generate executive summary\n",
    "        with get_openai_callback() as cb:\n",
    "            executive_summary = self.llm.invoke(\n",
    "                self.executive_prompt.format(\n",
    "                    documents=\"\\n\\n\".join(document_summaries)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # Compile report\n",
    "        report = {\n",
    "            \"report_title\": f\"Executive Report: {topic.title()}\",\n",
    "            \"generated_at\": datetime.now().isoformat(),\n",
    "            \"topic\": topic,\n",
    "            \"documents_analyzed\": len(all_documents),\n",
    "            \"time_period\": f\"Last {days_back} days\",\n",
    "            \"executive_summary\": executive_summary.content,\n",
    "            \"source_documents\": [\n",
    "                {\n",
    "                    \"name\": doc.metadata.get('name', 'Unknown'),\n",
    "                    \"path\": doc.metadata.get('path', 'Unknown'),\n",
    "                    \"size\": doc.metadata.get('size', 'Unknown'),\n",
    "                    \"modified\": doc.metadata.get('last_modified', 'Unknown')\n",
    "                }\n",
    "                for doc in all_documents[:10]\n",
    "            ],\n",
    "            \"api_usage\": {\n",
    "                \"total_tokens\": cb.total_tokens,\n",
    "                \"prompt_tokens\": cb.prompt_tokens,\n",
    "                \"completion_tokens\": cb.completion_tokens,\n",
    "                \"total_cost\": cb.total_cost\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        logger.info(f\"Executive report generated. API cost: ${cb.total_cost:.4f}\")\n",
    "        return report\n",
    "    \n",
    "    def save_report(self, report: Dict[str, Any], filename: str = None) -> str:\n",
    "        \"\"\"Save executive report to file.\"\"\"\n",
    "        \n",
    "        if not filename:\n",
    "            timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            topic_clean = report['topic'].replace(' ', '_').lower()\n",
    "            filename = f\"executive_report_{topic_clean}_{timestamp}.json\"\n",
    "        \n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(report, f, indent=2, default=str)\n",
    "        \n",
    "        logger.info(f\"Executive report saved to {filename}\")\n",
    "        return filename\n",
    "\n",
    "print(\"Executive Report Generator created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate executive report\n",
    "exec_generator = ExecutiveReportGenerator(\n",
    "    egnyte_domain=os.getenv(\"EGNYTE_DOMAIN\"),\n",
    "    egnyte_token=os.getenv(\"EGNYTE_USER_TOKEN\"),\n",
    "    openai_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Generate report for quarterly performance\n",
    "exec_report = exec_generator.generate_executive_report(\n",
    "    topic=\"quarterly performance metrics\",\n",
    "    folder_paths=[\"/Shared/Reports\", \"/Shared/Finance\"],\n",
    "    days_back=90\n",
    ")\n",
    "\n",
    "print(\"EXECUTIVE REPORT GENERATED\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Title: {exec_report['report_title']}\")\n",
    "print(f\"Documents Analyzed: {exec_report['documents_analyzed']}\")\n",
    "print(f\"Time Period: {exec_report['time_period']}\")\n",
    "print(f\"API Cost: ${exec_report['api_usage']['total_cost']:.4f}\")\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(exec_report['executive_summary'])\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(f\"\\n Source Documents ({len(exec_report['source_documents'])}):\")\n",
    "for i, doc in enumerate(exec_report['source_documents'][:5], 1):\n",
    "    print(f\"{i}. {doc['name']} - {doc['path']}\")\n",
    "\n",
    "# Save report\n",
    "report_file = exec_generator.save_report(exec_report)\n",
    "print(f\"\\n Report saved to: {report_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Enterprise Workflow Orchestration\n",
    "\n",
    "Orchestrate multiple workflows for comprehensive enterprise automation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnterpriseWorkflowOrchestrator:\n",
    "    \"\"\"Orchestrate multiple enterprise workflows.\"\"\"\n",
    "    \n",
    "    def __init__(self, egnyte_domain: str, egnyte_token: str, openai_key: str):\n",
    "        self.domain = egnyte_domain\n",
    "        self.token = egnyte_token\n",
    "        self.openai_key = openai_key\n",
    "        \n",
    "        # Initialize all workflow components\n",
    "        self.doc_intelligence = DocumentIntelligencePipeline(egnyte_domain, egnyte_token, openai_key)\n",
    "        self.compliance_monitor = ComplianceMonitor(egnyte_domain, egnyte_token, openai_key)\n",
    "        self.exec_generator = ExecutiveReportGenerator(egnyte_domain, egnyte_token, openai_key)\n",
    "    \n",
    "    def run_daily_workflow(self) -> Dict[str, Any]:\n",
    "        \"\"\"Run comprehensive daily enterprise workflow.\"\"\"\n",
    "        \n",
    "        logger.info(\"Starting daily enterprise workflow\")\n",
    "        workflow_start = datetime.now()\n",
    "        \n",
    "        results = {\n",
    "            \"workflow_start\": workflow_start.isoformat(),\n",
    "            \"status\": \"running\",\n",
    "            \"components\": {}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # 1. Document Intelligence Analysis\n",
    "            logger.info(\"Running document intelligence analysis...\")\n",
    "            doc_insights = self.doc_intelligence.process_documents(\"daily reports\", max_docs=15)\n",
    "            intelligence_report = self.doc_intelligence.generate_intelligence_report(doc_insights)\n",
    "            \n",
    "            results[\"components\"][\"document_intelligence\"] = {\n",
    "                \"status\": \"completed\",\n",
    "                \"insights_generated\": len(doc_insights),\n",
    "                \"high_risk_docs\": len(intelligence_report['high_risk_documents']),\n",
    "                \"avg_confidence\": intelligence_report['average_confidence']\n",
    "            }\n",
    "            \n",
    "            # 2. Compliance Monitoring\n",
    "            logger.info(\"Running compliance monitoring...\")\n",
    "            violations = self.compliance_monitor.scan_documents(days_back=1)  # Daily scan\n",
    "            compliance_report = self.compliance_monitor.generate_compliance_report(violations)\n",
    "            \n",
    "            results[\"components\"][\"compliance_monitoring\"] = {\n",
    "                \"status\": \"completed\",\n",
    "                \"violations_found\": len(violations),\n",
    "                \"critical_violations\": compliance_report['severity_breakdown']['critical'],\n",
    "                \"recommendations\": len(compliance_report['recommendations'])\n",
    "            }\n",
    "            \n",
    "            # 3. Executive Reporting (weekly)\n",
    "            if workflow_start.weekday() == 0:  # Monday\n",
    "                logger.info(\"Generating weekly executive report...\")\n",
    "                exec_report = self.exec_generator.generate_executive_report(\n",
    "                    topic=\"weekly business summary\",\n",
    "                    days_back=7\n",
    "                )\n",
    "                \n",
    "                results[\"components\"][\"executive_reporting\"] = {\n",
    "                    \"status\": \"completed\",\n",
    "                    \"documents_analyzed\": exec_report['documents_analyzed'],\n",
    "                    \"api_cost\": exec_report['api_usage']['total_cost']\n",
    "                }\n",
    "            else:\n",
    "                results[\"components\"][\"executive_reporting\"] = {\n",
    "                    \"status\": \"skipped\",\n",
    "                    \"reason\": \"Weekly report only generated on Mondays\"\n",
    "                }\n",
    "            \n",
    "            # 4. Generate Workflow Summary\n",
    "            workflow_summary = self._generate_workflow_summary(results)\n",
    "            results[\"workflow_summary\"] = workflow_summary\n",
    "            \n",
    "            results[\"status\"] = \"completed\"\n",
    "            results[\"workflow_end\"] = datetime.now().isoformat()\n",
    "            results[\"duration_minutes\"] = (datetime.now() - workflow_start).total_seconds() / 60\n",
    "            \n",
    "            logger.info(f\"Daily workflow completed in {results['duration_minutes']:.2f} minutes\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Workflow failed: {e}\")\n",
    "            results[\"status\"] = \"failed\"\n",
    "            results[\"error\"] = str(e)\n",
    "            results[\"workflow_end\"] = datetime.now().isoformat()\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _generate_workflow_summary(self, results: Dict[str, Any]) -> str:\n",
    "        \"\"\"Generate human-readable workflow summary.\"\"\"\n",
    "        \n",
    "        summary_parts = []\n",
    "        \n",
    "        # Document Intelligence Summary\n",
    "        doc_intel = results[\"components\"].get(\"document_intelligence\", {})\n",
    "        if doc_intel.get(\"status\") == \"completed\":\n",
    "            summary_parts.append(\n",
    "                f\"📊 Document Intelligence: Analyzed {doc_intel['insights_generated']} documents, \"\n",
    "                f\"found {doc_intel['high_risk_docs']} high-risk items \"\n",
    "                f\"(avg confidence: {doc_intel['avg_confidence']:.2f})\"\n",
    "            )\n",
    "        \n",
    "        # Compliance Summary\n",
    "        compliance = results[\"components\"].get(\"compliance_monitoring\", {})\n",
    "        if compliance.get(\"status\") == \"completed\":\n",
    "            summary_parts.append(\n",
    "                f\"🛡️ Compliance: Found {compliance['violations_found']} violations, \"\n",
    "                f\"{compliance['critical_violations']} critical issues requiring immediate attention\"\n",
    "            )\n",
    "        \n",
    "        # Executive Reporting Summary\n",
    "        exec_report = results[\"components\"].get(\"executive_reporting\", {})\n",
    "        if exec_report.get(\"status\") == \"completed\":\n",
    "            summary_parts.append(\n",
    "                f\" Executive Report: Generated from {exec_report['documents_analyzed']} documents \"\n",
    "                f\"(API cost: ${exec_report['api_cost']:.4f})\"\n",
    "            )\n",
    "        elif exec_report.get(\"status\") == \"skipped\":\n",
    "            summary_parts.append(\" Executive Report: Skipped (weekly schedule)\")\n",
    "        \n",
    "        return \"\\n\".join(summary_parts)\n",
    "    \n",
    "    def schedule_workflow(self, schedule_type: str = \"daily\") -> str:\n",
    "        \"\"\"Generate cron schedule for workflow automation.\"\"\"\n",
    "        \n",
    "        schedules = {\n",
    "            \"daily\": \"0 6 * * *\",  # 6 AM daily\n",
    "            \"hourly\": \"0 * * * *\",  # Every hour\n",
    "            \"weekly\": \"0 6 * * 1\",  # 6 AM Monday\n",
    "            \"business_hours\": \"0 9-17 * * 1-5\"  # 9 AM to 5 PM, weekdays\n",
    "        }\n",
    "        \n",
    "        return schedules.get(schedule_type, schedules[\"daily\"])\n",
    "\n",
    "print(\" Enterprise Workflow Orchestrator created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run enterprise workflow\n",
    "orchestrator = EnterpriseWorkflowOrchestrator(\n",
    "    egnyte_domain=os.getenv(\"EGNYTE_DOMAIN\"),\n",
    "    egnyte_token=os.getenv(\"EGNYTE_USER_TOKEN\"),\n",
    "    openai_key=os.getenv(\"OPENAI_API_KEY\")\n",
    ")\n",
    "\n",
    "# Execute daily workflow\n",
    "workflow_results = orchestrator.run_daily_workflow()\n",
    "\n",
    "print(\" ENTERPRISE WORKFLOW RESULTS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Status: {workflow_results['status'].upper()}\")\n",
    "print(f\"Duration: {workflow_results.get('duration_minutes', 0):.2f} minutes\")\n",
    "print(f\"\\n Workflow Summary:\")\n",
    "print(workflow_results.get('workflow_summary', 'No summary available'))\n",
    "\n",
    "print(f\"\\n🔧 Component Status:\")\n",
    "for component, details in workflow_results['components'].items():\n",
    "    status_emoji = \"✅\" if details['status'] == 'completed' else \"⏭️\" if details['status'] == 'skipped' else \"❌\"\n",
    "    print(f\"  {status_emoji} {component.replace('_', ' ').title()}: {details['status']}\")\n",
    "\n",
    "# Generate cron schedule\n",
    "cron_schedule = orchestrator.schedule_workflow(\"daily\")\n",
    "print(f\"\\n Cron Schedule (daily): {cron_schedule}\")\n",
    "print(\"   Add this to your crontab for automated execution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Production Deployment Considerations\n",
    "\n",
    "### Infrastructure Requirements:\n",
    "\n",
    "1. **Compute Resources**: 4+ CPU cores, 8GB+ RAM for concurrent processing\n",
    "2. **Storage**: Persistent storage for reports and logs\n",
    "3. **Network**: Reliable internet for Egnyte and OpenAI API calls\n",
    "4. **Security**: Secure credential management (AWS Secrets Manager, Azure Key Vault)\n",
    "\n",
    "### Monitoring & Alerting:\n",
    "\n",
    "1. **Health Checks**: Monitor API availability and response times\n",
    "2. **Error Tracking**: Comprehensive error logging and alerting\n",
    "3. **Cost Monitoring**: Track OpenAI API usage and costs\n",
    "4. **Compliance Alerts**: Immediate notifications for critical violations\n",
    "\n",
    "### Scalability:\n",
    "\n",
    "1. **Horizontal Scaling**: Multiple worker instances for large document volumes\n",
    "2. **Queue Management**: Redis/RabbitMQ for job queuing\n",
    "3. **Caching**: Redis for API response caching\n",
    "4. **Load Balancing**: Distribute requests across instances\n",
    "\n",
    "### Security Best Practices:\n",
    "\n",
    "1. **Credential Rotation**: Regular API key rotation\n",
    "2. **Network Security**: VPC/firewall configuration\n",
    "3. **Data Encryption**: Encrypt sensitive data at rest and in transit\n",
    "4. **Audit Logging**: Comprehensive audit trail for compliance\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "- **[Multi-Modal Analysis](04-multimodal-analysis.ipynb)**: Working with different document types\n",
    "- **[Security & Compliance](05-security-compliance.ipynb)**: Advanced security patterns\n",
    "- **[Performance Optimization](06-performance-optimization.ipynb)**: Scaling for enterprise workloads"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
