{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Egnyte-LangChain Integration\n",
    "\n",
    "This notebook demonstrates the basic integration between Egnyte and LangChain for document retrieval and AI-powered analysis.\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "1. **Egnyte Account**: Access to an Egnyte domain\n",
    "2. **API Credentials**: User token or OAuth setup\n",
    "3. **Python Environment**: Python 3.8+ with required packages\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "pip install egnyte-langchain-connector\n",
    "pip install langchain-openai  # For AI model integration\n",
    "pip install python-dotenv    # For environment variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup\n",
    "\n",
    "Create a `.env` file with your Egnyte credentials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Verify credentials are available\n",
    "EGNYTE_DOMAIN = os.getenv(\"EGNYTE_DOMAIN\")\n",
    "EGNYTE_USER_TOKEN = os.getenv(\"EGNYTE_USER_TOKEN\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(f\"Egnyte Domain: {EGNYTE_DOMAIN}\")\n",
    "print(f\"Token Available: {'Yes' if EGNYTE_USER_TOKEN else 'No'}\")\n",
    "print(f\"OpenAI Key Available: {'Yes' if OPENAI_API_KEY else 'No'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Document Retrieval\n",
    "\n",
    "Let's start with basic document retrieval from Egnyte:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_egnyte import EgnyteRetriever, EgnyteSearchOptions\n",
    "\n",
    "# Create the retriever\n",
    "retriever = EgnyteRetriever(\n",
    "    domain=EGNYTE_DOMAIN,\n",
    "    user_token=EGNYTE_USER_TOKEN\n",
    ")\n",
    "\n",
    "print(f\"âœ… Retriever created for domain: {EGNYTE_DOMAIN}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a basic search\n",
    "query = \"project proposal\"\n",
    "documents = retriever.invoke(query)\n",
    "\n",
    "print(f\"Found {len(documents)} documents for query: '{query}'\")\n",
    "print(\"\\nðŸ“„ Document Results:\")\n",
    "\n",
    "for i, doc in enumerate(documents[:3], 1):  # Show first 3 results\n",
    "    print(f\"\\n{i}. {doc.metadata.get('name', 'Unknown')}\")\n",
    "    print(f\"   Path: {doc.metadata.get('path', 'Unknown')}\")\n",
    "    print(f\"   Size: {doc.metadata.get('size', 'Unknown')} bytes\")\n",
    "    print(f\"   Modified: {doc.metadata.get('last_modified', 'Unknown')}\")\n",
    "    print(f\"   Content Preview: {doc.page_content[:100]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Search Options\n",
    "\n",
    "Use search options to refine your queries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "from langchain_egnyte import create_folder_search_options, create_date_range_search_options\n",
    "\n",
    "# Search in specific folder\n",
    "folder_options = create_folder_search_options(\n",
    "    folder_path=\"/Shared/Projects\",\n",
    "    limit=10\n",
    ")\n",
    "\n",
    "retriever_with_folder = EgnyteRetriever(\n",
    "    domain=EGNYTE_DOMAIN,\n",
    "    user_token=EGNYTE_USER_TOKEN,\n",
    "    search_options=folder_options\n",
    ")\n",
    "\n",
    "folder_documents = retriever_with_folder.invoke(\"budget analysis\")\n",
    "print(f\"Found {len(folder_documents)} documents in /Shared/Projects folder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search with date range (last 30 days)\n",
    "thirty_days_ago = datetime.now() - timedelta(days=30)\n",
    "\n",
    "date_options = create_date_range_search_options(\n",
    "    created_after=thirty_days_ago,\n",
    "    limit=5\n",
    ")\n",
    "\n",
    "retriever_with_date = EgnyteRetriever(\n",
    "    domain=EGNYTE_DOMAIN,\n",
    "    user_token=EGNYTE_USER_TOKEN,\n",
    "    search_options=date_options\n",
    ")\n",
    "\n",
    "recent_documents = retriever_with_date.invoke(\"meeting notes\")\n",
    "print(f\"Found {len(recent_documents)} recent documents (last 30 days)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AI-Powered Document Analysis\n",
    "\n",
    "Now let's integrate with OpenAI for intelligent document analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Initialize OpenAI model\n",
    "llm = ChatOpenAI(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    temperature=0.1,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")\n",
    "\n",
    "# Create a custom prompt for document analysis\n",
    "prompt_template = \"\"\"\n",
    "You are an expert document analyst. Based on the following documents from Egnyte, \n",
    "provide a comprehensive answer to the question.\n",
    "\n",
    "Context from Egnyte documents:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Please provide a detailed answer based on the document content, including:\n",
    "1. Key findings from the documents\n",
    "2. Relevant details and data points\n",
    "3. Source document references\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=prompt_template,\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Create the QA chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    chain_type_kwargs={\"prompt\": PROMPT},\n",
    "    return_source_documents=True\n",
    ")\n",
    "\n",
    "print(\"AI-powered QA chain created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask questions about your documents\n",
    "question = \"What are the key project milestones mentioned in the documents?\"\n",
    "\n",
    "result = qa_chain.invoke({\"query\": question})\n",
    "\n",
    "print(f\"AI Analysis for: '{question}'\")\n",
    "print(\"\\nAnswer:\")\n",
    "print(result[\"result\"])\n",
    "\n",
    "print(\"\\nSource Documents:\")\n",
    "for i, doc in enumerate(result[\"source_documents\"], 1):\n",
    "    print(f\"{i}. {doc.metadata.get('name', 'Unknown')} - {doc.metadata.get('path', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Async Operations for Better Performance\n",
    "\n",
    "For high-performance applications, use async operations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def async_document_search():\n",
    "    \"\"\"Demonstrate async document retrieval.\"\"\"\n",
    "    \n",
    "    # Multiple concurrent searches\n",
    "    queries = [\n",
    "        \"financial report\",\n",
    "        \"project timeline\",\n",
    "        \"team meeting\"\n",
    "    ]\n",
    "    \n",
    "    # Execute searches concurrently\n",
    "    tasks = [retriever.ainvoke(query) for query in queries]\n",
    "    results = await asyncio.gather(*tasks)\n",
    "    \n",
    "    print(\"Async Search Results:\")\n",
    "    for query, docs in zip(queries, results):\n",
    "        print(f\"  '{query}': {len(docs)} documents found\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run async search\n",
    "async_results = await async_document_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error Handling and Best Practices\n",
    "\n",
    "Implement robust error handling for production use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_egnyte import (\n",
    "    AuthenticationError,\n",
    "    ValidationError,\n",
    "    RateLimitError,\n",
    "    ConnectionError\n",
    ")\n",
    "\n",
    "def safe_document_search(query: str, max_retries: int = 3):\n",
    "    \"\"\"Perform document search with comprehensive error handling.\"\"\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            documents = retriever.invoke(query)\n",
    "            print(f\"Search successful: {len(documents)} documents found\")\n",
    "            return documents\n",
    "            \n",
    "        except AuthenticationError as e:\n",
    "            print(f\"Authentication failed: {e}\")\n",
    "            print(\"Please check your Egnyte credentials\")\n",
    "            break\n",
    "            \n",
    "        except ValidationError as e:\n",
    "            print(f\"Invalid query: {e}\")\n",
    "            break\n",
    "            \n",
    "        except RateLimitError as e:\n",
    "            print(f\"Rate limit exceeded (attempt {attempt + 1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                import time\n",
    "                time.sleep(2 ** attempt)  # Exponential backoff\n",
    "                \n",
    "        except ConnectionError as e:\n",
    "            print(f\"Connection error (attempt {attempt + 1}/{max_retries}): {e}\")\n",
    "            if attempt < max_retries - 1:\n",
    "                import time\n",
    "                time.sleep(1)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            break\n",
    "    \n",
    "    return []\n",
    "\n",
    "# Test error handling\n",
    "safe_results = safe_document_search(\"test query\")\n",
    "print(f\"Safe search returned {len(safe_results)} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Monitoring\n",
    "\n",
    "Monitor performance for optimization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import List\n",
    "from langchain.schema import Document\n",
    "\n",
    "def benchmark_search(queries: List[str]) -> dict:\n",
    "    \"\"\"Benchmark search performance across multiple queries.\"\"\"\n",
    "    \n",
    "    results = {\n",
    "        \"total_queries\": len(queries),\n",
    "        \"total_documents\": 0,\n",
    "        \"total_time\": 0,\n",
    "        \"average_time_per_query\": 0,\n",
    "        \"queries_per_second\": 0\n",
    "    }\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for query in queries:\n",
    "        query_start = time.time()\n",
    "        documents = retriever.invoke(query)\n",
    "        query_time = time.time() - query_start\n",
    "        \n",
    "        results[\"total_documents\"] += len(documents)\n",
    "        print(f\"Query: '{query}' - {len(documents)} docs in {query_time:.2f}s\")\n",
    "    \n",
    "    results[\"total_time\"] = time.time() - start_time\n",
    "    results[\"average_time_per_query\"] = results[\"total_time\"] / len(queries)\n",
    "    results[\"queries_per_second\"] = len(queries) / results[\"total_time\"]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Benchmark performance\n",
    "test_queries = [\"report\", \"meeting\", \"project\", \"budget\", \"analysis\"]\n",
    "performance = benchmark_search(test_queries)\n",
    "\n",
    "print(\"\\nPerformance Results:\")\n",
    "print(f\"Total Queries: {performance['total_queries']}\")\n",
    "print(f\"Total Documents: {performance['total_documents']}\")\n",
    "print(f\"Total Time: {performance['total_time']:.2f}s\")\n",
    "print(f\"Average Time per Query: {performance['average_time_per_query']:.2f}s\")\n",
    "print(f\"Queries per Second: {performance['queries_per_second']:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "This notebook covered the basics of Egnyte-LangChain integration. For more advanced use cases, check out:\n",
    "\n",
    "1. **[Advanced RAG Patterns](02-advanced-rag-patterns.ipynb)** - Complex retrieval-augmented generation\n",
    "2. **[Enterprise Workflows](03-enterprise-workflows.ipynb)** - Production deployment patterns\n",
    "3. **[Multi-Modal Analysis](04-multimodal-analysis.ipynb)** - Working with different document types\n",
    "\n",
    "## Resources\n",
    "\n",
    "- **Documentation**: [Egnyte-LangChain Connector Docs](https://github.com/your-repo/docs)\n",
    "- **API Reference**: [Egnyte Public API](https://developers.egnyte.com)\n",
    "- **LangChain Docs**: [LangChain Documentation](https://docs.langchain.com)\n",
    "- **Support**: [GitHub Issues](https://github.com/your-repo/issues)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
