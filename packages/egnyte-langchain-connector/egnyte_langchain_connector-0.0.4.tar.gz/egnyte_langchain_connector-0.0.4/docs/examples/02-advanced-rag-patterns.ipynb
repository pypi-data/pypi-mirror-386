{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG Patterns with Egnyte-LangChain\n",
    "\n",
    "This notebook demonstrates advanced Retrieval-Augmented Generation (RAG) patterns using Egnyte as the knowledge base and LangChain for AI processing.\n",
    "\n",
    "## Advanced Patterns Covered\n",
    "\n",
    "1. **Multi-Query RAG**: Generate multiple queries for comprehensive retrieval\n",
    "2. **Hierarchical RAG**: Organize documents by importance and relevance\n",
    "3. **Contextual Compression**: Compress retrieved content for better AI processing\n",
    "4. **Self-Querying**: Let AI determine search parameters\n",
    "5. **Ensemble Retrieval**: Combine multiple retrieval strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup and imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain.retrievers import (\n",
    "    MultiQueryRetriever,\n",
    "    ContextualCompressionRetriever,\n",
    "    EnsembleRetriever\n",
    ")\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Egnyte imports\n",
    "from langchain_egnyte import EgnyteRetriever, EgnyteSearchOptions\n",
    "\n",
    "# Load environment\n",
    "load_dotenv()\n",
    "\n",
    "EGNYTE_DOMAIN = os.getenv(\"EGNYTE_DOMAIN\")\n",
    "EGNYTE_USER_TOKEN = os.getenv(\"EGNYTE_USER_TOKEN\")\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "print(\"Advanced RAG Setup Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multi-Query RAG\n",
    "\n",
    "Generate multiple related queries to improve retrieval coverage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize base components\n",
    "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "base_retriever = EgnyteRetriever(\n",
    "    domain=EGNYTE_DOMAIN,\n",
    "    user_token=EGNYTE_USER_TOKEN\n",
    ")\n",
    "\n",
    "# Create multi-query retriever\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=base_retriever,\n",
    "    llm=llm,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Multi-Query Retriever created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test multi-query retrieval\n",
    "user_question = \"What are the financial projections for next quarter?\"\n",
    "\n",
    "print(f\"Original Question: {user_question}\")\n",
    "print(\"\\nMulti-Query Retrieval in progress...\")\n",
    "\n",
    "# This will generate multiple related queries automatically\n",
    "multi_docs = multi_query_retriever.invoke(user_question)\n",
    "\n",
    "print(f\"\\nRetrieved {len(multi_docs)} documents from multiple queries\")\n",
    "for i, doc in enumerate(multi_docs[:3], 1):\n",
    "    print(f\"{i}. {doc.metadata.get('name', 'Unknown')} - {doc.metadata.get('path', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Contextual Compression RAG\n",
    "\n",
    "Compress retrieved documents to focus on relevant content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create contextual compression retriever\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=base_retriever\n",
    ")\n",
    "\n",
    "print(\"Contextual Compression Retriever created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare regular vs compressed retrieval\n",
    "query = \"project budget allocation\"\n",
    "\n",
    "# Regular retrieval\n",
    "regular_docs = base_retriever.invoke(query)\n",
    "print(f\"Regular Retrieval: {len(regular_docs)} documents\")\n",
    "if regular_docs:\n",
    "    print(f\"   First doc length: {len(regular_docs[0].page_content)} characters\")\n",
    "\n",
    "# Compressed retrieval\n",
    "compressed_docs = compression_retriever.invoke(query)\n",
    "print(f\"\\nCompressed Retrieval: {len(compressed_docs)} documents\")\n",
    "if compressed_docs:\n",
    "    print(f\"   First doc length: {len(compressed_docs[0].page_content)} characters\")\n",
    "    print(f\"   Compression ratio: {len(compressed_docs[0].page_content) / len(regular_docs[0].page_content):.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Hierarchical Document Organization\n",
    "\n",
    "Organize documents by relevance and importance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HierarchicalRetriever:\n",
    "    \"\"\"Custom retriever that organizes documents hierarchically.\"\"\"\n",
    "    \n",
    "    def __init__(self, base_retriever: EgnyteRetriever, llm: ChatOpenAI):\n",
    "        self.base_retriever = base_retriever\n",
    "        self.llm = llm\n",
    "        \n",
    "    def _score_relevance(self, doc: Document, query: str) -> float:\n",
    "        \"\"\"Score document relevance using LLM.\"\"\"\n",
    "        prompt = f\"\"\"\n",
    "        Rate the relevance of this document to the query on a scale of 0-1.\n",
    "        \n",
    "        Query: {query}\n",
    "        Document: {doc.page_content[:500]}...\n",
    "        \n",
    "        Relevance score (0-1):\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = self.llm.invoke(prompt)\n",
    "            score = float(response.content.strip())\n",
    "            return max(0, min(1, score))  # Clamp to 0-1\n",
    "        except:\n",
    "            return 0.5  # Default score\n",
    "    \n",
    "    def retrieve_hierarchical(self, query: str, max_docs: int = 10) -> Dict[str, List[Document]]:\n",
    "        \"\"\"Retrieve and organize documents hierarchically.\"\"\"\n",
    "        \n",
    "        # Get base documents\n",
    "        docs = self.base_retriever.invoke(query)\n",
    "        \n",
    "        # Score each document\n",
    "        scored_docs = []\n",
    "        for doc in docs[:max_docs]:\n",
    "            score = self._score_relevance(doc, query)\n",
    "            scored_docs.append((doc, score))\n",
    "        \n",
    "        # Sort by score\n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # Organize into tiers\n",
    "        hierarchy = {\n",
    "            \"high_relevance\": [doc for doc, score in scored_docs if score >= 0.8],\n",
    "            \"medium_relevance\": [doc for doc, score in scored_docs if 0.5 <= score < 0.8],\n",
    "            \"low_relevance\": [doc for doc, score in scored_docs if score < 0.5]\n",
    "        }\n",
    "        \n",
    "        return hierarchy\n",
    "\n",
    "# Create hierarchical retriever\n",
    "hierarchical_retriever = HierarchicalRetriever(base_retriever, llm)\n",
    "print(\"Hierarchical Retriever created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test hierarchical retrieval\n",
    "query = \"quarterly sales performance\"\n",
    "hierarchy = hierarchical_retriever.retrieve_hierarchical(query)\n",
    "\n",
    "print(f\"Hierarchical Results for: '{query}'\")\n",
    "print(f\"\\nHigh Relevance: {len(hierarchy['high_relevance'])} documents\")\n",
    "for doc in hierarchy['high_relevance']:\n",
    "    print(f\"   - {doc.metadata.get('name', 'Unknown')}\")\n",
    "\n",
    "print(f\"\\nMedium Relevance: {len(hierarchy['medium_relevance'])} documents\")\n",
    "for doc in hierarchy['medium_relevance'][:3]:  # Show first 3\n",
    "    print(f\"   - {doc.metadata.get('name', 'Unknown')}\")\n",
    "\n",
    "print(f\"\\nLow Relevance: {len(hierarchy['low_relevance'])} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Self-Querying Retriever\n",
    "\n",
    "Let the AI determine optimal search parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelfQueryingEgnyteRetriever:\n",
    "    \"\"\"Self-querying retriever that optimizes search parameters.\"\"\"\n",
    "    \n",
    "    def __init__(self, domain: str, user_token: str, llm: ChatOpenAI):\n",
    "        self.domain = domain\n",
    "        self.user_token = user_token\n",
    "        self.llm = llm\n",
    "    \n",
    "    def _generate_search_strategy(self, user_query: str) -> Dict[str, Any]:\n",
    "        \"\"\"Generate optimal search strategy based on user query.\"\"\"\n",
    "        \n",
    "        strategy_prompt = f\"\"\"\n",
    "        Analyze this user query and suggest optimal search parameters for an enterprise document search.\n",
    "        \n",
    "        User Query: \"{user_query}\"\n",
    "        \n",
    "        Suggest:\n",
    "        1. Refined search terms (comma-separated)\n",
    "        2. Likely folder paths (if any specific departments/projects mentioned)\n",
    "        3. Document limit (5-20 based on query complexity)\n",
    "        4. Time relevance (recent, any, or specific period)\n",
    "        \n",
    "        Format your response as:\n",
    "        TERMS: [refined terms]\n",
    "        FOLDERS: [folder paths or \"any\"]\n",
    "        LIMIT: [number]\n",
    "        TIME: [recent/any/specific]\n",
    "        \"\"\"\n",
    "        \n",
    "        response = self.llm.invoke(strategy_prompt)\n",
    "        return self._parse_strategy_response(response.content)\n",
    "    \n",
    "    def _parse_strategy_response(self, response: str) -> Dict[str, Any]:\n",
    "        \"\"\"Parse LLM strategy response.\"\"\"\n",
    "        strategy = {\n",
    "            \"terms\": None,\n",
    "            \"folders\": None,\n",
    "            \"limit\": 10,\n",
    "            \"time_filter\": None\n",
    "        }\n",
    "        \n",
    "        lines = response.strip().split('\\n')\n",
    "        for line in lines:\n",
    "            if line.startswith('TERMS:'):\n",
    "                strategy['terms'] = line.replace('TERMS:', '').strip()\n",
    "            elif line.startswith('FOLDERS:'):\n",
    "                folders = line.replace('FOLDERS:', '').strip()\n",
    "                strategy['folders'] = None if folders.lower() == 'any' else folders\n",
    "            elif line.startswith('LIMIT:'):\n",
    "                try:\n",
    "                    strategy['limit'] = int(line.replace('LIMIT:', '').strip())\n",
    "                except:\n",
    "                    strategy['limit'] = 10\n",
    "        \n",
    "        return strategy\n",
    "    \n",
    "    def retrieve(self, user_query: str) -> List[Document]:\n",
    "        \"\"\"Perform self-querying retrieval.\"\"\"\n",
    "        \n",
    "        # Generate search strategy\n",
    "        strategy = self._generate_search_strategy(user_query)\n",
    "        print(f\"AI Strategy: {strategy}\")\n",
    "        \n",
    "        # Create search options based on strategy\n",
    "        search_options = EgnyteSearchOptions(\n",
    "            limit=strategy['limit']\n",
    "        )\n",
    "        \n",
    "        if strategy['folders']:\n",
    "            search_options.folder_path = strategy['folders']\n",
    "        \n",
    "        # Create retriever with optimized options\n",
    "        retriever = EgnyteRetriever(\n",
    "            domain=self.domain,\n",
    "            user_token=self.user_token,\n",
    "            search_options=search_options\n",
    "        )\n",
    "        \n",
    "        # Use refined terms or original query\n",
    "        search_query = strategy['terms'] if strategy['terms'] else user_query\n",
    "        \n",
    "        return retriever.invoke(search_query)\n",
    "\n",
    "# Create self-querying retriever\n",
    "self_query_retriever = SelfQueryingEgnyteRetriever(\n",
    "    domain=EGNYTE_DOMAIN,\n",
    "    user_token=EGNYTE_USER_TOKEN,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "print(\"Self-Querying Retriever created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test self-querying\n",
    "complex_query = \"I need to find information about our marketing campaign performance from the last quarter, specifically focusing on digital channels and ROI metrics\"\n",
    "\n",
    "print(f\"Complex Query: {complex_query}\")\n",
    "print(\"\\nAI is analyzing and optimizing search strategy...\")\n",
    "\n",
    "self_query_docs = self_query_retriever.retrieve(complex_query)\n",
    "\n",
    "print(f\"\\nSelf-Query Results: {len(self_query_docs)} documents\")\n",
    "for i, doc in enumerate(self_query_docs[:3], 1):\n",
    "    print(f\"{i}. {doc.metadata.get('name', 'Unknown')} - {doc.metadata.get('path', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Ensemble Retrieval\n",
    "\n",
    "Combine multiple retrieval strategies for optimal results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create multiple specialized retrievers\n",
    "recent_retriever = EgnyteRetriever(\n",
    "    domain=EGNYTE_DOMAIN,\n",
    "    user_token=EGNYTE_USER_TOKEN,\n",
    "    search_options=EgnyteSearchOptions(\n",
    "        limit=5,\n",
    "        created_after=datetime.now() - timedelta(days=30)\n",
    "    )\n",
    ")\n",
    "\n",
    "comprehensive_retriever = EgnyteRetriever(\n",
    "    domain=EGNYTE_DOMAIN,\n",
    "    user_token=EGNYTE_USER_TOKEN,\n",
    "    search_options=EgnyteSearchOptions(limit=15)\n",
    ")\n",
    "\n",
    "# Create ensemble retriever\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[recent_retriever, comprehensive_retriever, compression_retriever],\n",
    "    weights=[0.3, 0.4, 0.3]  # Weight recent docs, comprehensive search, and compressed content\n",
    ")\n",
    "\n",
    "print(\"Ensemble Retriever created with 3 strategies\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test ensemble retrieval\n",
    "ensemble_query = \"team productivity metrics\"\n",
    "\n",
    "print(f\"Ensemble Query: {ensemble_query}\")\n",
    "print(\"\\nRunning multiple retrieval strategies...\")\n",
    "\n",
    "ensemble_docs = ensemble_retriever.invoke(ensemble_query)\n",
    "\n",
    "print(f\"\\nEnsemble Results: {len(ensemble_docs)} documents\")\n",
    "print(\"\\nTop Results:\")\n",
    "for i, doc in enumerate(ensemble_docs[:5], 1):\n",
    "    print(f\"{i}. {doc.metadata.get('name', 'Unknown')}\")\n",
    "    print(f\"   Path: {doc.metadata.get('path', 'Unknown')}\")\n",
    "    print(f\"   Size: {doc.metadata.get('size', 'Unknown')} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Advanced RAG Chain with All Patterns\n",
    "\n",
    "Combine all patterns into a sophisticated RAG system:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedRAGSystem:\n",
    "    \"\"\"Advanced RAG system combining multiple retrieval patterns.\"\"\"\n",
    "    \n",
    "    def __init__(self, domain: str, user_token: str, llm: ChatOpenAI):\n",
    "        self.domain = domain\n",
    "        self.user_token = user_token\n",
    "        self.llm = llm\n",
    "        \n",
    "        # Initialize all retrievers\n",
    "        self.base_retriever = EgnyteRetriever(domain, user_token)\n",
    "        self.multi_query = MultiQueryRetriever.from_llm(self.base_retriever, llm)\n",
    "        self.hierarchical = HierarchicalRetriever(self.base_retriever, llm)\n",
    "        self.self_query = SelfQueryingEgnyteRetriever(domain, user_token, llm)\n",
    "        \n",
    "        # Create advanced prompt\n",
    "        self.prompt = PromptTemplate(\n",
    "            template=\"\"\"\n",
    "            You are an expert analyst with access to enterprise documents from Egnyte.\n",
    "            \n",
    "            CONTEXT FROM MULTIPLE RETRIEVAL STRATEGIES:\n",
    "            {context}\n",
    "            \n",
    "            QUESTION: {question}\n",
    "            \n",
    "            Provide a comprehensive analysis that:\n",
    "            1. Synthesizes information from all retrieved documents\n",
    "            2. Identifies key insights and patterns\n",
    "            3. Highlights any conflicting information\n",
    "            4. Provides actionable recommendations\n",
    "            5. Cites specific documents for each claim\n",
    "            \n",
    "            ANALYSIS:\n",
    "            \"\"\",\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "    \n",
    "    def analyze(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"Perform advanced RAG analysis.\"\"\"\n",
    "        \n",
    "        print(f\"Analyzing: {question}\")\n",
    "        \n",
    "        # 1. Multi-query retrieval\n",
    "        print(\"\\nMulti-Query Retrieval...\")\n",
    "        multi_docs = self.multi_query.invoke(question)\n",
    "        \n",
    "        # 2. Hierarchical organization\n",
    "        print(\"Hierarchical Organization...\")\n",
    "        hierarchy = self.hierarchical.retrieve_hierarchical(question)\n",
    "        \n",
    "        # 3. Self-querying optimization\n",
    "        print(\"Self-Querying Optimization...\")\n",
    "        self_query_docs = self.self_query.retrieve(question)\n",
    "        \n",
    "        # Combine all documents\n",
    "        all_docs = multi_docs + hierarchy['high_relevance'] + self_query_docs\n",
    "        \n",
    "        # Remove duplicates based on path\n",
    "        unique_docs = []\n",
    "        seen_paths = set()\n",
    "        for doc in all_docs:\n",
    "            path = doc.metadata.get('path', '')\n",
    "            if path not in seen_paths:\n",
    "                unique_docs.append(doc)\n",
    "                seen_paths.add(path)\n",
    "        \n",
    "        print(f\"\\nCombined {len(unique_docs)} unique documents\")\n",
    "        \n",
    "        # Create context\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"Document: {doc.metadata.get('name', 'Unknown')}\\n{doc.page_content}\"\n",
    "            for doc in unique_docs[:10]  # Limit to top 10 for context window\n",
    "        ])\n",
    "        \n",
    "        # Generate analysis\n",
    "        print(\"Generating AI Analysis...\")\n",
    "        analysis_prompt = self.prompt.format(context=context, question=question)\n",
    "        analysis = self.llm.invoke(analysis_prompt)\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"analysis\": analysis.content,\n",
    "            \"source_documents\": unique_docs,\n",
    "            \"document_count\": len(unique_docs),\n",
    "            \"hierarchy\": {\n",
    "                \"high\": len(hierarchy['high_relevance']),\n",
    "                \"medium\": len(hierarchy['medium_relevance']),\n",
    "                \"low\": len(hierarchy['low_relevance'])\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Create advanced RAG system\n",
    "advanced_rag = AdvancedRAGSystem(\n",
    "    domain=EGNYTE_DOMAIN,\n",
    "    user_token=EGNYTE_USER_TOKEN,\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "print(\"Advanced RAG System initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the advanced RAG system\n",
    "complex_business_question = \"What are the key challenges and opportunities in our current market position, and what strategic recommendations can be made based on recent performance data?\"\n",
    "\n",
    "result = advanced_rag.analyze(complex_business_question)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ADVANCED RAG ANALYSIS RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nAnalysis Summary:\")\n",
    "print(f\"   Documents Analyzed: {result['document_count']}\")\n",
    "print(f\"   High Relevance: {result['hierarchy']['high']}\")\n",
    "print(f\"   Medium Relevance: {result['hierarchy']['medium']}\")\n",
    "print(f\"   Low Relevance: {result['hierarchy']['low']}\")\n",
    "\n",
    "print(f\"\\nAI Analysis:\")\n",
    "print(result['analysis'])\n",
    "\n",
    "print(f\"\\nSource Documents ({len(result['source_documents'])})\")\n",
    "for i, doc in enumerate(result['source_documents'][:5], 1):\n",
    "    print(f\"{i}. {doc.metadata.get('name', 'Unknown')} - {doc.metadata.get('path', 'Unknown')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison\n",
    "\n",
    "Compare different RAG approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def compare_rag_approaches(question: str):\n",
    "    \"\"\"Compare different RAG approaches for performance and quality.\"\"\"\n",
    "    \n",
    "    approaches = {\n",
    "        \"Basic RAG\": base_retriever,\n",
    "        \"Multi-Query RAG\": multi_query_retriever,\n",
    "        \"Compressed RAG\": compression_retriever,\n",
    "        \"Ensemble RAG\": ensemble_retriever\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, retriever in approaches.items():\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            docs = retriever.invoke(question)\n",
    "            duration = time.time() - start_time\n",
    "            \n",
    "            results[name] = {\n",
    "                \"documents\": len(docs),\n",
    "                \"duration\": duration,\n",
    "                \"avg_doc_length\": sum(len(doc.page_content) for doc in docs) / len(docs) if docs else 0,\n",
    "                \"success\": True\n",
    "            }\n",
    "        except Exception as e:\n",
    "            results[name] = {\n",
    "                \"error\": str(e),\n",
    "                \"success\": False\n",
    "            }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare approaches\n",
    "comparison_query = \"project status update\"\n",
    "comparison = compare_rag_approaches(comparison_query)\n",
    "\n",
    "print(f\"Performance Comparison for: '{comparison_query}'\")\n",
    "print(\"\\n\" + \"-\"*60)\n",
    "\n",
    "for approach, metrics in comparison.items():\n",
    "    if metrics['success']:\n",
    "        print(f\"{approach:20} | {metrics['documents']:3d} docs | {metrics['duration']:5.2f}s | {metrics['avg_doc_length']:6.0f} chars\")\n",
    "    else:\n",
    "        print(f\"{approach:20} | ERROR: {metrics['error']}\")\n",
    "\n",
    "print(\"-\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Best Practices and Recommendations\n",
    "\n",
    "### When to Use Each Pattern:\n",
    "\n",
    "1. **Multi-Query RAG**: Complex questions requiring comprehensive coverage\n",
    "2. **Contextual Compression**: Large documents with specific information needs\n",
    "3. **Hierarchical RAG**: When document relevance varies significantly\n",
    "4. **Self-Querying**: Complex user queries that need optimization\n",
    "5. **Ensemble RAG**: Critical applications requiring maximum accuracy\n",
    "\n",
    "### Performance Considerations:\n",
    "\n",
    "- **Latency**: Basic RAG < Compressed < Multi-Query < Ensemble\n",
    "- **Accuracy**: Basic RAG < Multi-Query < Compressed < Ensemble\n",
    "- **Cost**: Basic RAG < Compressed < Multi-Query < Ensemble\n",
    "\n",
    "### Production Deployment:\n",
    "\n",
    "1. **Caching**: Implement aggressive caching for repeated queries\n",
    "2. **Async Processing**: Use async operations for better throughput\n",
    "3. **Load Balancing**: Distribute requests across multiple instances\n",
    "4. **Monitoring**: Track performance metrics and error rates\n",
    "\n",
    "## Next Steps\n",
    "\n",
    "Explore more advanced patterns:\n",
    "- **[Enterprise Workflows](03-enterprise-workflows.ipynb)**: Production deployment patterns\n",
    "- **[Multi-Modal Analysis](04-multimodal-analysis.ipynb)**: Working with different document types\n",
    "- **[Security & Compliance](05-security-compliance.ipynb)**: Enterprise security patterns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
