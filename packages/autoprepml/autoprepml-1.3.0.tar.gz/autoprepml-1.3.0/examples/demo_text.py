"""
Demo: Text/NLP Data Preprocessing with AutoPrepML
Example: Cleaning and analyzing customer reviews/tweets
"""

import pandas as pd
from autoprepml.text import TextPrepML

# Sample data: Customer product reviews
reviews_data = {
    'review_id': list(range(1, 16)),
    'review_text': [
        'This product is AMAZING! Best purchase ever!!! http://example.com/review',
        'Terrible quality. Complete waste of money. Do NOT buy! üò°',
        '<html><body>Great product, highly recommended!</body></html>',
        'Contact support@company.com for issues. Not satisfied.',
        'ok',  # Too short
        'The product arrived quickly and works as expected. Very happy with the purchase.',
        'This product is AMAZING! Best purchase ever!!!',  # Duplicate
        'Absolutely love it! 5 stars ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê',
        'Meh... it\'s okay I guess. Nothing special really.',
        'WORST PRODUCT EVER!!! AVOID AT ALL COSTS!!!',
        '   ',  # Empty
        'Good value for money. Shipping was fast. Would recommend to friends and family.',
        'Broke after 2 weeks. Poor quality control. Very disappointed!!!',
        'Perfect! Exactly what I needed. Great customer service too.',
        'Not bad, but could be better. The price is a bit high for what you get.'
    ]
}

df = pd.DataFrame(reviews_data)

print("=" * 80)
print("üìù TEXT/NLP PREPROCESSING DEMO - Customer Reviews")
print("=" * 80)

# Initialize TextPrepML
print("\n1Ô∏è‚É£  Initializing TextPrepML...")
prep = TextPrepML(df, text_column='review_text')
print(f"‚úì Loaded {len(prep.df)} reviews")

# Detect issues
print("\n2Ô∏è‚É£  Detecting text data quality issues...")
issues = prep.detect_issues()
print(f"‚úì Missing text: {issues['missing_text']}")
print(f"‚úì Empty strings: {issues['empty_strings']}")
print(f"‚úì Very short texts (< 5 chars): {issues['very_short']}")
print(f"‚úì Contains URLs: {issues['contains_urls']}")
print(f"‚úì Contains emails: {issues['contains_emails']}")
print(f"‚úì Contains HTML: {issues['contains_html']}")
print(f"‚úì Average length: {issues['avg_length']:.1f} characters")

# Clean text
print("\n3Ô∏è‚É£  Cleaning text data...")
prep.clean_text(
    lowercase=True,
    remove_urls=True,
    remove_emails=True,
    remove_html=True,
    remove_extra_spaces=True
)
print("‚úì Cleaned text: removed URLs, emails, HTML tags, normalized case")

# Show before/after examples
print("\n   Before: ", df['review_text'].iloc[0][:60])
print("   After:  ", prep.df['review_text'].iloc[0][:60])

# Remove stopwords
print("\n4Ô∏è‚É£  Removing stopwords...")
prep.remove_stopwords()
print("‚úì Removed common English stopwords")

# Filter by length
print("\n5Ô∏è‚É£  Filtering by length...")
original_count = len(prep.df)
prep.filter_by_length(min_length=10, max_length=500)
print(f"‚úì Kept {len(prep.df)}/{original_count} reviews (removed too short/long)")

# Remove duplicates
print("\n6Ô∏è‚É£  Removing duplicate reviews...")
original_count = len(prep.df)
prep.remove_duplicates(keep='first')
print(f"‚úì Kept {len(prep.df)}/{original_count} reviews (removed duplicates)")

# Extract features
print("\n7Ô∏è‚É£  Extracting text features...")
prep.extract_features()
print("‚úì Added features:")
feature_cols = [col for col in prep.df.columns if col.startswith('review_text_')]
for col in feature_cols[:6]:
    print(f"   - {col}")

# Tokenize
print("\n8Ô∏è‚É£  Tokenizing text...")
prep.tokenize(method='word')
print("‚úì Created word tokens")

# Sample token output
sample_tokens = prep.df['review_text_tokens'].iloc[0][:5]
print(f"   Sample tokens: {sample_tokens}")

# Get vocabulary
print("\n9Ô∏è‚É£  Analyzing vocabulary...")
vocab = prep.get_vocabulary(top_n=10)
print("‚úì Top 10 most common words:")
for word, count in list(vocab.items())[:10]:
    print(f"   {word}: {count}")

# Detect language
print("\nüîü Detecting language...")
prep.detect_language()
english_count = (prep.df['review_text_language'] == 'english').sum()
print(f"‚úì Detected {english_count}/{len(prep.df)} reviews as English")

# Generate report
print("\nüìä Generating preprocessing report...")
report = prep.report()
print(f"‚úì Original shape: {report['original_shape']}")
print(f"‚úì Current shape:  {report['current_shape']}")
print(f"‚úì Operations performed: {len(report['logs'])}")

# Display sample results
print("\nüìÑ Sample Cleaned Reviews:")
print("-" * 80)
for idx in range(min(3, len(prep.df))):
    row = prep.df.iloc[idx]
    print(f"\n#{row['review_id']}")
    print(f"Text: {row['review_text'][:70]}...")
    print(f"Length: {row['review_text_length']} chars, "
          f"Words: {row['review_text_word_count']}, "
          f"Language: {row['review_text_language']}")

# Save cleaned data
output_file = 'reviews_cleaned.csv'
prep.df.to_csv(output_file, index=False)
print(f"\nüíæ Saved cleaned reviews to: {output_file}")

# Summary statistics
print("\n" + "=" * 80)
print("‚ú® TEXT PREPROCESSING COMPLETE!")
print("=" * 80)
print("üìà Statistics:")
print(f"   ‚Ä¢ Total reviews processed: {len(prep.df)}")
print(f"   ‚Ä¢ Average review length: {prep.df['review_text_length'].mean():.1f} chars")
print(f"   ‚Ä¢ Average words per review: {prep.df['review_text_word_count'].mean():.1f}")
print(f"   ‚Ä¢ Total vocabulary size: {len(vocab)} unique words")

print("\nüí° Use Cases:")
print("   ‚Ä¢ Sentiment analysis")
print("   ‚Ä¢ Topic modeling")
print("   ‚Ä¢ Text classification")
print("   ‚Ä¢ Customer feedback analysis")
print("=" * 80)
