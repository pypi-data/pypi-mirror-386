# -*- coding: utf-8 -*-
# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: proto/api/v0/luminarycloud/inference/inference.proto
"""Generated protocol buffer code."""
from google.protobuf.internal import enum_type_wrapper
from google.protobuf import descriptor as _descriptor
from google.protobuf import descriptor_pool as _descriptor_pool
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()


from google.api import annotations_pb2 as google_dot_api_dot_annotations__pb2
from google.protobuf import empty_pb2 as google_dot_protobuf_dot_empty__pb2
from luminarycloud._proto.inferenceservice import inferenceservice_pb2 as proto_dot_inferenceservice_dot_inferenceservice__pb2


DESCRIPTOR = _descriptor_pool.Default().AddSerializedFile(b'\n4proto/api/v0/luminarycloud/inference/inference.proto\x12-luminary.proto.api.v0.luminarycloud.inference\x1a\x1cgoogle/api/annotations.proto\x1a\x1bgoogle/protobuf/empty.proto\x1a-proto/inferenceservice/inferenceservice.proto\"\xaf\x01\n CreateInferenceServiceJobRequest\x12\x18\n\x10model_version_id\x18\x08 \x01(\t\x12\x0f\n\x07stl_url\x18\x02 \x01(\t\x12\x10\n\x08settings\x18\x07 \x01(\x0c\x12\x12\n\nconditions\x18\x03 \x01(\x0c\x12\x12\n\nproject_id\x18\x05 \x01(\t\x12 \n\x18write_visualization_data\x18\x06 \x01(\x08J\x04\x08\x04\x10\x05\"\x8e\x01\n!CreateInferenceServiceJobResponse\x12\x45\n\x06status\x18\x01 \x01(\x0e\x32\x35.luminary.proto.api.v0.luminarycloud.inference.Status\x12\x15\n\x08response\x18\x02 \x01(\x0cH\x00\x88\x01\x01\x42\x0b\n\t_response*D\n\x06Status\x12\x12\n\x0eSTATUS_PENDING\x10\x00\x12\x12\n\x0eSTATUS_SUCCESS\x10\x01\x12\x12\n\x0eSTATUS_FAILURE\x10\x02\x32\xea\x01\n\x10InferenceService\x12\xd5\x01\n\x19\x43reateInferenceServiceJob\x12O.luminary.proto.api.v0.luminarycloud.inference.CreateInferenceServiceJobRequest\x1aP.luminary.proto.api.v0.luminarycloud.inference.CreateInferenceServiceJobResponse\"\x15\x82\xd3\xe4\x93\x02\x0f\"\r/v0/inferenceB=Z;luminarycloud.com/core/proto/api/v0/luminarycloud/inferenceb\x06proto3')

_STATUS = DESCRIPTOR.enum_types_by_name['Status']
Status = enum_type_wrapper.EnumTypeWrapper(_STATUS)
STATUS_PENDING = 0
STATUS_SUCCESS = 1
STATUS_FAILURE = 2


_CREATEINFERENCESERVICEJOBREQUEST = DESCRIPTOR.message_types_by_name['CreateInferenceServiceJobRequest']
_CREATEINFERENCESERVICEJOBRESPONSE = DESCRIPTOR.message_types_by_name['CreateInferenceServiceJobResponse']
CreateInferenceServiceJobRequest = _reflection.GeneratedProtocolMessageType('CreateInferenceServiceJobRequest', (_message.Message,), {
  'DESCRIPTOR' : _CREATEINFERENCESERVICEJOBREQUEST,
  '__module__' : 'proto.api.v0.luminarycloud.inference.inference_pb2'
  # @@protoc_insertion_point(class_scope:luminary.proto.api.v0.luminarycloud.inference.CreateInferenceServiceJobRequest)
  })
_sym_db.RegisterMessage(CreateInferenceServiceJobRequest)

CreateInferenceServiceJobResponse = _reflection.GeneratedProtocolMessageType('CreateInferenceServiceJobResponse', (_message.Message,), {
  'DESCRIPTOR' : _CREATEINFERENCESERVICEJOBRESPONSE,
  '__module__' : 'proto.api.v0.luminarycloud.inference.inference_pb2'
  # @@protoc_insertion_point(class_scope:luminary.proto.api.v0.luminarycloud.inference.CreateInferenceServiceJobResponse)
  })
_sym_db.RegisterMessage(CreateInferenceServiceJobResponse)

_INFERENCESERVICE = DESCRIPTOR.services_by_name['InferenceService']
if _descriptor._USE_C_DESCRIPTORS == False:

  DESCRIPTOR._options = None
  DESCRIPTOR._serialized_options = b'Z;luminarycloud.com/core/proto/api/v0/luminarycloud/inference'
  _INFERENCESERVICE.methods_by_name['CreateInferenceServiceJob']._options = None
  _INFERENCESERVICE.methods_by_name['CreateInferenceServiceJob']._serialized_options = b'\202\323\344\223\002\017\"\r/v0/inference'
  _STATUS._serialized_start=532
  _STATUS._serialized_end=600
  _CREATEINFERENCESERVICEJOBREQUEST._serialized_start=210
  _CREATEINFERENCESERVICEJOBREQUEST._serialized_end=385
  _CREATEINFERENCESERVICEJOBRESPONSE._serialized_start=388
  _CREATEINFERENCESERVICEJOBRESPONSE._serialized_end=530
  _INFERENCESERVICE._serialized_start=603
  _INFERENCESERVICE._serialized_end=837
# @@protoc_insertion_point(module_scope)
