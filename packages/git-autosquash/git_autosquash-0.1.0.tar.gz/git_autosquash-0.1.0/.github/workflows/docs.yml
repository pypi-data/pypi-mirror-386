name: Deploy Documentation

on:
  push:
    branches: [main]
    paths: 
      - 'docs/**'
      - 'mkdocs.yml'
      - 'src/**/*.py'
      - '.github/workflows/docs.yml'
  pull_request:
    paths:
      - 'docs/**' 
      - 'mkdocs.yml'
      - 'src/**/*.py'
      - '.github/workflows/docs.yml'

permissions:
  contents: read
  pages: write
  id-token: write

# Allow only one concurrent deployment, skipping runs queued between the run in-progress and latest queued.
concurrency:
  group: "pages"
  cancel-in-progress: false

jobs:
  # Build documentation
  build:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
        with:
          # Fetch full history for accurate git blame analysis in docs
          fetch-depth: 0

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          enable-cache: true

      - name: Install documentation dependencies
        run: |
          uv pip install --system \
            mkdocs-material==9.5.* \
            mkdocstrings[python]==0.26.* \
            mkdocs-mermaid2-plugin==1.1.* \
            pymdown-extensions==10.11.*

      - name: Install project dependencies for API docs
        run: |
          uv pip install --system -e .

      - name: Setup Pages
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        uses: actions/configure-pages@v5

      - name: Build documentation
        run: mkdocs build --strict

      - name: Upload documentation artifact
        if: github.event_name == 'push' && github.ref == 'refs/heads/main'
        uses: actions/upload-pages-artifact@v3
        with:
          path: site/

  # Deploy to GitHub Pages (only on main branch pushes)
  deploy:
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    runs-on: ubuntu-latest
    needs: build
    steps:
      - name: Deploy to GitHub Pages
        id: deployment
        uses: actions/deploy-pages@v4

  # Link checker for pull requests and main branch
  link-check:
    runs-on: ubuntu-latest
    needs: build
    if: always() && needs.build.result == 'success'
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install link checker
        run: |
          pip install requests beautifulsoup4 urllib3

      - name: Download built site
        uses: actions/download-artifact@v4
        with:
          name: github-pages
          path: site-archive/

      - name: Extract site archive
        run: |
          cd site-archive
          ls -la
          # GitHub Pages artifact is a tar file, but may be compressed differently
          if [ -f artifact.tar ]; then
            # Try different extraction methods
            tar -tf artifact.tar | head -5
            tar -xf artifact.tar
            ls -la
            # Move the extracted contents
            if [ -d "site" ]; then
              mv site ../site
            else
              # If no site directory, assume all files are the site content
              mkdir -p ../site
              mv * ../site/ 2>/dev/null || true
            fi
          else
            # Files are already extracted by download-artifact
            mkdir -p ../site
            mv * ../site/ 2>/dev/null || true
          fi

      - name: Check internal links
        run: |
          python - << 'EOF'
          import os
          import re
          from pathlib import Path
          from urllib.parse import urljoin, urlparse
          import requests
          from bs4 import BeautifulSoup
          
          def check_internal_links(site_dir):
              """Check all internal links in the built documentation."""
              site_path = Path(site_dir)
              broken_links = []
              
              for html_file in site_path.rglob("*.html"):
                  with open(html_file, 'r', encoding='utf-8') as f:
                      soup = BeautifulSoup(f.read(), 'html.parser')
                  
                  # Check all links
                  for link in soup.find_all('a', href=True):
                      href = link['href']
                      
                      # Skip external links, fragments, and JavaScript links
                      if href.startswith(('http', 'mailto:', '#', 'javascript:')):
                          continue
                      
                      # Skip empty or invalid links
                      if not href or href in ['', '.', './', '/git-autosquash/.']:
                          continue
                      
                      # Resolve relative paths
                      if href.startswith('/'):
                          # Remove site base path if present
                          cleaned_href = href.lstrip('/')
                          if cleaned_href.startswith('git-autosquash/'):
                              cleaned_href = cleaned_href[len('git-autosquash/'):]
                          # Skip if cleaned path is empty or just a dot
                          if not cleaned_href or cleaned_href in ['.', './']:
                              continue
                          target_path = site_path / cleaned_href
                      else:
                          target_path = html_file.parent / href
                      
                      # Remove fragment identifier
                      if '#' in str(target_path):
                          target_path = Path(str(target_path).split('#')[0])
                      
                      # Check if target exists
                      if not target_path.exists():
                          # Try with .html extension for directory links
                          if target_path.is_dir():
                              target_path = target_path / 'index.html'
                          elif not target_path.suffix:
                              target_path = target_path.with_suffix('.html')
                      
                      if not target_path.exists():
                          broken_links.append(f"{html_file.relative_to(site_path)}: {href}")
              
              return broken_links
          
          broken = check_internal_links('site')
          if broken:
              print("Broken internal links found:")
              for link in broken:
                  print(f"  - {link}")
              exit(1)
          else:
              print("All internal links are valid!")
          EOF

  # Spell check for documentation content
  spell-check:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Install aspell
        run: |
          sudo apt-get update
          sudo apt-get install -y aspell aspell-en

      - name: Create aspell dictionary
        run: |
          cat > .aspell.en.pws << 'EOF'
          personal_ws-1.1 en 50
          autosquash
          mkdocs
          textual
          TUI
          CLI
          API
          hunk
          hunks
          squash
          squashing
          rebase
          rebasing
          diff
          diffs
          pipx
          uv
          pymdownx
          mermaid
          docstrings
          GitOps
          BlameAnalyzer
          HunkParser
          RebaseManager
          DiffHunk
          HunkMappingWidget
          DiffViewer
          ProgressIndicator
          AutoSquashApp
          ApprovalScreen
          github
          workflow
          workflows
          argcomplete
          setuptools
          pyproject
          TOML
          EOF

      - name: Check spelling
        run: |
          find docs -name "*.md" -exec aspell --home-dir=. --personal=.aspell.en.pws --lang=en_US --mode=markdown --check {} \;
        continue-on-error: true