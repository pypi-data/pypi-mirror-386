{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bb7f461",
   "metadata": {},
   "source": [
    "# Basic usage of the ``h5pywrappers`` library #"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6d9cd7",
   "metadata": {},
   "source": [
    "## A NOTE BEFORE STARTING ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880421e9",
   "metadata": {},
   "source": [
    "Since the ``h5pywrappers`` git repository tracks this notebook under its\n",
    "original basename ``basic_usage.ipynb``, we recommend that you copy the original\n",
    "notebook and rename it to any other basename that is not one of the original\n",
    "basenames that appear in the ``<root>/examples`` directory before executing any\n",
    "of the notebook cells below, where ``<root>`` is the root of the\n",
    "``h5pywrappers`` repository. This way you can explore the notebook by executing\n",
    "and modifying cells without changing the original notebook, which is being\n",
    "tracked by git."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a264a2e0-607c-443f-9968-4899fe545825",
   "metadata": {},
   "source": [
    "## Table of contents ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad66d2a-582e-4146-b38b-161d4902df01",
   "metadata": {},
   "source": [
    "- [Import necessary modules](#Import-necessary-modules)\n",
    "- [Introduction](#Introduction)\n",
    "- [Using the ``h5pywrappers`` function](#Using-the-h5pywrappers-function)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b0d90e",
   "metadata": {},
   "source": [
    "## Import necessary modules ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e7f9787f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For general array handling and constructing random number generators.\n",
    "import numpy as np\n",
    "\n",
    "# For converting objects.\n",
    "import czekitout.convert\n",
    "\n",
    "\n",
    "\n",
    "# The library that is the subject of this demonstration.\n",
    "import h5pywrappers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e667d633",
   "metadata": {},
   "source": [
    "## Introduction ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec7f2509",
   "metadata": {},
   "source": [
    "In this notebook, we demonstrate how one can use each function and class in the\n",
    "``h5pywrappers`` library.\n",
    "\n",
    "In order to execute the cells in this notebook as intended, a set of Python\n",
    "libraries need to be installed in the Python environment within which the cells\n",
    "of the notebook are to be executed. For this particular notebook, users need to\n",
    "install:\n",
    "\n",
    "    h5pywrappers\n",
    "    jupyter\n",
    "\n",
    "Users can install these libraries either via `pip`:\n",
    "\n",
    "    pip install h5pywrappers[examples]\n",
    "\n",
    "or `conda`:\n",
    "\n",
    "    conda install -y h5pywrappers jupyter -c conda-forge\n",
    "\n",
    "You can find the documentation for the ``h5pywrappers`` library\n",
    "[here](https://mrfitzpa.github.io/h5pywrappers/_autosummary/h5pywrappers.html).\n",
    "It is recommended that you consult the documentation of this library as you\n",
    "explore the notebook. Moreover, users should execute the cells in the order that\n",
    "they appear, i.e. from top to bottom, as some cells reference variables that are\n",
    "set in other cells above them. **Users should make sure to navigate the\n",
    "documentation for the version of ``h5pywrappers`` that they are currently\n",
    "using.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0993ba",
   "metadata": {},
   "source": [
    "## Using the ``h5pywrappers`` function ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21a2529c",
   "metadata": {},
   "source": [
    "Let's create an HDF5 file and populate it with HDF5 data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2306116d-f0cb-44b5-bead-37a1ef6efbf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"filename\": \"file_1.h5\", \"path_in_file\": \"/group_A/dataset_A\"}\n",
    "obj_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "float_array = np.random.rand(2, 5, 4, 6)\n",
    "\n",
    "kwargs = {\"dataset\": float_array, \"dataset_id\": obj_id, \"write_mode\": \"w\"}\n",
    "h5pywrappers.dataset.save(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0833aa4c-a421-4e4d-8d23-bfd2c7a6aaa6",
   "metadata": {},
   "source": [
    "The above notebook cell will create a new file at the file path \n",
    "``\"./file_1.h5\"``, with any file at the same file path being replaced by the new\n",
    "file. In the new file, an HDF5 dataset is located at the HDF5 path \n",
    "``\"/group_A/dataset_A\"``.\n",
    "\n",
    "If we try to re-save the HDF5 dataset when ``write_mode == \"w-\"``, an exception\n",
    "will be raised. The following notebook cell should raise an exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f53104e-cbd8-40ea-8efa-763584273680",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save the dataset to the HDF5 path ``'/group_A/dataset_A'`` of the HDF5 file at the file path ``'file_1.h5'`` because an HDF5 file already exists at said file path and the parameter ``write_mode`` was set to ``'w-'``, which prohibits modifying any such pre-existing file.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m kwargs = {\u001b[33m\"\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m\"\u001b[39m: float_array, \u001b[33m\"\u001b[39m\u001b[33mdataset_id\u001b[39m\u001b[33m\"\u001b[39m: obj_id, \u001b[33m\"\u001b[39m\u001b[33mwrite_mode\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mw-\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mh5pywrappers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/mfitzpat/miniconda3/envs/h5pywrappers/lib/python3.12/site-packages/h5pywrappers/dataset.py:262\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(dataset, dataset_id, write_mode)\u001b[39m\n\u001b[32m    259\u001b[39m     params[param_name] = func_alias(params)\n\u001b[32m    261\u001b[39m kwargs = params\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m obj = \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/mfitzpat/miniconda3/envs/h5pywrappers/lib/python3.12/site-packages/h5pywrappers/dataset.py:269\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(dataset, dataset_id, write_mode)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_save\u001b[39m(dataset, dataset_id, write_mode):\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     \u001b[43m_pre_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m     dataset_id_core_attrs = dataset_id.get_core_attrs(deep_copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    272\u001b[39m     filename = dataset_id_core_attrs[\u001b[33m\"\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/mfitzpat/miniconda3/envs/h5pywrappers/lib/python3.12/site-packages/h5pywrappers/dataset.py:301\u001b[39m, in \u001b[36m_pre_save\u001b[39m\u001b[34m(dataset, dataset_id, write_mode)\u001b[39m\n\u001b[32m    299\u001b[39m         unformatted_err_msg = \u001b[38;5;28mglobals\u001b[39m()[current_func_name+\u001b[33m\"\u001b[39m\u001b[33m_err_msg_1\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    300\u001b[39m         err_msg = unformatted_err_msg.format(path_in_file, filename)\n\u001b[32m--> \u001b[39m\u001b[32m301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(err_msg)\n\u001b[32m    302\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m h5py.File(filename, \u001b[33m\"\u001b[39m\u001b[33mw\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file_obj:\n\u001b[32m    303\u001b[39m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: Cannot save the dataset to the HDF5 path ``'/group_A/dataset_A'`` of the HDF5 file at the file path ``'file_1.h5'`` because an HDF5 file already exists at said file path and the parameter ``write_mode`` was set to ``'w-'``, which prohibits modifying any such pre-existing file."
     ]
    }
   ],
   "source": [
    "kwargs = {\"dataset\": float_array, \"dataset_id\": obj_id, \"write_mode\": \"w-\"}\n",
    "h5pywrappers.dataset.save(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aa3fff-286f-479f-8019-0aefbb832dc3",
   "metadata": {},
   "source": [
    "Let's now load the HDF5 dataset that we just successfully saved, but in \n",
    "read-only mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "90d9de28-03b9-46f2-8667-cba8b05f9080",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"dataset_A\": shape (2, 5, 4, 6), type \"<f8\">"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = {\"dataset_id\": obj_id, \"read_only\": True}\n",
    "dataset = h5pywrappers.dataset.load(**kwargs)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52eb9655-b787-4af4-9903-e6593e27fea5",
   "metadata": {},
   "source": [
    "Let's check that the data is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed2ff4da-e777-42da-a62f-30136000efcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bool(np.all(dataset[()] == float_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d678b89f-1983-49b5-9eac-421dc6301399",
   "metadata": {},
   "source": [
    "Since we are in read-only mode, we cannot modify the HDF5 dataset that we \n",
    "loaded. The following notebook cell should raise an exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5659b50d-b861-42aa-a81e-b6dd6b20c12e",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Can't synchronously write data (no write intent on file)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m = \u001b[32m0\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:54\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:55\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/mfitzpat/miniconda3/envs/h5pywrappers/lib/python3.12/site-packages/h5py/_hl/dataset.py:1081\u001b[39m, in \u001b[36mDataset.__setitem__\u001b[39m\u001b[34m(self, args, val)\u001b[39m\n\u001b[32m   1079\u001b[39m mspace = h5s.create_simple(selection.expand_shape(mshape))\n\u001b[32m   1080\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fspace \u001b[38;5;129;01min\u001b[39;00m selection.broadcast(mshape):\n\u001b[32m-> \u001b[39m\u001b[32m1081\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mid\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfspace\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdxpl\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dxpl\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:54\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:55\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/h5d.pyx:305\u001b[39m, in \u001b[36mh5py.h5d.DatasetID.write\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_proxy.pyx:119\u001b[39m, in \u001b[36mh5py._proxy.dset_rw\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mOSError\u001b[39m: Can't synchronously write data (no write intent on file)"
     ]
    }
   ],
   "source": [
    "dataset[0, 0, 0, 0] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e794398-0f8f-4355-bfe8-0ad7b6db08b8",
   "metadata": {},
   "source": [
    "Let's create a new HDF5 file and populate it with the same HDF5 dataset of the\n",
    "previous HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a1d16295-4b2f-4f96-bb5b-f477ac617a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"filename\": \"file_2.h5\", \"path_in_file\": \"/group_A/dataset_A\"}\n",
    "obj_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "kwargs = {\"dataset\": dataset, \"dataset_id\": obj_id, \"write_mode\": \"w\"}\n",
    "h5pywrappers.dataset.save(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01ace5c7-fb37-46bc-8457-88e59e56d589",
   "metadata": {},
   "source": [
    "Let's now close the first HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "607adc87-4d6a-4125-9be8-2674aba81de3",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065f9dc2-f834-4e6e-8468-a486d1fdf2a9",
   "metadata": {},
   "source": [
    "Let's load the HDF5 dataset in the second HDF5 file, modify it, and then close \n",
    "the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baa6fc03-f8f0-4ce7-ad09-9fc77d5d06e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"dataset_id\": obj_id, \"read_only\": False}\n",
    "dataset = h5pywrappers.dataset.load(**kwargs)\n",
    "dataset[0, 0, 0, 0] += 1\n",
    "dataset.file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c206641-36f9-4244-bee7-b64ec40c69f7",
   "metadata": {},
   "source": [
    "We can also store strings as HDF5 datasets. Let's append such an HDF5 dataset to\n",
    "the first HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1fed17e-88eb-4e1f-869a-efb2ca825fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"filename\": \"file_1.h5\", \n",
    "          \"path_in_file\": \"/group_B/dataset_B\"}\n",
    "obj_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "string = \"hello world\"\n",
    "\n",
    "kwargs = {\"dataset\": string, \"dataset_id\": obj_id, \"write_mode\": \"a\"}\n",
    "h5pywrappers.dataset.save(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6adcc54-c92e-447c-96e2-86c388fc807c",
   "metadata": {},
   "source": [
    "Let's load the new HDF5 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "63d3fc28-97c0-4c88-a59c-6967c28c95e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"dataset_B\": shape (), type \"|O\">"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = {\"dataset_id\": obj_id, \"read_only\": True}\n",
    "dataset = h5pywrappers.dataset.load(**kwargs)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71234557-1ffe-4847-95ea-fbf2a589a1ce",
   "metadata": {},
   "source": [
    "Let's check that the contents of the HDF5 file are correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "db5853cb-ee4a-4ed5-adc4-b8caf1637208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hello world'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = {\"obj\": dataset[()], \"obj_name\": \"dataset\"}\n",
    "czekitout.convert.to_str_from_str_like(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e2d86d-6652-47f9-8ee4-196f92fefde2",
   "metadata": {},
   "source": [
    "Let's now close the HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1af9bb10-a491-4d21-bac7-f3b3ca044466",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc59544f-69e4-4b00-8c0d-2969b23c6ba8",
   "metadata": {},
   "source": [
    "The last optional write mode for HDF5 datasets is toggled by setting\n",
    "``write_mode`` to ``\"a-\"``. In this mode, the HDF5 dataset of interest is saved\n",
    "unless an HDF5 object already exists at the target HDF5 path of the target HDF5\n",
    "file, in which case an error is raised. The following notebook cell should raise\n",
    "an exception:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "77a603a2-a269-408f-9e5d-64eef963cd33",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save the dataset to the HDF5 path ``'/group_B/dataset_B'`` of the HDF5 file at the file path ``'file_1.h5'`` because an HDF5 object already exists there and the parameter ``write_mode`` was set to ``'a-'``, which prohibits replacing any such pre-existing HDF5 object.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m kwargs = {\u001b[33m\"\u001b[39m\u001b[33mdataset\u001b[39m\u001b[33m\"\u001b[39m: string, \u001b[33m\"\u001b[39m\u001b[33mdataset_id\u001b[39m\u001b[33m\"\u001b[39m: obj_id, \u001b[33m\"\u001b[39m\u001b[33mwrite_mode\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33ma-\u001b[39m\u001b[33m\"\u001b[39m}\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mh5pywrappers\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/mfitzpat/miniconda3/envs/h5pywrappers/lib/python3.12/site-packages/h5pywrappers/dataset.py:262\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(dataset, dataset_id, write_mode)\u001b[39m\n\u001b[32m    259\u001b[39m     params[param_name] = func_alias(params)\n\u001b[32m    261\u001b[39m kwargs = params\n\u001b[32m--> \u001b[39m\u001b[32m262\u001b[39m obj = \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/mfitzpat/miniconda3/envs/h5pywrappers/lib/python3.12/site-packages/h5pywrappers/dataset.py:269\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(dataset, dataset_id, write_mode)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_save\u001b[39m(dataset, dataset_id, write_mode):\n\u001b[32m--> \u001b[39m\u001b[32m269\u001b[39m     \u001b[43m_pre_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwrite_mode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    271\u001b[39m     dataset_id_core_attrs = dataset_id.get_core_attrs(deep_copy=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    272\u001b[39m     filename = dataset_id_core_attrs[\u001b[33m\"\u001b[39m\u001b[33mfilename\u001b[39m\u001b[33m\"\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/mnt/mfitzpat/miniconda3/envs/h5pywrappers/lib/python3.12/site-packages/h5pywrappers/dataset.py:310\u001b[39m, in \u001b[36m_pre_save\u001b[39m\u001b[34m(dataset, dataset_id, write_mode)\u001b[39m\n\u001b[32m    308\u001b[39m             unformatted_err_msg = \u001b[38;5;28mglobals\u001b[39m()[current_func_name+\u001b[33m\"\u001b[39m\u001b[33m_err_msg_2\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    309\u001b[39m             err_msg = unformatted_err_msg.format(path_in_file, filename)\n\u001b[32m--> \u001b[39m\u001b[32m310\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m(err_msg)\n\u001b[32m    311\u001b[39m         \u001b[38;5;28;01mdel\u001b[39;00m file_obj[path_in_file]\n\u001b[32m    313\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mOSError\u001b[39m: Cannot save the dataset to the HDF5 path ``'/group_B/dataset_B'`` of the HDF5 file at the file path ``'file_1.h5'`` because an HDF5 object already exists there and the parameter ``write_mode`` was set to ``'a-'``, which prohibits replacing any such pre-existing HDF5 object."
     ]
    }
   ],
   "source": [
    "kwargs = {\"dataset\": string, \"dataset_id\": obj_id, \"write_mode\": \"a-\"}\n",
    "h5pywrappers.dataset.save(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0667b254-9119-4a35-889d-108513941d38",
   "metadata": {},
   "source": [
    "Let's add an HDF5 attribute to one of the HDF5 groups in the first HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a328c92f-df55-474a-aedf-d07fa24d0a00",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"filename\": \"file_1.h5\", \"path_in_file\": \"/group_A\"}\n",
    "obj_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "attr = \"foobar\"\n",
    "attr_name = \"alternate_group_name\"\n",
    "\n",
    "kwargs = {\"obj_id\": obj_id, \"attr_name\": attr_name}\n",
    "attr_id = h5pywrappers.attr.ID(**kwargs)\n",
    "\n",
    "kwargs = {\"attr\": \"foobar\", \"attr_id\": attr_id, \"write_mode\": \"a\"}\n",
    "h5pywrappers.attr.save(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9748075-107a-4b69-a067-87b36c9366c5",
   "metadata": {},
   "source": [
    "Note that users can also save HDF5 attributes in the write mode ``\"a-\"``.\n",
    "\n",
    "Let's load the HDF5 attribute and check that it is set to the correct value.\n",
    "Note that there is no need to close the HDF5 file as it is done automatically in\n",
    "this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "326919dd-333e-4c7c-97bf-da8aeacacd27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'foobar'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = {\"attr_id\": attr_id}\n",
    "h5pywrappers.attr.load(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb16ca24-a31a-4dfc-9df6-094ce7527b44",
   "metadata": {},
   "source": [
    "Let's create a new empty HDF5 group in the first HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "862657da-78dc-4da1-aa4a-e25060a7c746",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"filename\": \"file_1.h5\", \"path_in_file\": \"/group_C\"}\n",
    "obj_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "kwargs = {\"group\": None, \"group_id\": obj_id, \"write_mode\": \"a\"}\n",
    "h5pywrappers.group.save(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56db4ce2-9bc7-446d-ac70-064c4ea5dad6",
   "metadata": {},
   "source": [
    "Note that the same writing modes are available for HDF5 groups as those of HDF5\n",
    "datasets.\n",
    "\n",
    "Let's load the newly created HDF5 group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "07621eb3-b074-4057-b9a4-0ca2a50a8efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 group \"/group_C\" (0 members)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = {\"group_id\": obj_id, \"read_only\": True}\n",
    "group = h5pywrappers.group.load(**kwargs)\n",
    "group"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "369b610d-32cb-4367-8d34-9dcf3837ebe3",
   "metadata": {},
   "source": [
    "Let's save the newly created HDF5 group to the second HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a8722dd8-c332-4375-8d61-9fcbb9516ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"filename\": \"file_2.h5\", \"path_in_file\": \"/group_B\"}\n",
    "obj_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "kwargs = {\"group\": group, \"group_id\": obj_id, \"write_mode\": \"a-\"}\n",
    "h5pywrappers.group.save(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ed7ce3-1a1d-4ca3-a7ae-93855b1bae68",
   "metadata": {},
   "source": [
    "Let's close the first HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "49903f66-ecbd-4650-9eb9-c95ef403d4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "group.file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50df6ba5-b242-42ac-bdf0-b067e85b3ce1",
   "metadata": {},
   "source": [
    "Let's save an HDF5 \"scalar\" to the first HDF5 file. By \"scalar\", we mean a\n",
    "single numerical data element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "117dbc55-3d86-4c62-a0c2-da9002040cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"filename\": \"file_1.h5\", \"path_in_file\": \"/group_C/scalar_C\"}\n",
    "obj_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "scalar = 2.3+0.5j\n",
    "\n",
    "kwargs = {\"scalar\": scalar, \"scalar_id\": obj_id, \"write_mode\": \"a-\"}\n",
    "h5pywrappers.scalar.save(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da8eb96-9383-490b-95db-3239e09ab813",
   "metadata": {},
   "source": [
    "Note that the same writing modes are available for HDF5 scalars as those of HDF5\n",
    "datasets.\n",
    "\n",
    "Let's load the HDF5 scalar and check that the correct value was stored. Note \n",
    "that there is no need to close the HDF5 file as it is done automatically in this\n",
    "case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d447e4b-c489-4818-a5e6-36cfe6df6418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.3+0.5j)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = {\"scalar_id\": obj_id}\n",
    "h5pywrappers.scalar.load(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04ed23c8-e9fb-4007-aa7f-931b752f179d",
   "metadata": {},
   "source": [
    "Let's save an HDF5 \"datasubset\" to the first HDF5 file. By \"datasubset\", we mean\n",
    "an array obtained by taking a multidimensional slice of an HDF5 dataset in an\n",
    "HDF5 file. In other words, let's modify a multidimensional slice of one of the\n",
    "previously saved HDF5 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "44ccfbda-cf70-421d-bae1-ae3a43d6ea83",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"filename\": \"file_1.h5\", \"path_in_file\": \"/group_A/dataset_A\"}\n",
    "obj_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "multi_dim_slice = (1, slice(1, None, 1), [3, 0], slice(None))\n",
    "\n",
    "kwargs = {\"dataset_id\": obj_id, \"multi_dim_slice\": multi_dim_slice}\n",
    "datasubset_id = h5pywrappers.datasubset.ID(**kwargs)\n",
    "\n",
    "float_array = np.random.rand(2, 4, 6)\n",
    "\n",
    "kwargs = {\"datasubset\": float_array, \"datasubset_id\": datasubset_id}\n",
    "h5pywrappers.datasubset.save(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955e46cc-29ae-4129-82d9-d78f9e9bb5e4",
   "metadata": {},
   "source": [
    "Let's load the HDF5 datasubset and check that its contents are correct. Note \n",
    "that there is no need to close the HDF5 file as it is done automatically in this\n",
    "case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "556084d7-2a30-4b53-8992-5fea09ff195b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = {\"datasubset_id\": datasubset_id}\n",
    "bool(np.all(h5pywrappers.datasubset.load(**kwargs) == float_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4ffb8a-a090-4e9d-af8d-0512e8b8eafe",
   "metadata": {},
   "source": [
    "Note that users can use alternatively ``h5pywrappers.obj.load`` to load any HDF5\n",
    "group or dataset. Let's try loading an HDF5 dataset from the first file using \n",
    "this alternative loading function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a59d1094-1978-4319-bbc8-8eeca2aba84e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<HDF5 dataset \"dataset_A\": shape (2, 5, 4, 6), type \"<f8\">"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = {\"obj_id\": obj_id, \"read_only\": True}\n",
    "dataset = h5pywrappers.obj.load(**kwargs)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18107f36-17e0-4662-9583-e4ea0e344f62",
   "metadata": {},
   "source": [
    "Let's close the first HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c89dd728-7dbb-415f-bea0-b8374911a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11778db3-e2d9-4e41-8948-8070cbf3def3",
   "metadata": {},
   "source": [
    "One of the features of ``h5pywrappers`` is that the classes \n",
    "``h5pywrappers.obj.ID``, ``h5pywrappers.attr.ID``, and\n",
    "``h5pywrappers.datasubset.ID`` are subclasses of\n",
    "``fancytypes.PreSerializableAndUpdatable``, which is a special class that is\n",
    "both updatable, and \"pre-serializable\". See [here](https://mrfitzpa.github.io/fancytypes/_autosummary/fancytypes.PreSerializableAndUpdatable.html), for a\n",
    "description of the class ``fancytypes.PreSerializableAndUpdatable``, and a\n",
    "definition of \"pre-serialization\" (as well as de-pre-serialization).\n",
    "\n",
    "Let's pre-serialize ``datasubset_id`` as a demonstration of pre-serialization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "28447bbf-5918-43c5-9fb3-03ab7dea6650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_id': {'filename': 'file_1.h5', 'path_in_file': '/group_A/dataset_A'},\n",
       " 'multi_dim_slice': (1,\n",
       "  {'start': 1, 'stop': None, 'step': 1},\n",
       "  [3, 0],\n",
       "  {'start': None, 'stop': None, 'step': None})}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json_document = datasubset_id.pre_serialize()\n",
    "json_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fd5728-9b37-4906-b747-7661363bf310",
   "metadata": {},
   "source": [
    "Let's save the above JSON document ``json_document`` to the first HDF5 file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "76329fb2-15e4-42ea-9c18-4a59d5b6e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\"filename\": \"file_1.h5\", \"path_in_file\": \"/group_D/json_document_D\"}\n",
    "obj_id = h5pywrappers.obj.ID(**kwargs)\n",
    "\n",
    "kwargs = {\"json_document\": json_document, \n",
    "          \"json_document_id\": obj_id, \n",
    "          \"write_mode\": \"a-\"}\n",
    "h5pywrappers.json.document.save(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2d2ee6-72dd-4471-8cda-ddd5bcc59a78",
   "metadata": {},
   "source": [
    "Note that the same writing modes are available for JSON document as those of \n",
    "HDF5 datasets.\n",
    "\n",
    "Let's load the JSON document and check that its contents are correct. Note that\n",
    "there is no need to close the HDF5 file as it is done automatically in this \n",
    "case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "97cc23f2-3dac-4299-af82-0e6cfd150fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dataset_id': {'filename': 'file_1.h5', 'path_in_file': '/group_A/dataset_A'},\n",
       " 'multi_dim_slice': [1,\n",
       "  {'start': 1, 'stop': None, 'step': 1},\n",
       "  [3, 0],\n",
       "  {'start': None, 'stop': None, 'step': None}]}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kwargs = {\"json_document_id\": obj_id}\n",
    "h5pywrappers.json.document.load(**kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
