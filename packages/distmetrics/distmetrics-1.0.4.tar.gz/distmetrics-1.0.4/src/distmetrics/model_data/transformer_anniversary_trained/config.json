{
  "patch_size": 8,
  "input_size": 16,
  "data_dim": 128,
  "d_model": 256,
  "nhead": 4,
  "num_encoder_layers": 4,
  "dim_feedforward": 768,
  "max_seq_len": 20,
  "dropout": 0.2,
  "activation": "relu"
}
