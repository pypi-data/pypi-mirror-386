{
  "patch_size": 8,
  "input_size": 16,
  "data_dim": 128,
  "d_model": 128,
  "nhead": 16,
  "num_encoder_layers": 2,
  "dim_feedforward": 384,
  "max_seq_len": 10,
  "dropout": 0.2,
  "activation": "relu"
} 