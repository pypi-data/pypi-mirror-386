name: Integration Tests

on:
  push:
    branches: [main]
    paths:
      - 'src/**'
      - 'tests/**'
      - '.github/workflows/**'
      - 'pyproject.toml'
  workflow_dispatch:
    inputs:
      filter:
        description: the arg you want to pass to pytest filter (-k)
        type: string

jobs:
  expected-providers:
    runs-on: ubuntu-latest
    outputs:
      providers: ${{ steps.set_providers.outputs.expected_providers }}
      local_providers: ${{ steps.set_local_providers.outputs.expected_providers }}
    steps:
      - name: Set expected providers for integration tests
        id: set_providers
        run: echo "expected_providers=anthropic,bedrock,cerebras,cohere,databricks,deepseek,fireworks,gemini,groq,huggingface,inception,llama,mistral,moonshot,nebius,openai,openrouter,perplexity,portkey,together,sambanova,voyage,xai" >> $GITHUB_OUTPUT

      - name: Set expected providers for local integration tests
        id: set_local_providers
        run: echo "expected_providers=ollama,llamacpp,llamafile" >> $GITHUB_OUTPUT

  run-integration-tests:
    needs: expected-providers
    if: github.event.inputs.filter == '' || contains(needs.expected-providers.outputs.providers, github.event.inputs.filter)
    timeout-minutes: 30
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v5

      - uses: astral-sh/setup-uv@v7
        with:
          python-version: 3.13
          activate-environment: true

      - name: Install dependencies
        run: |
          uv sync --group tests --extra all

      - if: github.event.inputs.filter == '' || contains(github.event.inputs.filter, 'huggingface')
        env:
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
        run: python scripts/wake_up_hf_endpoint.py --retry=5

      - name: Run Integration tests (parallel with xdist)
        env:
          # When you add a new key, also add the provider name to the EXPECTED_PROVIDERS environment variable
          ANTHROPIC_API_KEY: ${{ secrets.ANTHROPIC_API_KEY }}
          AWS_BEARER_TOKEN_BEDROCK: ${{ secrets.AWS_BEARER_TOKEN_BEDROCK }}
          CEREBRAS_API_KEY: ${{ secrets.CEREBRAS_API_KEY }}
          COHERE_API_KEY: ${{ secrets.COHERE_API_KEY }}
          DATABRICKS_TOKEN: ${{ secrets.DATABRICKS_TOKEN }}
          DEEPSEEK_API_KEY: ${{ secrets.DEEPSEEK_API_KEY }}
          FIREWORKS_API_KEY: ${{ secrets.FIREWORKS_API_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          GROQ_API_KEY: ${{ secrets.GROQ_API_KEY }}
          HF_TOKEN: ${{ secrets.HF_TOKEN }}
          INCEPTION_API_KEY: ${{ secrets.INCEPTION_API_KEY }}
          LLAMA_API_KEY: ${{ secrets.LLAMA_API_KEY }}
          MISTRAL_API_KEY: ${{ secrets.MISTRAL_API_KEY }}
          MOONSHOT_API_KEY: ${{ secrets.MOONSHOT_API_KEY }}
          NEBIUS_API_KEY: ${{ secrets.NEBIUS_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          OPENROUTER_API_KEY: ${{ secrets.OPENROUTER_API_KEY }}
          PERPLEXITY_API_KEY: ${{ secrets.PERPLEXITY_API_KEY }}
          PORTKEY_API_KEY: ${{ secrets.PORTKEY_API_KEY }}
          SAMBANOVA_API_KEY: ${{ secrets.SAMBANOVA_API_KEY }}
          TOGETHER_API_KEY: ${{ secrets.TOGETHER_API_KEY }}
          VOYAGE_API_KEY: ${{ secrets.VOYAGE_API_KEY }}
          XAI_API_KEY: ${{ secrets.XAI_API_KEY }}
          EXPECTED_PROVIDERS: ${{ needs.expected-providers.outputs.providers }}
          INCLUDE_LOCAL_PROVIDERS: "false"
        run: |
          if [ -n "${{ inputs.filter }}" ]; then
            pytest tests/integration -v -n auto --cov --cov-report=xml -k "${{ inputs.filter }}"
          else
            pytest tests/integration -v -n auto --cov --cov-report=xml
          fi

      - name: Upload coverage reports to Codecov
        if: always()
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }}

  run-local-integration-tests:
    needs: expected-providers
    if: github.event.inputs.filter == '' || contains(needs.expected-providers.outputs.local_providers, github.event.inputs.filter)
    timeout-minutes: 45
    runs-on: ubuntu-latest

    steps:
      - uses: actions/checkout@v5

      - uses: astral-sh/setup-uv@v7
        with:
          python-version: 3.13
          activate-environment: true

      - name: Install dependencies
        run: |
          uv sync --group tests --extra all

      - uses: actions/cache@v4
        if: github.event.inputs.filter == '' || contains(github.event.inputs.filter, 'ollama')
        with:
          path: ~/.ollama
          key: ${{ runner.os }}-ollama-models-${{ hashFiles('tests/conftest.py') }}
          restore-keys: |
            ${{ runner.os }}-ollama-models-

      - uses: actions/cache@v4
        with:
          path: ~/.llamafile
          key: ${{ runner.os }}-llamafile-${{ hashFiles('tests/conftest.py') }}
          restore-keys: |
            ${{ runner.os }}-llamafile-

      - name: Setup Ollama
        if: github.event.inputs.filter == '' || contains(github.event.inputs.filter, 'ollama')
        uses: ai-action/setup-ollama@v1

      - name: Run ollama
        if: github.event.inputs.filter == '' || contains(github.event.inputs.filter, 'ollama')
        run: |
          ollama serve &
          ollama pull llama3.2:1b
          ollama pull qwen3:0.6b
          ollama pull llava:7b

      - name: Wait for Ollama to be ready
        if: github.event.inputs.filter == '' || contains(github.event.inputs.filter, 'ollama')
        run: |
          timeout 60 bash -c 'until curl -s http://localhost:11434/api/tags >/dev/null; do sleep 1; done'

      - name: Download and Run LlamaFile
        if: github.event.inputs.filter == '' || contains(github.event.inputs.filter, 'llamafile')
        run: |
          mkdir -p ~/.llamafile
          LLAMAFILE_PATH="$HOME/.llamafile/Qwen_Qwen3-0.6B-Q4_K_M.llamafile"

          if [ ! -f "$LLAMAFILE_PATH" ]; then
            echo "Downloading llamafile (not found in cache)..."
            wget -O "$LLAMAFILE_PATH" https://huggingface.co/Mozilla/Qwen3-0.6B-llamafile/resolve/main/Qwen_Qwen3-0.6B-Q4_K_M.llamafile
            chmod +x "$LLAMAFILE_PATH"
          else
            echo "Using cached llamafile"
          fi

          "$LLAMAFILE_PATH" --server --v2 &
          echo $! > llamafile.pid

      - name: Wait for LlamaFile to be ready
        if: github.event.inputs.filter == '' || contains(github.event.inputs.filter, 'llamafile')
        run: |
          timeout 60 bash -c 'until curl -s http://localhost:8080/v1/models >/dev/null; do sleep 1; done'

      - name : Download and Run LlamaCpp
        if: github.event.inputs.filter == '' || contains(github.event.inputs.filter, 'llamacpp')
        run: docker run -d -p 8090:8090 ghcr.io/ggml-org/llama.cpp:server -hf ggml-org/Qwen3-1.7B-GGUF --jinja --port 8090

      - name: Wait for LlamaCpp to be ready
        if: github.event.inputs.filter == '' || contains(github.event.inputs.filter, 'llamacpp')
        run: |
          timeout 60 bash -c 'until curl -s http://localhost:8090/health >/dev/null; do sleep 1; done'

      - name: Run Local Provider Integration tests
        env:
          INCLUDE_LOCAL_PROVIDERS: "true"
          INCLUDE_NON_LOCAL_PROVIDERS: "false"
        run: |
          if [ -n "${{ inputs.filter }}" ]; then
            pytest tests/integration -v --cov --cov-report=xml -k "${{ inputs.filter }}"
          else
            pytest tests/integration -v --cov --cov-report=xml
          fi

      - name: Cleanup LlamaFile process
        if: always()
        run: |
          if [ -f llamafile.pid ]; then
            kill $(cat llamafile.pid) || true
            rm llamafile.pid
          fi

      - name: Upload coverage reports to Codecov
        if: always()
        uses: codecov/codecov-action@v5
        with:
          token: ${{ secrets.CODECOV_TOKEN }}
