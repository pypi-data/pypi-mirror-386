# Standard library imports
from functools import lru_cache
import math
from typing import List, Tuple, Callable

# Third-party imports
from numba import jit
import numpy as np
from numpy.typing import NDArray

# Local application imports
from random_allocation.comparisons.utils import search_function_with_bounds, FunctionType, BoundType
from random_allocation.other_schemes.local import Gaussian_epsilon, Gaussian_delta
from random_allocation.comparisons.definitions import PrivacyParams, SchemeConfig, Direction, Verbosity
from random_allocation.random_allocation_scheme.random_allocation_utils import print_alpha, handle_directions

# Type aliases
Partition = Tuple[int, ...]
NumericFunction = Callable[[float], float]

# ==================== Add ====================
def allocation_epsilon_direct_add(params: PrivacyParams, config: SchemeConfig) -> float:
    """
    Compute the epsilon value of the allocation scheme in the add direction.
    
    Args:
        sigma: Gaussian noise scale
        delta: Target delta value for differential privacy
        num_steps: Number of steps in the allocation scheme
        num_epochs: Number of epochs
        
    Returns:
        Computed epsilon value
    """    
    num_steps_per_round = int(np.ceil(params.num_steps/params.num_selected))
    num_rounds = int(np.ceil(params.num_steps/num_steps_per_round))
    epsilon = Gaussian_epsilon(sigma=params.sigma*math.sqrt(num_steps_per_round/(num_rounds*params.num_epochs)), 
                               delta=params.delta) + (1-1.0/num_steps_per_round)/(2*params.sigma**2)
    return float(epsilon)

def allocation_delta_direct_add(params: PrivacyParams, config: SchemeConfig) -> float:
    """
    Compute the delta value of the allocation scheme in the add direction.
    
    Args:
        params: Privacy parameters (must include epsilon)
        config: Scheme configuration parameters        
    Returns:
        Computed delta value
    """
    num_steps_per_round = int(np.ceil(params.num_steps/params.num_selected))
    num_rounds = int(np.ceil(params.num_steps/num_steps_per_round))
    delta = Gaussian_delta(sigma=params.sigma*math.sqrt(num_steps_per_round/(num_rounds*params.num_epochs)), 
                           epsilon=params.epsilon - (1-1.0/num_steps_per_round)/(2*params.sigma**2))
    return float(delta)

# ==================== Remove ====================
@lru_cache(maxsize=None)
def generate_partitions(n: int, max_size: int) -> List[Tuple[int, ...]]:
    """
    Generate all integer partitions of [1, ..., n] with a maximum number of elements in the partition.
    
    Args:
        n: The number to partition
        max_size: The maximum number of elements in each partition
        
    Returns:
        List of partitions, where each partition is a tuple of integers
    """
    # Convert n to int to handle cases where it's passed as numpy.float64
    n = int(n)
    max_size = int(max_size)
    
    partitions: List[List[Tuple[int, ...]]] = [[] for _ in range(n + 1)]
    partitions[0].append(())

    for i in range(1, n):
        partitions[i] = generate_partitions(n=i, max_size=max_size)
    for j in range(n, 0, -1):
        for p in partitions[n - j]:
            if (not p or j <= p[0]) and len(p) < max_size:  # Ensure descending order
                partitions[n].append((j,) + p)
    return partitions[n]

@jit(nopython=True, cache=True)
def log_factorial(n: int) -> float:
    """
    Compute the natural logarithm of n!.
    """
    if n <= 1:
        return 0.0
    return float(np.sum(np.log(np.arange(1, n + 1))))

@jit(nopython=True, cache=True)
def log_factorial_range(n: int, m: int) -> float:
    """
    Compute the natural logarithm of (n! / (n-m)!).
    """
    if n <= 1:
        return 0.0
    return float(np.sum(np.log(np.arange(n - m + 1, n + 1))))

@jit(nopython=True, cache=True)
def calc_partition_sum_square(arr: Tuple[int, ...]) -> float:
    """
    Compute the sum of squares of an array.
    """
    result = 0.0
    for x in arr:
        result += x * x
    return float(result)

@jit(nopython=True, cache=True)
def calc_log_multinomial(partition: Tuple[int, ...], n: int) -> float:
    """
    Compute the log of the multinomial coefficient for a given partition.

    """
    log_prod_factorial = 0.0
    for p in partition:
        log_prod_factorial += log_factorial(n=p)
    return float(log_factorial(n=n) - log_prod_factorial)

@jit(nopython=True, cache=True)
def calc_counts_log_multinomial(partition: Tuple[int, ...], n: int) -> float:
    """
    Compute the counts of each unique integer in a partition and calculate the multinomial coefficient.
    """
    sum_partition = sum(partition)

    # Count frequencies
    counts = np.zeros(sum_partition + 1, dtype=np.int64)  # type: np.ndarray
    for x in partition:
        counts[x] += 1
    sum_counts = sum(counts)

    # Compute multinomial
    log_counts_factorial = 0.0
    for i in range(1, sum_partition + 1):
        if counts[i] > 0:
            log_counts_factorial += log_factorial(n=counts[i])

    return float(log_factorial_range(n=n, m=sum_counts) - log_counts_factorial)

@jit(nopython=True, cache=True)
def compute_exp_term(partition: Tuple[int, ...], alpha: int, num_steps: int, sigma: float) -> float:
    """
    Compute the exponent term that is summed up inside the log term in the first of Corollary 6.2.
    """
    counts_log_multinomial = calc_counts_log_multinomial(partition=partition, n=num_steps)
    partition_log_multinomial = calc_log_multinomial(partition=partition, n=alpha)
    partition_sum_square = calc_partition_sum_square(arr=partition) / (2 * sigma**2)
    return float(counts_log_multinomial + partition_log_multinomial + partition_sum_square)

@lru_cache(maxsize=None)
def allocation_RDP_remove(alpha: int, sigma: float, num_steps: int) -> float:
    """
    Compute the RDP value of the allocation scheme in the remove direction for a given alpha.
    
    Args:
        alpha: Alpha order for RDP
        sigma: Gaussian noise scale
        num_steps: Number of steps in the allocation scheme
        
    Returns:
        Computed RDP value
    """
    assert isinstance(alpha, int), "alpha must be an integer"
    assert isinstance(num_steps, int), "num_steps must be an integer"
    assert alpha > 1, "alpha must be > 1"
        
    partitions = generate_partitions(n=alpha, max_size=num_steps)
    exp_terms = [compute_exp_term(partition=partition, alpha=alpha, num_steps=num_steps, sigma=sigma) for partition in partitions]

    max_val = max(exp_terms)
    log_sum = np.log(sum(np.exp(term - max_val) for term in exp_terms))

    return float((log_sum - alpha*(1/(2*sigma**2) + np.log(num_steps)) + max_val) / (alpha - 1))

def allocation_epsilon_direct_remove(params: PrivacyParams, config: SchemeConfig) -> float:
    """
    Compute the epsilon value of the allocation scheme in the remove direction using RÃ©nyi Differential Privacy (RDP).
    This function is based on Lemma 2.4, and utilizes the improvement stated in Claim 6.4.
    
    Args:       
        params: Privacy parameters (must include delta)
        config: Scheme configuration parameters        
    Returns:
        Computed epsilon value
    """
    # Ensure alpha_orders has at least one element
    if config.allocation_direct_alpha_orders is None:
        raise ValueError("allocation_direct_alpha_orders must be provided in SchemeConfig for 'remove' direction")
    assert len(config.allocation_direct_alpha_orders) > 0, "alpha_orders must have at least one element"

    num_steps_per_round = int(np.ceil(params.num_steps/params.num_selected))
    num_rounds = int(np.ceil(params.num_steps/num_steps_per_round))

    alpha_orders = config.allocation_direct_alpha_orders
    alpha = alpha_orders[0]
    alpha_RDP = allocation_RDP_remove(alpha=alpha, sigma=params.sigma, num_steps=num_steps_per_round) * num_rounds*params.num_epochs
    epsilon = alpha_RDP + max(math.log1p(-1/alpha) - math.log(params.delta * alpha)/(alpha-1), 0)
    used_alpha = alpha
    for alpha in alpha_orders:
        alpha_RDP = allocation_RDP_remove(alpha=alpha, sigma=params.sigma, num_steps=num_steps_per_round) * num_rounds*params.num_epochs
        if alpha_RDP > epsilon:
            break
        else:
            new_eps = alpha_RDP + max(math.log1p(-1/alpha) - math.log(params.delta * alpha)/(alpha-1), 0)
            if new_eps < epsilon:
                epsilon = new_eps
                used_alpha = alpha

    print_alpha(used_alpha, alpha_orders[0], alpha_orders[-1], config.verbosity, "remove", params)
    return float(epsilon)

def allocation_delta_direct_remove(params: PrivacyParams, config: SchemeConfig) -> float:
    if config.allocation_direct_alpha_orders is None:
        raise ValueError("allocation_direct_alpha_orders must be provided in SchemeConfig for 'remove' or 'both' direction")
    
    result = search_function_with_bounds(
        func=lambda delta: allocation_epsilon_direct(
            params=PrivacyParams(
                sigma=params.sigma,
                num_steps=params.num_steps,
                num_selected=params.num_selected,
                num_epochs=params.num_epochs,
                epsilon=None,
                delta=delta
            ),
            config=config,
            direction=Direction.REMOVE,
        ),
        y_target=params.epsilon,
        bounds=(2*config.delta_tolerance, 1-2*config.delta_tolerance),
        tolerance=config.delta_tolerance,
        function_type=FunctionType.DECREASING,
        verbosity=config.verbosity,
    )
    return float(result) if result is not None else 1.0

# ==================== Both ====================
def allocation_epsilon_direct(params: PrivacyParams, config: SchemeConfig, direction: Direction = Direction.BOTH) -> float:
    """
    Compute epsilon for the direct allocation scheme.
    
    Args:
        params: Privacy parameters (must include delta)
        config: Scheme configuration parameters
        direction: The direction of privacy. Can be ADD, REMOVE, or BOTH.
    
    Returns:
        Computed epsilon value
    """
    if params.delta is None:
        raise ValueError("Delta must be provided to compute epsilon")


    return handle_directions(params=params,
                             config=config,
                             direction=direction,
                             add_func=allocation_epsilon_direct_add,
                             remove_func=allocation_epsilon_direct_remove,
                             var_name="epsilon")

def allocation_delta_direct(params: PrivacyParams, config: SchemeConfig, direction: Direction = Direction.BOTH) -> float:
    """
    Compute the delta value of the allocation scheme using RÃ©nyi Differential Privacy (RDP).
    This function can compute delta for both the add and remove directions, or maximum of both.
    
    Args:
        params: Privacy parameters (must include epsilon)
        config: Scheme configuration parameters
        direction: The direction of privacy. Can be ADD, REMOVE, or BOTH.
    
    Returns:
        Computed delta value
    """
    if params.epsilon is None:
        raise ValueError("Epsilon must be provided to compute delta")
    
    return handle_directions(params=params,
                             config=config,
                             direction=direction,
                             add_func=allocation_delta_direct_add,
                             remove_func=allocation_delta_direct_remove,
                             var_name="delta")

# ===================== Legacy RDP add ====================

def allocation_RDP_add(sigma: float, 
                       num_steps: int, 
                       num_epochs: int,
                       ) -> Tuple[NDArray[np.float64], NDArray[np.float64]]:
    # Compute RDP values
    alpha_orders = np.concatenate((np.arange(2, 202, dtype=np.float64), np.exp(np.linspace(np.log(202), np.log(10_000), 50))))
    alpha_RDP = num_epochs * (alpha_orders + num_steps - 1) / (2 * num_steps * sigma**2)
    return alpha_orders, alpha_RDP

def allocation_epsilon_RDP_add(params: PrivacyParams, config: SchemeConfig) -> float:
    if params.delta is None:
        raise ValueError("Delta must be provided to compute epsilon")
    
    num_steps_per_round = int(np.ceil(params.num_steps/params.num_selected))
    num_rounds = int(np.ceil(params.num_steps/num_steps_per_round))

    alpha_orders, alpha_RDP = allocation_RDP_add(sigma=params.sigma,
                                                 num_steps=num_steps_per_round,
                                                 num_epochs=num_rounds*params.num_epochs)
    alpha_epsilons = np.maximum(0.0, alpha_RDP + np.log1p(-1 / alpha_orders) - np.log(params.delta * alpha_orders) / (alpha_orders - 1))
    idx = np.argmin(alpha_epsilons)
    used_alpha = alpha_orders[idx]
    print_alpha(used_alpha, alpha_orders[0], alpha_orders[-1], config.verbosity, "add", params)
    return float(alpha_epsilons[idx])

def allocation_delta_RDP_add(params: PrivacyParams, config: SchemeConfig) -> float:
    if params.epsilon is None:
        raise ValueError("Epsilon must be provided to compute delta")

    num_steps_per_round = int(np.ceil(params.num_steps/params.num_selected))
    num_rounds = int(np.ceil(params.num_steps/num_steps_per_round))

    alpha_orders, alpha_RDP = allocation_RDP_add(sigma=params.sigma,
                                                 num_steps=num_steps_per_round,
                                                 num_epochs=num_rounds*params.num_epochs)
    alpha_deltas = np.exp((alpha_orders-1) * (alpha_RDP - params.epsilon))*(1-1/alpha_orders)**alpha_orders / (alpha_orders-1)
    idx = np.argmin(alpha_deltas)
    used_alpha = alpha_orders[idx]
    print_alpha(used_alpha, alpha_orders[0], alpha_orders[-1], config.verbosity, "add", params)
    # Protected conversion: ensure delta doesn't exceed 1.0
    return float(np.minimum(1.0, alpha_deltas[idx]))