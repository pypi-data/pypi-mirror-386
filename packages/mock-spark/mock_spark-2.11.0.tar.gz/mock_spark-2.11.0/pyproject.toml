[build-system]
requires = ["setuptools>=61.0", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "mock-spark"
version = "2.11.0"
description = "Lightning-fast PySpark testing without JVM - 10x faster with 100% API compatibility"
readme = "README.md"
license = {text = "MIT"}
authors = [
    {name = "Odos Matthews", email = "odosmatthews@gmail.com"}
]
maintainers = [
    {name = "Odos Matthews", email = "odosmatthews@gmail.com"}
]
keywords = [
    "spark", "pyspark", "mock", "testing", "development", "data-engineering",
    "dataframe", "spark-session", "unit-testing", "type-safe", "mypy",
    "error-simulation", "performance-testing", "data-generation", "enterprise"
]
classifiers = [
    "Development Status :: 5 - Production/Stable",
    "Intended Audience :: Developers",
    "License :: OSI Approved :: MIT License",
    "Operating System :: OS Independent",
    "Programming Language :: Python :: 3",
    "Programming Language :: Python :: 3.8",
    "Programming Language :: Python :: 3.9",
    "Programming Language :: Python :: 3.10",
    "Topic :: Software Development :: Testing",
    "Topic :: Software Development :: Libraries :: Python Modules",
    "Topic :: Scientific/Engineering :: Information Analysis",
]
requires-python = ">=3.8"
dependencies = [
    "spark-ddl-parser>=0.1.0",
    "duckdb>=0.9.0",
    "duckdb-engine>=0.15.0",
    "sqlalchemy[mypy]>=2.0.0",
    "sqlglot>=20.0.0",
    "psutil>=5.8.0",
]

[project.optional-dependencies]
pandas = [
    "pandas>=1.3.0",
    "pandas-stubs>=2.0.0",
]
analytics = [
    "pandas>=1.3.0",
    "pandas-stubs>=2.0.0",
    "numpy>=1.20.0",
]
dev = [
    "pytest>=7.0.0",
    "pytest-cov>=4.0.0",
    "black>=23.0.0",
    "isort>=5.12.0",
    "mypy>=0.990,<1.0",  # Pin to <1.0 for python_version=3.8 support
    "pandas>=1.3.0",
    "pandas-stubs>=2.0.0",
    "types-psutil>=6.0.0",
]
test = [
    "pytest>=7.0.0",
    "pytest-cov>=4.0.0",
    "pytest-xdist>=3.0.0",  # Parallel test execution with isolated JVM workers
    "hypothesis>=6.0.0",
    "pyspark>=3.2.0,<3.3.0",  # PySpark 3.2.x for Java 11 compatibility
    "delta-spark>=2.0.0,<2.2.0",  # Delta Lake compatible with PySpark 3.2.x
]
# PySpark version compatibility modes
# Note: These extras are markers for documentation purposes.
# To enable version-specific APIs, set the environment variable:
#   export MOCK_SPARK_PYSPARK_VERSION=3.0
# Or in Python before importing:
#   from mock_spark._version_compat import set_pyspark_version
#   set_pyspark_version('3.0')
pyspark-3-0 = []  # PySpark 3.0 API compatibility (use MOCK_SPARK_PYSPARK_VERSION=3.0)
pyspark-3-1 = []  # PySpark 3.1 API compatibility (use MOCK_SPARK_PYSPARK_VERSION=3.1)
pyspark-3-2 = []  # PySpark 3.2 API compatibility (use MOCK_SPARK_PYSPARK_VERSION=3.2)
pyspark-3-3 = []  # PySpark 3.3 API compatibility (use MOCK_SPARK_PYSPARK_VERSION=3.3)
pyspark-3-4 = []  # PySpark 3.4 API compatibility (use MOCK_SPARK_PYSPARK_VERSION=3.4)
pyspark-3-5 = []  # PySpark 3.5 API compatibility (use MOCK_SPARK_PYSPARK_VERSION=3.5)

[project.urls]
Homepage = "https://github.com/eddiethedean/mock-spark"
Repository = "https://github.com/eddiethedean/mock-spark"
Issues = "https://github.com/eddiethedean/mock-spark/issues"

[tool.setuptools.packages.find]
where = ["."]
include = ["mock_spark*"]

[tool.setuptools.package-data]
mock_spark = ["py.typed"]

[tool.black]
line-length = 100
target-version = ['py38']
include = '\.pyi?$'
extend-exclude = '''
/(
  # directories
  \.eggs
  | \.git
  | \.hg
  | \.mypy_cache
  | \.tox
  | \.venv
  | build
  | dist
)/
'''

[tool.isort]
profile = "black"
line_length = 100
multi_line_output = 3
include_trailing_comma = true
force_grid_wrap = 0
use_parentheses = true
ensure_newline_before_comments = true

[tool.mypy]
python_version = "3.8"
warn_return_any = true
warn_unused_configs = true
disallow_untyped_defs = true
disallow_incomplete_defs = true
check_untyped_defs = true
disallow_untyped_decorators = true
no_implicit_optional = true
warn_redundant_casts = true
warn_unused_ignores = true
warn_no_return = true
warn_unreachable = true
strict_equality = true
show_error_codes = true

[[tool.mypy.overrides]]
module = [
    "pandas.*",
    "psutil.*",

]
ignore_missing_imports = true

[[tool.mypy.overrides]]
module = [
    "mock_spark.testing.*",
]
ignore_errors = true

[[tool.mypy.overrides]]
module = [
    "mock_spark.dataframe.grouped_data",
]
warn_unreachable = false

[tool.pytest.ini_options]
testpaths = ["tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = [
    "-v",
    "--tb=short",
    "--strict-markers",
    "--disable-warnings",
    "--cov=mock_spark",
    "--cov-report=term-missing",
    "--cov-report=html",
    "--cov-report=xml",
]
markers = [
    "slow: marks tests as slow (deselect with '-m \"not slow\"')",
    "integration: marks tests as integration tests",
    "unit: marks tests as unit tests",
    "compatibility: marks tests as compatibility tests with real PySpark",
    "fast: marks tests as fast unit tests without PySpark",
    "delta: marks tests that require Delta Lake JARs (run serially for isolation)",
    "performance: marks tests that measure performance (run serially for stable timing)",
]
