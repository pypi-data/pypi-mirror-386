{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading PyArrow Dataset from S3\n",
    "\n",
    "This example demonstrates how to use fsspec-utils to read PyArrow datasets from \n",
    "S3-compatible storage systems including AWS S3, Cloudflare R2, and self-hosted MinIO.\n",
    "\n",
    "The example shows:\n",
    "1. Configuring storage options for different S3-compatible services\n",
    "2. Creating PyArrow datasets from these storage systems\n",
    "3. Reading data into PyArrow tables\n",
    "4. Working with partitioned datasets\n",
    "5. Using optimized dataset reading with metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyarrow as pa\n",
    "import pyarrow.dataset as pds\n",
    "\n",
    "from fsspec_utils import filesystem\n",
    "from fsspec_utils.storage_options import AwsStorageOptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AWS S3 Configuration\n",
    "\n",
    "Configure AWS S3 storage options and create a PyArrow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure AWS S3 storage options\n",
    "# Replace with your actual AWS credentials and region\n",
    "s3_options = AwsStorageOptions(\n",
    "    access_key_id=\"YOUR_AWS_ACCESS_KEY_ID\",      # Replace with your AWS access key\n",
    "    secret_access_key=\"YOUR_AWS_SECRET_ACCESS_KEY\",  # Replace with your AWS secret key\n",
    "    region=\"us-east-1\",                      # AWS region\n",
    ")\n",
    "\n",
    "# Create fsspec filesystem instance from storage options\n",
    "fs = filesystem(\"s3\", storage_options=s3_options)\n",
    "\n",
    "# Create PyArrow dataset from S3 bucket\n",
    "# Assumes Parquet data in s3://your-bucket/data/\n",
    "try:\n",
    "    dataset = fs.pyarrow_dataset(\"s3://your-bucket/data/\")\n",
    "    \n",
    "    # Read data from the dataset into a PyArrow table\n",
    "    table = dataset.to_table()\n",
    "    \n",
    "    print(f\"Dataset schema: {dataset.schema}\")\n",
    "    print(f\"Table shape: {table.shape}\")\n",
    "    print(f\"First few rows:\\n{table.slice(0, 5)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading from AWS S3: {e}\")\n",
    "    print(\"Make sure you have valid AWS credentials and the bucket exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cloudflare R2 Configuration\n",
    "\n",
    "Configure Cloudflare R2 storage options and create a PyArrow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Cloudflare R2 storage options\n",
    "# R2 is S3-compatible, so we use AwsStorageOptions with a custom endpoint\n",
    "# Replace with your actual R2 credentials and account ID\n",
    "r2_options = AwsStorageOptions(\n",
    "    access_key_id=\"YOUR_R2_ACCESS_KEY_ID\",    # Replace with your R2 access key\n",
    "    secret_access_key=\"YOUR_R2_SECRET_KEY\",  # Replace with your R2 secret key\n",
    "    endpoint_url=\"https://YOUR_ACCOUNT_ID.r2.cloudflarestorage.com\",  # R2 endpoint URL\n",
    "    # Note: R2 doesn't use AWS regions in the same way\n",
    ")\n",
    "\n",
    "# Create fsspec filesystem instance for R2\n",
    "r2_fs = filesystem(\"s3\", storage_options=r2_options)\n",
    "\n",
    "# Create PyArrow dataset from R2 bucket\n",
    "try:\n",
    "    r2_dataset = r2_fs.pyarrow_dataset(\"your-bucket-name/data/\")\n",
    "    \n",
    "    # Read data from the R2 dataset\n",
    "    r2_table = r2_dataset.to_table()\n",
    "    \n",
    "    print(f\"R2 Dataset schema: {r2_dataset.schema}\")\n",
    "    print(f\"R2 Table shape: {r2_table.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading from Cloudflare R2: {e}\")\n",
    "    print(\"Make sure you have valid R2 credentials and the bucket exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MinIO Configuration\n",
    "\n",
    "Configure MinIO storage options and create a PyArrow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure MinIO storage options\n",
    "# MinIO is S3-compatible, so we use AwsStorageOptions with custom endpoint and credentials\n",
    "# Replace with your actual MinIO credentials and endpoint\n",
    "minio_options = AwsStorageOptions(\n",
    "    access_key_id=\"YOUR_MINIO_ACCESS_KEY\",               # Your MinIO access key\n",
    "    secret_access_key=\"YOUR_MINIO_SECRET_KEY\",           # Your MinIO secret key\n",
    "    endpoint_url=\"http://localhost:9000\",     # MinIO server endpoint\n",
    "    allow_http=True,                          # Allow HTTP (not HTTPS) for local development\n",
    "    # Note: MinIO doesn't require AWS regions\n",
    ")\n",
    "\n",
    "# Create fsspec filesystem instance for MinIO\n",
    "minio_fs = filesystem(\"s3\", storage_options=minio_options)\n",
    "\n",
    "# Create PyArrow dataset from MinIO bucket\n",
    "try:\n",
    "    minio_dataset = minio_fs.pyarrow_dataset(\"your-bucket/data/\")\n",
    "    \n",
    "    # Read data from the MinIO dataset\n",
    "    minio_table = minio_dataset.to_table()\n",
    "    \n",
    "    print(f\"MinIO Dataset schema: {minio_dataset.schema}\")\n",
    "    print(f\"MinIO Table shape: {minio_table.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading from MinIO: {e}\")\n",
    "    print(\"Make sure you have a MinIO server running and the bucket exists.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Partitioned Datasets\n",
    "\n",
    "Example of working with partitioned datasets and using partition pruning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For partitioned Parquet data (e.g., data partitioned by date)\n",
    "try:\n",
    "    partitioned_dataset = fs.pyarrow_dataset(\n",
    "        \"s3://your-bucket/partitioned-data/\",\n",
    "        partitioning=[\"year\", \"month\", \"day\"]  # Hive-style partitioning\n",
    "    )\n",
    "    \n",
    "    # Query with partition pruning - only read specific partitions\n",
    "    filtered_table = partitioned_dataset.to_table(\n",
    "        filter=(\n",
    "            (partitioned_dataset.field(\"year\") == 2024) &\n",
    "            (partitioned_dataset.field(\"month\") == 1) &\n",
    "            (partitioned_dataset.field(\"day\") > 15)\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\"Filtered table shape: {filtered_table.shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error working with partitioned dataset: {e}\")\n",
    "    print(\"Make sure you have a partitioned dataset in the specified location.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Parquet Dataset Reading\n",
    "\n",
    "Using `pyarrow_parquet_dataset` for optimized reading with metadata files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you have a _metadata file in your dataset directory\n",
    "try:\n",
    "    parquet_dataset = fs.pyarrow_parquet_dataset(\"s3://your-bucket/data-with-metadata/\")\n",
    "    \n",
    "    # This automatically uses the _metadata file for optimized reading\n",
    "    optimized_table = parquet_dataset.to_table()\n",
    "    \n",
    "    print(f\"Optimized table shape: {optimized_table.shape}\")\n",
    "    print(f\"Dataset files: {parquet_dataset.files}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error reading optimized Parquet dataset: {e}\")\n",
    "    print(\"Make sure you have a dataset with a _metadata file in the specified location.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}