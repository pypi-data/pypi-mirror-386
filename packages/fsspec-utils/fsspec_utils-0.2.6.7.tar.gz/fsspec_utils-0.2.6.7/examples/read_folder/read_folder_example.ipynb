{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Folder of Files into PyArrow Table\n",
    "\n",
    "This example demonstrates how to read a folder of parquet, csv, or json files \n",
    "into a PyArrow table using fsspec-utils.\n",
    "\n",
    "The example shows:\n",
    "1. Creating sample data files in different formats (Parquet, CSV, JSON)\n",
    "2. Reading all files of a specific format from a directory into a PyArrow table\n",
    "3. Demonstrating the differences between reading different file formats\n",
    "4. Showing how to handle various data types and structures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "import shutil\n",
    "import os\n",
    "import polars as pl\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import json\n",
    "\n",
    "# Import fsspec-utils\n",
    "from fsspec_utils import filesystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_data(temp_dir):\n",
    "    \"\"\"Create sample Parquet, CSV, and JSON files in subdirectories.\"\"\"\n",
    "    \n",
    "    # Create subdirectories\n",
    "    subdir1 = os.path.join(temp_dir, \"subdir1\")\n",
    "    subdir2 = os.path.join(temp_dir, \"subdir2\")\n",
    "    os.makedirs(subdir1, exist_ok=True)\n",
    "    os.makedirs(subdir2, exist_ok=True)\n",
    "    \n",
    "    # Sample data\n",
    "    data1 = {\n",
    "        \"id\": [1, 2, 3],\n",
    "        \"name\": [\"Alice\", \"Bob\", \"Charlie\"],\n",
    "        \"value\": [10.5, 20.3, 30.7]\n",
    "    }\n",
    "    \n",
    "    data2 = {\n",
    "        \"id\": [4, 5, 6],\n",
    "        \"name\": [\"David\", \"Eve\", \"Frank\"],\n",
    "        \"value\": [40.2, 50.8, 60.1]\n",
    "    }\n",
    "    \n",
    "    # Create Parquet files\n",
    "    df1 = pl.DataFrame(data1)\n",
    "    df2 = pl.DataFrame(data2)\n",
    "    \n",
    "    # Save Parquet files\n",
    "    df1.write_parquet(os.path.join(subdir1, \"data1.parquet\"))\n",
    "    df2.write_parquet(os.path.join(subdir2, \"data2.parquet\"))\n",
    "    \n",
    "    # Create CSV files\n",
    "    df1.write_csv(os.path.join(subdir1, \"data1.csv\"))\n",
    "    df2.write_csv(os.path.join(subdir2, \"data2.csv\"))\n",
    "    \n",
    "    # Create JSON files\n",
    "    with open(os.path.join(subdir1, \"data1.json\"), \"w\") as f:\n",
    "        json.dump(data1, f)\n",
    "    \n",
    "    with open(os.path.join(subdir2, \"data2.json\"), \"w\") as f:\n",
    "        json.dump(data2, f)\n",
    "    \n",
    "    print(f\"Created sample data in {temp_dir}\")\n",
    "    print(f\"  - {subdir1}/data1.parquet\")\n",
    "    print(f\"  - {subdir2}/data2.parquet\")\n",
    "    print(f\"  - {subdir1}/data1.csv\")\n",
    "    print(f\"  - {subdir2}/data2.csv\")\n",
    "    print(f\"  - {subdir1}/data1.json\")\n",
    "    print(f\"  - {subdir2}/data2.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_parquet_reading(temp_dir):\n",
    "    \"\"\"Demonstrate reading Parquet files into a PyArrow Table.\"\"\"\n",
    "    print(\"\\n=== Reading Parquet Files ===\")\n",
    "    \n",
    "    # Create a filesystem instance\n",
    "    fs = filesystem(temp_dir)\n",
    "    \n",
    "    # Read all Parquet files in the directory and subdirectories\n",
    "    # This returns a PyArrow Table directly\n",
    "    parquet_table = fs.read_parquet(\"**/*.parquet\", concat=True)\n",
    "    \n",
    "    print(f\"Successfully read Parquet files into PyArrow Table\")\n",
    "    print(f\"Table schema: {parquet_table.schema}\")\n",
    "    print(f\"Table shape: {parquet_table.num_rows} rows x {parquet_table.num_columns} columns\")\n",
    "    print(\"First 3 rows:\")\n",
    "    print(parquet_table.slice(0, 3).to_pandas())\n",
    "    \n",
    "    return parquet_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_csv_reading(temp_dir):\n",
    "    \"\"\"Demonstrate reading CSV files into a PyArrow Table.\"\"\"\n",
    "    print(\"\\n=== Reading CSV Files ===\")\n",
    "    \n",
    "    # Create a filesystem instance\n",
    "    fs = filesystem(temp_dir)\n",
    "    \n",
    "    # Read all CSV files in the directory and subdirectories\n",
    "    # This returns a Polars DataFrame when concat=True\n",
    "    csv_df = fs.read_csv(\"**/*.csv\", concat=True)\n",
    "    \n",
    "    print(f\"Successfully read CSV files into Polars DataFrame\")\n",
    "    print(f\"DataFrame shape: {csv_df.shape}\")\n",
    "    print(\"First 3 rows:\")\n",
    "    print(csv_df.head(3))\n",
    "    \n",
    "    # Convert Polars DataFrame to PyArrow Table\n",
    "    csv_table = csv_df.to_arrow()\n",
    "    \n",
    "    print(f\"\\nConverted to PyArrow Table\")\n",
    "    print(f\"Table schema: {csv_table.schema}\")\n",
    "    print(f\"Table shape: {csv_table.num_rows} rows x {csv_table.num_columns} columns\")\n",
    "    \n",
    "    return csv_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_json_reading(temp_dir):\n",
    "    \"\"\"Demonstrate reading JSON files into a PyArrow Table.\"\"\"\n",
    "    print(\"\\n=== Reading JSON Files ===\")\n",
    "    \n",
    "    # Create a filesystem instance\n",
    "    fs = filesystem(temp_dir)\n",
    "    \n",
    "    # Read all JSON files in the directory and subdirectories\n",
    "    # This returns a Polars DataFrame when as_dataframe=True and concat=True\n",
    "    json_df = fs.read_json(\"**/*.json\", as_dataframe=True, concat=True)\n",
    "    \n",
    "    print(f\"Successfully read JSON files into Polars DataFrame\")\n",
    "    print(f\"DataFrame shape: {json_df.shape}\")\n",
    "    print(\"First 3 rows:\")\n",
    "    print(json_df.head(3))\n",
    "    \n",
    "    # Convert Polars DataFrame to PyArrow Table\n",
    "    json_table = json_df.to_arrow()\n",
    "    \n",
    "    print(f\"\\nConverted to PyArrow Table\")\n",
    "    print(f\"Table schema: {json_table.schema}\")\n",
    "    print(f\"Table shape: {json_table.num_rows} rows x {json_table.num_columns} columns\")\n",
    "    \n",
    "    return json_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a temporary directory\n",
    "temp_dir = tempfile.mkdtemp()\n",
    "print(f\"Created temporary directory: {temp_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "create_sample_data(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demonstrate reading different file formats\n",
    "parquet_table = demonstrate_parquet_reading(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_table = demonstrate_csv_reading(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_table = demonstrate_json_reading(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify that all tables have the same data\n",
    "print(\"\\n=== Verification ===\")\n",
    "print(f\"All tables have the same number of rows: \"\n",
    "      f\"{parquet_table.num_rows == csv_table.num_rows == json_table.num_rows}\")\n",
    "print(f\"All tables have the same number of columns: \"\n",
    "      f\"{parquet_table.num_columns == csv_table.num_columns == json_table.num_columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up temporary directory\n",
    "shutil.rmtree(temp_dir)\n",
    "print(f\"\\nCleaned up temporary directory: {temp_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}