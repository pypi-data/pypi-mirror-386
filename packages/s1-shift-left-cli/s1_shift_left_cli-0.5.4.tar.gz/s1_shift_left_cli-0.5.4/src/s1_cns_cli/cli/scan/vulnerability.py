import json
import logging
import os
import subprocess
import sys
import tempfile
import time
import uuid
from datetime import datetime, timezone

from tabulate import tabulate

from s1_cns_cli.cli.config import get_config_manager
from s1_cns_cli.cli.registry import MissingRequiredFlagsException, LogColors, MissingDependenciesException, \
    SENTINELONE_JSON, DEFECT_DOJO_GENERIC_FORMAT, OUTPUT_FORMAT, SEVERITY_MAP, HttpMethod, DOWNLOAD_VULN_DB_INFO_API, \
    ScanNotEnabledException, ScanFailedException
from s1_cns_cli.cli.utils import read_json_file, print_output_on_file, get_config_path, get_severity_color, \
    get_version, get_exit_code_on_crash, get_os_and_architecture, get_wrapping_length, wrap_text, get_priority, \
    get_output_file_and_format, get_sarif_payload, make_request, get_actionable_and_non_actionable

LOGGER = logging.getLogger("cli")


def read_result(output_file):
    if os.path.exists(output_file) and os.path.getsize(output_file) > 0:
        issues = read_json_file(output_file)
        issue_cnt = len(issues) if issues is not None else 0
        return issues, issue_cnt
    else:
        return [], 0


def print_sbom(data):
    required_length = get_wrapping_length(4)
    if len(data) > 0:
        table_data = []
        for row in data:
            table_data.append({
                "Name": wrap_text(row["name"], required_length),
                "Version": wrap_text(row["version"], required_length),
                "Type": wrap_text(row["type"], required_length),
                "Purl": wrap_text(row["purl"], required_length)
            })
        print(tabulate(table_data, headers="keys", tablefmt="psql"))
    else:
        LOGGER.info("No packages detected")
    return 0


def vulnerability_parser(args, cache_directory):
    vulnerability_pre_evaluation(args, cache_directory)

    manager = get_config_manager()
    global_config_data = manager.get_global()
    vulnerability_config_data = manager.get_vuln()

    output_file = os.path.join(tempfile.gettempdir(), f"{uuid.uuid4()}.json")
    if args.generate_sbom:
        output_file, _ = get_output_file_and_format(args)

    call_vulnerability_scanner(args, cache_directory, output_file, global_config_data)

    # means error occurred during vulnerability scanning / sbom generation
    if not os.path.exists(output_file):
        return get_exit_code_on_crash(cache_directory)

    if args.generate_sbom:
        if args.sbom_format == SENTINELONE_JSON:
            with open(output_file, 'r') as file:
                data = json.load(file)
                print_sbom(data["artifacts"])
        LOGGER.info(f"Sbom generated at path: {output_file}")
        return 0
    else:
        issues, issue_cnt = read_result(output_file)
        os.remove(output_file)
        if issue_cnt > 0:
            return vulnerability_post_evaluation(args, issues, global_config_data, vulnerability_config_data)
        else:
            print(LogColors.OKGREEN + "RESULT\tScan completed. No issue found!" + LogColors.ENDC)
        return 0


def vulnerability_pre_evaluation(args, cache_directory):
    operating_sys, arch = get_os_and_architecture()
    runtime = f"{operating_sys}/{arch}"
    not_supported_runtimes = ["windows/386", "windows/arm"]
    if runtime in not_supported_runtimes:
        LOGGER.info(f"{runtime} runtime not supported for vulnerability scanning")
        sys.exit(0)

    global_config_data = read_json_file(get_config_path(cache_directory))
    output_file, output_format = get_output_file_and_format(args)

    if args.generate_sbom and len(output_file) == 0:
        LOGGER.error("--output-file is required to generate sbom")
        sys.exit(1)

    if args.vuln_format == DEFECT_DOJO_GENERIC_FORMAT and len(output_file) == 0:
        LOGGER.error("--output-file is required")
        sys.exit(1)

    if args.generate_sbom and args.sbom_format is None:
        args.sbom_format = SENTINELONE_JSON

    if args.sbom_format is not None and not args.generate_sbom:
        LOGGER.warning("--sbom-format should be used with --generate-sbom")

    if args.directory == "" and args.docker_image == "":
        raise MissingRequiredFlagsException("Either --d/--directory or --docker-image is required with vuln")

    if output_format == OUTPUT_FORMAT.SARIF or output_format == OUTPUT_FORMAT.CSV:
        if args.generate_sbom:
            LOGGER.error(f"output-format: {output_format} not supported for sbom")
            sys.exit(1)

        if args.vuln_format == DEFECT_DOJO_GENERIC_FORMAT:
            LOGGER.error(f"output-format: {output_format} not supported for DEFECT_DOJO_GENERIC_FORMAT")
            sys.exit(1)

    manager = get_config_manager()
    vuln_config = manager.get_vuln()
    vuln_scan_enabled = vuln_config.get("enabled", False)
    if not vuln_scan_enabled:
        raise ScanNotEnabledException("vuln scan is disabled")


def group_vulnerabilities_by_cve(findings):
    """
    Group vulnerability findings by their unique characteristics (CVE ID, package, version, etc.)
    and combine paths from duplicate entries into a single paths array.
    """
    grouped = {}
    
    for finding in findings:
        # Create a unique key based on CVE characteristics
        key = (
            finding.get("id", ""),
            finding.get("package", ""),
            finding.get("version", ""),
        )
        
        if key in grouped:
            # Merge paths from duplicate entries
            existing_paths = set(grouped[key].get("paths", []))
            new_paths = set(finding.get("paths", []))
            combined_paths = sorted(list(existing_paths.union(new_paths)))
            grouped[key]["paths"] = combined_paths
        else:
            # First occurrence of this CVE, store it
            grouped[key] = finding.copy()
    
    return list(grouped.values())


def vulnerability_post_evaluation(args, findings, global_config_data, vulnerability_config_data):
    exit_code = 0
    if len(findings) > 0:
        # Group findings by CVE ID to combine duplicate entries with different paths
        grouped_findings = group_vulnerabilities_by_cve(findings)
        sorted_findings = sorted(grouped_findings, key=get_priority)
        actionable_findings, _ = get_actionable_and_non_actionable(sorted_findings,
                                                                   vulnerability_config_data["severity"])

        # remove issues that are matching with cve-exceptions
        if "cveExceptions" in vulnerability_config_data:
            actionable_findings = evaluate_cve_exceptions_on_issues(args, actionable_findings, vulnerability_config_data)

        exit_code = 1 if len(actionable_findings) > 0 else 0

        if args.show_all_findings:
            actionable_findings = sorted_findings

        if args.vuln_format == SENTINELONE_JSON:
            print_issue_on_console(actionable_findings, args.quiet, args.verbose)

        if args.vuln_format == DEFECT_DOJO_GENERIC_FORMAT:
            actionable_findings = {
                "findings": actionable_findings
            }
        save_issues_on_file(args, actionable_findings, global_config_data)
        print("RESULT\tScan completed. Found " + str(len(actionable_findings)) + " issues.")
    else:
        print(LogColors.OKGREEN + "RESULT\tScan completed. No issue found!" + LogColors.ENDC)

    return exit_code


def evaluate_cve_exceptions_on_issues(args, issues, vulnerability_config_data) -> list:
    exceptions = vulnerability_config_data["cveExceptions"]

    if len(exceptions) > 0:
        cves_to_exclude = build_excluded_cve_set(exceptions)

        if args.vuln_format == SENTINELONE_JSON:
            issues = [issue for issue in issues if issue.get("id", "") not in cves_to_exclude]

        if args.vuln_format == DEFECT_DOJO_GENERIC_FORMAT:
            issues = [issue for issue in issues if issue.get("cve", "") not in cves_to_exclude]

    return issues


def build_excluded_cve_set(exceptions: list) -> set:
    current_time = int(time.time())
    cve_to_exclude = set()

    for exception in exceptions:
        cve_ids = exception.get("cveIds", [])
        # convert expiry time to second
        expiry_time = exception.get("expiry", 0) // 1000
        excludePermanently = exception.get("excludePermanently", False)
        reason = exception.get('reason', '')
        if excludePermanently or current_time < expiry_time:
            if excludePermanently:
                LOGGER.debug(f"Excluding permanent muted cve's from issues ({', '.join(cve_ids)}), reason: {reason}")
            else:
                LOGGER.debug(f"{datetime.fromtimestamp(current_time)}\tExcluding muted cve's from issues ({', '.join(cve_ids)}) till {datetime.fromtimestamp(expiry_time, timezone.utc)} UTC, reason: {reason} ")
            cve_to_exclude.update(cve_ids)

    return cve_to_exclude


def get_vuln_db(global_config_data):
    try:
        management_console_url = global_config_data["management_console_url"]
        api_token = global_config_data["service_user_api_token"]
        vuln_db_url = management_console_url + DOWNLOAD_VULN_DB_INFO_API
        query_params = {
            "scopeType": global_config_data["scope_type"],
            "scopeIds": global_config_data["scope_id"]
        }
        response = make_request(HttpMethod.GET, vuln_db_url, api_token, query_params)
        return response.json()["data"]
    except Exception as e:
        LOGGER.debug(f"failed to fetch metadata for latest vuln db, error: {e}")
        return {"version": 0}


def call_vulnerability_scanner(args, cache_directory, output_file, global_config_data):
    if os.path.exists(output_file):
        os.remove(output_file)

    vuln_db = get_vuln_db(global_config_data)

    command = generate_command(args, cache_directory, output_file, global_config_data, vuln_db)
    result = subprocess.run(command)
    exit_code = result.returncode
    if exit_code != 0:
        raise ScanFailedException()


def generate_command(args, cache_directory, output_path, global_config_data, vuln_db):
    cli_version = get_version()
    binary_path = os.path.join(cache_directory, "bin", cli_version, "bin_vulnerability_scanner")
    if not os.path.exists(binary_path):
        raise MissingDependenciesException(f"Missing Vulnerability Scanner Binary: {cli_version}")

    scan_path = args.directory
    if args.docker_image:
        scan_path = args.docker_image

    command = [binary_path, "-p", scan_path, "--output-path", output_path]
    skipped_path = args.skip_paths + global_config_data["pathToIgnore"]

    if args.debug:
        command.append("--debug")

    if args.only_fixed:
        command.append("--only-fixed")

    if args.generate_sbom:
        command.append("--generate-sbom")

    if args.sbom_format:
        command.extend(["--sbom-format", args.sbom_format])

    if args.vuln_format:
        command.extend(["--vuln-format", args.vuln_format])

    if args.docker_image:
        command.extend(["--scan-image", "--platform", args.platform, "--registry", args.registry])

        if args.username != "":
            command.extend(["--username", args.username])
        if args.password != "":
            command.extend(["--password", args.password])
            command.extend(["--auth-token", args.password])

        if not args.all_layers:
            command.append("--squashed")

    if "version" in vuln_db and vuln_db["version"] > 0:
        command.extend(["--db-version", str(vuln_db["version"])])
        command.extend(["--db-url", vuln_db["signed_url"]])
        command.extend(["--db-built-timestamp-tag", vuln_db["built_timestamp"]])
        command.extend(["--db-sha", vuln_db["checksum"]])

    if len(skipped_path) > 0:
        for path in skipped_path:
            if not args.docker_image:
                if os.path.isabs(path):
                    LOGGER.error("absolute paths are not allowed in skip-path")
                    sys.exit(1)

                """
                in secretScanner we join skip path with base-dir using
                filepath.join() 
                behaviour of filepath.join()
                 1: join(A, *) => A/*
                 2: join(A, ./*) => A/*
                 3: join(A, */) => A/*
                  
                To make it consistent with other scanners we replace
                */ with ./* and **/ with ./**
                inside sbom-scanner => ./ prefix is removed from skip-path

                so final behaviour (same for iac and secret-scanning as well)
                 * = *         ** = **
                ./* = *       ./** = **
                */ = *        **/ = **
                """
                if path == "*" or path == "**":
                    path = "./" + path

                if path == "*/" or path == "**/":
                    path = "./" + path[:-1]

                # done as vul scanner only allows ./, */, **/, if user just want to pass the dir or filename
                if not path.startswith("*/") and not path.startswith("**/") and not path.startswith("./"):
                    path = "./" + path
            command.extend(["--skip-path", path])
    return command

def print_issue_on_console(issues, quiet, verbose):
    if verbose:
        print(json.dumps(issues, indent=4))
        return

    table_data = []
    for issue in issues:
        if quiet:
            print(
                LogColors.FAIL + f'[ISSUE]\tFound {issue["id"]} inside package {issue["package"]} for version {issue["version"]}' + LogColors.ENDC)
        else:
            table_data.append(generate_table_row(issue))

    if len(table_data) > 0:
        print(tabulate(table_data, headers="keys", tablefmt="psql"))


def generate_table_row(issue):
    severity_color = get_severity_color(issue["severity"].upper())
    fixed_version = "-"
    if issue["fixedVersions"] != "":
        fixed_version = issue["fixedVersions"]

    required_length = get_wrapping_length(7)
    return {
        "Id": wrap_text(str(issue["id"]), required_length),
        "Severity": wrap_text(severity_color + issue["severity"] + LogColors.ENDC, required_length),
        "Package": wrap_text(str(issue["package"]), required_length),
        "Version": wrap_text(str(issue["version"]), required_length),
        "Fixed In": wrap_text(str(fixed_version), required_length),
        "CVSSScore": wrap_text(str(issue["CVSSScore"]), required_length),
        "Link": wrap_text(issue["link"], required_length)
    }


def save_issues_on_file(args, filtered_issues, global_config_data):
    output_file, output_format = get_output_file_and_format(args)
    if output_format == OUTPUT_FORMAT.SARIF:
        filtered_issues = convert_issues_to_sarif(filtered_issues)
    print_output_on_file(filtered_issues, output_file, output_format)


def convert_issues_to_sarif(issues):
    rules = []
    results = []
    sarif_result = get_sarif_payload("SentinelOne CNS Vulnerability Scanner")

    for issue in issues:
        rule, result = get_sarif_rule_and_result(issue)
        rules.append(rule)
        results.append(result)
    sarif_result["runs"][0]["results"] = results
    sarif_result["runs"][0]["tool"]["driver"]["rules"] = rules
    return sarif_result


def get_sarif_rule_and_result(issue):
    title = issue.get("id", "")
    short_description = issue.get("id", "")
    full_description = issue.get("description", "")
    help_uri = issue.get("link", "")
    severity = issue.get("severity", "LOW").upper()
    file_path = issue.get("paths", "")[0]
    package = issue.get("package", "")
    version = issue.get("version", "")
    fixed_versions = issue.get("fixedVersions", "")

    return {
        "id": title,
        "name": title,
        "shortDescription": {
            "text": short_description
        },
        "fullDescription": {
            "text": full_description
        },
        **({"helpUri": help_uri} if help_uri is not None and len(help_uri) != 0 else {}),
        "properties": {
            "security-severity": SEVERITY_MAP[severity]
        },
        "help": {
            "text": f"Id: {title}\nSeverity: {severity}\nPackage: {package}\nVersion: {version}\nFix Version: {fixed_versions}\nLocation: {file_path}\nLink: {help_uri}\n",
            "markdown": f"| Id | Severity | Package | Version | Fix Version | Location | Link |\n| --- | --- | --- | --- | --- | --- | --- |\n| {title} | {severity} | {package} | {version} | {fixed_versions} | {file_path} | [url]({help_uri})|\n"
        }
    }, {
        "ruleId": title,
        "message": {
            "text": title,
        },
        "locations": [{
            "physicalLocation": {
                "artifactLocation": {
                    "uri": file_path
                }
            }
        }]
    }
