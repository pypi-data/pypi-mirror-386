# This file was generated by Nuitka

# Stubs included by default
from __future__ import annotations
from __future__ import annotations
from cfg import DLModelManager, WellLogTrainConfig
from components import Decoder1D, Encoder1D
from quantizers import VectorQuantizerEMA
from sklearn.preprocessing import StandardScaler
from typing import Any, Optional, Sequence
from typing_extensions import Self
import math
import numpy
import torch
import torch.nn
import torch.nn.functional

def _build_sinusoidal_pos_embed(n_positions: int, dim: int) -> torch.Tensor:
    ...

class VQVAE1D(DLModelManager):
    def __init__(self: Self, cfg_dict: Dict[str, Any], scaler_mean: list[float], scaler_std: list[float]) -> None: ...
    def get_scaler(self: Self, scaler_mean: Any, scaler_std: Any) -> Any: ...
    def encode(self: Self, x: torch.Tensor) -> Any: ...
    def _forward(self: Self, x_ch_all: torch.Tensor) -> Any: ...
    def _dtw_loss(self: Self, quary: Any, q_z_reshaped: torch.Tensor, dense_info: torch.Tensor) -> Any: ...
    def forward(self: Self, x_ch_all: torch.Tensor) -> Any: ...
    def predict(self: Self, x_ch_all: torch.Tensor) -> Any: ...
    def training_step(self: Self, batch: Any, batch_idx: Any) -> Any: ...
    def validation_step(self: Self, batch: Any, batch_idx: Any) -> Any: ...
    def test_step(self: Self, batch: Any, batch_idx: Any) -> Any: ...
    def predict_step(self: Self, batch: Any, batch_idx: Any) -> Any: ...
    def configure_optimizers(self: Self) -> Any: ...

class WellLogModel:
    def __init__(self: Self, cfg_dict: Dict[str, Any], input_size: int) -> None: ...
    def forward(self: Self, x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor: ...

class WellLogModel_Lightning(DLModelManager):
    def __init__(self: Self, cfg_dict: Dict[str, Any], scaler_mean: list[float], scaler_std: list[float], label_ch_idxs: list[int], label_ch_names: list[str], mlm_prob: float, block_size: int, density_threshold: float) -> None: ...
    def get_scaler(self: Self) -> Any: ...
    @staticmethod
    def _masked_mean(x: torch.Tensor, mask: torch.Tensor) -> torch.Tensor: ...
    @staticmethod
    def _compute_masked_mae(pred: torch.Tensor, target: torch.Tensor, valid_mask: torch.Tensor) -> torch.Tensor: ...
    @staticmethod
    def _compute_masked_rmse(pred: torch.Tensor, target: torch.Tensor, valid_mask: torch.Tensor) -> torch.Tensor: ...
    def _generate_density_block_mask(self: Self, orig_mask: torch.Tensor, block_size: int) -> torch.Tensor: ...
    def forward(self: Self, x: torch.Tensor, cls_mask: torch.Tensor) -> torch.Tensor: ...
    def training_step(self: Self, batch: Any, batch_idx: Any) -> Any: ...
    def validation_step(self: Self, batch: Any, batch_idx: Any) -> Any: ...
    def test_step(self: Self, batch: Any, batch_idx: Any) -> Any: ...
    def configure_optimizers(self: Self) -> Any: ...
    def predict_step(self: Self, batch: Any, batch_idx: Any) -> Any: ...


__name__ = ...



# Modules used internally, to allow implicit dependencies to be seen:
import __future__
import math
import sklearn
import sklearn.preprocessing
import sklearn.preprocessing.StandardScaler
import numpy
import typing
import torch
import torch.nn
import torch.nn.functional