# This file was generated by Nuitka

# Stubs included by default
from __future__ import annotations
from abc import ABC, abstractmethod
from typing import Any
from typing_extensions import Self
import math
import torch
import torch.nn
import torch.nn.functional

def sinkhorn(cost: Any, n_iters: Any, epsilon: Any, is_distributed: Any) -> Any:
    ...

class BaseQuantizer(ABC):
    def __init__(self: Self, num_embeddings: Any, embedding_dim: Any, commitment_cost: Any, track_indices: Any) -> None: ...
    def reset_used_indices(self: Self) -> Any: ...
    def get_used_indices(self: Self) -> Any: ...
    def prune_codebook_for_inference(self: Self, used_indices: Any) -> Any: ...
    def _quantize(self: Self, z: Any) -> Any: ...
    def _compute_perplexity(self: Self, encodings: Any) -> Any: ...
    def forward(self: Self, z: Any) -> Any: ...

class GlobalPattenQuery(BaseQuantizer):
    def __init__(self: Self, num_embeddings: Any, embedding_dim: Any, commitment_cost: Any, track_indices: Any) -> None: ...
    def _quantize(self: Self, z: Any) -> Any: ...

class VectorQuantizer(BaseQuantizer):
    def __init__(self: Self, num_embeddings: Any, embedding_dim: Any, commitment_cost: Any, track_indices: Any) -> None: ...
    def _quantize(self: Self, z: Any) -> Any: ...

class VectorQuantizerEMA(BaseQuantizer):
    def __init__(self: Self, num_embeddings: Any, embedding_dim: Any, commitment_cost: Any, decay: Any, epsilon: Any, track_indices: Any) -> None: ...
    def _quantize(self: Self, z: Any) -> Any: ...

class VectorQuantizerOptVQ(BaseQuantizer):
    def __init__(self: Self, num_embeddings: Any, embedding_dim: Any, commitment_cost: Any, loss_q_type: Any, use_norm: Any, track_indices: Any) -> None: ...
    def compute_codebook_loss(self: Self, query: Any, indices: Any, nearest_ref: Any, query2ref: Any) -> Any: ...
    def _quantize(self: Self, z: Any) -> Any: ...

class VectorQuantizerSinkhorn(BaseQuantizer):
    def __init__(self: Self, num_embeddings: Any, embedding_dim: Any, commitment_cost: Any, loss_q_type: Any, epsilon: Any, n_iters: Any, normalize_mode: Any, use_prob: Any, use_norm: Any, track_indices: Any) -> None: ...
    def normalize(self: Self, A: Any, dim: Any, mode: Any) -> Any: ...
    def compute_codebook_loss(self: Self, query: Any, indices: Any, nearest_ref: Any, query2ref: Any) -> Any: ...
    def _quantize(self: Self, z: Any) -> Any: ...

class VectorQuantizerSinkhornEMA(BaseQuantizer):
    def __init__(self: Self, num_embeddings: Any, embedding_dim: Any, commitment_cost: Any, epsilon: Any, n_iters: Any, normalize_mode: Any, use_prob: Any, decay: Any, epsilon_ema: Any, track_indices: Any) -> None: ...
    def normalize(self: Self, A: Any, dim: Any, mode: Any) -> Any: ...
    def _quantize(self: Self, z: Any) -> Any: ...

class ResidualVQ:
    def __init__(self: Self) -> None: ...
    def reset_used_indices(self: Self) -> Any: ...
    def get_used_indices(self: Self) -> Any: ...
    def prune_codebook_for_inference(self: Self, all_used_indices: Any) -> Any: ...
    def forward(self: Self, z: Any) -> Any: ...


__name__ = ...



# Modules used internally, to allow implicit dependencies to be seen:
import math
import torch
import torch.nn
import torch.nn.functional
import abc
import torch.distributed