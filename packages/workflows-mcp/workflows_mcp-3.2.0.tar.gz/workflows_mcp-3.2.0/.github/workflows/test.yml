name: Integration Tests

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:  # Allow manual trigger

jobs:
  integration-tests:
    name: Run Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 15

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          enable-cache: true
          cache-dependency-glob: "pyproject.toml"

      - name: Install dependencies
        run: uv sync --all-extras

      - name: Run integration tests
        run: |
          uv run pytest tests/integration/ -v \
            -m "not slow" \
            --cov=workflows_mcp \
            --cov-report=xml \
            --cov-report=term-missing \
            --tb=short

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        if: always()
        with:
          file: ./coverage.xml
          flags: integration
          name: integration-tests
          fail_ci_if_error: false

      - name: Test report summary
        if: always()
        run: |
          echo "## Integration Test Results" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "✅ Integration tests completed" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### Coverage" >> $GITHUB_STEP_SUMMARY
          echo "See coverage report in artifacts" >> $GITHUB_STEP_SUMMARY

  unit-tests:
    name: Run Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          enable-cache: true
          cache-dependency-glob: "pyproject.toml"

      - name: Install dependencies
        run: uv sync --all-extras

      - name: Run unit tests
        run: |
          uv run pytest tests/ -v \
            --ignore=tests/integration/ \
            --cov=workflows_mcp \
            --cov-report=xml \
            --cov-report=term-missing \
            --tb=short

      - name: Upload coverage to Codecov
        uses: codecov/codecov-action@v4
        if: always()
        with:
          file: ./coverage.xml
          flags: unit
          name: unit-tests
          fail_ci_if_error: false

  quality-gates:
    name: Code Quality Gates
    runs-on: ubuntu-latest
    timeout-minutes: 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          enable-cache: true

      - name: Install dependencies
        run: uv sync --all-extras

      - name: Run ruff linting
        run: uv run ruff check src/workflows_mcp/ tests/

      - name: Run ruff formatting check
        run: uv run ruff format --check src/workflows_mcp/ tests/

      - name: Run mypy type checking
        run: uv run mypy src/workflows_mcp/
        continue-on-error: true

  workflow-validation:
    name: Validate Built-in Workflows
    runs-on: ubuntu-latest
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.12'

      - name: Install uv
        uses: astral-sh/setup-uv@v4
        with:
          enable-cache: true

      - name: Install dependencies
        run: uv sync --all-extras

      - name: Validate workflow YAML syntax
        run: |
          uv run python -c "
          from pathlib import Path
          import yaml

          templates_dir = Path('src/workflows_mcp/templates')
          workflows = list(templates_dir.rglob('*.yaml'))

          print(f'Validating {len(workflows)} workflow files...')
          errors = []

          for workflow_file in workflows:
              try:
                  with open(workflow_file) as f:
                      yaml.safe_load(f)
                  print(f'✅ {workflow_file.relative_to(templates_dir)}')
              except Exception as e:
                  errors.append(f'❌ {workflow_file.relative_to(templates_dir)}: {e}')
                  print(errors[-1])

          if errors:
              print(f'\n{len(errors)} workflow(s) failed validation')
              exit(1)
          else:
              print(f'\n✅ All {len(workflows)} workflows validated successfully')
          "

      - name: Test workflow loading
        run: |
          uv run python -c "
          from workflows_mcp.engine.registry import WorkflowRegistry
          from pathlib import Path

          registry = WorkflowRegistry()
          templates_dir = Path('src/workflows_mcp/templates')
          result = registry.load_from_directory(str(templates_dir))

          if not result.is_success:
              print(f'❌ Failed to load workflows: {result.error}')
              exit(1)

          workflows = registry.list_all_metadata()
          print(f'✅ Successfully loaded {len(workflows)} workflows')

          for wf in sorted(workflows, key=lambda x: x['name']):
              print(f'  - {wf[\"name\"]} ({wf.get(\"tags\", [])})')
          "

  test-summary:
    name: Test Suite Summary
    runs-on: ubuntu-latest
    needs: [integration-tests, unit-tests, quality-gates, workflow-validation]
    if: always()

    steps:
      - name: Check test results
        run: |
          echo "## Test Suite Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test Suite | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|------------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Integration Tests | ${{ needs.integration-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Unit Tests | ${{ needs.unit-tests.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Quality Gates | ${{ needs.quality-gates.result }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Workflow Validation | ${{ needs.workflow-validation.result }} |" >> $GITHUB_STEP_SUMMARY

          # Fail if any critical job failed
          if [ "${{ needs.integration-tests.result }}" != "success" ] || \
             [ "${{ needs.unit-tests.result }}" != "success" ] || \
             [ "${{ needs.quality-gates.result }}" != "success" ] || \
             [ "${{ needs.workflow-validation.result }}" != "success" ]; then
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "❌ **Some tests failed - see details above**" >> $GITHUB_STEP_SUMMARY
            exit 1
          else
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "✅ **All tests passed successfully**" >> $GITHUB_STEP_SUMMARY
          fi
