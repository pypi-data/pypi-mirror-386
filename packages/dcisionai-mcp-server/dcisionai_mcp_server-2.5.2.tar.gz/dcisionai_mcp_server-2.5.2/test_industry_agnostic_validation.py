#!/usr/bin/env python3
"""
Comprehensive Test Suite for Industry-Agnostic Validation
Tests the enhanced pattern-breaking strategies and security fixes.
"""

import pytest
import asyncio
import json
from typing import Dict, Any, List
import sys
import os

# Add the package to the path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'dcisionai_mcp_server'))

from tools import DcisionAITools


class TestIndustryAgnosticValidation:
    """Test suite for validating industry-agnostic AI reasoning."""
    
    @pytest.fixture
    def tools(self):
        """Initialize DcisionAI tools for testing."""
        return DcisionAITools()
    
    @pytest.mark.asyncio
    async def test_novel_problem_archaeology(self, tools):
        """Test completely novel problem - archaeology."""
        problem = """
        I'm an archaeologist planning excavation sites. I have 10 potential 
        dig sites, 5 teams, and a budget of $500K. Each site has different 
        expected historical value and required time. Maximize archaeological 
        value while staying within budget and time constraints.
        """
        
        # Test intent classification
        intent_result = await tools.classify_intent(problem)
        assert intent_result['status'] == 'success'
        intent = intent_result['result']
        assert 'archaeology' in intent.get('intent', '').lower() or \
               'excavation' in intent.get('intent', '').lower()
        
        # Test data analysis
        data_result = await tools.analyze_data(problem, intent)
        assert data_result['status'] == 'success'
        data = data_result['result']
        assert data['readiness_score'] > 0.7
        
        # Test model building
        model_result = await tools.build_model(problem, intent, data)
        assert model_result['status'] == 'success'
        model_spec = model_result['result']
        
        # Validate model has no manufacturing templates
        variables = model_spec.get('variables', [])
        assert len(variables) > 0
        
        # Check reasoning steps exist and are problem-specific
        reasoning = model_spec.get('reasoning_steps', {})
        assert 'step1_decision_analysis' in reasoning
        assert 'step2_constraint_analysis' in reasoning
        assert 'step3_objective_analysis' in reasoning
        assert 'step4_variable_design' in reasoning
        assert 'step5_constraint_formulation' in reasoning
        assert 'step6_objective_formulation' in reasoning
        assert 'step7_validation' in reasoning
        
        # Check that reasoning is archaeology-specific
        decision_analysis = reasoning['step1_decision_analysis'].lower()
        assert 'archaeological' in decision_analysis or \
               'excavation' in decision_analysis or \
               'dig' in decision_analysis or \
               'site' in decision_analysis
        
        print("âœ… Archaeology test passed - AI reasoned from first principles")
    
    @pytest.mark.asyncio
    async def test_manufacturing_pattern_breaking(self, tools):
        """Test that manufacturing doesn't use learned patterns."""
        problem = """
        I manage a factory with 2 machines. Machine A produces 100 units/hour 
        at $50/hour. Machine B produces 90 units/hour at $60/hour. I need 
        500 units. Minimize cost.
        """
        
        # Test model building
        model_result = await tools.build_model(problem)
        assert model_result['status'] == 'success'
        model_spec = model_result['result']
        
        # Check reasoning shows explicit decision analysis
        reasoning = model_spec.get('reasoning_steps', {})
        assert reasoning, "Reasoning steps must be present"
        
        # Check that AI shows step-by-step reasoning
        assert 'step1_decision_analysis' in reasoning
        assert 'step2_constraint_analysis' in reasoning
        assert 'step3_objective_analysis' in reasoning
        
        # Variables should be used in constraints
        variables = {v['name'] for v in model_spec.get('variables', [])}
        constraints = model_spec.get('constraints', [])
        
        # Every variable should appear in at least one constraint or objective
        objective_expr = model_spec.get('objective', {}).get('expression', '')
        constraint_exprs = [c.get('expression', '') for c in constraints]
        
        for var in variables:
            appears_in_objective = var in objective_expr
            appears_in_constraint = any(var in expr for expr in constraint_exprs)
            assert appears_in_objective or appears_in_constraint, \
                f"Variable {var} is not used anywhere!"
        
        print("âœ… Manufacturing pattern-breaking test passed")
    
    @pytest.mark.asyncio
    async def test_simulation_identifies_uncertainty(self, tools):
        """Test that Monte Carlo identifies problem-specific uncertainty."""
        
        # Portfolio problem
        portfolio_solution = {
            'objective_value': 0.12,
            'optimal_values': {'x1': 0.4, 'x2': 0.3, 'x3': 0.3}
        }
        
        portfolio_desc = "Allocate $100K across 3 stocks to maximize return while limiting risk to 15%"
        
        # Run simulation
        mc_result = tools._run_monte_carlo_simulation(
            portfolio_solution, 
            portfolio_desc, 
            num_trials=1000
        )
        
        # Check that uncertainty sources are identified
        assert 'uncertainty_sources' in mc_result
        uncertainties = mc_result['uncertainty_sources']
        
        # For portfolio, should identify return/volatility uncertainty
        uncertainty_names = [u['name'].lower() for u in uncertainties]
        has_financial_uncertainty = any(
            term in ' '.join(uncertainty_names) 
            for term in ['return', 'volatility', 'market', 'price']
        )
        assert has_financial_uncertainty, \
            f"Portfolio simulation should identify financial uncertainty, got: {uncertainty_names}"
        
        # Manufacturing problem
        manufacturing_solution = {
            'objective_value': 250,
            'optimal_values': {'z1': 5, 'z2': 0}
        }
        
        manufacturing_desc = "Produce 500 units using 2 production lines to minimize cost"
        
        mc_result2 = tools._run_monte_carlo_simulation(
            manufacturing_solution,
            manufacturing_desc,
            num_trials=1000
        )
        
        uncertainties2 = mc_result2['uncertainty_sources']
        uncertainty_names2 = [u['name'].lower() for u in uncertainties2]
        
        # For manufacturing, should identify demand/capacity uncertainty
        has_manufacturing_uncertainty = any(
            term in ' '.join(uncertainty_names2)
            for term in ['demand', 'capacity', 'production', 'breakdown']
        )
        assert has_manufacturing_uncertainty, \
            f"Manufacturing simulation should identify operational uncertainty, got: {uncertainty_names2}"
        
        print("âœ… Simulation uncertainty identification test passed")
    
    @pytest.mark.asyncio
    async def test_business_validation_no_keywords(self, tools):
        """Test that business validation uses AI, not keywords."""
        
        # Portfolio with sum > 1.0 (invalid)
        optimal_values = {'x1': 0.5, 'x2': 0.6, 'x3': 0.3}  # Sum = 1.4
        problem = "Allocate investment across 3 stocks"
        model_spec = {
            'variables': [
                {'name': 'x1', 'description': 'Allocation to stock A'},
                {'name': 'x2', 'description': 'Allocation to stock B'},
                {'name': 'x3', 'description': 'Allocation to stock C'}
            ]
        }
        
        validation = tools._validate_business_logic(optimal_values, problem, model_spec)
        
        # Should catch that allocations don't sum to 1.0
        assert not validation['is_valid'], "Should detect invalid portfolio allocation"
        assert len(validation['errors']) > 0, "Should provide error message"
        
        print("âœ… Business validation AI reasoning test passed")
    
    def test_safe_expression_evaluation(self, tools):
        """Test that expression evaluation is safe from code injection."""
        
        # Test normal expression
        result = tools._calculate_objective_value("50 * x1 + 60 * x2", {"x1": 5, "x2": 3})
        assert result == 50 * 5 + 60 * 3  # 430
        
        # Test complex expression
        result = tools._calculate_objective_value("(x1 + x2) * x3", {"x1": 2, "x2": 3, "x3": 4})
        assert result == (2 + 3) * 4  # 20
        
        # Test that malicious code is not executed
        try:
            # This should fail safely, not execute code
            result = tools._calculate_objective_value("__import__('os').system('echo hacked')", {"x1": 1})
            # If we get here, the malicious code was not executed (good)
            assert True, "Malicious code was safely blocked"
        except ValueError:
            # This is expected - the expression should be rejected
            assert True, "Malicious code was safely rejected"
        
        print("âœ… Safe expression evaluation test passed")
    
    def test_robust_json_parsing(self, tools):
        """Test robust JSON parsing with nested structures."""
        
        # Test normal JSON
        json_text = '{"key": "value", "nested": {"inner": "data"}}'
        result = tools._safe_json_parse(json_text)
        assert result["key"] == "value"
        assert result["nested"]["inner"] == "data"
        
        # Test JSON in code block
        json_text = '```json\n{"key": "value"}\n```'
        result = tools._safe_json_parse(json_text)
        assert result["key"] == "value"
        
        # Test malformed JSON
        json_text = '{"key": "value", "incomplete": }'
        result = tools._safe_json_parse(json_text)
        assert "parse_error" in result or "raw_response" in result
        
        print("âœ… Robust JSON parsing test passed")
    
    @pytest.mark.asyncio
    async def test_diverse_problem_types(self, tools):
        """Test that the platform works for diverse problem types."""
        
        test_problems = [
            {
                "name": "Blood Bank Inventory",
                "problem": "I manage a blood bank with 4 blood types (A+, A-, B+, B-). Each type has different shelf life and demand. Minimize waste while meeting demand.",
                "expected_keywords": ["blood", "inventory", "shelf", "demand"]
            },
            {
                "name": "Music Festival Scheduling",
                "problem": "I'm organizing a music festival with 20 bands, 3 stages, and 8-hour schedule. Each band has different setup time and popularity. Maximize audience satisfaction.",
                "expected_keywords": ["band", "stage", "schedule", "audience"]
            },
            {
                "name": "Space Mission Planning",
                "problem": "I'm planning a Mars mission with 5 payloads, 3 rockets, and $2B budget. Each payload has different weight and scientific value. Maximize scientific return.",
                "expected_keywords": ["payload", "rocket", "budget", "scientific"]
            }
        ]
        
        for test_case in test_problems:
            # Test model building
            model_result = await tools.build_model(test_case["problem"])
            assert model_result['status'] == 'success', f"Failed for {test_case['name']}"
            
            model_spec = model_result['result']
            reasoning = model_spec.get('reasoning_steps', {})
            
            # Check that reasoning is problem-specific
            decision_analysis = reasoning.get('step1_decision_analysis', '').lower()
            has_expected_keywords = any(
                keyword in decision_analysis 
                for keyword in test_case["expected_keywords"]
            )
            assert has_expected_keywords, \
                f"Reasoning for {test_case['name']} should contain problem-specific keywords"
            
            print(f"âœ… {test_case['name']} test passed")
        
        print("âœ… Diverse problem types test passed")


def run_tests():
    """Run all tests."""
    print("ðŸ§ª Running Industry-Agnostic Validation Tests...")
    print("=" * 60)
    
    # Create test instance
    test_instance = TestIndustryAgnosticValidation()
    tools = DcisionAITools()
    
    # Run async tests
    async def run_async_tests():
        await test_instance.test_novel_problem_archaeology(tools)
        await test_instance.test_manufacturing_pattern_breaking(tools)
        await test_instance.test_simulation_identifies_uncertainty(tools)
        await test_instance.test_business_validation_no_keywords(tools)
        await test_instance.test_diverse_problem_types(tools)
    
    # Run sync tests
    test_instance.test_safe_expression_evaluation(tools)
    test_instance.test_robust_json_parsing(tools)
    
    # Run async tests
    asyncio.run(run_async_tests())
    
    print("=" * 60)
    print("ðŸŽ‰ All tests passed! Industry-agnostic validation is working correctly.")


if __name__ == "__main__":
    run_tests()
