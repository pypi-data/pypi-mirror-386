[job]
model = "Qwen/Qwen3-32B"
# Optionally set here; you can also pass --dataset
# data = "/abs/path/to/train.jsonl"

[compute]
gpu_type = "H100"
gpu_count = 4
nodes = 1

[data]
# Optional; forwarded into metadata.effective_config.data.topology
topology = { container_count = 4 }

[training]
mode = "sft_offline"
use_qlora = true

[training.validation]
enabled = true
evaluation_strategy = "steps"
eval_steps = 20
save_best_model_at_end = true
metric_for_best_model = "val.loss"
greater_is_better = false

[hyperparameters]
n_epochs = 1
per_device_batch = 1
gradient_accumulation_steps = 64
sequence_length = 4096
learning_rate = 5e-6
warmup_ratio = 0.03

[hyperparameters.parallelism]
use_deepspeed = true
deepspeed_stage = 2
bf16 = true
fp16 = false
fsdp = false
