# General
AGENTMAKE_USERNAME=
AGENTMAKE_ASSISTANT_NAME=
DEVELOPER_MODE=FALSE
DEFAULT_AI_BACKEND=ollama
# DEFAULT_SYSTEM_MESSAGE could be set to `auto`, `reasoning`, a pre-defined tag or a custom string
DEFAULT_SYSTEM_MESSAGE=
DEFAULT_FOLLOW_UP_PROMPT="Ask a follow-up question and answer it."
DEFAULT_OPEN_COMMAND=
DEFAULT_TEXT_EDITOR=etextedit
DEFAULT_REFINE_INSTRUCTION=
DEFAULT_CUSTOM_LABEL=
# e.g. search/searxng is a better alternative, but extra setup is needed
DEFAULT_ONLINE_SEARCH_TOOL=search/google
DEFAULT_MAXIMUM_ONLINE_SEARCHES=5
DEFAULT_FABRIC_PATTERNS_PATH=
# Markdown theme option['abap', 'algol', 'algol_nu', 'arduino', 'autumn', 'bw', 'borland', 'coffee', 'colorful', 'default', 'dracula', 'emacs', 'friendly_grayscale', 'friendly', 'fruity', 'github-dark', 'gruvbox-dark', 'gruvbox-light', 'igor', 'inkpot', 'lightbulb', 'lilypond', 'lovelace', 'manni', 'material', 'monokai', 'murphy', 'native', 'nord-darker', 'nord', 'one-dark', 'paraiso-dark', 'paraiso-light', 'pastie', 'perldoc', 'rainbow_dash', 'rrt', 'sas', 'solarized-dark', 'solarized-light', 'staroffice', 'stata-dark', 'stata-light', 'tango', 'trac', 'vim', 'vs', 'xcode', 'zenburn']
DEFAULT_MARKDOWN_THEME="github-dark"

# Backend: anthropic
ANTHROPIC_API_KEY=
ANTHROPIC_MODEL=claude-3-5-sonnet-latest
ANTHROPIC_TEMPERATURE=0.3
ANTHROPIC_MAX_TOKENS=8192

# Backend: azure
# Deploy AI models with Azure service: https://ai.azure.com/github
# use Azure OpenAI Service endpoint for running OpenAI models; the endpoint should look like https://resource_name.openai.azure.com/
AZURE_OPENAI_API_VERSION=2024-10-21
AZURE_OPENAI_API_KEY=
AZURE_OPENAI_API_ENDPOINT=
AZURE_OPENAI_MODEL=gpt-5-chat
AZURE_OPENAI_TEMPERATURE=0.3
AZURE_OPENAI_MAX_TOKENS=16384
# use Azure AI inference endpoint for running DeepSeek-R1 and Phi-4; the endpoint should look like https://resource_name.services.ai.azure.com/models
AZURE_ANY_API_KEY=
AZURE_ANY_API_ENDPOINT=
AZURE_ANY_MODEL=grok-3
AZURE_ANY_TEMPERATURE=0.3
AZURE_ANY_MAX_TOKENS=16384
AZURE_DEEPSEEK_API_KEY=
AZURE_DEEPSEEK_API_ENDPOINT=
AZURE_DEEPSEEK_MODEL=DeepSeek-R1
AZURE_DEEPSEEK_TEMPERATURE=0.3
AZURE_DEEPSEEK_MAX_TOKENS=8000
AZURE_PHI_API_KEY=
AZURE_PHI_API_ENDPOINT=
AZURE_PHI_MODEL=Phi-4
AZURE_PHI_TEMPERATURE=0.3
AZURE_PHI_MAX_TOKENS=8000
AZURE_EMBEDDING_MODEL=azure-text-embedding-3-large
AZURE_VISUAL_MODEL=gpt-4o
AZURE_VISUAL_TEMPERATURE=0.3
AZURE_VISUAL_MAX_TOKENS=4096
AZURE_DALLE_API_KEY=
AZURE_DALLE_API_ENDPOINT=
AZURE_DALLE_MODEL=dall-e-3
AZURE_WHISPER_API_KEY=
AZURE_WHISPER_API_ENDPOINT=
AZURE_WHISPER_MODEL=whisper

# Backend: cohere
# support multiple Cohere API keys, use comma ',' as separator
COHERE_API_KEY=
COHERE_MODEL=command-r-plus
COHERE_TEMPERATURE=0.3
COHERE_MAX_TOKENS=4000

# Backend: custom
# e.g. agentmake
CUSTOM_API_KEY=
# e.g. "http://localhost:11434/v1" for Ollama
CUSTOM_API_ENDPOINT=
CUSTOM_MODEL=
CUSTOM_TEMPERATURE=0.3
CUSTOM_MAX_TOKENS=4000

# Backend: deepseek
DEEPSEEK_API_KEY=
DEEPSEEK_MODEL=deepseek-chat
DEEPSEEK_TEMPERATURE = 0.3
DEEPSEEK_MAX_TOKENS = 8000

# Backend: github
# support multiple Github API keys, use comma ',' as separator
GITHUB_API_KEY=
GITHUB_MODEL=gpt-4o
GITHUB_TEMPERATURE=0.3
GITHUB_MAX_TOKENS=4000
GITHUB_ANY_API_KEY=
GITHUB_ANY_MODEL=DeepSeek-V3
GITHUB_ANY_TEMPERATURE=0.3
GITHUB_ANY_MAX_TOKENS=4000
GITHUB_VISUAL_MODEL=gpt-4o
GITHUB_VISUAL_TEMPERATURE=0.3
GITHUB_VISUAL_MAX_TOKENS=4000

# Backend: googleai
GOOGLEAI_API_KEY=${GEMINI_API_KEY}
GOOGLEAI_MODEL=gemini-2.5-flash
GOOGLEAI_TEMPERATURE=0.3
GOOGLEAI_MAX_TOKENS=8192
GOOGLEAI_VISUAL_MODEL=gemini-2.5-flash
GOOGLEAI_VISUAL_TEMPERATURE=0.3
GOOGLEAI_VISUAL_MAX_TOKENS=4096

# Backend: vertexai
# Enter the path of your google application credentials json file as the API key
# Default JSON credentials path: ~/agentmake/google_application_credentials.json
VERTEXAI_API_KEY=${GOOGLEAI_API_KEY}
VERTEXAI_API_PROJECT_ID=
VERTEXAI_API_SERVICE_LOCATION=us-central1
VERTEXAI_MODEL=gemini-2.5-flash
VERTEXAI_TEMPERATURE=0.3
VERTEXAI_MAX_TOKENS=8192
VERTEXAI_VISUAL_MODEL=gemini-2.5-flash
VERTEXAI_VISUAL_TEMPERATURE=0.3
VERTEXAI_VISUAL_MAX_TOKENS=8192
VERTEXAI_IMAGEN_MODEL=imagen-3.0-generate-002

# Backend: groq
# support multiple Groq API keys, use comma ',' as separator
GROQ_API_KEY=
GROQ_MODEL=llama-3.3-70b-versatile
GROQ_TEMPERATURE=0.3
GROQ_MAX_TOKENS=32768
GROQ_VISUAL_MODEL=llama-3.2-90b-vision-preview
GROQ_VISUAL_TEMPERATURE=0.3
GROQ_VISUAL_MAX_TOKENS=8192

# Backend: llamacpp
LLAMACPP_API_ENDPOINT=http://127.0.0.1:8080/v1
LLAMACPP_TEMPERATURE=0.3
LLAMACPP_MAX_TOKENS=2048

# Backend: mistral
# support multiple Mistral API keys, use comma ',' as separator
MISTRAL_API_KEY=
MISTRAL_MODEL=mistral-large-latest
MISTRAL_TEMPERATURE=0.3
MISTRAL_MAX_TOKENS=8000
MISTRAL_VISUAL_MODEL=pixtral-large-latest
MISTRAL_VISUAL_TEMPERATURE=0.3
MISTRAL_VISUAL_MAX_TOKENS=8000

# Backend: ollama
# Empty the value of OLLAMA_ENDPOINT if you want to use dynamic local ip
OLLAMA_ENDPOINT="http://127.0.0.1:11434"
OLLAMA_MODEL=llama3.2
OLLAMA_TEMPERATURE=0.3
OLLAMA_MAX_TOKENS=-1
OLLAMA_CONTEXT_WINDOW=2048
OLLAMA_BATCH_SIZE=512
OLLAMA_KEEP_ALIVE=5m
OLLAMA_VISUAL_MODEL=granite3.2-vision
OLLAMA_VISUAL_TEMPERATURE=0.3
OLLAMA_VISUAL_MAX_TOKENS=-1

# Backend: openai
OPENAI_API_KEY=
OPENAI_MODEL=gpt-5
OPENAI_TEMPERATURE=1
OPENAI_MAX_TOKENS=128000
OPENAI_VISUAL_MODEL=gpt-4o
OPENAI_VISUAL_TEMPERATURE=0.3
OPENAI_VISUAL_MAX_TOKENS=4096

# Backend: xai
XAI_API_KEY=
XAI_MODEL=grok-4
XAI_TEMPERATURE=0.3
# maximum context window size: 131072 tokens, including both input and output tokens
XAI_MAX_TOKENS=8192
XAI_VISUAL_MODEL=grok-2-vision-latest
XAI_VISUAL_TEMPERATURE=0.3
XAI_VISUAL_MAX_TOKENS=4096

# Other APIs
# support multiple Tavily API keys, use comma ',' as separator
TAVILY_API_KEY=
# support multiple OpenWeatherMap API keys, use comma ',' as separator
OPENWEATHERMAP_API_KEY=

# Tool: RAG
# openai models: "text-embedding-3-small", "text-embedding-3-large", "text-embedding-ada-002"
# azure openai models: "azure-text-embedding-3-small", "azure-text-embedding-3-large", "azure-text-embedding-ada-002"
# cohere models: "embed-english-v3.0", "embed-english-light-v3.0", "embed-multilingual-v3.0", "embed-multilingual-light-v3.0"
# mistral models: "mistral-embed"
# genai/vertexai/googleai models: "text-embedding-004"
# ollama models: https://ollama.com/search?c=embedding
RAG_EMBEDDING_MODEL="paraphrase-multilingual"
RAG_CHUNK_SIZE=1200
RAG_CHUNK_OVERLAP_SIZE=200
RAG_QUERY_TOP_K=5

# Tool: Perplexica
PERPLEXICA_HOST="http://127.0.0.1"
PERPLEXICA_PORT=3000
# local embedding options "xenova-bge-small-en-v1.5", "xenova-gte-small", "xenova-bert-base-multilingual-uncased"
PERPLEXICA_LOCAL_EMBEDDING="xenova-bge-small-en-v1.5"
# optimizationMode: 'speed' | 'balanced'
PERPLEXICA_OPTIMIZATION_MODE=speed
# focusMode: `webSearch`, `academicSearch`, `writingAssistant`, `wolframAlphaSearch`, `youtubeSearch`, `redditSearch`
PERPLEXICA_FOCUS_MODE=webSearch

# Tool: SearXNG
SEARXNG_HOST="http://127.0.0.1"
SEARXNG_PORT=4000

# Tool: Stable Diffusion CPP
FLUX_IMAGE_MODEL="flux1-dev-q4_k.gguf"
FLUX_IMAGE_WIDTH=1920
FLUX_IMAGE_HEIGHT=1088
FLUX_IMAGE_SAMPLE_STEPS=20

# Tool: whisper
# https://github.com/openai/whisper/tree/main#available-models-and-languages
# "tiny", "base", "small", "medium", "large"
WHISPER_MODEL=base

# Tool: install python package
# specify the path of `pip` command located in the virtual environment, if any
PIP_PATH=

# Tool: Memory
MEMORY_TYPES="general,instruction,fact,event,concept"

# Tool: UBA API
UBA_API_LOCAL_PORT=8080
UBA_API_ENDPOINT="https://bible.gospelchurchuk.org/plain"
UBA_API_TIMEOUT=10
UBA_API_PRIVATE_KEY=

# BibleMate AI
BIBLEMATE_STATIC_TOKEN=
BIBLEMATE_MCP_PUBLIC_KEY=
BIBLEMATE_MCP_PRIVATE_KEY=
BIBLEMATE_MCP_ISSUER=
BIBLEMATE_MCP_AUDIENCE=

# Tool: Text-to-speech
TTS_EDGE_VOICE="en-GB-SoniaNeural"
TTS_EDGE_RATE=1.0
TTS_USE_VLC=
TTS_VLC_RATE=1.0
TTS_USE_MPV=
# https://cloud.google.com/text-to-speech/docs/voices
TTS_TERMUX_LANGUAGE="en-US"
TTS_TERMUX_RATE=1.0
TTS_PIPER_VOICE="en_GB-aru-medium"
TTS_PIPER_OPTIONS=

# Agents
MAXIMUM_ACTION_ROUND=20
MAXIMUM_AUTO_HEALING=3
# DEFAULT_TOOL_CHOICES="@chat @search/google @files/extract_text @install_python_package @magic"
DEFAULT_TOOL_CHOICES=

# Custom instruction for running interactive mode
CUSTOM_INSTRUCTION_1=
CUSTOM_INSTRUCTION_2=
CUSTOM_INSTRUCTION_3=
CUSTOM_INSTRUCTION_4=
CUSTOM_INSTRUCTION_5=
CUSTOM_INSTRUCTION_6=
CUSTOM_INSTRUCTION_7=
CUSTOM_INSTRUCTION_8=
CUSTOM_INSTRUCTION_9=
CUSTOM_INSTRUCTION_10=