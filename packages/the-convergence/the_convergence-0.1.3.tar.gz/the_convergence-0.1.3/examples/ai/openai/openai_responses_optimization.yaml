# OpenAI Responses API Optimization Configuration
# For GPT-5, O3, and other next-generation models
# Documentation: https://platform.openai.com/docs/api-reference/responses
#
# The Responses API is OpenAI's recommended API for all new projects.
# It provides better reasoning support, tool calling, and more flexible output formats.
#
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# SETUP (3 Steps):
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
#
# 1. Get API key: https://platform.openai.com/api-keys
#
# 2. Set environment variable (choose one):
#    a) Export manually:
#       export OPENAI_API_KEY="sk-..."
#    
#    b) Create .env file:
#       echo "OPENAI_API_KEY=sk-..." > .env
#    
#    c) Let CLI prompt you (interactive):
#       convergence optimize [this file]  # It will ask for the key
#
# 3. Run optimization:
#    convergence optimize examples/ai/openai/openai_responses_optimization.yaml
#
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
# API KEYS FOR AGENT SOCIETY (Optional - see society section below):
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━
#
# If you enable agent society features (RLP/SAO), you need an LLM for those agents.
#
# Option A: Reuse same API key (test OpenAI, agents use OpenAI):
#   society.llm.api_key_env: "OPENAI_API_KEY"  ← Same key for both!
#
# Option B: Use different provider (test OpenAI, agents use Gemini):
#   export GEMINI_API_KEY="AIza..."  ← Separate key
#   society.llm.model: "gemini/gemini-2.0-flash-exp"
#   society.llm.api_key_env: "GEMINI_API_KEY"
#
# The CLI will prompt you for any missing keys when you run the optimization.
# ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━

api:
  name: "openai_responses"
  endpoint: "https://api.openai.com/v1/responses"
  auth:
    type: "bearer"
    token_env: "OPENAI_API_KEY"
  
  request:
    method: "POST"
    headers:
      Content-Type: "application/json"
    timeout_seconds: 60
  
  response:
    success_field: "status"  # Check if status == "completed"
    result_field: "output"   # Array containing the response message
    error_field: "error"
  
  # Rate limit handling
  # OpenAI enforces: RPM (requests/min), TPM (tokens/min), RPD, TPD
  # The system uses exponential backoff retries automatically
  # Free tier: ~60 RPM, ~150K TPM | Tier 1: ~500 RPM, ~200K TPM

search_space:
  parameters:
    # Model selection
    # Note: Update these to match your available models
    model:
      type: "categorical"
      values:
        - "gpt-4.1-mini"        # Fast, cost-efficient
        - "gpt-4.1-nano"        # Even smaller/faster
        # Common alternatives:
        # - "gpt-4o"            # Most capable
        # - "gpt-4o-mini"       # Balanced
        # - "gpt-4-turbo"       # High performance

      
# Evaluation configuration
evaluation:
  # Test cases
  test_cases:
    path: "openai_responses_tests.json"
    # Path is relative to this YAML file (examples/ai/openai/)
    # Each test case has:
    # - input: prompt/instructions for the model
    # - expected: criteria for evaluation
    # - metadata: category, difficulty, weight
  
  # Local evaluator (in same folder as this YAML)
  custom_evaluator:
    enabled: true
    module: "openai_responses"
    function: "score_openai_response"
    # For reasoning models, you can also use:
    # function: "score_reasoning_response"
  
  # Metrics to optimize
  metrics:
    # Response quality (most important)
    response_quality:
      weight: 0.40
      type: "higher_is_better"
      function: "custom"
      # Uses the custom evaluator above
    
    # Latency (response time)
    latency_ms:
      weight: 0.25
      type: "lower_is_better"
      threshold: 5000
      # Prefer responses under 5 seconds
    
    # Cost per call
    cost_per_call:
      weight: 0.20
      type: "lower_is_better"
      budget_per_call: 0.10
      # Target: under $0.10 per call
    
    # Token efficiency (output quality per token)
    token_efficiency:
      weight: 0.15
      type: "higher_is_better"
      function: "custom"
      # Measures quality/cost ratio

# Optimization strategy
# MINIMAL TEST MODE - Just ~10 API calls to test the system
optimization:
  algorithm: "mab_evolution"
  
  # Multi-Armed Bandit (exploration phase)
  mab:
    strategy: "thompson_sampling"
    exploration_rate: 0.2
  
  # Evolutionary Algorithm (refinement phase)
  evolution:
    population_size: 2    # Just 2 configs (the 2 models)
    generations: 1        # Single generation (no evolution)
    mutation_rate: 0.0
    crossover_rate: 0.0
    elite_size: 1
  
  # Execution settings
  execution:
    experiments_per_generation: 1   # LITERALLY JUST 2 API CALLS
    parallel_workers: 1              # Sequential (no parallel to avoid rate limits)
    max_retries: 3                   # Retry with exponential backoff
    early_stopping:
      enabled: false
    
    # ULTRA-CONSERVATIVE MODE FOR TESTING
    # - 2 API calls total (one per model)
    # - Sequential execution (no parallel)
    # - Max retries with backoff for any errors

# Output configuration
output:
  save_path: "./results/openai_responses_optimization"
  save_all_experiments: true
  formats: ["json", "markdown", "csv"]
  
  visualizations:
    - "score_over_time"
    - "parameter_importance"
    - "pareto_front"
    - "cost_vs_quality"
  
  export_best_config:
    enabled: true
    format: "python"
    output_path: "./best_openai_config.py"

# Advanced: Agent Society (optional)
# Enables AI agents to help optimize using RLP (Reasoning Learning Process) and SAO (Self-Alignment)
society:
  enabled: true  # Set to true to enable
  auto_generate_agents: true
  
  # LLM configuration for agent society (RLP/SAO features)
  llm:
    # Option 1: Use same provider as API being tested
    model: "openai/gpt-4o-mini"           # LiteLLM format: provider/model
    api_key_env: "OPENAI_API_KEY"         # Reuse same API key!
    
    # Option 2: Use different provider (e.g., cheaper Gemini for society)
    # model: "gemini/gemini-2.0-flash-exp"
    # api_key_env: "GEMINI_API_KEY"       # Set separately: export GEMINI_API_KEY="AIza..."
    
    temperature: 0.7
    max_tokens: 1000
  
  learning:
    rlp_enabled: true   # Reasoning-based Learning Process
    sao_enabled: true   # Self-Alignment Optimization
  collaboration:
    enabled: true
    strategy: "debate"

# Legacy tracking (session-based optimization history)
legacy:
  enabled: true
  
  # Session ID to group related runs
  session_id: "openai_responses_optimization"
  
  # Tracking backend (SQLite + CSV by default)
  tracking_backend: "builtin"
  
  # Storage paths
  sqlite_path: "./data/legacy.db"
  export_dir: "./legacy_exports"
  export_formats: ["csv", "json"]
  
  # Optional: MLflow integration
  mlflow_config:
    enabled: false
    tracking_uri: "http://localhost:5000"
    experiment_name: "openai-optimization"
  
  # Optional: Aim integration  
  aim_config:
    enabled: false
    repo_path: "./aim_repo"
  
  # Optional: Weave integration (separate from main weave config)
  weave_config:
    enabled: false
    project_name: "openai-legacy-tracking"

# Weave integration (observability)
weave:
  enabled: true
  project_name: "openai-responses-optimization"
  # Track all experiments in Weights & Biases Weave

# Usage:
# 1. Set your OpenAI API key:
#    export OPENAI_API_KEY="sk-..."
#
# 2. Run optimization:
#    convergence optimize examples/ai/openai_responses_optimization.yaml
#
# 3. View results in ./results/openai_responses_optimization/
#
# 4. Check legacy tracking:
#    sqlite3 ./data/legacy.db "SELECT * FROM runs;"
#    ls -la ./legacy_exports/
#
# ULTRA-MINIMAL TEST MODE:
# - LITERALLY JUST 2 API CALLS (one per model)
# - Sequential execution (no parallel requests)
# - Perfect for testing auth, debugging errors
# - Cost: ~$0.001-0.01
#
# Common errors to check:
# - 401: Invalid API key (check OPENAI_API_KEY env var)
# - 429: Rate limit hit (wait 60 seconds, or reduce parallel_workers)
# - 404: Model not available (check model names in values list)
# - 400: Invalid request format (check API compatibility)
#
# For real optimization, change:
#   experiments_per_generation: 50+
#   parallel_workers: 5+
#   generations: 5+
