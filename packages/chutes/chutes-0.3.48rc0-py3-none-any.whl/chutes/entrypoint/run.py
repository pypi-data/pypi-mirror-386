"""
Run a chute, automatically handling encryption/decryption via GraVal.
"""

import os
import re
import asyncio
import aiohttp
import sys
import jwt
import ctypes
import time
import uuid
import inspect
import typer
import psutil
import base64
import socket
import secrets
import subprocess
import threading
import traceback
import orjson as json
from functools import lru_cache
from loguru import logger
from typing import Optional, Any
from datetime import datetime
from pydantic import BaseModel
from ipaddress import ip_address
from uvicorn import Config, Server
from fastapi import FastAPI, Request, Response, status, HTTPException
from fastapi.responses import ORJSONResponse
from starlette.middleware.base import BaseHTTPMiddleware
from prometheus_client import generate_latest, CONTENT_TYPE_LATEST
from substrateinterface import Keypair, KeypairType
from chutes.entrypoint._shared import load_chute, miner, authenticate_request
from chutes.entrypoint.ssh import setup_ssh_access
from chutes.chute import ChutePack, Job
from chutes.util.context import is_local
from cryptography.hazmat.primitives.ciphers import Cipher, algorithms, modes
from cryptography.hazmat.backends import default_backend
from cryptography.hazmat.primitives import padding


CFSV_PATH = os.path.join(os.path.dirname(__file__), "..", "cfsv")
graval_middleware_instance = None


class ConcurrencyUpdate(BaseModel):
    concurrency: int


@lru_cache(maxsize=1)
def get_netnanny_ref():
    netnanny = ctypes.CDLL(None, ctypes.RTLD_GLOBAL)
    netnanny.generate_challenge_response.argtypes = [ctypes.c_char_p]
    netnanny.generate_challenge_response.restype = ctypes.c_char_p
    netnanny.verify_challenge.argtypes = [ctypes.c_char_p, ctypes.c_char_p]
    netnanny.verify_challenge.restype = ctypes.c_int
    netnanny.initialize_network_control.argtypes = []
    netnanny.initialize_network_control.restype = ctypes.c_int
    netnanny.unlock_network.argtypes = []
    netnanny.unlock_network.restype = ctypes.c_int
    netnanny.lock_network.argtypes = []
    netnanny.lock_network.restype = ctypes.c_int
    return netnanny


async def update_concurrency(request: Request) -> dict[str, Any]:
    """
    Dynamically update the concurrency value of a chute.
    """
    global graval_middleware_instance
    update_data = ConcurrencyUpdate(**request.state.decrypted)
    new_concurrency = update_data.concurrency
    if new_concurrency < 1 or new_concurrency > 128:
        raise ValueError("Concurrency must be between 1 and 128")
    if graval_middleware_instance is not None:
        old_concurrency = getattr(graval_middleware_instance, "concurrency", None)
        graval_middleware_instance.concurrency = new_concurrency
        if hasattr(graval_middleware_instance, "_semaphore"):
            graval_middleware_instance._semaphore = asyncio.Semaphore(new_concurrency)
        logger.info(f"Updated concurrency from {old_concurrency} to {new_concurrency}")
        response_data = {
            "success": True,
            "old_concurrency": old_concurrency,
            "new_concurrency": new_concurrency,
        }
    else:
        response_data = {"success": False, "message": "Middleware not initialized"}
    if hasattr(request.state, "_encrypt"):
        return {"json": request.state._encrypt(json.dumps(response_data))}
    return response_data


async def get_concurrency(request: Request) -> dict[str, Any]:
    """
    Get the current concurrency value.
    """
    global graval_middleware_instance
    current_concurrency = None
    if graval_middleware_instance:
        current_concurrency = getattr(graval_middleware_instance, "concurrency", None)
    response_data = {"concurrency": current_concurrency}
    if hasattr(request.state, "_encrypt"):
        return {"json": request.state._encrypt(json.dumps(response_data))}
    return response_data


def get_all_process_info():
    """
    Return running process info.
    """
    processes = {}
    for proc in psutil.process_iter(["pid", "name", "cmdline", "open_files", "create_time"]):
        try:
            info = proc.info
            info["open_files"] = [f.path for f in proc.open_files()]
            info["create_time"] = datetime.fromtimestamp(proc.create_time()).strftime(
                "%Y-%m-%d %H:%M:%S"
            )
            info["environ"] = dict(proc.environ())
            processes[str(proc.pid)] = info
        except (psutil.NoSuchProcess, psutil.AccessDenied):
            continue
    return Response(
        content=json.dumps(processes).decode(),
        media_type="application/json",
    )


def get_env_sig(request: Request):
    """
    Environment signature check.
    """
    import chutes.envcheck as envcheck

    return Response(
        content=envcheck.signature(request.state.decrypted["salt"]),
        media_type="text/plain",
    )


def get_env_dump(request: Request):
    """
    Base level environment check, running processes and things.
    """
    import chutes.envcheck as envcheck

    key = bytes.fromhex(request.state.decrypted["key"])
    return Response(
        content=envcheck.dump(key),
        media_type="text/plain",
    )


async def get_metrics():
    """
    Get the latest prometheus metrics.
    """
    return Response(generate_latest(), media_type=CONTENT_TYPE_LATEST)


async def get_devices():
    """
    Fetch device information.
    """
    return [miner().get_device_info(idx) for idx in range(miner()._device_count)]


async def process_device_challenge(request: Request, challenge: str):
    """
    Process a GraVal device info challenge string.
    """
    return Response(
        content=miner().process_device_info_challenge(challenge),
        media_type="text/plain",
    )


async def process_fs_challenge(request: Request):
    """
    Process a filesystem challenge.
    """
    challenge = FSChallenge(**request.state.decrypted)
    return Response(
        content=miner().process_filesystem_challenge(
            filename=challenge.filename,
            offset=challenge.offset,
            length=challenge.length,
        ),
        media_type="text/plain",
    )


async def process_netnanny_challenge(request: Request):
    """
    Process a NetNanny challenge.
    """
    challenge = request.state.decrypted.get("challenge", "foo")
    netnanny = get_netnanny_ref()
    return Response(
        content=netnanny.generate_challenge_response(challenge),
        media_type="text/plain",
    )


async def handle_slurp(request: Request, chute_module):
    """
    Read part or all of a file.
    """
    slurp = Slurp(**request.state.decrypted)
    if slurp.path == "__file__":
        source_code = inspect.getsource(chute_module)
        return Response(
            content=base64.b64encode(source_code.encode()).decode(),
            media_type="text/plain",
        )
    elif slurp.path == "__run__":
        source_code = inspect.getsource(sys.modules[__name__])
        return Response(
            content=base64.b64encode(source_code.encode()).decode(),
            media_type="text/plain",
        )
    if not os.path.isfile(slurp.path):
        if os.path.isdir(slurp.path):
            if hasattr(request.state, "_encrypt"):
                return {"json": request.state._encrypt(json.dumps({"dir": os.listdir(slurp.path)}))}
            return {"dir": os.listdir(slurp.path)}
        raise HTTPException(
            status_code=status.HTTP_404_NOT_FOUND,
            detail="Path not found: {slurp.path}",
        )
    response_bytes = None
    with open(slurp.path, "rb") as f:
        f.seek(slurp.start_byte)
        if slurp.end_byte is None:
            response_bytes = f.read()
        else:
            response_bytes = f.read(slurp.end_byte - slurp.start_byte)
    response_data = {"contents": base64.b64encode(response_bytes).decode()}
    if hasattr(request.state, "_encrypt"):
        return {"json": request.state._encrypt(json.dumps(response_data))}
    return response_data


async def pong(request: Request) -> dict[str, Any]:
    """
    Echo incoming request as a liveness check.
    """
    if hasattr(request.state, "_encrypt"):
        return {"json": request.state._encrypt(json.dumps(request.state.decrypted))}
    return request.state.decrypted


async def get_token(request: Request) -> dict[str, Any]:
    """
    Fetch a token, useful in detecting proxies between the real deployment and API.
    """
    endpoint = request.state.decrypted.get(
        "endpoint", "https://api.chutes.ai/instances/token_check"
    )
    salt = request.state.decrypted.get("salt", 42)
    async with aiohttp.ClientSession(trust_env=True) as session:
        async with session.get(endpoint, params={"salt": salt}) as resp:
            if hasattr(request.state, "_encrypt"):
                return {"json": request.state._encrypt(await resp.text())}
            return await resp.json()


async def generate_filesystem_hash(salt: str, exclude_file: str, mode: str = "full"):
    """
    Generate a hash of the filesystem, in either sparse or full mode.
    """
    fsv_hash = None
    logger.info(
        f"Running filesystem verification challenge in {mode=} using {salt=} excluding {exclude_file}"
    )
    process = await asyncio.create_subprocess_exec(
        CFSV_PATH,
        "challenge",
        "/",
        salt,
        mode,
        "/etc/chutesfs.index",
        exclude_file,
        stdout=asyncio.subprocess.PIPE,
        stderr=asyncio.subprocess.PIPE,
    )
    stdout, stderr = await process.communicate()
    if process.returncode != 0:
        raise subprocess.CalledProcessError(
            process.returncode,
            [
                CFSV_PATH,
                "challenge",
                "/",
                salt,
                mode,
                "/etc/chutesfs.index",
                exclude_file,
            ],
            output=stdout.decode("utf-8"),
            stderr=stderr.decode("utf-8"),
        )
    stdout_text = stdout.decode("utf-8")
    for line in stdout_text.strip().split("\n"):
        if line.startswith("RESULT:"):
            fsv_hash = line.split("RESULT:")[1].strip()
            logger.success(f"Filesystem verification hash: {fsv_hash}")
            break
    if not fsv_hash:
        logger.warning("Failed to extract filesystem verification hash from cfsv output")
        raise Exception("Failed to generate filesystem challenge response.")
    return fsv_hash


class Slurp(BaseModel):
    path: str
    start_byte: Optional[int] = 0
    end_byte: Optional[int] = None


class FSChallenge(BaseModel):
    filename: str
    length: int
    offset: int


class DevMiddleware(BaseHTTPMiddleware):
    async def dispatch(self, request: Request, call_next):
        """
        Dev/dummy dispatch.
        """
        args = await request.json() if request.method in ("POST", "PUT", "PATCH") else None
        request.state.serialized = False
        request.state.decrypted = args
        return await call_next(request)


class GraValMiddleware(BaseHTTPMiddleware):
    def __init__(self, app: FastAPI, concurrency: int = 1, symmetric_key: str = None):
        """
        Initialize a semaphore for concurrency control/limits.
        """
        super().__init__(app)
        self.concurrency = concurrency
        self.lock = asyncio.Lock()
        self.requests_in_flight = {}
        self.symmetric_key = symmetric_key
        self.app = app

    async def _dispatch(self, request: Request, call_next):
        """
        Transparently handle decryption and verification.
        """
        if request.client.host == "127.0.0.1":
            return await call_next(request)

        # Internal endpoints.
        path = request.scope.get("path", "")
        if path.endswith("/_metrics"):
            ip = ip_address(request.client.host)
            is_private = (
                ip.is_private
                or ip.is_loopback
                or ip.is_link_local
                or ip.is_multicast
                or ip.is_reserved
                or ip.is_unspecified
            )
            if not is_private:
                return ORJSONResponse(
                    status_code=status.HTTP_401_UNAUTHORIZED,
                    content={"detail": "go away (internal)"},
                )
            else:
                return await call_next(request)

        # Authentication...
        body_bytes, failure_response = await authenticate_request(request)
        if failure_response:
            return failure_response

        # Decrypt using the symmetric key we exchanged via GraVal.
        if request.method in ("POST", "PUT", "PATCH"):
            try:
                iv = bytes.fromhex(body_bytes[:32].decode())
                cipher = Cipher(
                    algorithms.AES(self.symmetric_key),
                    modes.CBC(iv),
                    backend=default_backend(),
                )
                unpadder = padding.PKCS7(128).unpadder()
                decryptor = cipher.decryptor()
                decrypted_data = (
                    decryptor.update(base64.b64decode(body_bytes[32:])) + decryptor.finalize()
                )
                unpadded_data = unpadder.update(decrypted_data) + unpadder.finalize()
                try:
                    request.state.decrypted = json.loads(unpadded_data)
                except Exception:
                    request.state.decrypted = json.loads(unpadded_data.rstrip(bytes(range(1, 17))))
                request.state.iv = iv
            except ValueError as exc:
                return ORJSONResponse(
                    status_code=status.HTTP_451_UNAVAILABLE_FOR_LEGAL_REASONS,
                    content={"detail": f"Decryption failed: {exc}"},
                )

            def _encrypt(plaintext: bytes):
                if isinstance(plaintext, str):
                    plaintext = plaintext.encode()
                padder = padding.PKCS7(128).padder()
                cipher = Cipher(
                    algorithms.AES(self.symmetric_key),
                    modes.CBC(iv),
                    backend=default_backend(),
                )
                padded_data = padder.update(plaintext) + padder.finalize()
                encryptor = cipher.encryptor()
                encrypted_data = encryptor.update(padded_data) + encryptor.finalize()
                return base64.b64encode(encrypted_data).decode()

            request.state._encrypt = _encrypt

        return await call_next(request)

    async def dispatch(self, request: Request, call_next):
        """
        Rate-limiting wrapper around the actual dispatch function.
        """
        request.request_id = str(uuid.uuid4())
        request.state.serialized = request.headers.get("X-Chutes-Serialized") is not None

        # Pass regular, special paths through.
        if (
            request.scope.get("path", "").endswith(
                (
                    "/_fs_challenge",
                    "/_fs_hash",
                    "/_metrics",
                    "/_ping",
                    "/_procs",
                    "/_slurp",
                    "/_device_challenge",
                    "/_devices",
                    "/_env_sig",
                    "/_env_dump",
                    "/_token",
                    "/_dump",
                    "/_sig",
                    "/_toca",
                    "/_eslurp",
                )
            )
            or request.client.host == "127.0.0.1"
        ):
            return await self._dispatch(request, call_next)

        # Decrypt encrypted paths, which could be one of the above as well.
        path = request.scope.get("path", "")
        try:
            iv = bytes.fromhex(path[1:33])
            cipher = Cipher(
                algorithms.AES(self.symmetric_key),
                modes.CBC(iv),
                backend=default_backend(),
            )
            unpadder = padding.PKCS7(128).unpadder()
            decryptor = cipher.decryptor()
            decrypted_data = decryptor.update(bytes.fromhex(path[33:])) + decryptor.finalize()
            actual_path = unpadder.update(decrypted_data) + unpadder.finalize()
            actual_path = actual_path.decode().rstrip("?")
            logger.info(f"Decrypted request path: {actual_path} from input path: {path}")
            request.scope["path"] = actual_path
        except ValueError:
            return ORJSONResponse(
                status_code=status.HTTP_404_NOT_FOUND,
                content={"detail": f"Bad path: {path}"},
            )

        # Now pass the decrypted special paths through.
        if request.scope.get("path", "").endswith(
            (
                "/_fs_challenge",
                "/_fs_hash",
                "/_metrics",
                "/_ping",
                "/_procs",
                "/_slurp",
                "/_device_challenge",
                "/_devices",
                "/_env_sig",
                "/_env_dump",
                "/_token",
                "/_dump",
                "/_sig",
                "/_toca",
                "/_eslurp",
            )
        ):
            return await self._dispatch(request, call_next)

        # Concurrency control with timeouts in case it didn't get cleaned up properly.
        async with self.lock:
            now = time.time()
            if len(self.requests_in_flight) >= self.concurrency:
                purge_keys = []
                for key, val in self.requests_in_flight.items():
                    if now - val >= 600:
                        logger.warning(
                            f"Assuming this request is no longer in flight, killing: {key}"
                        )
                        purge_keys.append(key)
                if purge_keys:
                    for key in purge_keys:
                        self.requests_in_flight.pop(key, None)
                    self.requests_in_flight[request.request_id] = now
                else:
                    return ORJSONResponse(
                        status_code=status.HTTP_429_TOO_MANY_REQUESTS,
                        content={
                            "error": "RateLimitExceeded",
                            "detail": f"Max concurrency exceeded: {self.concurrency}, try again later.",
                        },
                    )
            else:
                self.requests_in_flight[request.request_id] = now

        # Perform the actual request.
        response = None
        try:
            response = await self._dispatch(request, call_next)
            if hasattr(response, "body_iterator"):
                original_iterator = response.body_iterator

                async def wrapped_iterator():
                    try:
                        async for chunk in original_iterator:
                            yield chunk
                    except Exception as exc:
                        logger.warning(f"Unhandled exception in body iterator: {exc}")
                        self.requests_in_flight.pop(request.request_id, None)
                        raise
                    finally:
                        self.requests_in_flight.pop(request.request_id, None)

                response.body_iterator = wrapped_iterator()
                return response
            return response
        finally:
            if not response or not hasattr(response, "body_iterator"):
                self.requests_in_flight.pop(request.request_id, None)


def start_dummy_socket(port_mapping, symmetric_key):
    """
    Start a dummy socket based on the port mapping configuration to validate ports.
    """
    proto = port_mapping["proto"].lower()
    internal_port = port_mapping["internal_port"]
    response_text = f"response from {proto} {internal_port}"
    if proto in ["tcp", "http"]:
        return start_tcp_dummy(internal_port, symmetric_key, response_text)
    return start_udp_dummy(internal_port, symmetric_key, response_text)


def encrypt_response(symmetric_key, plaintext):
    """
    Encrypt the response using AES-CBC with PKCS7 padding.
    """
    padder = padding.PKCS7(128).padder()
    new_iv = secrets.token_bytes(16)
    cipher = Cipher(
        algorithms.AES(symmetric_key),
        modes.CBC(new_iv),
        backend=default_backend(),
    )
    padded_data = padder.update(plaintext.encode()) + padder.finalize()
    encryptor = cipher.encryptor()
    encrypted_data = encryptor.update(padded_data) + encryptor.finalize()
    response_cipher = base64.b64encode(encrypted_data).decode()
    return new_iv, response_cipher


def start_tcp_dummy(port, symmetric_key, response_plaintext):
    """
    TCP port check socket.
    """

    def tcp_handler():
        sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        try:
            sock.bind(("0.0.0.0", port))
            sock.listen(1)
            logger.info(f"TCP socket listening on port {port}")
            conn, addr = sock.accept()
            logger.info(f"TCP connection from {addr}")
            data = conn.recv(1024)
            logger.info(f"TCP received: {data.decode('utf-8', errors='ignore')}")
            iv, encrypted_response = encrypt_response(symmetric_key, response_plaintext)
            full_response = f"{iv.hex()}|{encrypted_response}".encode()
            conn.send(full_response)
            logger.info(f"TCP sent encrypted response on port {port}: {full_response=}")
            conn.close()
        except Exception as e:
            logger.info(f"TCP socket error on port {port}: {e}")
            raise
        finally:
            sock.close()
            logger.info(f"TCP socket on port {port} closed")

    thread = threading.Thread(target=tcp_handler, daemon=True)
    thread.start()
    return thread


def start_udp_dummy(port, symmetric_key, response_plaintext):
    """
    UDP port check socket.
    """

    def udp_handler():
        sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
        sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)
        try:
            sock.bind(("0.0.0.0", port))
            logger.info(f"UDP socket listening on port {port}")
            data, addr = sock.recvfrom(1024)
            logger.info(f"UDP received from {addr}: {data.decode('utf-8', errors='ignore')}")
            iv, encrypted_response = encrypt_response(symmetric_key, response_plaintext)
            full_response = f"{iv.hex()}|{encrypted_response}".encode()
            sock.sendto(full_response, addr)
            logger.info(f"UDP sent encrypted response on port {port}")
        except Exception as e:
            logger.info(f"UDP socket error on port {port}: {e}")
            raise
        finally:
            sock.close()
            logger.info(f"UDP socket on port {port} closed")

    thread = threading.Thread(target=udp_handler, daemon=True)
    thread.start()
    return thread


async def _gather_devices_and_initialize(
    token: str,
    host: str,
    port_mappings: list[dict[str, Any]],
    chute_module: Any,
) -> tuple[str, str, dict[str, Any]]:
    """
    Gather the GPU info assigned to this pod, submit with our one-time token to get GraVal seed.
    """
    from chutes.envdump import DUMPER

    # Build the GraVal request based on the GPUs that were actually assigned to this pod.
    logger.info("Collecting GPUs and port mappings...")
    body = {"gpus": [], "port_mappings": port_mappings, "host": host}
    for idx in range(miner()._device_count):
        body["gpus"].append(miner().get_device_info(idx))
    token_data = jwt.decode(token, options={"verify_signature": False})
    url = token_data.get("url")
    key = token_data.get("env_key", "a" * 32)
    exclude_file = "/dev/null"
    if chute_module and hasattr(chute_module, "__file__"):
        exclude_file = os.path.abspath(chute_module.__file__)
        logger.info(f"Excluding chute file from verification: {exclude_file}")

    logger.info("Collecting full envdump...")
    body["env"] = DUMPER.dump(key)
    body["code"] = DUMPER.slurp(key, exclude_file, 0, 0)
    body["fsv"] = await generate_filesystem_hash(token_data["sub"], exclude_file, mode="full")

    # Disk space.
    disk_gb = token_data.get("disk_gb", 10)
    logger.info(f"Checking disk space availability: {disk_gb}GB required")
    try:
        _ = subprocess.run(
            [CFSV_PATH, "sizetest", "/tmp", str(disk_gb)],
            capture_output=True,
            text=True,
            check=True,
        )
        logger.success(f"Disk space check passed: {disk_gb}GB available in /tmp")
    except subprocess.CalledProcessError as e:
        logger.error(f"Disk space check failed: {e.stderr}")
        raise Exception(f"Insufficient disk space: {disk_gb}GB required in /tmp")
    except Exception as e:
        logger.error(f"Error checking disk space: {e}")
        raise Exception(f"Failed to verify disk space availability: {e}")

    # Fetch the challenges.
    async with aiohttp.ClientSession(raise_for_status=True) as session:
        logger.info(f"Collected all environment data, submitting to validator: {url}")
        async with session.post(url, headers={"Authorization": token}, json=body) as resp:
            init_params = await resp.json()
            logger.success(f"Successfully fetched initialization params: {init_params=}")

            # First, we initialize graval on all GPUs from the provided seed.
            miner()._graval_seed = init_params["seed"]
            iterations = init_params.get("iterations", 1)
            logger.info(f"Generating proofs from seed={miner()._graval_seed}")
            proofs = miner().prove(miner()._graval_seed, iterations=iterations)

            # Use GraVal to extract the symmetric key from the challenge.
            sym_key = init_params["symmetric_key"]
            bytes_ = base64.b64decode(sym_key["ciphertext"])
            iv = bytes_[:16]
            cipher = bytes_[16:]
            logger.info("Decrypting payload via proof challenge matrix...")
            device_index = [
                miner().get_device_info(i)["uuid"] for i in range(miner()._device_count)
            ].index(sym_key["uuid"])
            symmetric_key = bytes.fromhex(
                miner().decrypt(
                    init_params["seed"],
                    cipher,
                    iv,
                    len(cipher),
                    device_index,
                )
            )

            # Now, we can respond to the URL by encrypting a payload with the symmetric key and sending it back.
            plaintext = sym_key["response_plaintext"]
            new_iv, response_cipher = encrypt_response(symmetric_key, plaintext)
            logger.success(
                f"Completed PoVW challenge, sending back: {plaintext=} "
                f"as {response_cipher=} where iv={new_iv.hex()}"
            )

            # Start up dummy sockets to test port mappings.
            dummy_socket_threads = []
            for port_map in port_mappings:
                if port_map.get("default"):
                    continue
                dummy_socket_threads.append(start_dummy_socket(port_map, symmetric_key))

            # Post the response to the challenge, which returns job data (if any).
            async with session.put(
                url,
                headers={"Authorization": token},
                json={
                    "response": response_cipher,
                    "iv": new_iv.hex(),
                    "proof": proofs,
                },
                raise_for_status=False,
            ) as resp:
                if resp.ok:
                    logger.success("Successfully negotiated challenge response!")
                    return exclude_file, symmetric_key, await resp.json()
                else:
                    # log down the reason of failure to the challenge
                    detail = await resp.text(encoding="utf-8", errors="replace")
                    logger.error(f"Failed: {resp.reason} ({resp.status}) {detail}")
                    resp.raise_for_status()


# Run a chute (which can be an async job or otherwise long-running process).
def run_chute(
    chute_ref_str: str = typer.Argument(
        ..., help="chute to run, in the form [module]:[app_name], similar to uvicorn"
    ),
    miner_ss58: str = typer.Option(None, help="miner hotkey ss58 address"),
    validator_ss58: str = typer.Option(None, help="validator hotkey ss58 address"),
    host: str | None = typer.Option("0.0.0.0", help="host to bind to"),
    port: int | None = typer.Option(8000, help="port to listen on"),
    logging_port: int | None = typer.Option(8001, help="logging port"),
    keyfile: str | None = typer.Option(None, help="path to TLS key file"),
    certfile: str | None = typer.Option(None, help="path to TLS certificate file"),
    debug: bool = typer.Option(False, help="enable debug logging"),
    dev: bool = typer.Option(False, help="dev/local mode"),
    dev_job_data_path: str = typer.Option(None, help="dev mode: job payload JSON path"),
    dev_job_method: str = typer.Option(None, help="dev mode: job method"),
):
    async def _run_chute():
        """
        Run the chute (or job).
        """
        # Configure net-nanny.
        try:
            netnanny = get_netnanny_ref()
            challenge = secrets.token_hex(16).encode("utf-8")
            response = netnanny.generate_challenge_response(challenge)
            if not response:
                logger.error("NetNanny validation failed: no response")
                sys.exit(137)
            if netnanny.verify_challenge(challenge, response) != 1:
                logger.error("NetNanny validation failed: invalid response")
                sys.exit(137)
            if netnanny.initialize_network_control() != 0:
                logger.error("Failed to initialize network control")
                sys.exit(137)
            if netnanny.unlock_network() != 0:
                logger.error("Failed to unlock network")
                sys.exit(137)
            logger.debug("NetNanny initialized and network unlocked")
        except (OSError, AttributeError) as e:
            logger.error(f"NetNanny library not properly loaded: {e}")
            sys.exit(137)
        if not dev and os.getenv("CHUTES_NETNANNY_UNSAFE", "") == "1":
            logger.error("NetNanny library not loaded system wide!")
            sys.exit(137)
        if not dev and os.getpid() != 1:
            logger.error(f"Must be PID 1 (container entrypoint), but got PID {os.getpid()}")
            sys.exit(137)

        # Load the chute.
        if dev:
            os.environ["CHUTES_DEV_MODE"] = "true"
        chute_module, chute = load_chute(chute_ref_str=chute_ref_str, config_path=None, debug=debug)
        if is_local():
            logger.error("Cannot run chutes in local context!")
            sys.exit(1)

        chute = chute.chute if isinstance(chute, ChutePack) else chute

        # Load token and port mappings from the environment.
        token = os.getenv("CHUTES_LAUNCH_JWT")
        port_mappings = [
            # Main chute pod.
            {
                "proto": "tcp",
                "internal_port": port,
                "external_port": port,
                "default": True,
            },
            # Logging server.
            {
                "proto": "tcp",
                "internal_port": logging_port,
                "external_port": logging_port,
                "default": True,
            },
        ]
        external_host = os.getenv("CHUTES_EXTERNAL_HOST")
        primary_port = os.getenv("CHUTES_PORT_PRIMARY")
        if primary_port and primary_port.isdigit():
            port_mappings[0]["external_port"] = int(primary_port)
        ext_logging_port = os.getenv("CHUTES_PORT_LOGGING")
        if ext_logging_port and ext_logging_port.isdigit():
            port_mappings[1]["external_port"] = int(ext_logging_port)
        for key, value in os.environ.items():
            port_match = re.match(r"^CHUTES_PORT_(TCP|UDP|HTTP)_([0-9]+)", key)
            if port_match and value.isdigit():
                port_mappings.append(
                    {
                        "proto": port_match.group(1),
                        "internal_port": int(port_match.group(2)),
                        "external_port": int(value),
                        "default": False,
                    }
                )

        # GPU verification plus job fetching.
        job_data: dict | None = None
        symmetric_key: str | None = None
        job_id: str | None = None
        job_obj: Job | None = None
        job_method: str | None = None
        job_status_url: str | None = None
        activation_url: str | None = None
        exclude_file: str | None = None
        if token:
            (
                exclude_file,
                symmetric_key,
                response,
            ) = await _gather_devices_and_initialize(
                token, external_host, port_mappings, chute_module
            )
            job_id = response.get("job_id")
            job_method = response.get("job_method")
            job_status_url = response.get("job_status_url")
            if job_method:
                job_obj = next(j for j in chute._jobs if j.name == job_method)
            job_data = response.get("job_data")
            activation_url = response.get("activation_url")

            # Secret environment variables, e.g. HF tokens for private models.
            if response.get("secrets"):
                for secret_key, secret_value in response["secrets"].items():
                    os.environ[secret_key] = secret_value

        elif not dev:
            logger.error("No GraVal token supplied!")
            sys.exit(1)

        # Configure dev method job payload/method/etc.
        if dev and dev_job_data_path:
            with open(dev_job_data_path) as infile:
                job_data = json.loads(infile.read())
            job_id = str(uuid.uuid4())
            job_method = dev_job_method
            job_obj = next(j for j in chute._jobs if j.name == dev_job_method)
            logger.info(f"Creating task, dev mode, for {job_method=}")

        # Run the chute's initialization code.
        await chute.initialize()

        # Encryption/rate-limiting middleware setup.
        if dev:
            chute.add_middleware(DevMiddleware)
        else:

            class DynamicGraValMiddleware(GraValMiddleware):
                def __init__(self, app, **kwargs):
                    super().__init__(app, **kwargs)
                    global graval_middleware_instance
                    graval_middleware_instance = self

            chute.add_middleware(
                DynamicGraValMiddleware,
                concurrency=chute.concurrency,
                symmetric_key=symmetric_key,
            )

        # Slurps and processes.
        async def _handle_slurp(request: Request):
            nonlocal chute_module

            return await handle_slurp(request, chute_module)

        @chute.on_event("startup")
        async def activate_on_startup():
            if not activation_url or dev:
                if not chute.allow_external_egress:
                    if netnanny.lock_network() != 0:
                        logger.error("Failed to unlock network")
                        sys.exit(137)
                    logger.success("Successfully enabled NetNanny.")
                return
            activated = False
            for attempt in range(10):
                await asyncio.sleep(attempt)
                try:
                    async with aiohttp.ClientSession(raise_for_status=True) as session:
                        async with session.get(
                            activation_url, headers={"Authorization": token}
                        ) as resp:
                            if resp.ok:
                                logger.success(f"Instance activated: {await resp.text()}")
                                activated = True
                                if not chute.allow_external_egress:
                                    if netnanny.lock_network() != 0:
                                        logger.error("Failed to unlock network")
                                        sys.exit(137)
                                    logger.success("Successfully enabled NetNanny.")
                                break
                            logger.error(
                                f"Instance activation failed: {resp.status=}: {await resp.text()}"
                            )
                            if resp.status == 423:
                                break

                except Exception as e:
                    logger.error(f"Unexpected error attempting to activate instance: {str(e)}")
            if not activated:
                raise Exception("Failed to activate instance, aborting...")

        async def _handle_fs_hash_challenge(request: Request):
            nonlocal exclude_file
            data = request.state.decrypted
            return {
                "result": await generate_filesystem_hash(
                    data["salt"], exclude_file, mode=data.get("mode", "sparse")
                )
            }

        # Validation endpoints.
        chute.add_api_route("/_ping", pong, methods=["POST"])
        chute.add_api_route("/_token", get_token, methods=["POST"])
        chute.add_api_route("/_metrics", get_metrics, methods=["GET"])
        chute.add_api_route("/_slurp", _handle_slurp, methods=["POST"])
        chute.add_api_route("/_procs", get_all_process_info, methods=["GET"])
        chute.add_api_route("/_env_sig", get_env_sig, methods=["POST"])
        chute.add_api_route("/_env_dump", get_env_dump, methods=["POST"])
        chute.add_api_route("/_devices", get_devices, methods=["GET"])
        chute.add_api_route("/_device_challenge", process_device_challenge, methods=["GET"])
        chute.add_api_route("/_fs_challenge", process_fs_challenge, methods=["POST"])
        chute.add_api_route("/_fs_hash", _handle_fs_hash_challenge, methods=["POST"])
        chute.add_api_route("/_netnanny_challenge", process_netnanny_challenge, methods=["POST"])
        chute.add_api_route("/_update_concurrency", update_concurrency, methods=["POST"])
        chute.add_api_route("/_get_concurrency", get_concurrency, methods=["GET"])

        # New envdump endpoints.
        import chutes.envdump as envdump

        chute.add_api_route("/_dump", envdump.handle_dump, methods=["POST"])
        chute.add_api_route("/_sig", envdump.handle_sig, methods=["POST"])
        chute.add_api_route("/_toca", envdump.handle_toca, methods=["POST"])
        chute.add_api_route("/_eslurp", envdump.handle_slurp, methods=["POST"])

        logger.success("Added all chutes internal endpoints.")

        # Job shutdown/kill endpoint.
        async def _shutdown():
            nonlocal job_obj, server
            if not job_obj:
                raise HTTPException(
                    status_code=status.HTTP_404_NOT_FOUND,
                    detail="Job task not found",
                )
            logger.warning("Shutdown requested.")
            if job_obj and not job_obj.cancel_event.is_set():
                job_obj.cancel_event.set()
            server.should_exit = True
            return {"ok": True}

        # Jobs can't be started until the full suite of validation tests run,
        # so we need to provide an endpoint for the validator to use to kick
        # it off.
        if job_id:
            job_task = None

            async def start_job_with_monitoring(**kwargs):
                nonlocal job_task
                ssh_process = None
                job_task = asyncio.create_task(job_obj.run(job_status_url=job_status_url, **kwargs))

                async def monitor_job():
                    try:
                        result = await job_task
                        logger.info(f"Job completed with result: {result}")
                    except Exception as e:
                        logger.error(f"Job failed with error: {e}")
                    finally:
                        logger.info("Job finished, shutting down server...")
                        if ssh_process:
                            try:
                                ssh_process.terminate()
                                await asyncio.sleep(0.5)
                                if ssh_process.poll() is None:
                                    ssh_process.kill()
                                logger.info("SSH server stopped")
                            except Exception as e:
                                logger.error(f"Error stopping SSH server: {e}")
                        server.should_exit = True

                # If the pod defines SSH access, enable it.
                if job_obj.ssh and job_data.get("_ssh_public_key"):
                    ssh_process = await setup_ssh_access(job_data["_ssh_public_key"])

                asyncio.create_task(monitor_job())

            await start_job_with_monitoring(**job_data)
            logger.info("Started job!")

            chute.add_api_route("/_shutdown", _shutdown, methods=["POST"])
            logger.info("Added shutdown endpoint")

        # Start the uvicorn process, whether in job mode or not.
        config = Config(
            app=chute,
            host=host or "0.0.0.0",
            port=port or 8000,
            limit_concurrency=1000,
            ssl_certfile=certfile,
            ssl_keyfile=keyfile,
        )
        server = Server(config)
        await server.serve()

    # Kick everything off
    async def _logged_run():
        """
        Wrap the actual chute execution with the logging process, which is
        kept alive briefly after the main process terminates.
        """
        from chutes.entrypoint.logger import launch_server

        if not dev:
            miner()._miner_ss58 = miner_ss58
            miner()._validator_ss58 = validator_ss58
            miner()._keypair = Keypair(ss58_address=validator_ss58, crypto_type=KeypairType.SR25519)

        def run_logging_server():
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)
            loop.run_until_complete(
                launch_server(
                    host=host or "0.0.0.0",
                    port=logging_port,
                    dev=dev,
                    certfile=certfile,
                    keyfile=keyfile,
                )
            )

        logging_thread = threading.Thread(target=run_logging_server, daemon=True)
        logging_thread.start()

        await asyncio.sleep(3)
        exception_raised = False
        try:
            await _run_chute()
        except Exception as exc:
            logger.error(
                f"Unexpected error executing _run_chute(): {str(exc)}\n{traceback.format_exc()}"
            )
            exception_raised = True
            await asyncio.sleep(60)
            raise
        finally:
            if not exception_raised:
                await asyncio.sleep(30)

    asyncio.run(_logged_run())
